<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Hive应用-面试题"><meta name="keywords" content="Hive,面试题"><meta name="author" content="WangXun"><meta name="copyright" content="WangXun"><title>Hive应用-面试题 | Wake</title><link rel="shortcut icon" href="/img/favicon-blank.svg"><link rel="stylesheet" href="/css/index.css?version=1.9.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"HSM2EINL2X","apiKey":"6f1478b12150efd917d5ecfcddfb8b8b","indexName":"wangxun","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%9F%BA%E7%A1%80"><span class="toc-number">1.</span> <span class="toc-text">基础</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BC%98%E5%8C%96"><span class="toc-number">2.</span> <span class="toc-text">优化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%AE%9E%E6%93%8D"><span class="toc-number">3.</span> <span class="toc-text">实操</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E5%BB%BA%E6%A8%A1"><span class="toc-number">4.</span> <span class="toc-text">建模</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">WangXun</div><div class="author-info__description text-center"></div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/w749">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">71</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">20</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">6</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/top-img.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Wake</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">Hive应用-面试题</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-10-23</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Software/">Software</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">5.6k</span><span class="post-meta__separator">|</span><span>Reading time: 21 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>常见的 Hive 面试题，记录不只为面试，加深对底层环境的理解</p>
<span id="more"></span>

<h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><ol>
<li><p>Hive SQL执行流程（详细）</p>
<ul>
<li><p>Parser：Hive 使用 Antlr 进行语法和词法解析，解析的结果就是 AST 抽象语法树，会进行语法校验，使用<code>explain extented select * from test</code>查看详细的语法树</p>
</li>
<li><p>Analyzer：语法分析，这里会去元数据库关联查找相关的表和字段是否存在，然后生成 QB（query block），QB 就是一条 SQL 最基本的组成单元，包括输入源、计算过程、输出三个部分</p>
</li>
<li><p>Logical Plan：逻辑执行计划解析，由 QB 生成一堆 Operator Tree，基本的操作符包括TableScanOperator、SelectOperator、FilterOperator、JoinOperator、GroupByOperator、ReduceSinkOperator</p>
</li>
<li><p>Logical Optimizer：逻辑执行计划优化，对上一步的执行计划进行优化，还是返回 Operator Tree</p>
</li>
<li><p>Physical Paln：物理执行计划，生成 Task Tree</p>
</li>
<li><p>Physical Optimizer：物理执行计划优化，同样返回 Task Tree</p>
</li>
<li><p>将优化后的 Task Tree 提交给 Map Reduce 或者其他引擎去执行</p>
</li>
</ul>
</li>
<li><p>简单介绍 SQL 执行映射 MR 流程</p>
<ul>
<li><p>首先是过滤类的语句，不带任何聚合和 join 操作，整个过程是没有 reduce 的，类似 ETL 操作，其中 map 的数量是由文件分片数量决定的，分区条件直接在数据读取的时候过滤</p>
</li>
<li><p>分组聚合类查询，本质和 Word Count 执行差不多，combiner 是每个 Map 局部的 reduce，好处是减少 shuffle 数据量，但并不是所有的场景都会发生 combiner，例如求均值</p>
</li>
</ul>
</li>
<li><p>简单介绍一下 Hive 架构和原理以及特点</p>
<ul>
<li>Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类 SQL 查询功能</li>
<li>Hive 的数据存储在 HDFS 上，或者说 Hive 的数据就是在 HDFS 上的映射，表就是文件，数据库就是文件夹，只不过底层是分布式存储的；计算引擎默认是 Map Reduce，Hive3.0 开始已经不推荐使用了；资源调度和程序的执行是放在 Yarn 上的</li>
<li>Hive 是优点是可存储数据量大，可扩展性强，数据安全性较高，结合元数据可对底层数据进行较好的存储和管理。缺点是只可增删查不可改，如需改只能查询修改后全部覆盖或者使用拉链表等其他手段；还有就是计算效率较慢，因为 Map Reduce 计算引擎会产生 Shuffle，多次落盘读盘会导致效率低下；还有一个缺点是不支持索引，如果不建分区表它会暴力扫描整个表；所以 Hive 的使用一定是建立在良好的优化方法之上的</li>
</ul>
</li>
<li><p>查看 hive 自带的函数以及详细使用方法</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查看支持的函数</span></span><br><span class="line"><span class="keyword">show</span> functions;</span><br><span class="line"><span class="keyword">show</span> functions <span class="keyword">like</span> <span class="string">&#x27;unix*&#x27;</span>;</span><br><span class="line"><span class="comment">-- 查看一个函数的详细使用方法</span></span><br><span class="line"><span class="keyword">desc</span> <span class="keyword">function</span> extended unix_timestamp;</span><br></pre></td></tr></table></figure>
</li>
<li><p>内部表和外部表的区别</p>
<ul>
<li>创建内部表时会把数据移动到指定的目录或者默认的目录下，删除的时候会把元数据以及数据全部删除且无法恢复；创建外部表时不会将数据移动到默认的目录下，只是记录数据所在位置，删除的时候只会删除元数据，数据不动</li>
<li>创建内部表（默认就是内部表）：<code>create table test...</code>，可以不指定 location 数据存储位置；创建外部表：<code>create external table test...</code> location ‘…’，必须指定数据所在位置</li>
</ul>
</li>
<li><p>介绍一下分区表和分桶表的作用、原理以及构建语句</p>
<ul>
<li>分区表是通过将数据按指定字段分区存储的方法减少查询时数据的扫描量，减少不必要的数据扫描，底层是在表所在的 HDFS 目录下再建文件夹存入每个分区的数据</li>
<li>分桶表则是为了提高大表 join 时的效率，通过对需要 join 的两张表中相同的字段进行分桶，将它们按数量指定分在不同的桶内，按照分桶字段的 hash 值取模除以分桶的个数确定数据存在哪个桶中，最好是排序分桶，这样可以保证两表相同的数据被分在同一个桶内，底层还是存储在不同的文件内。分桶表还有一个作用就是使取样更高效，开发时可以先取某一个桶的数据测试查询</li>
<li>分区表字段在表中不存在，而分桶表在表中存在；分桶表只能通过查询的方式插入，不可以使用 load 直接导入，建表语句使用<code>...clustered by (state) sorted by (cases desc) into 5 buckets...</code></li>
</ul>
</li>
<li><p>查看表结构、创建语句</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查看表详细信息</span></span><br><span class="line"><span class="keyword">desc</span> formatted test;</span><br><span class="line"><span class="comment">-- 表的创建语句</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">create</span> <span class="keyword">table</span> test;</span><br></pre></td></tr></table></figure>
</li>
<li><p>表有哪些分区、对应的 hdfs 路径、分区数据量大小、分区文件总数</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查看表的所有分区</span></span><br><span class="line"><span class="keyword">show</span> partitions hero_all;</span><br><span class="line"><span class="comment">-- 查看某一个分区的详细信息（在Detailed Partition Information中）</span></span><br><span class="line"><span class="keyword">desc</span> extended hero_all <span class="keyword">partition</span>(role<span class="operator">=</span><span class="string">&#x27;warrior&#x27;</span>);</span><br><span class="line"><span class="comment">-- 去MySQL元数据库查看分区数量</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="number">1</span>) `分区数量`</span><br><span class="line"><span class="keyword">from</span> PARTITIONS</span><br><span class="line"><span class="keyword">where</span> TBL_ID <span class="keyword">in</span> (</span><br><span class="line">    <span class="keyword">select</span> TBL_ID <span class="keyword">from</span> TBLS <span class="keyword">where</span> TBL_NAME <span class="operator">=</span> <span class="string">&#x27;students&#x27;</span>);</span><br><span class="line"><span class="comment">-- 查看表或者某一个分区数据量大小</span></span><br><span class="line">hadoop fs <span class="operator">-</span>du <span class="operator">-</span>s <span class="operator">-</span>h <span class="operator">/</span>warehouse<span class="operator">/</span>test<span class="operator">/</span>uscovid<span class="operator">/</span>us_covid</span><br></pre></td></tr></table></figure>
</li>
<li><p>hive 支持的基本数据类型</p>
<table>
<thead>
<tr>
<th>数据类型</th>
<th>长度</th>
<th>说明</th>
<th>示例</th>
</tr>
</thead>
<tbody><tr>
<td>TINYINT</td>
<td>1byte</td>
<td>-128 ~ 127</td>
<td>100Y</td>
</tr>
<tr>
<td>SMALLINT</td>
<td>2byte</td>
<td>-32768 ~ 32767</td>
<td>100S</td>
</tr>
<tr>
<td>INT</td>
<td>4byte</td>
<td>-2^32~ 2^32-1</td>
<td>100</td>
</tr>
<tr>
<td>BIGINT</td>
<td>8byte</td>
<td>-2^64~ 2^64-1</td>
<td>100L</td>
</tr>
<tr>
<td>FLOAT</td>
<td>4byte</td>
<td>单精度浮点数</td>
<td>5.21</td>
</tr>
<tr>
<td>DOUBLE</td>
<td>8byte</td>
<td>双精度浮点数</td>
<td>5.21</td>
</tr>
<tr>
<td>DECIMAL</td>
<td>-</td>
<td>高精度浮点数</td>
<td>DECIMAL(9,8)</td>
</tr>
<tr>
<td>BOOLEAN</td>
<td>-</td>
<td>布尔型</td>
<td>true&#x2F;false</td>
</tr>
<tr>
<td>BINARY</td>
<td>-</td>
<td>字节数组</td>
<td>-</td>
</tr>
<tr>
<td>STRING</td>
<td>-</td>
<td>常用字符串类型</td>
<td>‘abc’</td>
</tr>
<tr>
<td>VARCHAR</td>
<td>1-65535</td>
<td>可变字符串，需要指定长度</td>
<td>‘abc’</td>
</tr>
<tr>
<td>CHAR</td>
<td>1-255</td>
<td>不够长度的使用空格填充</td>
<td>‘abc’</td>
</tr>
<tr>
<td>DATE</td>
<td>-</td>
<td>yyyy-MM-dd</td>
<td>2020-07-04</td>
</tr>
<tr>
<td>TIMESTAMPS</td>
<td>-</td>
<td>yyyy-MM-dd HH:mm:ss.fffffffff</td>
<td>2020-07-04 12:36:25.111</td>
</tr>
<tr>
<td>STRUCT</td>
<td>-</td>
<td>结构体</td>
<td><code>struct&lt;name:string,weight:double&gt;</code></td>
</tr>
<tr>
<td>ARRAY</td>
<td>-</td>
<td>相同数据类型的集合</td>
<td>array<Int></td>
</tr>
<tr>
<td>MAP</td>
<td>-</td>
<td>键值对的组合</td>
<td>map&lt;string,string&gt;</td>
</tr>
</tbody></table>
</li>
<li><p>确定一个 hive sql 的 map 数量和 reduce 数量</p>
<ul>
<li><p>map 数量通过 split 数量确定，split 数量等于文件大小 &#x2F; split size，split size 默认是128M；reduce 数量不指定的情况下是 -1，是一个动态计算的值，也可以通过<code>set mapred.reduce.tasks=10;</code>手动设定</p>
</li>
<li><p>map 数量并不是越大越好，如果有很多小文件，每个小文件都被当作一个块，那么就会造成资源浪费，解决这个问题可以对小文件进行合并</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 每个map最大输入大小</span></span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size<span class="operator">=</span><span class="number">100000000</span>;</span><br><span class="line"><span class="comment">-- 节点中可以处理的最小文件大小,100M</span></span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node<span class="operator">=</span><span class="number">100000000</span>;</span><br><span class="line"><span class="comment">-- 机架中可以处理的最小的文件大小</span></span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack<span class="operator">=</span><span class="number">100000000</span>;</span><br><span class="line"><span class="comment">-- 表示执行map前对小文件进行合并</span></span><br><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure>
</li>
<li><p>reduce 数量的确定，同 map 一样也不是越多越好，有多少 reduce 就会产生多少小文件，这些小文件作为下一个任务的输入就会造成资源的浪费，小文件合并后面再说</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 下面两个参数确定reduce的数量</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer;  <span class="comment">-- 每个reduce任务处理的数据量，默认256M</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.reducers.max;  <span class="comment">-- 每个任务最大的reduce数量，默认1009</span></span><br><span class="line"><span class="comment">-- 根据上面两个参数，reduce数量=min(1009, 总输入数据大小/256M)</span></span><br><span class="line"><span class="comment">-- 如果需要减少reduce个数，可以调大每个reduce任务处理的数据量或者自行设定</span></span><br><span class="line"><span class="keyword">set</span> mapred.reduce.tasks <span class="operator">=</span> <span class="number">5</span>;</span><br></pre></td></tr></table></figure></li>
</ul>
</li>
<li><p>Hive 命令常用参数</p>
<ul>
<li><code>hive</code>：进入 hive 交互式 shell</li>
<li><code>hive --help</code>：查看 hive 命令都支持哪些参数</li>
<li><code>hive --hiveconf a=b,c=d</code>…：传入变量，新版可以使用 –define </li>
<li><code>hive -i test.init...</code>：从文件初始化 hive，文件中存放 hive sql 初始化语句</li>
<li><code>hive -e &quot;use test;select * from test;&quot;</code>：不进入交互式 shell 运行 sql 并返回结果</li>
<li><code>hive -f ./test.sql</code>：不进入交互式 shell 运行一个 sql 文件</li>
</ul>
</li>
<li><p>解释 sql 运行步骤，以及如何优化</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, b.name <span class="keyword">from</span> a <span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.id <span class="operator">=</span> b.id <span class="keyword">where</span> a.dt <span class="operator">=</span> <span class="string">&#x27;2021-10-10&#x27;</span> <span class="keyword">and</span> b.dt <span class="operator">=</span> <span class="string">&#x27;2021-10-10&#x27;</span></span><br></pre></td></tr></table></figure>

<p>首先是看 on 和 where，它俩的区别是 on 是生成 left join 完成后的临时表用来过滤的条件，就算条件不满足也会返回左表的所有结果，而 where 则是在生成临时表后再对这个结果进行过滤，最终返回过滤后的结果</p>
<p>根据这个特性可以将 where 条件前置，直接 join 两个筛选后的表，可以减少 join 的数据量</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> aa.id, bb.name </span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> id, name <span class="keyword">from</span> a <span class="keyword">where</span> dt <span class="operator">=</span> <span class="string">&#x27;2021-10-10&#x27;</span>) aa </span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> (</span><br><span class="line">    <span class="keyword">select</span> id, name <span class="keyword">from</span> b <span class="keyword">where</span> dt <span class="operator">=</span> <span class="string">&#x27;2021-10-10&#x27;</span>) bb </span><br><span class="line"><span class="keyword">on</span> aa.id <span class="operator">=</span> bb.id</span><br></pre></td></tr></table></figure>
</li>
<li><p>下面的 sql 语句的区别</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.<span class="operator">*</span> <span class="keyword">from</span> a <span class="keyword">left</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.key <span class="operator">=</span> b.key <span class="keyword">and</span> a.ds <span class="operator">=</span> xxx <span class="keyword">and</span> b.ds <span class="operator">=</span> xxx</span><br><span class="line"><span class="keyword">select</span> a.<span class="operator">*</span> <span class="keyword">from</span> a <span class="keyword">left</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.key <span class="operator">=</span> b.key <span class="keyword">and</span> b.ds <span class="operator">=</span> xxx</span><br><span class="line"><span class="keyword">select</span> a.<span class="operator">*</span> <span class="keyword">from</span> a <span class="keyword">left</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.key <span class="operator">=</span> b.key <span class="keyword">and</span> a.ds <span class="operator">=</span> xxx <span class="keyword">where</span> b.ds <span class="operator">=</span> xxx</span><br><span class="line"><span class="keyword">select</span> a.<span class="operator">*</span> <span class="keyword">from</span> a <span class="keyword">left</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.key <span class="operator">=</span> b.key <span class="keyword">where</span> a.ds <span class="operator">=</span> xxx <span class="keyword">and</span> b.ds <span class="operator">=</span> xxx</span><br></pre></td></tr></table></figure>

<p>这个问题其实和上面那个问题差不多，主要是考察 on 和 where 的理解和使用，on 和 where 同时存在时 on 的优先级是高于 where 的，这样就表示如果要对连接后的结果再做聚合可以使用 on 连接两张表再 where 过滤掉不需要的记录，可以减少不必要的计算量。如果要在结果记录中保留某一张表的所有记录，则把连接条件和过滤条件全部放在 on 中，这样就保证了聚合时数据的完整性。</p>
</li>
</ol>
<h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><ol>
<li><p>a 表 left join b 表，b 表为小表，可以怎样优化</p>
<ul>
<li>这里考察的是 map join 的知识，我们一般理解的默认 join 操作是先进行 map 分组，再通过 shuffle 将相同 key 的值划分到同一个 reduce 中去 join。如果其中有一张表比较小的情况下不用采取这种操作，map join 的原理是将两张表中数据量较少的一张表复制到内存中，大表的每一个 map（split）都去和这个小表去 join，最后把结果拼接一下就可以了，这样减少了 shuffle 和 reduce 的操作</li>
<li>hive 0.7 版本以前还需手动使用<code>/*+ mapjoin(table) */</code>提示才会执行 map join，之后的版本由参数<code>hive.auto.convert.join</code>控制，默认为 true，无需手动开启，自动转为 map join 的条件是其中一张表的数据量大小，由<code>hive.mapjoin.smalltable.filesize</code>来决定，默认是25M，意思若其中有一张表小于25M则自动开启 map join</li>
<li>其实 hive 现在也支持本地模式，若是两张表数据量都比较小，就会在本地执行任务，区分方式通过任务号中是否含有 local，通过<code>set mapreduce.framework.name=local;set hive.exec.mode.local.auto=true;</code>开启，<code>mapreduce.framework.name=local</code>会让所有任务都走本地</li>
</ul>
</li>
<li><p>两个大表连接，发生数据倾斜，有几个 reduce 无法完成，怎样定位发生数据倾斜的原因，怎样优化</p>
</li>
<li><p>两个大表连接，发生数据倾斜，一个reduce无法完成，发现有一张表中 id &#x3D; ‘’ 的记录有很多，其他都是非重复，应该怎样优化</p>
<ul>
<li>首先如果其中一张表中 id 包含许多记录为空的情况下，若是后续计算不需要这部分数据，则可以在 join 之前就把这部分数据过滤掉，减少 shuffle 的时候产生的笛卡尔积数量，而且空记录在分配到一个 reduce 的时候也会对效率产生影响</li>
<li>第二种情况就是在需要保留这些记录的时候，就可以对这些数据打散处理，避免被分到一个 reduce 当中。方法是在 join 之前使用 case 判断若为空的时候分配给一个随机值，这样就会被分到不同的 reduce 当中，提高处理效率</li>
</ul>
</li>
<li><p>sort by、distribute by、cluster by 和 order by 的区别</p>
<ul>
<li>首先它们都是排序相关的函数，输入和输出数据量是一样的，不要和 group by 搞混了。order by 就是我们熟知的全局排序，在 hive 中使用它的后果就是会导致只有一个 reduce，数据量较大时会造成运算的压力</li>
<li>distribute by 是先计算这个字段内数据的 hash 值，hash 值相同的分到一个 reduce 内，若是数据量较小或者指定一个 reduce 的时候就不用考虑这些了，都会分到一个 reduce 中，一般配合 sort by 使用</li>
<li>sort by 是局部排序，一般配合 distribute by 使用，将 hash 值相同的数据分到一个 reduce 内方便排序。它和 order by 不同的地方在于 order by 是全局排序，sort by 是局部排序，若只有一个 reduce 时它俩的结果相同，若有多个 reduce 时并且字段内所分区的数量原大于 reduce 个数就会导致多个分区的数据分到一个 reduce 内最终排序结果会出现问题，所以需要 sort by 分区的字段加排序的字段</li>
<li>cluster by 是在 distribute by 和 sort by 的字段相同时使用，缺点是只能升序排序</li>
</ul>
</li>
<li><p>文件存储格式和数据压缩的优化</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 开启hive中间传输数据压缩功能</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.intermediate<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 开启mapreduce中map输出压缩功能</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.map.output.compress<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 设置mapreduce中map输出数据的压缩方式</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.map.output.compress.codec<span class="operator">=</span> org.apache.hadoop.io.compress.SnappyCodec;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 开启hive最终输出数据压缩功能</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.output<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 开启mapreduce最终输出数据压缩</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 设置mapreduce最终数据输出压缩方式</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.codec <span class="operator">=</span> org.apache.hadoop.io.compress.SnappyCodec;</span><br><span class="line"><span class="comment">-- 设置mapreduce最终数据输出压缩为块压缩</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.type<span class="operator">=</span>BLOCK;</span><br></pre></td></tr></table></figure>
</li>
<li><p>其他解决数据倾斜的方法 - 包括 join、group by 等一般场景</p>
</li>
</ol>
<h3 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h3><ol>
<li><p>使用 select 查询时，使用哪个函数给值为 null 的数据设置默认值</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 使用nvl或者coalesce对值为null的直接填充，或者使用if函数判断为null再填充</span></span><br><span class="line"><span class="keyword">select</span> nvl(id, <span class="string">&#x27;123&#x27;</span>) <span class="keyword">from</span> test;</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">coalesce</span>(id, <span class="string">&#x27;123&#x27;</span>) <span class="keyword">from</span> test;</span><br><span class="line"><span class="keyword">select</span> if(isnull(id), <span class="string">&#x27;123&#x27;</span>, id) <span class="keyword">from</span> test;</span><br></pre></td></tr></table></figure>
</li>
<li><p>a、b、c 三个表内连接，连接字段都是 key，写出连接语句</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> a <span class="keyword">inner</span> <span class="keyword">join</span> b <span class="keyword">inner</span> <span class="keyword">join</span> c <span class="keyword">on</span> a.key <span class="operator">=</span> b.key <span class="keyword">and</span> a.key <span class="operator">=</span> c.key;</span><br></pre></td></tr></table></figure>
</li>
<li><p>已知 a 是一张内部表，如何将它转为外部表，写出 hive sql 语句</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 直接修改表属性中的EXTERNAL即可</span></span><br><span class="line"><span class="keyword">show</span> formatted test;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> test <span class="keyword">set</span> tblproperties(<span class="string">&#x27;EXTERNAL&#x27;</span><span class="operator">=</span><span class="string">&#x27;TRUE&#x27;</span>);  <span class="comment">-- 内部转外部</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> test <span class="keyword">set</span> tblproperties(<span class="string">&#x27;EXTERNAL&#x27;</span><span class="operator">=</span><span class="string">&#x27;FALSE&#x27;</span>);  <span class="comment">-- 外部转内部</span></span><br></pre></td></tr></table></figure>
</li>
<li><p>行转列以及列转行函数以及使用方法</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 行转列使用侧视图lateral view和explode炸裂（假设需要把列C按逗号拆分再打散）</span></span><br><span class="line"><span class="comment">-- explode函数需要传入array，如果字段本就是array就不用split了</span></span><br><span class="line"><span class="keyword">select</span> explode(<span class="keyword">array</span>(<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>)) <span class="keyword">as</span> col;  <span class="comment">-- col：a,b,c</span></span><br><span class="line"><span class="keyword">select</span> A, B, C_new <span class="keyword">from</span> test a <span class="keyword">lateral</span> <span class="keyword">view</span> explode(split(C, <span class="string">&#x27;,&#x27;</span>)) b <span class="keyword">as</span> C_new;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 列转行使用concat_ws，同样输入array</span></span><br><span class="line"><span class="keyword">select</span> concat_ws(<span class="string">&#x27;|&#x27;</span>,<span class="keyword">array</span>(<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>)) <span class="keyword">as</span> col;  <span class="comment">-- col:a|b|c</span></span><br><span class="line"><span class="keyword">select</span> A, concat_ws(<span class="string">&#x27;,&#x27;</span>, collect_set(c_new)) <span class="keyword">as</span> C <span class="keyword">from</span> test <span class="keyword">group</span> <span class="keyword">by</span> A; </span><br></pre></td></tr></table></figure>
</li>
<li><p>常用的开窗函数</p>
<p>参照以前写的 MySQL 中的[开窗函数](<a href="https://wangxukun.top/blogs/language/sql-overchuang-kou-han-shu.html">SQL OVER窗口函数 | 汪寻 (wangxukun.top)</a>)</p>
</li>
<li><p>rank、dense_rank 和 row_number 函数的区别</p>
<p>参照以前写的 MySQL 中的[开窗函数](<a href="https://wangxukun.top/blogs/language/sql-overchuang-kou-han-shu.html">SQL OVER窗口函数 | 汪寻 (wangxukun.top)</a>)</p>
</li>
<li><p>将字符串 “k1&#x3D;v1&amp;k2&#x3D;v2&amp;…&amp;kn&#x3D;vn” 进行分割并存入一个字段，可以查出任意 kn 对应的 vn 值，并计算共有多少个 k</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 首先使用str_to_map函数将字符串转为map，随后就可以使用[key]的方式取出对应的value，如果没有会返回NULL</span></span><br><span class="line"><span class="keyword">select</span> str_to_map(<span class="string">&#x27;k1=v1&amp;k2=v2&amp;k3=v3&#x27;</span>, <span class="string">&#x27;&amp;&#x27;</span>, <span class="string">&#x27;=&#x27;</span>)[<span class="string">&#x27;k1&#x27;</span>];</span><br><span class="line"><span class="comment">-- 只返回map的key或者value使用map_keys和map_values</span></span><br><span class="line"><span class="keyword">select</span> map_keys(str_to_map(<span class="string">&#x27;k1=v1&amp;k2=v2&amp;k3=v3&#x27;</span>, <span class="string">&#x27;&amp;&#x27;</span>, <span class="string">&#x27;=&#x27;</span>)) map_key_cnt;</span><br><span class="line"><span class="comment">-- 返回map的长度使用size</span></span><br><span class="line"><span class="keyword">select</span> size(str_to_map(<span class="string">&#x27;k1=v1&amp;k2=v2&amp;k3=v3&#x27;</span>, <span class="string">&#x27;&amp;&#x27;</span>, <span class="string">&#x27;=&#x27;</span>)) map_key_cnt;</span><br></pre></td></tr></table></figure>
</li>
<li><p>全量用户登录日志表 login_all，字段信息 login_time、openid；新增用户登录日志表 login_new，字段信息 login_time、openid，计算每天新增用户次日、七日、30日留存率</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tmp <span class="keyword">as</span> (  <span class="comment">-- 字段分别是openid，登录间隔时间，昨日新增用户（用于计算留存率）</span></span><br><span class="line">	<span class="keyword">select</span> new.openid, datediff(new.login_time, alll.login_time) diff_time, <span class="built_in">count</span>(<span class="keyword">distinct</span> new.openid) <span class="keyword">over</span>() all_user</span><br><span class="line">	<span class="keyword">from</span></span><br><span class="line">	(    <span class="comment">-- 减少数据量，一天只留一条登录记录</span></span><br><span class="line">		<span class="keyword">select</span> openid, to_date(login_time) login_time</span><br><span class="line">		<span class="keyword">from</span> user_stay_new</span><br><span class="line">		<span class="keyword">group</span> <span class="keyword">by</span> openid, to_date(login_time)  </span><br><span class="line">	) <span class="keyword">new</span></span><br><span class="line">	<span class="keyword">left</span> <span class="keyword">join</span></span><br><span class="line">	(    <span class="comment">-- 减少数据量，一天只留一条登录记录</span></span><br><span class="line">		<span class="keyword">select</span> openid, to_date(login_time) login_time</span><br><span class="line">		<span class="keyword">from</span> user_stay_all</span><br><span class="line">		<span class="keyword">where</span> datediff(login_time, <span class="built_in">current_date</span>()) <span class="operator">&lt;=</span> <span class="number">33</span></span><br><span class="line">		<span class="keyword">group</span> <span class="keyword">by</span> openid, to_date(login_time) </span><br><span class="line">	) alll</span><br><span class="line">	<span class="keyword">on</span> alll.openid <span class="operator">=</span> new.openid</span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span> diff_time, round(<span class="built_in">count</span>(<span class="keyword">distinct</span> openid) <span class="operator">/</span> <span class="built_in">max</span>(all_user), <span class="number">2</span>) stay_pct</span><br><span class="line"><span class="keyword">from</span> tmp</span><br><span class="line"><span class="keyword">where</span> diff_time <span class="keyword">in</span> (<span class="number">1</span>, <span class="number">7</span>, <span class="number">30</span>)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> diff_time;</span><br></pre></td></tr></table></figure>

<p>基本思路是先将 user_stay_new 表每个用户只留下一次登陆记录记为 new，再从 user_stay_all 表取出每人每日一次的登录记录记为 alll，将 new 和 alll 通过 openid 左连接得到所有用户最新的和之前三十三天的登录数据，随后计算两个登录时间的时间差以及 new 表中的总人数。将结果作为临时表，存的就是几日留存的人数以及昨日登录的总人数</p>
</li>
<li><p>发送消息流水表 chat_all：ctime、send_id、receiver_id、content；用户登录流水表 login_tb：ctime、id、location，输出每天有发送消息用户最近的发送消息时间、id、登录位置和登录时间</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tmp <span class="keyword">as</span> (</span><br><span class="line">	<span class="keyword">select</span> chat.receiver_id, chat.ctime chat_time, login.ctime login_time, login.location, </span><br><span class="line">    	<span class="built_in">row_number</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> chat.receiver_id, to_date(chat.ctime) <span class="keyword">order</span> <span class="keyword">by</span> chat.ctime <span class="keyword">desc</span>) <span class="keyword">rows</span></span><br><span class="line">    <span class="keyword">from</span> chat_all chat</span><br><span class="line">    <span class="keyword">inner</span> <span class="keyword">join</span> login_tb login</span><br><span class="line">    <span class="keyword">on</span> chat.receiver_id <span class="operator">=</span> login.id</span><br><span class="line">    <span class="keyword">and</span> to_date(chat.ctime) <span class="operator">=</span> to_date(login.ctime)</span><br><span class="line">) </span><br><span class="line"><span class="keyword">select</span> receiver_id, chat_time, login_time</span><br><span class="line"><span class="keyword">from</span> tmp</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">rows</span> <span class="operator">=</span> <span class="number">1</span>;</span><br></pre></td></tr></table></figure>

<p>思路是先将用户每日登录的位置和时间 inner join 到消息流水表，如果有多条登录记录先对 login_tb 表处理，这里假设每天只有一条，然后按用户 id 和日期开窗并按发送消息的时间降序排序，将最后一条发送的时间标记为1，最后再筛选出来标记为1的记录即可</p>
</li>
<li><p>一个分区表 T，字段 id、qq、age，按天分区，写出建表语句</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> `T`(</span><br><span class="line">	id <span class="type">int</span>,</span><br><span class="line">    qq string,</span><br><span class="line">    age <span class="type">int</span></span><br><span class="line">) <span class="type">row</span> format delimited</span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">partition</span> <span class="keyword">by</span>(dt string)</span><br><span class="line">stored <span class="keyword">as</span> orc</span><br><span class="line">location <span class="string">&#x27;/warehouse/test/T&#x27;</span></span><br><span class="line">tblproperties(<span class="string">&#x27;orc.compress&#x27;</span><span class="operator">=</span><span class="string">&#x27;SNAPPY&#x27;</span>);</span><br></pre></td></tr></table></figure>
</li>
<li><p>上方的分区表，求 20211021 这个分区中，年龄第 10 大的 qq 号列表</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tmp <span class="keyword">as</span>(</span><br><span class="line">	<span class="keyword">select</span> id, qq, age,</span><br><span class="line">        <span class="built_in">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> age <span class="keyword">asc</span>) ranks</span><br><span class="line">    <span class="keyword">from</span> T</span><br><span class="line">    <span class="keyword">where</span> dt <span class="operator">=</span> <span class="string">&#x27;20211021&#x27;</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tmp <span class="keyword">where</span> ranks <span class="operator">=</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>有一个网站的访问记录表 visit_tb，每条记录有 ctime 和 ip，计算过去五分钟内访问次数最多的 1000 个 ip</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tmp <span class="keyword">as</span> (  <span class="comment">-- 再计算这段时间内每个ip的访问次数</span></span><br><span class="line">    <span class="keyword">select</span> ip, size(collect_list(ip)) cnt <span class="comment">-- count(ip)</span></span><br><span class="line">    <span class="keyword">from</span> (  <span class="comment">-- 先计算当前时间和访问时间的时间差，单位为秒</span></span><br><span class="line">        <span class="keyword">select</span> ip, to_unix_timestamp(<span class="built_in">current_timestamp</span>()) <span class="operator">-</span> to_unix_timestamp(ctime) second_diff</span><br><span class="line">        <span class="keyword">from</span> visit_tb</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">where</span> second_diff <span class="operator">&lt;=</span> <span class="number">300</span></span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> ip</span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span> ip, cnt</span><br><span class="line"><span class="keyword">from</span> tmp</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> cnt <span class="keyword">desc</span></span><br><span class="line">limit <span class="number">1000</span>;</span><br></pre></td></tr></table></figure>
</li>
<li><p>解析 extra 中所有的字段：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">id,os,extra</span><br><span class="line"><span class="number">1</span>,iphone,[&#123;&quot;id&quot;:<span class="number">1001</span>,&quot;type&quot;:&quot;show&quot;,&quot;from&quot;:&quot;home&quot;&#125;,&#123;&quot;id&quot;:<span class="number">1002</span>,&quot;type&quot;:&quot;click&quot;,&quot;from&quot;:&quot;swan&quot;&#125;]</span><br><span class="line"><span class="number">2</span>,android,[&#123;&quot;id&quot;:<span class="number">1003</span>,&quot;type&quot;:&quot;slide&quot;,&quot;from&quot;:&quot;tool&quot;&#125;,&#123;&quot;id&quot;:<span class="number">1002</span>,&quot;type&quot;:&quot;del&quot;,&quot;from&quot;:&quot;wode&quot;&#125;,&#123;&quot;id&quot;:<span class="number">1004</span>,&quot;type&quot;:&quot;click&quot;,&quot;from&quot;:&quot;home&quot;&#125;]</span><br></pre></td></tr></table></figure>

<p>存储数据就不多说了，直接存储为 string 字符串，查询的时候再解析。思路是先去掉左右的数组符号，然后将数组 json 之间的分隔符由逗号替换成分号，接下来使用分号切分再使用侧视图炸裂，得到的就是一个个 json 字符串，最后再使用 json_tuple 从 json 字符串中取出多个值与原字段拼接在一起，或者使用 get_json_object 函数以此取出所需字段</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tmp <span class="keyword">as</span> (</span><br><span class="line">    <span class="keyword">select</span> id, os, b.json</span><br><span class="line">    <span class="keyword">from</span> json_array</span><br><span class="line">        <span class="keyword">lateral</span> <span class="keyword">view</span> explode(split(regexp_replace(regexp_replace(extra, <span class="string">&#x27;\\[|\\]&#x27;</span>, <span class="string">&#x27;&#x27;</span>), <span class="string">&#x27;\\&#125;\\,\\&#123;&#x27;</span>, <span class="string">&#x27;\\&#125;\\;\\&#123;&#x27;</span>), <span class="string">&#x27;\\;&#x27;</span>)) b <span class="keyword">as</span> json</span><br><span class="line">)</span><br><span class="line"><span class="comment">-- select id, os, get_json_object(json, &#x27;$.id&#x27;) iid, get_json_object(json, &#x27;$.type&#x27;) type, get_json_object(json, &#x27;$.from&#x27;) `from`</span></span><br><span class="line"><span class="keyword">select</span> id, os, tmp1.iid, tmp1.type, tmp1.`<span class="keyword">from</span>`</span><br><span class="line"><span class="keyword">from</span> tmp <span class="keyword">lateral</span> <span class="keyword">view</span> json_tuple(json, <span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;type&#x27;</span>, <span class="string">&#x27;from&#x27;</span>) tmp1 <span class="keyword">as</span> iid, type, `<span class="keyword">from</span>`;</span><br></pre></td></tr></table></figure>
</li>
<li><p>有一个表 subscribe_tb，两个字段分别是 gz 和 bgz，对应的 a 关注 b，数据如下：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">gz,bgz</span><br><span class="line"><span class="number">12</span>,<span class="number">34</span></span><br><span class="line"><span class="number">12</span>,<span class="number">56</span></span><br><span class="line"><span class="number">12</span>,<span class="number">78</span></span><br><span class="line"><span class="number">34</span>,<span class="number">56</span></span><br><span class="line"><span class="number">34</span>,<span class="number">12</span></span><br></pre></td></tr></table></figure>

<p>找出所有互相关注的记录</p>
<p>思路是使用关注账号和被关注账号自连接，得到新的字段被关注人所关注的账号，然后再筛选关注账号和被关注人所关注账号相同的行就是两个账号互相关注的记录</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tmp <span class="keyword">as</span> (</span><br><span class="line">	<span class="keyword">select</span> a.gz, a.bgz, b.gz bgzgz </span><br><span class="line">    <span class="keyword">from</span> subscribe_tb a</span><br><span class="line">    <span class="keyword">inner</span> <span class="keyword">join</span> subscribe_tb b</span><br><span class="line">    <span class="keyword">on</span> a.gz <span class="operator">=</span> b.bgz</span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span> gz, bgz</span><br><span class="line"><span class="keyword">from</span> tmp <span class="keyword">where</span> gz <span class="operator">=</span> bgzgz</span><br></pre></td></tr></table></figure>
</li>
<li><p>UDF、UDAF 和 UDTF 函数的区别以及怎么开发</p>
<p>UDF 是一进一出，类似于 round、year、hour、floor、abs 等函数；UDAF 则是多进一出，类似 sum、count、max、avg 等函数；UDTF 则是一进多处，在 Hive 中多用于解析 Map 或者 Array 类型并返回多行记录，类似于 explode、json_tuple 等函数</p>
<p>至于 UDF 开发写了一个 Scala 版本的 UDF，<a href="https://wangxukun.top/blogs/software/hiveru-men-zi-ding-yi-han-shu-scala-.html">详见这里</a>，UDAF 看不懂，有时间了可以看看<a target="_blank" rel="noopener" href="https://cwiki.apache.org/confluence/display/Hive/GenericUDAFCaseStudy">官网</a></p>
</li>
</ol>
<h3 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h3><ol>
<li>数仓的流程，你做的是什么</li>
<li>常见的建模方法有什么，你们用的是什么</li>
<li>维度建模有什么好处？比如业务需要增加一个维度，需要怎么做</li>
<li>怎样判断一个需求能不能实现，你的判断标准是什么，需求变更要做什么</li>
<li>ads 每天的数据量有多大，ads 层在 MySQL 中的表是怎样创建的，有什么注意事项，索引怎样创建</li>
<li>拉链表的原理</li>
<li>拉链表的整合方式</li>
<li>时点数和时期数</li>
<li>简述几种缓慢变化维度（SCD）的处理方法</li>
</ol>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">WangXun</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wangxukun.top/2021/10/23/Software/Hive应用-面试题/">https://wangxukun.top/2021/10/23/Software/Hive应用-面试题/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Hive/">Hive</a><a class="post-meta__tags" href="/tags/%E9%9D%A2%E8%AF%95%E9%A2%98/">面试题</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/10/25/Software/Hive%E5%85%A5%E9%97%A8-%E5%9B%9B%E4%B8%AAby%E8%AF%A6%E8%A7%A3/"><i class="fa fa-chevron-left">  </i><span>Hive入门-四个by详解</span></a></div><div class="next-post pull-right"><a href="/2021/10/23/Software/Spark%E5%BA%94%E7%94%A8-%E9%9D%A2%E8%AF%95%E9%A2%98/"><span>Spark应用-面试题</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'aaf2844e0aeef4917c17',
  clientSecret: '10b96d24dffda7d3b4544778cf620f81990b676d',
  repo: 'blog-issue',
  owner: 'w749',
  admin: 'w749',
  id: md5(decodeURI(location.pathname)),
  language: 'en'
})
gitalk.render('gitalk-container')</script></div></div><footer class="footer-bg" style="background-image: url(/img/top-img.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2022 By WangXun</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/lib/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.1"></script><script src="/js/fancybox.js?version=1.9.1"></script><script src="/js/sidebar.js?version=1.9.1"></script><script src="/js/copy.js?version=1.9.1"></script><script src="/js/fireworks.js?version=1.9.1"></script><script src="/js/transition.js?version=1.9.1"></script><script src="/js/scroll.js?version=1.9.1"></script><script src="/js/head.js?version=1.9.1"></script><script src="/js/search/algolia.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>