<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Spark入门-聚合算子详解"><meta name="keywords" content="Spark"><meta name="author" content="WangXun"><meta name="copyright" content="WangXun"><title>Spark入门-聚合算子详解 | Wake</title><link rel="shortcut icon" href="/img/favicon-blank.svg"><link rel="stylesheet" href="/css/index.css?version=1.9.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"HSM2EINL2X","apiKey":"6f1478b12150efd917d5ecfcddfb8b8b","indexName":"wangxun","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#Overview"><span class="toc-number">1.</span> <span class="toc-text">Overview</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#groupBy"><span class="toc-number">2.</span> <span class="toc-text">groupBy</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#groupByKey"><span class="toc-number">3.</span> <span class="toc-text">groupByKey</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#reduceByKey"><span class="toc-number">4.</span> <span class="toc-text">reduceByKey</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#aggregateByKey"><span class="toc-number">5.</span> <span class="toc-text">aggregateByKey</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#flodByKey"><span class="toc-number">6.</span> <span class="toc-text">flodByKey</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Compare"><span class="toc-number">7.</span> <span class="toc-text">Compare</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Application"><span class="toc-number">8.</span> <span class="toc-text">Application</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">WangXun</div><div class="author-info__description text-center"></div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/w749">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">71</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">20</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">6</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/top-img.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Wake</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">Spark入门-聚合算子详解</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-08-29</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Software/">Software</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">3.1k</span><span class="post-meta__separator">|</span><span>Reading time: 11 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>Spark 中常用的算子加起来有三四十个，其中针对 key 的聚合算子有五个，分别是<code>groupBy、groupByKey、reduceByKey、aggregateByKey 和 flodByKey</code>，有几个底层其实调用的都是一个方法，只不过传入的参数不一样产生了这几个算子，但我仍打算分开来详解每个算子的计算过程，加深理解。</p>
<span id="more"></span>

<h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>这几个聚合算子要解决的问题都是将所需操作的 RDD 中的 key 值相同的 value 值聚合在一起然后两两做计算也就是聚合最终得出一个结果，这里有三个点需要注意，一是两两聚合的初始值，是从外部传入还是使用默认值；二是分区内聚合方式，因为 RDD 默认是并行计算，会分成多个分区，每个分区内部可以指定聚合方式；三是分区间聚合方式，拿到分区内的聚合结果就要考虑分区间的聚合方式了，这个参数也可以指定。所以这几种算子的区别就是因为传入了不同的参数。</p>
<h2 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h2><p>先来说说 groupBy，它是最容易理解的，就是把 key 值相同的 value 值放在一起形成<code>(key, iter)</code>的键值对，聚合的话需要使用 map 再对每个 key 对应的 iter 内容做聚合。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">groupBy[<span class="type">K</span>](f: <span class="type">T</span> =&gt; <span class="type">K</span>)(<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">T</span>])]</span><br></pre></td></tr></table></figure>

<p>先来看看源码，需要传入一个 group 方法 f ，这个f方法传入待分组的元素，返回另外一个值 K，而这个 K 就是分组的依据，注意看最后 groupBy 返回的结果类型也是以 K 和相同 K 的初始元素生成的迭代器所组成的元组，需要对相同K下的 iter 进行聚合就需要再进行 map 操作。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 计算初始RDD不同首字母开头的元素数量</span></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.makeRDD(<span class="type">List</span>(<span class="string">&quot;Hello&quot;</span>, <span class="string">&quot;Java&quot;</span>, <span class="string">&quot;Python&quot;</span>, <span class="string">&quot;PHP&quot;</span>, <span class="string">&quot;Help&quot;</span>))</span><br><span class="line"><span class="comment">// (&#x27;H&#x27;, (&quot;Hello&quot;, &quot;Help&quot;)), (&#x27;J&#x27;, (&quot;Java&quot;)), (&#x27;P&#x27;, (&quot;Python&quot;, &quot;PHP&quot;))</span></span><br><span class="line"><span class="keyword">val</span> groupRDD: <span class="type">RDD</span>[(<span class="type">Char</span>, <span class="type">Iterable</span>[<span class="type">String</span>])] = rdd.groupBy(_.charAt(<span class="number">0</span>))</span><br><span class="line"><span class="comment">// (&#x27;H&#x27;, 2), (&#x27;J&#x27;, 1), (&#x27;P&#x27;, 2)</span></span><br><span class="line"><span class="keyword">val</span> sizeRDD: <span class="type">RDD</span>[(<span class="type">Char</span>, <span class="type">Int</span>)] = groupRDD.map(_._2.size)</span><br><span class="line">sizeRDD.collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p>具体过程可以参考下图，第二步添加了 File 落盘动作，因为 group 操作会计算每个分区所有单词的首字母并缓存下来，如果放在内存中若数据过多则会产生内存溢出；再就是第三步从文件读取回来，并不一定是三个分区，这里只是为了便于理解。</p>
<div align=center><img src="groupBy.png"></div>

<h2 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h2><p>groupByKey 相比于 groupBy 不同的是，groupBy 需要指定分组的 key ，而 groupByKey 是将元组这种类型的第一个值作为 key ，对第二个值进行分组的操作。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">groupByKey(): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br></pre></td></tr></table></figure>

<p>可以看到这个算子不需要传入参数，就是针对元组这种 KV 类型定义的，至于返回值的类型，K 就是元组的第一个值，<code>Iterable(V)</code>则是相同 K 值的所有 V 组成的迭代器，那么同时处理元组类型时 groupByKey 和 groupBy 的不同之处就是这里的 V 是元组内的第二个值，而 groupBy 是初始的元素值，具体看下面的例子：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 根据RDD内元组的第一个元素将数据分类并对第二个元素求和</span></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="comment">// (&quot;a&quot;, (1, 3, 4)), (&quot;b&quot;, (2))</span></span><br><span class="line"><span class="keyword">val</span> groupByKeyRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = sc.groupByKey()</span><br><span class="line"><span class="comment">// (&quot;a&quot;, ((&quot;a&quot;, 1), (&quot;a&quot;, 3), (&quot;a&quot;, 4))), (&quot;b&quot;, (&quot;b&quot;, 2))</span></span><br><span class="line"><span class="keyword">val</span> groupByRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Int</span>)])] = sc.groupBy(_._1)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 当然聚合方式也不相同</span></span><br><span class="line">groupByKeyRDD.map(_._2.sum)</span><br><span class="line">groupByRDD.map(_._2.map(_._2).sum)</span><br></pre></td></tr></table></figure>

<p>groupByKey 也需要落盘操作，会导致数据打乱重组，存在 shuffle 操作，效率相对来说比较低下，这也就引出了 reduceByKey，下面再详细比较两者的不同之处。</p>
<div align=center><img src="groupByKey.png"></div>

<h2 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h2><p>reduceByKey 相比于 groupByKey 就是把 map 操作集成在算子当中了，不需要再额外进行 map 操作，它和aggregateByKey以及 flodByKey 的操作类似，只不过细节之处需要传入不同的参数区分彼此不同的功能。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reduceByKey(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure>

<p>可以看到 reduceByKey 接收一个 func 参数，而这个 func 参数接收两个 V 类型的参数并返回一个 V 类型的结果，这里的 V 其实就是初始 RDD 中的元素，这里需要传入的 func 就是元素两两计算的逻辑。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 根据RDD内元组的第一个元素将数据分类并对第二个元素求和</span></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="keyword">val</span> <span class="type">ReduceByKeyRDD</span>: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = rdd.reduceByKey(_ + _)  <span class="comment">// (&quot;a&quot;, 8), (&quot;b&quot;, 2)</span></span><br><span class="line"><span class="type">ReduceByKeyRDD</span>.collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p>从下图中的第一张图看相对于 groupByKey 只是少了 map 的步骤将它整合在 reduceByKey 中，但是实际上 reduceByKey 的作用不止于此，第二张图才是实际的运行模式，它提供了 Combine 预聚合的功能，支持在分区中先进行聚合，称作分区内聚合，然后再落盘等待分区间聚合。这样下来它不只是减少了 map 的操作，同时提供了分区内聚合使得 shuffle 落盘时的数据量尽量小，IO 效率也会提高不少。最后它引出了分区内聚合和分区间聚合，reduceByKey 的分区内聚合和分区间聚合是一样的。</p>
<div align=center><img src="reduceByKey.png"></div>

<h2 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h2><p>aggregateByKey 是对 reduceByKey 的高级应用，它可以分开来指定分区内聚合和分区间聚合，并提供了一个计算初始值。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aggregateByKey[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">V</span>) =&gt; <span class="type">U</span>, combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">U</span>)]</span><br></pre></td></tr></table></figure>

<p>来看上方源码，它采用柯里化操作，第一个参数列表接收一个参数 zeroValue，它提供一个初始值，不同于 reduceByKey 直接开始计算第一个元素和第二个元素，aggregateByKey 允许先用初始值和第一个元素进行两两计算；第二个参数列表接收两个参数，第一个是 seqOp 表示分区内聚合方式，它接收两个参数返回一个参数，注意接收的参数一个 U 的类型和 zeroValue 类型相同，另外一个是初始元素的类型，返回类型是 U 类型，说明返回类型是由 zeroValue 决定的，这很重要；第二个参数 combOp 表示分区间聚合方式，接收两个 U 类型的参数并返回一个 U 类型的参数。最终返回初始元素和聚合后的元素。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 给定初始RDD并指定两个分区，分区内计算最大值分区间求和</span></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(</span><br><span class="line">    <span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">6</span>)), </span><br><span class="line">    numSlices = <span class="number">2</span></span><br><span class="line">)</span><br><span class="line"><span class="comment">// ((&quot;a&quot;, 8), (&quot;b&quot;, 8))</span></span><br><span class="line"><span class="keyword">val</span> aggregateByKeyRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = rdd.aggregateByKey(<span class="number">0</span>)(</span><br><span class="line">    (x, y) =&gt; math.max(x, y),  <span class="comment">// 分区内求最大值</span></span><br><span class="line">    (x, y) =&gt; x + y  <span class="comment">// 分区间求和</span></span><br><span class="line">)</span><br><span class="line">aggregateByKeyRDD.collect().foreach(println)</span><br></pre></td></tr></table></figure>

<p>看运行过程就更清晰了，相比于 reduceByKey 只是将分区内聚合和分区间聚合分开来了，并且提供了一个初始值，这个初始值作为第一个元素与初始 RDD 的第一个元素计算，这也就使得初始值不一样哪怕聚合方式相同结果也可能不一样，详情看下图。其次就是分区数量对结果的影响，上方例子如果按三个分区计算结果又不一样了，它作为 aggregateByKey 的第四个决定结果的隐形参数在聚合时也需要考虑在内。</p>
<div align=center><img src="aggregateByKey.png"></div>

<h2 id="flodByKey"><a href="#flodByKey" class="headerlink" title="flodByKey"></a>flodByKey</h2><p>flodByKey 是 aggregateByKey 的特例情况，在分区内聚合方式和分区间聚合方式相同的时候使用。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">foldByKey(zeroValue: <span class="type">V</span>)(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure>

<p>仍然是柯里化传参，第一个参数列表给定一个初始值，第二个参数列表传入一个聚合函数 func，在一定条件下和 reduceByKey 的结果和聚合方式是相同的。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 计算初始值与所有元素的和</span></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>)), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> foldByKeyRDD: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd.foldByKey(<span class="number">10</span>)(_ + _)  <span class="comment">// 10 + 1 + 3 + 10 + 4 + 2 = 30</span></span><br></pre></td></tr></table></figure>

<p>看运行过程还是比较容易理解的，尤其需要注意初始值的设定，不然会产生意想不到的结果。</p>
<div align=center><img src="foldByKey.png"></div>

<h2 id="Compare"><a href="#Compare" class="headerlink" title="Compare"></a>Compare</h2><p>前面把每个算子的详细计算过程都画了一遍，接下来从源码中函数的接收参数中继续看<code>reduceByKey、aggregateByKey和flodByKey</code>这三个算子的联系和不同之处，它们的源码中都调用了一个函数<code>combineByKeyWithClassTag</code>，接下来来看一看传入的参数。从它们源码调用的函数就可以很清楚的区分这几个算子了，结合不同环境使用不同的算子。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// reduceByKey</span></span><br><span class="line">combineByKeyWithClassTag[<span class="type">V</span>]((v: <span class="type">V</span>) =&gt; </span><br><span class="line">                            v,  <span class="comment">// 就是初始RDD的值，当作每个分区开始计算的初始值，不需要指定</span></span><br><span class="line">                            func,  <span class="comment">// 分区内聚合</span></span><br><span class="line">                            func,  <span class="comment">// 分区间聚合</span></span><br><span class="line">                            partitioner)</span><br><span class="line"><span class="comment">// aggregateByKey</span></span><br><span class="line">combineByKeyWithClassTag[<span class="type">U</span>]((v: <span class="type">V</span>) =&gt; </span><br><span class="line">                            cleanedSeqOp(createZero(), v),  <span class="comment">// 初始值，柯里化的第一个参数列表</span></span><br><span class="line">                            cleanedSeqOp,  <span class="comment">// 分区内聚合，柯里化的第二个参数列表</span></span><br><span class="line">                            combOp,  <span class="comment">// 分区间聚合，与分区内聚合不相同</span></span><br><span class="line">                            partitioner)</span><br><span class="line"><span class="comment">// flodByKey</span></span><br><span class="line">combineByKeyWithClassTag[<span class="type">V</span>]((v: <span class="type">V</span>) =&gt; </span><br><span class="line">                            cleanedFunc(createZero(), v),  <span class="comment">// 初始值，柯里化的第一个参数列表</span></span><br><span class="line">                            cleanedFunc,  <span class="comment">// 分区内聚合，柯里化的第二个参数列表</span></span><br><span class="line">                            cleanedFunc,  <span class="comment">// 分区间聚合，与分区内聚合相同</span></span><br><span class="line">                            partitioner)</span><br></pre></td></tr></table></figure>

<h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><ol>
<li>获取首字母相同 key 数据的和</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(</span><br><span class="line">  <span class="type">List</span>((<span class="string">&quot;Hello&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;Java&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;Python&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;PHP&quot;</span>, <span class="number">7</span>), (<span class="string">&quot;Help&quot;</span>, <span class="number">9</span>)))</span><br><span class="line"></span><br><span class="line">rdd.map(kv =&gt; (kv._1.charAt(<span class="number">0</span>), kv._2))  <span class="comment">// 先将原始RDD的首字母提出来</span></span><br><span class="line">  .reduceByKey(_ + _)  <span class="comment">// 再按照key进行求和</span></span><br><span class="line">  .collect()</span><br><span class="line">  .foreach(println)  <span class="comment">// (&quot;P&quot;,12), (&quot;H&quot;, 10), (&quot;C&quot;, 3)</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>获取相同 key 数据的平均值</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(</span><br><span class="line">  <span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">6</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">7</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">8</span>)), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">rdd.aggregateByKey((<span class="number">0.0</span>, <span class="number">0</span>))(  <span class="comment">// 元组第一个元素接收求和数据，0.0避免求均值强转为Int，第二个接收数据计数</span></span><br><span class="line">  (k, v) =&gt; (k._1 + v, k._2 + <span class="number">1</span>),  <span class="comment">// 分区内按key累加、计数</span></span><br><span class="line">  (k, v) =&gt; (k._1 + v._1, k._2 + v._2)  <span class="comment">// 分区间将分区内的统计结果累加</span></span><br><span class="line">)</span><br><span class="line">  .map(kv =&gt; (kv._1, kv._2._1 / kv._2._2))  <span class="comment">// 求均值</span></span><br><span class="line">  .collect()</span><br><span class="line">  .foreach(println)  <span class="comment">// (&quot;a&quot;,3.5), (&quot;b&quot;, 5.5)</span></span><br></pre></td></tr></table></figure>

<ol start="3">
<li>获取相同 key 的数据分区内求均值分区间求和的结果</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rdd.aggregateByKey((<span class="number">0.0</span>, <span class="number">0</span>))(</span><br><span class="line">  (k, v) =&gt; ((k._1 + v), k._2 + <span class="number">1</span>),</span><br><span class="line">  (k, v) =&gt; (k._1 / k._2 + v._1 / v._2, k._2 + v._2)  <span class="comment">// 直接在这一步先计算分区内均值再求和</span></span><br><span class="line">)</span><br><span class="line">  .collect()</span><br><span class="line">  .foreach(println) <span class="comment">// (&quot;a&quot;,(7.0, 4)), (&quot;b&quot;, (11.0, 4))</span></span><br></pre></td></tr></table></figure>

<ol start="4">
<li>数据如下所示每一行数据是一条点击记录，字段分别为（时间戳 省份 市 用户 广告），计算每个省份点击次数前三名的广告。</li>
</ol>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1516609143867 6 7 64 16</span><br><span class="line">1516609143869 9 4 75 18</span><br><span class="line">1516609143869 1 7 87 12</span><br><span class="line">1516609143869 2 8 92 9</span><br><span class="line">1516609143869 6 7 84 24</span><br><span class="line">1516609143869 1 8 95 5</span><br><span class="line">1516609143869 8 1 90 29</span><br><span class="line">1516609143869 3 3 36 16</span><br><span class="line">1516609143869 3 3 54 22</span><br><span class="line">1516609143869 7 6 33 5</span><br></pre></td></tr></table></figure>

<p>思考的重点是中间数据结构的转换，刚开始计算的 key 是省份 + 广告，后面的 key 就只有省份了，需要在省份内部做计算。计算过程如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> original: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;data/agent.log&quot;</span>)  <span class="comment">// 时间戳 省份 市 用户 广告</span></span><br><span class="line"><span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = original.map(str =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> strings: <span class="type">Array</span>[<span class="type">String</span>] = str.split(<span class="string">&quot; &quot;</span>)  <span class="comment">// 拆分数据并取到省份和广告字段</span></span><br><span class="line">  ((strings(<span class="number">1</span>), strings(<span class="number">4</span>)), <span class="number">1</span>)</span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment">// 计算每个省份每条广告的点击人数</span></span><br><span class="line"><span class="keyword">val</span> reduceRDD: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = mapRDD.reduceByKey(_ + _)  </span><br><span class="line"><span class="comment">// 转换数据结构，因为最终是计算每个省份内的，所以省份是key，将广告跟点击数放一起</span></span><br><span class="line"><span class="keyword">val</span> newMapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">String</span>, <span class="type">Int</span>))] = reduceRDD.map&#123; </span><br><span class="line">  <span class="keyword">case</span> ((pro, ad), sum) =&gt; (pro, (ad, sum)) </span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// groupBy省份，将所有省份下的所有广告点击数放一起</span></span><br><span class="line"><span class="keyword">val</span> groupRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Int</span>)])] = newMapRDD.groupByKey()</span><br><span class="line"><span class="comment">// 在每个省份内排序，取前三条数据</span></span><br><span class="line"><span class="keyword">val</span> mapValuesRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)])] = groupRDD.mapValues(iter =&gt; &#123;</span><br><span class="line">  iter.toList.sortWith(_._2 &gt; _._2).take(<span class="number">3</span>) <span class="comment">// sortBy(_._2)(Ordering.Int.reverse)</span></span><br><span class="line">&#125;)</span><br><span class="line">mapValuesRDD.collect().foreach(println)</span><br></pre></td></tr></table></figure>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">WangXun</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wangxukun.top/2021/08/29/Software/Spark入门-聚合算子详解/">https://wangxukun.top/2021/08/29/Software/Spark入门-聚合算子详解/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/09/07/Software/Spark%E5%85%A5%E9%97%A8-RDD%E3%80%81DataFrame%20and%20DataSet/"><i class="fa fa-chevron-left">  </i><span>Spark入门-RDD、DataFrame and DataSet</span></a></div><div class="next-post pull-right"><a href="/2021/08/06/Software/Hive%E5%B8%B8%E8%A7%81%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%96%B9%E5%BC%8F/"><span>Hive常见性能优化方式</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'aaf2844e0aeef4917c17',
  clientSecret: '10b96d24dffda7d3b4544778cf620f81990b676d',
  repo: 'blog-issue',
  owner: 'w749',
  admin: 'w749',
  id: md5(decodeURI(location.pathname)),
  language: 'en'
})
gitalk.render('gitalk-container')</script></div></div><footer class="footer-bg" style="background-image: url(/img/top-img.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2022 By WangXun</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/lib/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.1"></script><script src="/js/fancybox.js?version=1.9.1"></script><script src="/js/sidebar.js?version=1.9.1"></script><script src="/js/copy.js?version=1.9.1"></script><script src="/js/fireworks.js?version=1.9.1"></script><script src="/js/transition.js?version=1.9.1"></script><script src="/js/scroll.js?version=1.9.1"></script><script src="/js/head.js?version=1.9.1"></script><script src="/js/search/algolia.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>