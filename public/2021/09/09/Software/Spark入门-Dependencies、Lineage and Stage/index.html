<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Spark入门-Dependencies、Lineage and Stage"><meta name="keywords" content="Spark"><meta name="author" content="WangXun"><meta name="copyright" content="WangXun"><title>Spark入门-Dependencies、Lineage and Stage | Wake</title><link rel="shortcut icon" href="/img/favicon-blank.svg"><link rel="stylesheet" href="/css/index.css?version=1.9.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"HSM2EINL2X","apiKey":"6f1478b12150efd917d5ecfcddfb8b8b","indexName":"wangxun","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Lineage%EF%BC%88%E8%A1%80%E7%BC%98%E5%85%B3%E7%B3%BB%EF%BC%89"><span class="toc-number">1.</span> <span class="toc-text">Lineage（血缘关系）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Dependencies%EF%BC%88%E4%BE%9D%E8%B5%96%E5%85%B3%E7%B3%BB%EF%BC%89"><span class="toc-number">2.</span> <span class="toc-text">Dependencies（依赖关系）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Stage%EF%BC%88%E9%98%B6%E6%AE%B5%E5%88%92%E5%88%86%EF%BC%89"><span class="toc-number">3.</span> <span class="toc-text">Stage（阶段划分）</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#DAG"><span class="toc-number">3.1.</span> <span class="toc-text">DAG</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#DAG-%E5%88%92%E5%88%86-Stage"><span class="toc-number">3.2.</span> <span class="toc-text">DAG 划分 Stage</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BD%95%E8%A6%81%E5%88%92%E5%88%86-Stage"><span class="toc-number">3.3.</span> <span class="toc-text">为何要划分 Stage</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Job-%E5%92%8C-Task"><span class="toc-number">4.</span> <span class="toc-text">Job 和 Task</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">WangXun</div><div class="author-info__description text-center"></div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/w749">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">71</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">20</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">6</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/top-img.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Wake</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">Spark入门-Dependencies、Lineage and Stage</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-09-09</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Software/">Software</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1.8k</span><span class="post-meta__separator">|</span><span>Reading time: 6 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>Spark 中比较重要的一块就是血缘关系和阶段划分，虽说并不能像累加器或者广播变量解决特定的需求，但对于理解 Spark 计算的任务执行调度有很大的帮助。</p>
<span id="more"></span>

<h3 id="Lineage（血缘关系）"><a href="#Lineage（血缘关系）" class="headerlink" title="Lineage（血缘关系）"></a>Lineage（血缘关系）</h3><p>RDD 只支持粗粒度转换，即在大量记录上执行的单个操作。将创建 RDD 的一系列 Lineage (血统)记录下来，以便恢复丢失的分区。RDD 的 Lineage 会记录 RDD 的元数据信息和转换行为，当该 RDD 的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</p>
<p>RDD 不保存数据，在没有缓存和检查点的情况下如果需要重复使用 RDD 或者分区丢失只能通过依赖上游的血缘关系恢复当前 RDD 的操作。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;input/1.txt&quot;</span>)</span><br><span class="line">println(<span class="string">&quot;-----------textFile-----------&quot;</span>)</span><br><span class="line">println(fileRDD.toDebugString)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> wordRDD: <span class="type">RDD</span>[<span class="type">String</span>] = fileRDD.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">println(<span class="string">&quot;-----------flatMap-----------&quot;</span>)</span><br><span class="line">println(wordRDD.toDebugString)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordRDD.map((_,<span class="number">1</span>))</span><br><span class="line">println(<span class="string">&quot;-----------map-----------&quot;</span>)</span><br><span class="line">println(mapRDD.toDebugString)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = mapRDD.reduceByKey(_+_)</span><br><span class="line">println(<span class="string">&quot;-----------reduceByKey-----------&quot;</span>)</span><br><span class="line">println(resultRDD.toDebugString)</span><br><span class="line"></span><br><span class="line">resultRDD.collect()</span><br></pre></td></tr></table></figure>

<p>以上是 WordCount 的步骤，将每一个步骤中的 RDD 的血缘关系打印出来就会发现它们彼此之间存在联系相互依赖。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">-----------textFile-----------</span><br><span class="line">(2) data/WordCount01 MapPartitionsRDD[1] at textFile at RRR.scala:12 []</span><br><span class="line"> |  data/WordCount01 HadoopRDD[0] at textFile at RRR.scala:12 []</span><br><span class="line">-----------flatMap-----------</span><br><span class="line">(2) MapPartitionsRDD[2] at flatMap at RRR.scala:15 []</span><br><span class="line"> |  data/WordCount01 MapPartitionsRDD[1] at textFile at RRR.scala:12 []</span><br><span class="line"> |  data/WordCount01 HadoopRDD[0] at textFile at RRR.scala:12 []</span><br><span class="line">-----------map-----------</span><br><span class="line">(2) MapPartitionsRDD[3] at map at RRR.scala:18 []</span><br><span class="line"> |  MapPartitionsRDD[2] at flatMap at RRR.scala:15 []</span><br><span class="line"> |  data/WordCount01 MapPartitionsRDD[1] at textFile at RRR.scala:12 []</span><br><span class="line"> |  data/WordCount01 HadoopRDD[0] at textFile at RRR.scala:12 []</span><br><span class="line">-----------reduceByKey-----------</span><br><span class="line">(2) ShuffledRDD[4] at reduceByKey at RRR.scala:21 []</span><br><span class="line"> +-(2) MapPartitionsRDD[3] at map at RRR.scala:18 []</span><br><span class="line">    |  MapPartitionsRDD[2] at flatMap at RRR.scala:15 []</span><br><span class="line">    |  data/WordCount01 MapPartitionsRDD[1] at textFile at RRR.scala:12 []</span><br><span class="line">    |  data/WordCount01 HadoopRDD[0] at textFile at RRR.scala:12 []</span><br></pre></td></tr></table></figure>

<p>上方输出结果一目了然，下游依次依赖上游直至创建 RDD 的最初状态，看下图可以更直观的感受这个血缘关系。</p>
<div align=center><img src="血缘关系.png"></div>

<h3 id="Dependencies（依赖关系）"><a href="#Dependencies（依赖关系）" class="headerlink" title="Dependencies（依赖关系）"></a>Dependencies（依赖关系）</h3><p>这里所谓的依赖关系，其实就是两个相邻 RDD 之间的关系。查看依赖使用 dependencies 属性，不过并没有血缘关系展示的直观。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;data/WordCount01&quot;</span>)</span><br><span class="line">println(<span class="string">&quot;-----------textFile-----------&quot;</span>)</span><br><span class="line">println(fileRDD.dependencies)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> wordRDD: <span class="type">RDD</span>[<span class="type">String</span>] = fileRDD.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">println(<span class="string">&quot;-----------flatMap-----------&quot;</span>)</span><br><span class="line">println(wordRDD.dependencies)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordRDD.map((_,<span class="number">1</span>))</span><br><span class="line">println(<span class="string">&quot;-----------map-----------&quot;</span>)</span><br><span class="line">println(mapRDD.dependencies)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = mapRDD.reduceByKey(_+_)</span><br><span class="line">println(<span class="string">&quot;-----------reduceByKey-----------&quot;</span>)</span><br><span class="line">println(resultRDD.dependencies)</span><br><span class="line"></span><br><span class="line">resultRDD.collect()</span><br></pre></td></tr></table></figure>

<p>输出</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-----------textFile-----------</span><br><span class="line">List(org.apache.spark.OneToOneDependency@787e4357)</span><br><span class="line">-----------flatMap-----------</span><br><span class="line">List(org.apache.spark.OneToOneDependency@21ea996f)</span><br><span class="line">-----------map-----------</span><br><span class="line">List(org.apache.spark.OneToOneDependency@6af5b246)</span><br><span class="line">-----------reduceByKey-----------</span><br><span class="line">List(org.apache.spark.ShuffleDependency@3079c26a)</span><br></pre></td></tr></table></figure>

<p>RDD 之间的依赖又分为窄依赖和宽依赖，它其实是根据前后两个 RDD 之间的转变是否打乱分区决定的，看图</p>
<div align=center><img src="窄依赖.png"></div>

<p>上图为窄依赖，父 RDD 的一个分区只会被子 RDD 的一个分区依赖。RDD 操作前后的分区数和分区内的数据是不变的，不用打乱 Shuffle，一般 map、foreach 这种操作都会形成窄依赖。</p>
<div align=center><img src="宽依赖.png"></div>

<p>上图为宽依赖，父 RDD 的一个分区会被子 RDD 的多个分区依赖(涉及到 shuffle )。如果有 group、reduce 这些会产生 Shuffle 打乱原 RDD 分区的操作，那么两个 RDD 之间就是宽依赖。</p>
<p>那么为什么要设计宽窄依赖呢，对于窄依赖：它的多个分区可以并行计算，而且每一个分区的数据如果丢失只需要重新计算对应的分区的数据就可以了。而对于宽依赖：它是划分 Stage 的依据，宽依赖必须等到上一阶段计算完成才能计算下一阶段。</p>
<h3 id="Stage（阶段划分）"><a href="#Stage（阶段划分）" class="headerlink" title="Stage（阶段划分）"></a>Stage（阶段划分）</h3><h4 id="DAG"><a href="#DAG" class="headerlink" title="DAG"></a>DAG</h4><p>DAG(Directed Acyclic Graph 有向无环图)指的是数据转换执行的过程，有方向，无闭环(其实就是 RDD 执行的流程)；原始的 RDD 通过一系列的转换操作就形成了 DAG 有向无环图，任务执行时，可以按照 DAG 的描述，执行真正的计算(数据被操作的一个过程)。</p>
<p>DAG 的边界是通过 Action 行动算子来划分的，开始：通过 SparkContext 创建的 RDD；结束：触发 Action，一旦触发 Action 就形成了一个完整的 DAG。</p>
<h4 id="DAG-划分-Stage"><a href="#DAG-划分-Stage" class="headerlink" title="DAG 划分 Stage"></a>DAG 划分 Stage</h4><div align=center><img src="Stage.png"></div>

<ul>
<li>一个 Spark 程序中可以有多个 DAG（有几个 Action 算子就有几个 DAG，上图有一个 Action 算子就只有一个 DAG ），一个 DAG 可以有多个 Stage（根据宽依赖 &#x2F; Shuffle 进行划分）。</li>
<li>同一个 Stage 可以有多个 Task 并行执行（ Task 数&#x3D;分区数，上图有三个 Task 就有三个分区，需要注意这和有几个 executor 没关系，每台机器分配几个核就有几个 executor，然后根据这台机器上运行几个 Task 来决定每个 executor 运行几个 Task ）。可以看到上图 DAG 中只有 reduceByKey 操作是一个宽依赖，Spark 内核会以此为边界将其前后划分成不同的 Stage。</li>
<li>同时我们可以注意到，在图中 Stage1 中，从 textFile 到 flatMap 到 map 都是窄依赖，这几步操作可以形成一个流水线操作，通过 flatMap 操作生成的 partition 可以不用等待整个 RDD 计算结束，而是继续进行 map 操作，这样大大提高了计算的效率。</li>
</ul>
<h4 id="为何要划分-Stage"><a href="#为何要划分-Stage" class="headerlink" title="为何要划分 Stage"></a>为何要划分 Stage</h4><p>一个复杂的业务逻辑如果有 shuffle，那么就意味着前面阶段产生结果后，才能执行下一个阶段，即下一个阶段的计算要依赖上一个阶段的数据。那么我们按照 shuffle &#x2F; 宽依赖进行划分，就可以将一个 DAG 划分成多个 Stage &#x2F; 阶段，在同一个 Stage 中，会有多个算子操作，可以形成一个 pipeline 流水线，流水线内的多个平行的分区可以并行执行。</p>
<p>对于窄依赖划分 Stage 时，partition 的转换处理在 Stage 中完成计算，不划分(将窄依赖尽量放在在同一个 Stage 中，可以实现流水线计算)。对于宽依赖，由于有 Shuffle 的存在，只能在父 RDD 处理完成后，才能开始接下来的计算，也就是说需要划分 Stage 。</p>
<p>Spark 会根据 Shuffle &#x2F; 宽依赖使用回溯算法来对 DAG 进行 Stage 划分，从后往前，遇到宽依赖就断开，遇到窄依赖就把当前的 RDD 加入到当前的 Stage &#x2F; 阶段中，这一点可以使用 RDD 的 toDebugString 方法查看，看到<code>+-</code>符号就是断开划分阶段。</p>
<p>DAGScheduler 按照 ShuffleDependency 作为 Stage 的划分节点对 RDD 的 DAG 进行 Stage 划分（上游的 Stage 将为 ShuffleMapStage）。因此一个 Job 可能被划分为一到多个 Stage 。Stage 分为 ShuffleMapStage 和 ResultStage 两种。</p>
<h3 id="Job-和-Task"><a href="#Job-和-Task" class="headerlink" title="Job 和 Task"></a>Job 和 Task</h3><ul>
<li>Job：用户提交的作业。当 RDD 及其 DAG 被提交给 DAGScheduler 调度后，DAGScheduler 会将所有 RDD 中的转换及动作视为一个 Job。一个 Job 由一到多个 Task 组成。</li>
<li>Task：具体执行任务。一个 Job 在每个 Stage 内都会按照 RDD 的 Partition 数量，创建多个 Task。Task 分为 ShuffleMapTask 和 ResultTask 两种。ShuffleMapStage 中的 Task 为 ShuffleMapTask，而 ResultStage 中 的 Task 为 ResultTask。ShuffleMapTask 和 ResultTask 类似于 Hadoop 中的 Map 任务和 Reduce 任务。<br>oozie job -oozie <a target="_blank" rel="noopener" href="http://localhost:11000/oozie">http://localhost:11000/oozie</a> -rerun 0000411-180116183039102-oozie-hado-C</li>
</ul>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">WangXun</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wangxukun.top/2021/09/09/Software/Spark入门-Dependencies、Lineage and Stage/">https://wangxukun.top/2021/09/09/Software/Spark入门-Dependencies、Lineage and Stage/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/09/10/Software/Spark%E5%85%A5%E9%97%A8-Cache%E3%80%81Persist%20and%20Checkpoint/"><i class="fa fa-chevron-left">  </i><span>Spark入门-Cache、Persist and Checkpoint</span></a></div><div class="next-post pull-right"><a href="/2021/09/08/Software/Spark%E5%85%A5%E9%97%A8-UDF%E5%92%8CUDAF%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0/"><span>Spark入门-UDF和UDAF自定义函数</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'aaf2844e0aeef4917c17',
  clientSecret: '10b96d24dffda7d3b4544778cf620f81990b676d',
  repo: 'blog-issue',
  owner: 'w749',
  admin: 'w749',
  id: md5(decodeURI(location.pathname)),
  language: 'en'
})
gitalk.render('gitalk-container')</script></div></div><footer class="footer-bg" style="background-image: url(/img/top-img.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2022 By WangXun</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/lib/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.1"></script><script src="/js/fancybox.js?version=1.9.1"></script><script src="/js/sidebar.js?version=1.9.1"></script><script src="/js/copy.js?version=1.9.1"></script><script src="/js/fireworks.js?version=1.9.1"></script><script src="/js/transition.js?version=1.9.1"></script><script src="/js/scroll.js?version=1.9.1"></script><script src="/js/head.js?version=1.9.1"></script><script src="/js/search/algolia.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>