<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="Spark入门-RDD、DataFrame and DataSet"><meta name="keywords" content="Spark"><meta name="author" content="WangXun"><meta name="copyright" content="WangXun"><title>Spark入门-RDD、DataFrame and DataSet | Wake</title><link rel="shortcut icon" href="/img/favicon-blank.svg"><link rel="stylesheet" href="/css/index.css?version=1.9.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"HSM2EINL2X","apiKey":"6f1478b12150efd917d5ecfcddfb8b8b","indexName":"wangxun","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#RDD"><span class="toc-number">1.</span> <span class="toc-text">RDD</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataFrame"><span class="toc-number">2.</span> <span class="toc-text">DataFrame</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#DataSet"><span class="toc-number">3.</span> <span class="toc-text">DataSet</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%89%E8%80%85%E7%9A%84%E8%81%94%E7%B3%BB%E4%B8%8E%E5%8C%BA%E5%88%AB"><span class="toc-number">4.</span> <span class="toc-text">三者的联系与区别</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89%E8%80%85%E7%9A%84%E5%85%B1%E6%80%A7"><span class="toc-number">4.1.</span> <span class="toc-text">三者的共性</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%89%E8%80%85%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="toc-number">4.2.</span> <span class="toc-text">三者的区别</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B8%E4%BA%92%E8%BD%AC%E6%8D%A2"><span class="toc-number">4.3.</span> <span class="toc-text">相互转换</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">WangXun</div><div class="author-info__description text-center"></div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/w749">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">71</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">20</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">6</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/top-img.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Wake</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">Spark入门-RDD、DataFrame and DataSet</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2021-09-08</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Software/">Software</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1.9k</span><span class="post-meta__separator">|</span><span>Reading time: 6 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p><code>RDD(Resilient Distributed Dataset)</code>叫做弹性分布式数据集，是 Spark 中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行 计算的集合。而 DataFrame 和 DataSet 分别是 Spark1.3 版本和 1.6 版本开始支持的数据集类型。它们之间彼此依赖也可以互相转换，分别应用在不同的场景下。</p>
<span id="more"></span>

<h3 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h3><p>RDD 是 Spark 计算的基础数据集合，之后的 DataFrame 和 DataSet 底层也是封装了 RDD ，所以掌握 RDD 对是学习Spark的第一步，源码中列出了 RDD 的五个特征，分别是：</p>
<ul>
<li>A list of partitions</li>
<li>A function for computing each split</li>
<li>A list of dependencies on other RDDs</li>
<li>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)</li>
<li>Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)</li>
</ul>
<p>总结下来就是 RDD 数据结构中存在分区列表，用于执行任务时并行计算；Spark 在计算时使用的是分区函数对每个分区进行计算；RDD 是计算模型的封装，当需要将多个计算模型进行组合时，需要在 RDD 之间建立彼此之间的依赖关系；当数据为 KV 键值对时，可设定分区器自定义数据的分区，可选；计算时可根据节点的状态选择不同的节点位置进行计算，可选。</p>
<p>从代码或者数据方面考虑，RDD 只存储数据，不存在 Schema，例如给它由多个元组组成的 List，它并不知道每个元组的第一个元素代表姓名，第二个元素代表年龄，但我们可以使用样例类封装每个元组，让它知道每个元组代表一个 User，如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 元组对组成的RDD</span></span><br><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;Bob&quot;</span>, <span class="number">23</span>), (<span class="string">&quot;Alice&quot;</span>, <span class="number">22</span>), (<span class="string">&quot;John&quot;</span>, <span class="number">24</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 给每个元组对加上属性</span></span><br><span class="line"><span class="keyword">val</span> rdd01 = sc.makeRDD(<span class="type">List</span>(<span class="type">User</span>(<span class="string">&quot;Bob&quot;</span>, <span class="number">23</span>), <span class="type">User</span>(<span class="string">&quot;Alice&quot;</span>, <span class="number">22</span>), <span class="type">User</span>(<span class="string">&quot;John&quot;</span>, <span class="number">24</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)  <span class="comment">// 样例类要放在main方法外</span></span></span><br></pre></td></tr></table></figure>

<h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><p>DataFrame 则是为了使 Spark 可以处理结构化数据或者半结构化数据而产生的数据集合，它可以从多个数据源读取数据加以处理，处理方式也可以使用我们熟知的 SQL 或者 Spark 独有的 DSL 语言，你可以将它想象成 RDD 中的每个元素都拥有了 Schema，有了字段名描述数据，但是没有了属性，最新版的 Spark 已经把 DataFrame 作为 DataSet 的一种特殊形式来构建，源码：<code>type DataFrame = Dataset[Row]</code>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从RDD转换</span></span><br><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;Bob&quot;</span>, <span class="number">23</span>), (<span class="string">&quot;Alice&quot;</span>, <span class="number">22</span>), (<span class="string">&quot;John&quot;</span>, <span class="number">24</span>)))</span><br><span class="line"><span class="keyword">val</span> df = rdd.toDF(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从文件中获取</span></span><br><span class="line">spark.read.json(<span class="string">&quot;data/user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// DSL</span></span><br><span class="line">df.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>, $<span class="string">&quot;age&quot;</span> + <span class="number">1</span> as <span class="string">&quot;new_age&quot;</span>)</span><br><span class="line">df.filter($<span class="string">&quot;age&quot;</span> &gt; <span class="number">21</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;select *, age + 1 as new_age from user&quot;</span>).show()</span><br></pre></td></tr></table></figure>

<h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><p>DataSet 是具有强类型的数据集合，需要提供对应的类型信息。在后期的 Spark 版本中，DataSet 有可能会逐步取代 RDD 和 DataFrame 成为唯一的 API 接口。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._  <span class="comment">// spark是SparkSession对象名，如果涉及到转换，需要引入转换规则</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 从RDD创建</span></span><br><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="type">User</span>(<span class="string">&quot;Bob&quot;</span>, <span class="number">23</span>), <span class="type">User</span>(<span class="string">&quot;Alice&quot;</span>, <span class="number">22</span>), <span class="type">User</span>(<span class="string">&quot;John&quot;</span>, <span class="number">24</span>)))</span><br><span class="line">rdd.toDS  <span class="comment">// RDD具有属性时可以直接转，Schema和属性都有了</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 从DF创建</span></span><br><span class="line"><span class="keyword">val</span> df = rdd.toDF(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line">df.as[<span class="type">User</span>]  <span class="comment">// 需要给DF明确属性</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 需要注意的是从文件中导入的数据在没有指定属性的情况下默认都是DataFrame，加上属性转换就可以转换为DataSet</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)  <span class="comment">// 样例类要放在main方法外</span></span></span><br></pre></td></tr></table></figure>

<p>DataSet 和 DataFrame 的最大区别是每一行的类型，也就是强类型和弱类型，我们在处理数据的时候是按行处理的，对每一行的不同字段进行处理就需要定位指定的字段，而 DataFrame 每一行是没有属性的，或者说它是 DataSet 每一行属性固定为 Row 的特例，取指定的值只能通过位置，这有很大的不确定性，并且也不好维护；而 DataSet 在一行内可以使用属性.字段的方式定位属性并加以操作。在自定义 UDF 函数的时候就能感受到强类型和弱类型的区别了。</p>
<h3 id="三者的联系与区别"><a href="#三者的联系与区别" class="headerlink" title="三者的联系与区别"></a>三者的联系与区别</h3><p>三者的共性、区别和相互之间的转换。</p>
<h4 id="三者的共性"><a href="#三者的共性" class="headerlink" title="三者的共性"></a>三者的共性</h4><ul>
<li>RDD 、DataFrame 、DataSet 全都是 Spark 平台下的分布式弹性数据集，为处理超大型数据提供便利;</li>
<li>三者都有惰性机制，在进行创建、转换，如 map 方法时，不会立即执行，只有在遇到 Action 如 foreach 时，三者才会开始遍历运算;</li>
<li>三者有许多共同的函数，如 filter，排序等;</li>
<li>在对 DataFrame 和 Dataset 进行操作许多操作都需要这个包：<code>import spark.implicits._</code>(在创建好 SparkSession  对象后尽量直接导入)；</li>
<li>三者都会根据 Spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出；</li>
<li>三者都有 partition 的概念；</li>
<li>DataFrame 和 DataSet 均可使用模式匹配获取各个字段的值和类型。</li>
</ul>
<h4 id="三者的区别"><a href="#三者的区别" class="headerlink" title="三者的区别"></a>三者的区别</h4><ol>
<li><strong>RDD</strong><ul>
<li>RDD 一般和 Spark Mllib 同时使用；</li>
<li>RDD 不支持 Spark SQL 操作；</li>
</ul>
</li>
<li><strong>DataFrame</strong><ul>
<li>与 RDD 和 Dataset 不同，DataFrame 每一行的类型固定为 Row，每一列的值没法直接访问，只有通过解析才能获取各个字段的值；</li>
<li>DataFrame 与 DataSet 一般不与 Spark Mllib 同时使用；</li>
<li>DataFrame 与 DataSet 均支持 Spark SQL 的操作，比如 select，groupby 之类，还能注册临时表&#x2F;视窗，进行 SQL 语句操作；</li>
<li>DataFrame 与 DataSet 支持一些特别方便的保存方式，比如保存成 csv，可以带上表头；</li>
</ul>
</li>
<li><strong>DataSet</strong><ul>
<li>Dataset 和 DataFrame 拥有完全相同的成员函数，区别只是每一行的数据类型不同；</li>
<li>DataFrame 其实就是 DataSet 的一个特例 <code>type DataFrame = Dataset[Row]</code>；</li>
<li>DataFrame 也可以叫 Dataset[Row] ，每一行的类型是 Row ，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的 getAS 方法或者共性中的第七条提到的模式匹配拿出特定字段。而 Dataset 中，每一行是什么类型是不一定的，在自定义了 case class 之后可以很自由的获得每一行的信息。</li>
</ul>
</li>
</ol>
<h4 id="相互转换"><a href="#相互转换" class="headerlink" title="相互转换"></a>相互转换</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._  <span class="comment">// spark是SparkSession对象名，如果涉及到转换，需要引入转换规则</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// RDD</span></span><br><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;Bob&quot;</span>, <span class="number">23</span>), (<span class="string">&quot;Alice&quot;</span>, <span class="number">22</span>), (<span class="string">&quot;John&quot;</span>, <span class="number">24</span>)))</span><br><span class="line">rdd.toDF(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>)  <span class="comment">// RDD转DataFrame</span></span><br><span class="line"><span class="keyword">val</span> rdd01 = rdd.map(<span class="type">User</span>(_._1, _._2))  <span class="comment">// 需要给RDD带上属性才可以直接转DataSet</span></span><br><span class="line">rdd01.toDS  <span class="comment">// RDD转DataSet</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrame</span></span><br><span class="line"><span class="keyword">val</span> df = rdd.toDF(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line">df.rdd  <span class="comment">// DataFrame转RDD</span></span><br><span class="line">df.as[<span class="type">User</span>].toDS  <span class="comment">// Dataframe转DataSet，加上属性直接转</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// DataSet</span></span><br><span class="line"><span class="keyword">val</span> ds = rdd.toDs</span><br><span class="line">ds.rdd  <span class="comment">// DataSet转RDD</span></span><br><span class="line">ds.toDF  <span class="comment">// DataSet转DataFrame</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure>

<p>只需要记住每个数据集合的特征就可以灵活的相互转换。RDD 可以是无属性的数据元素，也可以是有属性的数据元素，但是没有 Schema；DataFrame 是有 Schema 的数据元素，但是没有属性；DataSet是有Schema有属性的数据集合，所以从 RDD 到 DataFrame 再到 DataSet 是依次递进的过程。</p>
</div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">WangXun</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wangxukun.top/2021/09/08/Software/Spark入门-RDD、DataFrame and DataSet/">https://wangxukun.top/2021/09/08/Software/Spark入门-RDD、DataFrame and DataSet/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Spark/">Spark</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2021/09/09/Software/Spark%E5%85%A5%E9%97%A8-UDF%E5%92%8CUDAF%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0/"><i class="fa fa-chevron-left">  </i><span>Spark入门-UDF和UDAF自定义函数</span></a></div><div class="next-post pull-right"><a href="/2021/08/30/Software/Spark%E5%85%A5%E9%97%A8-%E8%81%9A%E5%90%88%E7%AE%97%E5%AD%90%E8%AF%A6%E8%A7%A3/"><span>Spark入门-聚合算子详解</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'aaf2844e0aeef4917c17',
  clientSecret: '10b96d24dffda7d3b4544778cf620f81990b676d',
  repo: 'blog-issue',
  owner: 'w749',
  admin: 'w749',
  id: md5(decodeURI(location.pathname)),
  language: 'en'
})
gitalk.render('gitalk-container')</script></div></div><footer class="footer-bg" style="background-image: url(/img/top-img.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2022 By WangXun</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/lib/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.1"></script><script src="/js/fancybox.js?version=1.9.1"></script><script src="/js/sidebar.js?version=1.9.1"></script><script src="/js/copy.js?version=1.9.1"></script><script src="/js/fireworks.js?version=1.9.1"></script><script src="/js/transition.js?version=1.9.1"></script><script src="/js/scroll.js?version=1.9.1"></script><script src="/js/head.js?version=1.9.1"></script><script src="/js/search/algolia.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>