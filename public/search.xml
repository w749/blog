<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Spark进阶-内存管理</title>
      <link href="/2022/11/12/Software/Spark%E8%BF%9B%E9%98%B6-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/"/>
      <url>/2022/11/12/Software/Spark%E8%BF%9B%E9%98%B6-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>Spark内存管理，主要包括Spark的堆内内存和堆外内存、内存管理机制等</p><span id="more"></span><blockquote><p>源码版本是Spark 3.1.2</p></blockquote><h2 id="Spark堆内堆外内存"><a href="#Spark堆内堆外内存" class="headerlink" title="Spark堆内堆外内存"></a>Spark堆内堆外内存</h2><p>Spark内存分为堆内内存（ON_HEAP）和堆外内存（OFF_HEAP），其中堆内内存基于JVM内存模型，堆外内存通过调用底层JDK Unsafe API。两种内存类型统一由Spark MemoryManager实现</p><h3 id="堆内内存"><a href="#堆内内存" class="headerlink" title="堆内内存"></a>堆内内存</h3><p>Executo作为一个JVM进程，其内部基于JVM的内存管理模型。Spark在其之上封装了统一的内存管理接口MemoryManager，通过对JVM堆空间进行合理的规划(逻辑上)，完成对象实例内存空间的申请和释放。保证满足Spark运行机制的前提下，最大化利用内存空间</p><p>这里涉及到的JVM堆空间概念，简单描述就是在程序中，关于对象实例&#x2F;数组的创建、使用和释放的内存，都会在JVM中的一块被称作为”JVM堆”内存区域内进行管理分配。Spark程序在创建对象后，JVM会在堆内内存中分配一定大小的空间，创建Class对象并返回对象引用，Spark保存对象引用，同时记录占用的内存信息</p><p>Spark中堆内内存参数有: -executor-memory。通常是任务提交时在参数中进行定义，且与-executor-cores等相关配置一起被提交至ResourceManager中进行Executor的资源申请。在Worker节点创建一定数目的Executor，每个Executor被分配-executor-memory大小的堆内内存。Executor的堆内内存被所有的Task线程任务共享，多线程在内存中进行数据交换</p><p>Spark堆内内存主要分为Storage(存储内存)、Execution(执行内存)和Other(其他) 几部分</p><ul><li>Storage用于缓存RDD数据和broadcast广播变量的内存使用</li><li>Execution仅提供shuffle过程的内存使用</li><li>Other提供Spark内部对象、用户自定义对象的内存空间</li></ul><h3 id="堆外内存"><a href="#堆外内存" class="headerlink" title="堆外内存"></a>堆外内存</h3><p>Spark1.6在堆内内存的基础上引入了堆外内存，进一步优化了Spark内存的使用率。堆外内存其底层调用基于C的JDK Unsafe类方法，通过指针直接进行内存的操作，包括内存空间的申请、使用、删除释放等。</p><p>Spark在2.x之后，摒弃了之前版本的Tachyon，采用Java中常见的基于JDK Unsafe API来对堆外内存进行管理。此模式不在JVM中申请内存，而是直接操作系统内存，减少了JVM中内存空间切换的开销，降低了GC回收占用的消耗，实现对内存的精确管控。</p><p>堆外内存默认情况下是不开启的，需要在配置中将spark.memory.offHeap.enabled设为True,同时配置spark.memory.offHeap.size参数设置堆大小。对于堆外内存的划分，仅包含Execution(执行内存)和Storage(存储内存)两块区域，且被所有task线程任务共享。</p><h2 id="MemoryManager"><a href="#MemoryManager" class="headerlink" title="MemoryManager"></a>MemoryManager</h2><p>MemoryManager用来管理Spark的内存，主要包含计算内存和存储内存的管理，在Spark1.6版本之前使用的是静态内存管理机制（Static Memory Manager），Spark计算和存储使用的内存都是静态的，通过参数调整大小；Spark1.6以后考虑到内存管理的灵活性改成了统一内存管理(Unified Memory Manager)</p><h3 id="统一内存管理"><a href="#统一内存管理" class="headerlink" title="统一内存管理"></a>统一内存管理</h3><p>在看Spark3.1.2版本的代码时发现Spark只能使用统一内存管理机制进行内存管理，所以对静态内存管理不过多赘述</p><h4 id="on-heap"><a href="#on-heap" class="headerlink" title="on-heap"></a>on-heap</h4><div align=center><img src="on-heap.png"></div><p>堆内内存整体划分为Usable Memory(可用内存)和Reversed Memory(预留内存)两大部分。其中预留内存作为OOM等异常情况的内存使用区域，默认被分配300M的空间。可用内存可进一步分为(Unified Memory)统一内存和Other内存其他两部分，默认占比为6:4</p><p>统一内存中的Storage(存储内存)和Execution(执行内存)以及Other内存，其参数及使用范围均与静态内存模式一致，只是Storage、Execution之间启用了动态内存占用机制。动态内存占用机制：</p><ul><li>设置内存的初始值，即Execution和Storage均需设定各自的内存区域范围(默认参数0.5)</li><li>若存在一方内存不足，另一方内存空余时，可占用对方内存空间</li><li>双方内存均不足时，需落盘处理</li><li>Execution内存被占用时，Storage需将此部分转存硬盘并归还空间</li><li>Storage内存被占用时，Execution无需归还</li></ul><h4 id="off-heap"><a href="#off-heap" class="headerlink" title="off-heap"></a>off-heap</h4><div align=center><img src="off-heap.png"></div><p>和静态管理模式分配一致，堆外内存默认值为384M。整体分为Storage和Execution两部分，且启用动态内存占用机制，其中默认的初始化占比值均为0.5</p><h3 id="UnifiedMemoryManager初始化"><a href="#UnifiedMemoryManager初始化" class="headerlink" title="UnifiedMemoryManager初始化"></a>UnifiedMemoryManager初始化</h3><p>UnifiedMemoryManager是MemoryManager的实现类，它在SparkEnv中实例化，通过调用伴生对象的apply方法构建对象</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(conf: <span class="type">SparkConf</span>, numCores: <span class="type">Int</span>): <span class="type">UnifiedMemoryManager</span> = &#123;</span><br><span class="line">  <span class="comment">// 返回Execution和Storage之间共享的内存总量，以字节为单位</span></span><br><span class="line">  <span class="keyword">val</span> maxMemory = getMaxMemory(conf)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">UnifiedMemoryManager</span>(</span><br><span class="line">    conf,</span><br><span class="line">    maxHeapMemory = maxMemory,</span><br><span class="line">    onHeapStorageRegionSize =</span><br><span class="line">      (maxMemory * conf.get(config.<span class="type">MEMORY_STORAGE_FRACTION</span>)).toLong,</span><br><span class="line">    numCores = numCores)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>getMaxMemory方法则是根据配置参数确定执行和存储的内存</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getMaxMemory</span></span>(conf: <span class="type">SparkConf</span>): <span class="type">Long</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> systemMemory = conf.get(<span class="type">TEST_MEMORY</span>)</span><br><span class="line">  <span class="comment">// 预留内存就是RESERVED_SYSTEM_MEMORY_BYTES 300M</span></span><br><span class="line">  <span class="keyword">val</span> reservedMemory = conf.getLong(<span class="type">TEST_RESERVED_MEMORY</span>.key,</span><br><span class="line">    <span class="keyword">if</span> (conf.contains(<span class="type">IS_TESTING</span>)) <span class="number">0</span> <span class="keyword">else</span> <span class="type">RESERVED_SYSTEM_MEMORY_BYTES</span>)</span><br><span class="line">  <span class="keyword">val</span> minSystemMemory = (reservedMemory * <span class="number">1.5</span>).ceil.toLong</span><br><span class="line">  <span class="keyword">if</span> (systemMemory &lt; minSystemMemory) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">s&quot;System memory <span class="subst">$systemMemory</span> must &quot;</span> +</span><br><span class="line">      <span class="string">s&quot;be at least <span class="subst">$minSystemMemory</span>. Please increase heap size using the --driver-memory &quot;</span> +</span><br><span class="line">      <span class="string">s&quot;option or <span class="subst">$&#123;config.DRIVER_MEMORY.key&#125;</span> in Spark configuration.&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// SPARK-12759 Check executor memory to fail fast if memory is insufficient</span></span><br><span class="line">  <span class="keyword">if</span> (conf.contains(config.<span class="type">EXECUTOR_MEMORY</span>)) &#123;</span><br><span class="line">    <span class="keyword">val</span> executorMemory = conf.getSizeAsBytes(config.<span class="type">EXECUTOR_MEMORY</span>.key)</span><br><span class="line">    <span class="keyword">if</span> (executorMemory &lt; minSystemMemory) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">s&quot;Executor memory <span class="subst">$executorMemory</span> must be at least &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;<span class="subst">$minSystemMemory</span>. Please increase executor memory using the &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;--executor-memory option or <span class="subst">$&#123;config.EXECUTOR_MEMORY.key&#125;</span> in Spark configuration.&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> usableMemory = systemMemory - reservedMemory</span><br><span class="line">  <span class="comment">// config.MEMORY_FRACTION就是spark.memory.fraction参数，execution和storage的内存占usableMemory的比率</span></span><br><span class="line">  <span class="keyword">val</span> memoryFraction = conf.get(config.<span class="type">MEMORY_FRACTION</span>)</span><br><span class="line">  (usableMemory * memoryFraction).toLong</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>而在创建实例对象时又初始化了几个属性用来跟踪execution和storage内存的使用情况（在父类MemoryManager中初始化）</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 分别初始化了几个内存池用来跟踪内存使用情况</span></span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">val</span> onHeapStorageMemoryPool = <span class="keyword">new</span> <span class="type">StorageMemoryPool</span>(<span class="keyword">this</span>, <span class="type">MemoryMode</span>.<span class="type">ON_HEAP</span>)</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">val</span> offHeapStorageMemoryPool = <span class="keyword">new</span> <span class="type">StorageMemoryPool</span>(<span class="keyword">this</span>, <span class="type">MemoryMode</span>.<span class="type">OFF_HEAP</span>)</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">val</span> onHeapExecutionMemoryPool = <span class="keyword">new</span> <span class="type">ExecutionMemoryPool</span>(<span class="keyword">this</span>, <span class="type">MemoryMode</span>.<span class="type">ON_HEAP</span>)</span><br><span class="line"><span class="keyword">protected</span> <span class="keyword">val</span> offHeapExecutionMemoryPool = <span class="keyword">new</span> <span class="type">ExecutionMemoryPool</span>(<span class="keyword">this</span>, <span class="type">MemoryMode</span>.<span class="type">OFF_HEAP</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 初始化堆内内存池的可用内存，这是实例化时传入的参数</span></span><br><span class="line">onHeapStorageMemoryPool.incrementPoolSize(onHeapStorageMemory)</span><br><span class="line">onHeapExecutionMemoryPool.incrementPoolSize(onHeapExecutionMemory)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 堆外内存池初始化</span></span><br><span class="line"><span class="keyword">protected</span>[<span class="keyword">this</span>] <span class="keyword">val</span> maxOffHeapMemory = conf.get(<span class="type">MEMORY_OFFHEAP_SIZE</span>)</span><br><span class="line"><span class="keyword">protected</span>[<span class="keyword">this</span>] <span class="keyword">val</span> offHeapStorageMemory =</span><br><span class="line">  (maxOffHeapMemory * conf.get(<span class="type">MEMORY_STORAGE_FRACTION</span>)).toLong</span><br><span class="line"></span><br><span class="line">offHeapExecutionMemoryPool.incrementPoolSize(maxOffHeapMemory - offHeapStorageMemory)</span><br><span class="line">offHeapStorageMemoryPool.incrementPoolSize(offHeapStorageMemory)</span><br></pre></td></tr></table></figure><p>MemoryManager初始化完成后，它会被传到Driver和每个Executor的BlockManager和SparkEnv中等待调用</p><h3 id="MemoryManager的使用"><a href="#MemoryManager的使用" class="headerlink" title="MemoryManager的使用"></a>MemoryManager的使用</h3><p>以将数据写入堆内内存的广播变量为例，最终调用BlockManager.putBytes方法，接下来的调用逻辑看下图，这里只看与MemoryManager交互的部分</p><div align=center><img src="BlockManager-putBytes.png"></div><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">acquireStorageMemory</span></span>(</span><br><span class="line">    blockId: <span class="type">BlockId</span>,</span><br><span class="line">    numBytes: <span class="type">Long</span>,</span><br><span class="line">    memoryMode: <span class="type">MemoryMode</span>): <span class="type">Boolean</span> = synchronized &#123;</span><br><span class="line">  assertInvariants()</span><br><span class="line">  assert(numBytes &gt;= <span class="number">0</span>)</span><br><span class="line">  <span class="comment">// 广播变量的StorageLevel是MEMORY_AND_DISK_SER，所以它存储在堆内内存</span></span><br><span class="line">  <span class="keyword">val</span> (executionPool, storagePool, maxMemory) = memoryMode <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">MemoryMode</span>.<span class="type">ON_HEAP</span> =&gt; (</span><br><span class="line">      onHeapExecutionMemoryPool,</span><br><span class="line">      onHeapStorageMemoryPool,</span><br><span class="line">      maxOnHeapStorageMemory)</span><br><span class="line">    <span class="keyword">case</span> <span class="type">MemoryMode</span>.<span class="type">OFF_HEAP</span> =&gt; (</span><br><span class="line">      offHeapExecutionMemoryPool,</span><br><span class="line">      offHeapStorageMemoryPool,</span><br><span class="line">      maxOffHeapStorageMemory)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 如果需要存储的数据大小大于on-heap storage的最大内存那么直接返回false</span></span><br><span class="line">  <span class="keyword">if</span> (numBytes &gt; maxMemory) &#123;</span><br><span class="line">    logInfo(<span class="string">s&quot;Will not store <span class="subst">$blockId</span> as the required space (<span class="subst">$numBytes</span> bytes) exceeds our &quot;</span> +</span><br><span class="line">      <span class="string">s&quot;memory limit (<span class="subst">$maxMemory</span> bytes)&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 需要存储的数据大小大于用于storage的空闲内存时尝试占用execution的内存</span></span><br><span class="line">  <span class="keyword">if</span> (numBytes &gt; storagePool.memoryFree) &#123;</span><br><span class="line">    <span class="keyword">val</span> memoryBorrowedFromExecution = <span class="type">Math</span>.min(executionPool.memoryFree,</span><br><span class="line">      numBytes - storagePool.memoryFree)</span><br><span class="line">    <span class="comment">// 注意memoryBorrowedFromExecution计算方式，有可能从execution内存占用后仍然不够</span></span><br><span class="line">    executionPool.decrementPoolSize(memoryBorrowedFromExecution)</span><br><span class="line">    storagePool.incrementPoolSize(memoryBorrowedFromExecution)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 获取所需的内存，必要时删掉其他block</span></span><br><span class="line">  storagePool.acquireMemory(blockId, numBytes)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从execution占用内存后仍然不够尝试删掉其他block，如有必要（isUseDisk）则将它存到磁盘中</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">acquireMemory</span></span>(</span><br><span class="line">    blockId: <span class="type">BlockId</span>,</span><br><span class="line">    numBytesToAcquire: <span class="type">Long</span>,</span><br><span class="line">    numBytesToFree: <span class="type">Long</span>): <span class="type">Boolean</span> = lock.synchronized &#123;</span><br><span class="line">  assert(numBytesToAcquire &gt;= <span class="number">0</span>)</span><br><span class="line">  assert(numBytesToFree &gt;= <span class="number">0</span>)</span><br><span class="line">  assert(memoryUsed &lt;= poolSize)</span><br><span class="line">  <span class="keyword">if</span> (numBytesToFree &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="comment">// 占用execution内存后仍然不够，那么就删掉其他的block，删除大小由numBytesToFree控制</span></span><br><span class="line">    memoryStore.evictBlocksToFreeSpace(<span class="type">Some</span>(blockId), numBytesToFree, memoryMode)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> enoughMemory = numBytesToAcquire &lt;= memoryFree</span><br><span class="line">  <span class="keyword">if</span> (enoughMemory) &#123;</span><br><span class="line">    _memoryUsed += numBytesToAcquire</span><br><span class="line">  &#125;</span><br><span class="line">  enoughMemory</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>释放内存就是清空跟踪状态，真正的数据都在MemoryStore存着，MemoryManager只是跟踪记录存储大小状态而已</p><h2 id="MemoryStore"><a href="#MemoryStore" class="headerlink" title="MemoryStore"></a>MemoryStore</h2><p>数据以MemoryEntry的方形式存在<code>private val entries = new LinkedHashMap[BlockId, MemoryEntry[_]]</code>中，MemoryStore要做的就是对外提供接口对entries进行管理，由MemoryManager跟踪管理可用内存情况</p>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark进阶-BlockManager</title>
      <link href="/2022/11/02/Software/Spark%E8%BF%9B%E9%98%B6-BlockManager/"/>
      <url>/2022/11/02/Software/Spark%E8%BF%9B%E9%98%B6-BlockManager/</url>
      
        <content type="html"><![CDATA[<p>Spark BlockManager及相关组件和主要运行流程介绍</p><span id="more"></span><blockquote><p>源码版本是Spark 3.1.2</p></blockquote><h2 id="BlockManagerMaster"><a href="#BlockManagerMaster" class="headerlink" title="BlockManagerMaster"></a>BlockManagerMaster</h2><p>之所以以BlockManagerMaster为起点有两个原因，第一个是因为它的初始化早于BlockManager，第二个是因为它负责将BlockManager的请求操作发送给Driver的BlockManagerMasterEndpoint由它来处理一系列请求，driver和executor都有BlockManagerMaster</p><p>它的初始化代码在SparkEnv的create方法中，现在不管是driver还是executor都有自己的BlockManagerMaster，并且都拿到了BlockManagerMaster的RpcEndpointRef和BlockManagerMasterHeartbeat的RpcEndpointRef</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> blockManagerMaster = <span class="keyword">new</span> <span class="type">BlockManagerMaster</span>(</span><br><span class="line">  <span class="comment">// 创建或者查找对应的RpcEndpointRef，如果在driver端则需要注册，如果是executor则通过名称检索对应的RpcEndpointRef。这里返回BlockManagerMaster的RpcEndpointRef</span></span><br><span class="line">  registerOrLookupEndpoint(</span><br><span class="line">    <span class="type">BlockManagerMaster</span>.<span class="type">DRIVER_ENDPOINT_NAME</span>,</span><br><span class="line">    <span class="keyword">new</span> <span class="type">BlockManagerMasterEndpoint</span>(</span><br><span class="line">      rpcEnv,</span><br><span class="line">      isLocal,</span><br><span class="line">      conf,</span><br><span class="line">      listenerBus,</span><br><span class="line">      <span class="keyword">if</span> (conf.get(config.<span class="type">SHUFFLE_SERVICE_FETCH_RDD_ENABLED</span>)) &#123;</span><br><span class="line">        externalShuffleClient</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">None</span></span><br><span class="line">      &#125;, blockManagerInfo,</span><br><span class="line">      mapOutputTracker.asInstanceOf[<span class="type">MapOutputTrackerMaster</span>])),</span><br><span class="line">  <span class="comment">// 返回BlockManagerMasterHeartbeat的RpcEndpointRef，不重点关注</span></span><br><span class="line">  registerOrLookupEndpoint(</span><br><span class="line">    <span class="type">BlockManagerMaster</span>.<span class="type">DRIVER_HEARTBEAT_ENDPOINT_NAME</span>,</span><br><span class="line">    <span class="keyword">new</span> <span class="type">BlockManagerMasterHeartbeatEndpoint</span>(rpcEnv, isLocal, blockManagerInfo)),</span><br><span class="line">  conf,</span><br><span class="line">  isDriver)</span><br></pre></td></tr></table></figure><h2 id="BlockManagerMasterEndpoint"><a href="#BlockManagerMasterEndpoint" class="headerlink" title="BlockManagerMasterEndpoint"></a>BlockManagerMasterEndpoint</h2><p>它是的作用是跟踪并保存BlockManager的状态，处理由BlockManagerMaster发送来的一些操作请求并返回所需的状态数据，例如GetLocations（获取指定blockId位置）、RemoveBroadcast（从各BlockManager移除广播变量）等等。当然它只是个中转站，用来解析请求内容并将它发送给其他的BlockManager（driver或者executor）去处理</p><h3 id="重要属性"><a href="#重要属性" class="headerlink" title="重要属性"></a>重要属性</h3><ul><li>blockManagerInfo: mutable.Map[BlockManagerId, BlockManagerInfo] 维护BlockManagerId和BlockManagerInfo的关系，初始化时它是空的</li><li>executorIdToLocalDirs: Cache[String, Array[String]] 维护executorId和localDirs的映射关系</li><li>blockManagerIdByExecutor: mutable.HashMap[String, BlockManagerId] 维护executorId和BlockManagerId的映射关系</li><li>blockLocations: JHashMap[BlockId, mutable.HashSet[BlockManagerId]] 维护blockId对应拥有此block的BlockMangerId的映射关系</li></ul><h3 id="BlockManagerMasterEndpoint-register"><a href="#BlockManagerMasterEndpoint-register" class="headerlink" title="BlockManagerMasterEndpoint.register"></a>BlockManagerMasterEndpoint.register</h3><p>每个BlockManager都需要在BlockManagerMasterEndpoint注册，来看一下register方法都做了哪些事</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">register</span></span>(</span><br><span class="line">    idWithoutTopologyInfo: <span class="type">BlockManagerId</span>,</span><br><span class="line">    localDirs: <span class="type">Array</span>[<span class="type">String</span>],</span><br><span class="line">    maxOnHeapMemSize: <span class="type">Long</span>,</span><br><span class="line">    maxOffHeapMemSize: <span class="type">Long</span>,</span><br><span class="line">    storageEndpoint: <span class="type">RpcEndpointRef</span>): <span class="type">BlockManagerId</span> = &#123;</span><br><span class="line">  <span class="comment">// 重新封装BlockManagerId</span></span><br><span class="line">  <span class="keyword">val</span> id = <span class="type">BlockManagerId</span>(</span><br><span class="line">    idWithoutTopologyInfo.executorId,</span><br><span class="line">    idWithoutTopologyInfo.host,</span><br><span class="line">    idWithoutTopologyInfo.port,</span><br><span class="line">    topologyMapper.getTopologyForHost(idWithoutTopologyInfo.host))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> time = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">  <span class="comment">// 将executorId和磁盘地址的映射维护到executorIdToLocalDirs</span></span><br><span class="line">  executorIdToLocalDirs.put(id.executorId, localDirs)</span><br><span class="line">  <span class="comment">// register时blockManagerInfo和blockManagerIdByExecutor不能包含相同的id信息</span></span><br><span class="line">  <span class="keyword">if</span> (!blockManagerInfo.contains(id)) &#123;</span><br><span class="line">    blockManagerIdByExecutor.get(id.executorId) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(oldId) =&gt;</span><br><span class="line">        <span class="comment">// A block manager of the same executor already exists, so remove it (assumed dead)</span></span><br><span class="line">        logError(<span class="string">&quot;Got two different block manager registrations on same executor - &quot;</span></span><br><span class="line">            + <span class="string">s&quot; will replace old one <span class="subst">$oldId</span> with new one <span class="subst">$id</span>&quot;</span>)</span><br><span class="line">        removeExecutor(id.executorId)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">    &#125;</span><br><span class="line">    logInfo(<span class="string">&quot;Registering block manager %s with %s RAM, %s&quot;</span>.format(</span><br><span class="line">      id.hostPort, <span class="type">Utils</span>.bytesToString(maxOnHeapMemSize + maxOffHeapMemSize), id))</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 维护executorId和BlockManagerId</span></span><br><span class="line">    blockManagerIdByExecutor(id.executorId) = id</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> externalShuffleServiceBlockStatus =</span><br><span class="line">      <span class="keyword">if</span> (externalShuffleServiceRddFetchEnabled) &#123;</span><br><span class="line">        <span class="keyword">val</span> externalShuffleServiceBlocks = blockStatusByShuffleService</span><br><span class="line">          .getOrElseUpdate(externalShuffleServiceIdOnHost(id), <span class="keyword">new</span> <span class="type">JHashMap</span>[<span class="type">BlockId</span>, <span class="type">BlockStatus</span>])</span><br><span class="line">        <span class="type">Some</span>(externalShuffleServiceBlocks)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="type">None</span></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 维护BlockManagerId和BlockManagerInfo</span></span><br><span class="line">    blockManagerInfo(id) = <span class="keyword">new</span> <span class="type">BlockManagerInfo</span>(id, <span class="type">System</span>.currentTimeMillis(),</span><br><span class="line">      maxOnHeapMemSize, maxOffHeapMemSize, storageEndpoint, externalShuffleServiceBlockStatus)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (pushBasedShuffleEnabled) &#123;</span><br><span class="line">      addMergerLocation(id)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  listenerBus.post(<span class="type">SparkListenerBlockManagerAdded</span>(time, id, maxOnHeapMemSize + maxOffHeapMemSize,</span><br><span class="line">      <span class="type">Some</span>(maxOnHeapMemSize), <span class="type">Some</span>(maxOffHeapMemSize)))</span><br><span class="line">  id</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="以RemoveBroadcast为例跟踪消息流程"><a href="#以RemoveBroadcast为例跟踪消息流程" class="headerlink" title="以RemoveBroadcast为例跟踪消息流程"></a>以RemoveBroadcast为例跟踪消息流程</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">rddBroadcast.unpersist()  <span class="comment">// 程序入口</span></span><br><span class="line">unpersist(blocking = <span class="literal">false</span>)</span><br><span class="line">doUnpersist(blocking)</span><br><span class="line"><span class="type">TorrentBroadcast</span>.doUnpersist(blocking: <span class="type">Boolean</span>)</span><br><span class="line"><span class="type">TorrentBroadcast</span>.unpersist(id, removeFromDriver = <span class="literal">false</span>, blocking)</span><br><span class="line"><span class="comment">// 由driver的BlockManager的BlockManagerMaster发送RemoveBroadcast消息</span></span><br><span class="line"><span class="type">SparkEnv</span>.get.blockManager.master.removeBroadcast(id, removeFromDriver, blocking)</span><br><span class="line"><span class="comment">// 向BlockManagerMasterEndpoint发送RemoveBroadcast消息</span></span><br><span class="line">driverEndpoint.askSync[<span class="type">Future</span>[<span class="type">Seq</span>[<span class="type">Int</span>]]](<span class="type">RemoveBroadcast</span>(broadcastId, removeFromMaster))</span><br><span class="line"><span class="comment">// 消息会被这个方法处理，查看RemoveBroadcast消息的处理方法就行</span></span><br><span class="line"><span class="type">BlockManagerMasterEndpoint</span>.receiveAndReply(context: <span class="type">RpcCallContext</span>)</span><br><span class="line"><span class="type">BlockManagerMasterEndpoint</span>.removeBroadcast(broadcastId: <span class="type">Long</span>, removeFromDriver: <span class="type">Boolean</span>)</span><br><span class="line"><span class="comment">// BlockManagerMasterEndpoint处理不了，将它发送给BlockManagerStorageEndpoint，BlockManagerStorageEndpoint相关介绍往下看</span></span><br><span class="line">bm.storageEndpoint.ask[<span class="type">Int</span>](removeMsg).recover</span><br><span class="line"><span class="type">BlockManagerStorageEndpoint</span>.receiveAndReply(context: <span class="type">RpcCallContext</span>)</span><br><span class="line">blockManager.removeBroadcast(broadcastId, tellMaster = <span class="literal">true</span>)  <span class="comment">// 最终它会调用对应BlockManager的removeBroadcast方法</span></span><br></pre></td></tr></table></figure><p>例如RegisterBlockManager、GetLocations这些消息BlockManagerMasterEndpoint可以处理，因为所有的BlockManager都在它这注册，所以它自己处理完之后就可以返回，类似移除Block、移除Shuffle都由各自Executor的BlockManager完成所有发送给对应的BlockManagerStorageEndpoint</p><h2 id="BlockManagerStorageEndpoint"><a href="#BlockManagerStorageEndpoint" class="headerlink" title="BlockManagerStorageEndpoint"></a>BlockManagerStorageEndpoint</h2><p>它的作用是接收从BlockManagerMasterEndpoint接收消息并在自己对应的BlockManager执行对应的操作，例如上面的RemoveBroadcast，它和BlockManager是一对一对应的。下面看一下它的初始化</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 它的初始化在BlockManager实例化过程中已经完成</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> storageEndpoint = rpcEnv.setupEndpoint(</span><br><span class="line">  <span class="string">&quot;BlockManagerEndpoint&quot;</span> + <span class="type">BlockManager</span>.<span class="type">ID_GENERATOR</span>.next,</span><br><span class="line">  <span class="keyword">new</span> <span class="type">BlockManagerStorageEndpoint</span>(rpcEnv, <span class="keyword">this</span>, mapOutputTracker))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 向BlockManagerMasterEndpoint注册自己</span></span><br><span class="line"><span class="keyword">val</span> idFromMaster = master.registerBlockManager(</span><br><span class="line">  id,</span><br><span class="line">  diskBlockManager.localDirsString,</span><br><span class="line">  maxOnHeapMemory,</span><br><span class="line">  maxOffHeapMemory,</span><br><span class="line">  storageEndpoint)</span><br></pre></td></tr></table></figure><p>在向BlockManagerMasterEndpoint注册时就将BlockManagerStorageEndpoint一起已发送去了，随后又封装在blockManagerInfo对应的BlockManagerInfo中，这样BlockManagerMasterEndpoint就可以向任何一个BlockManager的BlockManagerStorageEndpoint发送消息了</p><h2 id="BlockManager"><a href="#BlockManager" class="headerlink" title="BlockManager"></a>BlockManager</h2><p>说了那么多终于到主角了，BlockManager运行在driver和executor上，它向用户提供了在磁盘或者内存中读取或者写入block数据的接口，在它之下仍然有更底层的实现（DiskStore、MemoryStore）</p><h3 id="BlockManager初始化"><a href="#BlockManager初始化" class="headerlink" title="BlockManager初始化"></a>BlockManager初始化</h3><p>创建BlockManager实例在SparkEnv中<code>val blockManager = new BlockManager(...</code>，创建实例的同时有许多组件也被实例化了，例如DiskBlockManager、MemoryStore、DiskStore、BlockManagerStorageEndpoint、BlockInfoManager等等。只有在调用它的initialize方法时该BlockManager才会真正被初始化</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(appId: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  blockTransferService.init(<span class="keyword">this</span>)</span><br><span class="line">  externalBlockStoreClient.foreach &#123; blockStoreClient =&gt;</span><br><span class="line">    blockStoreClient.init(appId)</span><br><span class="line">  &#125;</span><br><span class="line">  blockReplicationPolicy = &#123;</span><br><span class="line">    <span class="keyword">val</span> priorityClass = conf.get(config.<span class="type">STORAGE_REPLICATION_POLICY</span>)</span><br><span class="line">    <span class="keyword">val</span> clazz = <span class="type">Utils</span>.classForName(priorityClass)</span><br><span class="line">    <span class="keyword">val</span> ret = clazz.getConstructor().newInstance().asInstanceOf[<span class="type">BlockReplicationPolicy</span>]</span><br><span class="line">    logInfo(<span class="string">s&quot;Using <span class="subst">$priorityClass</span> for block replication policy&quot;</span>)</span><br><span class="line">    ret</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 初始化BlockManagerId</span></span><br><span class="line">  <span class="keyword">val</span> id = <span class="type">BlockManagerId</span>(executorId, blockTransferService.hostName, blockTransferService.port, <span class="type">None</span>)</span><br><span class="line">  <span class="comment">// 向BlockManagerMasterEndpoint注册自己</span></span><br><span class="line">  <span class="keyword">val</span> idFromMaster = master.registerBlockManager(</span><br><span class="line">    id,</span><br><span class="line">    diskBlockManager.localDirsString,</span><br><span class="line">    maxOnHeapMemory,</span><br><span class="line">    maxOffHeapMemory,</span><br><span class="line">    storageEndpoint)</span><br><span class="line"></span><br><span class="line">  blockManagerId = <span class="keyword">if</span> (idFromMaster != <span class="literal">null</span>) idFromMaster <span class="keyword">else</span> id</span><br><span class="line">  <span class="comment">// Shuffle的服务ID，一般也是当前BlockManagerId</span></span><br><span class="line">  shuffleServerId = <span class="keyword">if</span> (externalShuffleServiceEnabled) &#123;</span><br><span class="line">    logInfo(<span class="string">s&quot;external shuffle service port = <span class="subst">$externalShuffleServicePort</span>&quot;</span>)</span><br><span class="line">    <span class="type">BlockManagerId</span>(executorId, blockTransferService.hostName, externalShuffleServicePort)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    blockManagerId</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Register Executors&#x27; configuration with the local shuffle service, if one should exist.</span></span><br><span class="line">  <span class="keyword">if</span> (externalShuffleServiceEnabled &amp;&amp; !blockManagerId.isDriver) &#123;</span><br><span class="line">    registerWithExternalShuffleServer()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  hostLocalDirManager = &#123;</span><br><span class="line">    <span class="keyword">if</span> (conf.get(config.<span class="type">SHUFFLE_HOST_LOCAL_DISK_READING_ENABLED</span>) &amp;&amp;</span><br><span class="line">        !conf.get(config.<span class="type">SHUFFLE_USE_OLD_FETCH_PROTOCOL</span>)) &#123;</span><br><span class="line">      <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">HostLocalDirManager</span>(</span><br><span class="line">        futureExecutionContext,</span><br><span class="line">        conf.get(config.<span class="type">STORAGE_LOCAL_DISK_BY_EXECUTORS_CACHE_SIZE</span>),</span><br><span class="line">        blockStoreClient))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">None</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  logInfo(<span class="string">s&quot;Initialized BlockManager: <span class="subst">$blockManagerId</span>&quot;</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="write-block"><a href="#write-block" class="headerlink" title="write block"></a>write block</h3><p>write block最终都汇聚到两个方法，BlockStoreUpdater.save和BlockManager.doPutIterator，下面分别看一下这两个方法<br>BlockManager.doPutIterator写入iterator数据</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doPutIterator</span></span>[<span class="type">T</span>](</span><br><span class="line">    blockId: <span class="type">BlockId</span>,</span><br><span class="line">    iterator: () =&gt; <span class="type">Iterator</span>[<span class="type">T</span>],</span><br><span class="line">    level: <span class="type">StorageLevel</span>,</span><br><span class="line">    classTag: <span class="type">ClassTag</span>[<span class="type">T</span>],</span><br><span class="line">    tellMaster: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">    keepReadLock: <span class="type">Boolean</span> = <span class="literal">false</span>): <span class="type">Option</span>[<span class="type">PartiallyUnrolledIterator</span>[<span class="type">T</span>]] = &#123;</span><br><span class="line">  doPut(blockId, level, classTag, tellMaster = tellMaster, keepReadLock = keepReadLock) &#123; info =&gt;</span><br><span class="line">    <span class="keyword">val</span> startTimeNs = <span class="type">System</span>.nanoTime()</span><br><span class="line">    <span class="keyword">var</span> iteratorFromFailedMemoryStorePut: <span class="type">Option</span>[<span class="type">PartiallyUnrolledIterator</span>[<span class="type">T</span>]] = <span class="type">None</span></span><br><span class="line">    <span class="comment">// Size of the block in bytes</span></span><br><span class="line">    <span class="keyword">var</span> size = <span class="number">0</span>L</span><br><span class="line">    <span class="keyword">if</span> (level.useMemory) &#123;</span><br><span class="line">      <span class="keyword">if</span> (level.deserialized) &#123;</span><br><span class="line">        <span class="comment">// 将给定的iterator放入对应的blockId中，最终调用MemoryStore.putIterator方法</span></span><br><span class="line">        memoryStore.putIteratorAsValues(blockId, iterator(), classTag) <span class="keyword">match</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="type">Right</span>(s) =&gt;</span><br><span class="line">            size = s</span><br><span class="line">          <span class="comment">// 如果内存中没有足够空间放不下数据那么会尝试将数据放入磁盘中：StorageLevel.MEMORY_AND_DISK</span></span><br><span class="line">          <span class="keyword">case</span> <span class="type">Left</span>(iter) =&gt;</span><br><span class="line">            <span class="keyword">if</span> (level.useDisk) &#123;</span><br><span class="line">              logWarning(<span class="string">s&quot;Persisting block <span class="subst">$blockId</span> to disk instead.&quot;</span>)</span><br><span class="line">              diskStore.put(blockId) &#123; channel =&gt;</span><br><span class="line">                <span class="keyword">val</span> out = <span class="type">Channels</span>.newOutputStream(channel)</span><br><span class="line">                serializerManager.dataSerializeStream(blockId, out, iter)(classTag)</span><br><span class="line">              &#125;</span><br><span class="line">              size = diskStore.getSize(blockId)</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              iteratorFromFailedMemoryStorePut = <span class="type">Some</span>(iter)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123; <span class="comment">// !level.deserialized</span></span><br><span class="line">        memoryStore.putIteratorAsBytes(blockId, iterator(), classTag, level.memoryMode) <span class="keyword">match</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="type">Right</span>(s) =&gt;</span><br><span class="line">            size = s</span><br><span class="line">          <span class="keyword">case</span> <span class="type">Left</span>(partiallySerializedValues) =&gt;</span><br><span class="line">            <span class="comment">// Not enough space to unroll this block; drop to disk if applicable</span></span><br><span class="line">            <span class="keyword">if</span> (level.useDisk) &#123;</span><br><span class="line">              logWarning(<span class="string">s&quot;Persisting block <span class="subst">$blockId</span> to disk instead.&quot;</span>)</span><br><span class="line">              diskStore.put(blockId) &#123; channel =&gt;</span><br><span class="line">                <span class="keyword">val</span> out = <span class="type">Channels</span>.newOutputStream(channel)</span><br><span class="line">                partiallySerializedValues.finishWritingToStream(out)</span><br><span class="line">              &#125;</span><br><span class="line">              size = diskStore.getSize(blockId)</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              iteratorFromFailedMemoryStorePut = <span class="type">Some</span>(partiallySerializedValues.valuesIterator)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    <span class="comment">// 如果没使用内存那么直接将数据放入磁盘中</span></span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (level.useDisk) &#123;</span><br><span class="line">      diskStore.put(blockId) &#123; channel =&gt;</span><br><span class="line">        <span class="keyword">val</span> out = <span class="type">Channels</span>.newOutputStream(channel)</span><br><span class="line">        serializerManager.dataSerializeStream(blockId, out, iterator())(classTag)</span><br><span class="line">      &#125;</span><br><span class="line">      size = diskStore.getSize(blockId)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 如果写入成功这里会返回BlockStatus，包含写入级别、写入内存的数据大小和磁盘的数据大小（在MemoryStore和DiskStore中维护）</span></span><br><span class="line">    <span class="keyword">val</span> putBlockStatus = getCurrentBlockStatus(blockId, info)</span><br><span class="line">    <span class="keyword">val</span> blockWasSuccessfullyStored = putBlockStatus.storageLevel.isValid</span><br><span class="line">    <span class="keyword">if</span> (blockWasSuccessfullyStored) &#123;</span><br><span class="line">      info.size = size</span><br><span class="line">      <span class="comment">// 向BlockManagerMasterEndpoint发送更新Block状态的消息</span></span><br><span class="line">      <span class="keyword">if</span> (tellMaster &amp;&amp; info.tellMaster) &#123;</span><br><span class="line">        reportBlockStatus(blockId, putBlockStatus)</span><br><span class="line">      &#125;</span><br><span class="line">      addUpdatedBlockStatusToTaskMetrics(blockId, putBlockStatus)</span><br><span class="line">      logDebug(<span class="string">s&quot;Put block <span class="subst">$blockId</span> locally took <span class="subst">$&#123;Utils.getUsedTimeNs(startTimeNs)&#125;</span>&quot;</span>)</span><br><span class="line">      <span class="keyword">if</span> (level.replication &gt; <span class="number">1</span>) &#123;</span><br><span class="line">        <span class="keyword">val</span> remoteStartTimeNs = <span class="type">System</span>.nanoTime()</span><br><span class="line">        <span class="keyword">val</span> bytesToReplicate = doGetLocalBytes(blockId, info)</span><br><span class="line">        <span class="keyword">val</span> remoteClassTag = <span class="keyword">if</span> (!serializerManager.canUseKryo(classTag)) &#123;</span><br><span class="line">          scala.reflect.classTag[<span class="type">Any</span>]</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          classTag</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          replicate(blockId, bytesToReplicate, level, remoteClassTag)</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">          bytesToReplicate.dispose()</span><br><span class="line">        &#125;</span><br><span class="line">        logDebug(<span class="string">s&quot;Put block <span class="subst">$blockId</span> remotely took <span class="subst">$&#123;Utils.getUsedTimeNs(remoteStartTimeNs)&#125;</span>&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    assert(blockWasSuccessfullyStored == iteratorFromFailedMemoryStorePut.isEmpty)</span><br><span class="line">    iteratorFromFailedMemoryStorePut</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>BlockStoreUpdater.save写入bytes数据</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">save</span></span>(): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  doPut(blockId, level, classTag, tellMaster, keepReadLock) &#123; info =&gt;</span><br><span class="line">    <span class="keyword">val</span> startTimeNs = <span class="type">System</span>.nanoTime()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> replicationFuture = <span class="keyword">if</span> (level.replication &gt; <span class="number">1</span>) &#123;</span><br><span class="line">      <span class="type">Future</span> &#123;</span><br><span class="line">        <span class="comment">// 将block复制到另一个节点</span></span><br><span class="line">        replicate(blockId, blockData(), level, classTag)</span><br><span class="line">      &#125;(futureExecutionContext)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="literal">null</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (level.useMemory) &#123;</span><br><span class="line">      <span class="comment">// 将block写入内存中</span></span><br><span class="line">      <span class="keyword">val</span> putSucceeded = <span class="keyword">if</span> (level.deserialized) &#123;</span><br><span class="line">        <span class="comment">// 如果是未经序列化的数据那么将它封装为BlockData并获取输入流最终调用MemoryStore.putIterator方法</span></span><br><span class="line">        saveDeserializedValuesToMemoryStore(blockData().toInputStream())</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 如果是序列化的bytes调用MemoryStore.putBytes方法</span></span><br><span class="line">        saveSerializedValuesToMemoryStore(readToByteBuffer())</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="comment">// 如果内存写入失败那么写入到磁盘</span></span><br><span class="line">      <span class="keyword">if</span> (!putSucceeded &amp;&amp; level.useDisk) &#123;</span><br><span class="line">        logWarning(<span class="string">s&quot;Persisting block <span class="subst">$blockId</span> to disk instead.&quot;</span>)</span><br><span class="line">        saveToDiskStore()</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (level.useDisk) &#123;</span><br><span class="line">      <span class="comment">// 写入到磁盘，最终调用DiskStore.putBytes方法</span></span><br><span class="line">      saveToDiskStore()</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 下面的流程就和BlockManager.doPutIterator方法一样了</span></span><br><span class="line">    <span class="keyword">val</span> putBlockStatus = getCurrentBlockStatus(blockId, info)</span><br><span class="line">    <span class="keyword">val</span> blockWasSuccessfullyStored = putBlockStatus.storageLevel.isValid</span><br><span class="line">    <span class="keyword">if</span> (blockWasSuccessfullyStored) &#123;</span><br><span class="line">      info.size = blockSize</span><br><span class="line">      <span class="keyword">if</span> (tellMaster &amp;&amp; info.tellMaster) &#123;</span><br><span class="line">        reportBlockStatus(blockId, putBlockStatus)</span><br><span class="line">      &#125;</span><br><span class="line">      addUpdatedBlockStatusToTaskMetrics(blockId, putBlockStatus)</span><br><span class="line">    &#125;</span><br><span class="line">    logDebug(<span class="string">s&quot;Put block <span class="subst">$&#123;blockId&#125;</span> locally took <span class="subst">$&#123;Utils.getUsedTimeNs(startTimeNs)&#125;</span>&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (level.replication &gt; <span class="number">1</span>) &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="type">ThreadUtils</span>.awaitReady(replicationFuture, <span class="type">Duration</span>.<span class="type">Inf</span>)</span><br><span class="line">      &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">NonFatal</span>(t) =&gt;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">&quot;Error occurred while waiting for replication to finish&quot;</span>, t)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (blockWasSuccessfullyStored) &#123;</span><br><span class="line">      <span class="type">None</span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">Some</span>(blockSize)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;.isEmpty</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="read-block"><a href="#read-block" class="headerlink" title="read block"></a>read block</h3><p>read block同样分为两个方法，BlockManager.getLocalValues和BlockManager.getRemoteBlock，分别是从本地获取block和从远程获取block，当然内部也同样分为直接读取values和读取bytes两种方式</p><p>BlockManager.getLocalValues从本地获取block</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getLocalValues</span></span>(blockId: <span class="type">BlockId</span>): <span class="type">Option</span>[<span class="type">BlockResult</span>] = &#123;</span><br><span class="line">  logDebug(<span class="string">s&quot;Getting local block <span class="subst">$blockId</span>&quot;</span>)</span><br><span class="line">  blockInfoManager.lockForReading(blockId) <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">      logDebug(<span class="string">s&quot;Block <span class="subst">$blockId</span> was not found&quot;</span>)</span><br><span class="line">      <span class="type">None</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(info) =&gt;</span><br><span class="line">      <span class="keyword">val</span> level = info.level</span><br><span class="line">      logDebug(<span class="string">s&quot;Level for block <span class="subst">$blockId</span> is <span class="subst">$level</span>&quot;</span>)</span><br><span class="line">      <span class="keyword">val</span> taskContext = <span class="type">Option</span>(<span class="type">TaskContext</span>.get())</span><br><span class="line">      <span class="keyword">if</span> (level.useMemory &amp;&amp; memoryStore.contains(blockId)) &#123;</span><br><span class="line">        <span class="comment">// 从内存中获取block，如果未经序列化那么直接读取并返回，如果序列化了那么需要调用SerializerManager.dataDeserializeStream进行反序列化</span></span><br><span class="line">        <span class="keyword">val</span> iter: <span class="type">Iterator</span>[<span class="type">Any</span>] = <span class="keyword">if</span> (level.deserialized) &#123;</span><br><span class="line">          memoryStore.getValues(blockId).get</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          serializerManager.dataDeserializeStream(</span><br><span class="line">            blockId, memoryStore.getBytes(blockId).get.toInputStream())(info.classTag)</span><br><span class="line">        &#125;</span><br><span class="line">        </span><br><span class="line">        <span class="comment">// 包装iterator并释放读锁，返回BlockResult</span></span><br><span class="line">        <span class="keyword">val</span> ci = <span class="type">CompletionIterator</span>[<span class="type">Any</span>, <span class="type">Iterator</span>[<span class="type">Any</span>]](iter, &#123;</span><br><span class="line">          releaseLock(blockId, taskContext)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">BlockResult</span>(ci, <span class="type">DataReadMethod</span>.<span class="type">Memory</span>, info.size))</span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (level.useDisk &amp;&amp; diskStore.contains(blockId)) &#123;</span><br><span class="line">        <span class="comment">// 从磁盘读取block数据</span></span><br><span class="line">        <span class="keyword">val</span> diskData = diskStore.getBytes(blockId)</span><br><span class="line">        <span class="keyword">val</span> iterToReturn: <span class="type">Iterator</span>[<span class="type">Any</span>] = &#123;</span><br><span class="line">          <span class="keyword">if</span> (level.deserialized) &#123;</span><br><span class="line">            <span class="keyword">val</span> diskValues = serializerManager.dataDeserializeStream(</span><br><span class="line">              blockId,</span><br><span class="line">              diskData.toInputStream())(info.classTag)</span><br><span class="line">            <span class="comment">// 尝试将数据缓存到内存以加快后续读取速度</span></span><br><span class="line">            maybeCacheDiskValuesInMemory(info, blockId, level, diskValues)</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">val</span> stream = maybeCacheDiskBytesInMemory(info, blockId, level, diskData)</span><br><span class="line">              .map &#123; _.toInputStream(dispose = <span class="literal">false</span>) &#125;</span><br><span class="line">              .getOrElse &#123; diskData.toInputStream() &#125;</span><br><span class="line">            serializerManager.dataDeserializeStream(blockId, stream)(info.classTag)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 包装iterator并释放读锁，返回BlockResult</span></span><br><span class="line">        <span class="keyword">val</span> ci = <span class="type">CompletionIterator</span>[<span class="type">Any</span>, <span class="type">Iterator</span>[<span class="type">Any</span>]](iterToReturn, &#123;</span><br><span class="line">          releaseLockAndDispose(blockId, diskData, taskContext)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">BlockResult</span>(ci, <span class="type">DataReadMethod</span>.<span class="type">Disk</span>, info.size))</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        handleLocalReadFailure(blockId)</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>BlockManager.getRemoteBlock远程获取block</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">getRemoteBlock</span></span>[<span class="type">T</span>](</span><br><span class="line">    blockId: <span class="type">BlockId</span>,</span><br><span class="line">    bufferTransformer: <span class="type">ManagedBuffer</span> =&gt; <span class="type">T</span>): <span class="type">Option</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  logDebug(<span class="string">s&quot;Getting remote block <span class="subst">$blockId</span>&quot;</span>)</span><br><span class="line">  require(blockId != <span class="literal">null</span>, <span class="string">&quot;BlockId is null&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 因为所有的block都在driver注册，所以直接去driver查找block在本地运行的其他作业的location和status</span></span><br><span class="line">  <span class="keyword">val</span> locationsAndStatusOption = master.getLocationsAndStatus(blockId, blockManagerId.host)</span><br><span class="line">  <span class="keyword">if</span> (locationsAndStatusOption.isEmpty) &#123;</span><br><span class="line">    logDebug(<span class="string">s&quot;Block <span class="subst">$blockId</span> is unknown by block manager master&quot;</span>)</span><br><span class="line">    <span class="type">None</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> locationsAndStatus = locationsAndStatusOption.get</span><br><span class="line">    <span class="keyword">val</span> blockSize = locationsAndStatus.status.diskSize.max(locationsAndStatus.status.memSize)</span><br><span class="line"></span><br><span class="line">    locationsAndStatus.localDirs.flatMap &#123; localDirs =&gt;</span><br><span class="line">      <span class="comment">// 从在同一主机上运行的其他作业的本地目录中读取block数据，随后转换为ManagedBuffer</span></span><br><span class="line">      <span class="keyword">val</span> blockDataOption =</span><br><span class="line">        readDiskBlockFromSameHostExecutor(blockId, localDirs, locationsAndStatus.status.diskSize)</span><br><span class="line">      <span class="keyword">val</span> res = blockDataOption.flatMap &#123; blockData =&gt;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="type">Some</span>(bufferTransformer(blockData))</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">            logDebug(<span class="string">&quot;Block from the same host executor cannot be opened: &quot;</span>, e)</span><br><span class="line">            <span class="type">None</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      logInfo(<span class="string">s&quot;Read <span class="subst">$blockId</span> from the disk of a same host executor is &quot;</span> +</span><br><span class="line">        (<span class="keyword">if</span> (res.isDefined) <span class="string">&quot;successful.&quot;</span> <span class="keyword">else</span> <span class="string">&quot;failed.&quot;</span>))</span><br><span class="line">      res</span><br><span class="line">    &#125;.orElse &#123;</span><br><span class="line">      <span class="comment">// 如果为空就去driver或者其他executor查找，它从locationsAndStatus拿到block所在的host、port以及executorId获取对应的blockId数据</span></span><br><span class="line">      fetchRemoteManagedBuffer(blockId, blockSize, locationsAndStatus).map(bufferTransformer)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="remove-block"><a href="#remove-block" class="headerlink" title="remove block"></a>remove block</h3><p>remove block没什么好说的，分别从MemoryStore和DiskStore移除对应blockId的block数据</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">removeBlockInternal</span></span>(blockId: <span class="type">BlockId</span>, tellMaster: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> blockStatus = <span class="keyword">if</span> (tellMaster) &#123;</span><br><span class="line">    <span class="keyword">val</span> blockInfo = blockInfoManager.assertBlockIsLockedForWriting(blockId)</span><br><span class="line">    <span class="type">Some</span>(getCurrentBlockStatus(blockId, blockInfo))</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="type">None</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 分别从memory和disk移除block</span></span><br><span class="line">  <span class="keyword">val</span> removedFromMemory = memoryStore.remove(blockId)</span><br><span class="line">  <span class="keyword">val</span> removedFromDisk = diskStore.remove(blockId)</span><br><span class="line">  <span class="keyword">if</span> (!removedFromMemory &amp;&amp; !removedFromDisk) &#123;</span><br><span class="line">    logWarning(<span class="string">s&quot;Block <span class="subst">$blockId</span> could not be removed as it was not found on disk or in memory&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// blockInfoManager移除掉blockId和BlockInfo的映射关系</span></span><br><span class="line">  blockInfoManager.removeBlock(blockId)</span><br><span class="line">  <span class="keyword">if</span> (tellMaster) &#123;</span><br><span class="line">    reportBlockStatus(blockId, blockStatus.get.copy(storageLevel = <span class="type">StorageLevel</span>.<span class="type">NONE</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="其他组件"><a href="#其他组件" class="headerlink" title="其他组件"></a>其他组件</h3><ul><li>BlockInfoManager：管理当前BlockManager上面所有的BlockInfo，它维护了一个<code>infos: mutable.HashMap[BlockId, BlockInfo]</code>存储现有的block映射，<code>writeLocksByTask: mutable.HashMap[TaskAttemptId, mutable.Set[BlockId]]</code>用来维护每个task对block加的写锁，<code>readLocksByTask: mutable.HashMap[TaskAttemptId, ConcurrentHashMultiset[BlockId]]</code>维护每个task对block加的读锁，每个block可以有多个读锁。同时也提供这些block的读写锁的释放</li><li>BlockInfo：跟踪保存每个Block的元数据</li><li>BlockId：标识不同block的接口，它的实现有RDDBlockId、ShuffleBlockId、ShuffleDataBlockId等等</li><li>BlockData：抽象出block的读取方式，并提供读取底层数据的不同方法</li><li>DiskBlockManager：创建并维护每个block到文件地址的映射，一个block映射一个文件。主要的方法就是getFile，根据filename确定文件的路径并返回，还有创建SPARK_LOCAL_DIRS文件夹清理文件夹的功能</li><li>DiskStore：结合DiskBlockManager提供外部读取和写入block的接口，具体方式就是DiskBlockManager通过blockId计算其hash值的方式确定每个block存储的file</li><li>MemoryManager：管理计算的内存和存储的内存，唯一的实现类是UnifiedMemoryManager，它要解决的问题是需要提供多大的内存用于Store存储</li><li>MemoryStore：数据以MemoryEntry的方形式存在<code>private val entries = new LinkedHashMap[BlockId, MemoryEntry[_]]</code>中，MemoryStore要做的就是对外提供接口对entries进行管理。有关Spark内存的管理查看<a href="">Spark-Memory</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark源码-broadcast流程原理</title>
      <link href="/2022/10/24/Software/Spark%E6%BA%90%E7%A0%81-broadcast%E6%B5%81%E7%A8%8B%E5%8E%9F%E7%90%86/"/>
      <url>/2022/10/24/Software/Spark%E6%BA%90%E7%A0%81-broadcast%E6%B5%81%E7%A8%8B%E5%8E%9F%E7%90%86/</url>
      
        <content type="html"><![CDATA[<p>Spark broadcast流程原理，从创建和销毁看广播变量的管理</p><span id="more"></span><p>参考<a href="https://blog.51cto.com/u_15067227/2573521">Spark广播变量原理分析</a>，广播变量主要依赖BlockManager，相关介绍放在<a href="https://wangxukun.top/2022/11/02/Software/Spark%E6%BA%90%E7%A0%81-BlockManager/">BlockManager</a></p><h2 id="写入BroadCast到BlockManager"><a href="#写入BroadCast到BlockManager" class="headerlink" title="写入BroadCast到BlockManager"></a>写入BroadCast到BlockManager</h2><p>将需要广播的变量写入到Driver的BlockManager中</p><div align=center><img src="broadcast-write.png"></div><h3 id="SparkContext-broadcast"><a href="#SparkContext-broadcast" class="headerlink" title="SparkContext.broadcast"></a>SparkContext.broadcast</h3><p>SparkContext创建一个广播变量的入口，返回一个广播变量</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">broadcast</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](value: <span class="type">T</span>): <span class="type">Broadcast</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  assertNotStopped()  <span class="comment">// 确保Spark Context未停止</span></span><br><span class="line">  <span class="comment">// 如果尝试广播RDD则抛出一个异常</span></span><br><span class="line">  require(!classOf[<span class="type">RDD</span>[_]].isAssignableFrom(classTag[<span class="type">T</span>].runtimeClass),</span><br><span class="line">    <span class="string">&quot;Can not directly broadcast RDDs; instead, call collect() and broadcast the result.&quot;</span>)</span><br><span class="line">  <span class="comment">// 创建广播变量并返回</span></span><br><span class="line">  <span class="keyword">val</span> bc = env.broadcastManager.newBroadcast[<span class="type">T</span>](value, isLocal)</span><br><span class="line">  <span class="keyword">val</span> callSite = getCallSite</span><br><span class="line">  logInfo(<span class="string">&quot;Created broadcast &quot;</span> + bc.id + <span class="string">&quot; from &quot;</span> + callSite.shortForm)</span><br><span class="line">  cleaner.foreach(_.registerBroadcastForCleanup(bc))</span><br><span class="line">  bc</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="BroadcastManager-newBroadcast"><a href="#BroadcastManager-newBroadcast" class="headerlink" title="BroadcastManager.newBroadcast"></a>BroadcastManager.newBroadcast</h3><p>新建并返回Broadcast，broadcastFactory是BroadcastFactory的唯一实现类TorrentBroadcastFactory</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">newBroadcast</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](value_ : <span class="type">T</span>, isLocal: <span class="type">Boolean</span>): <span class="type">Broadcast</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> bid = nextBroadcastId.getAndIncrement()</span><br><span class="line">  value_ <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> pb: <span class="type">PythonBroadcast</span> =&gt; pb.setBroadcastId(bid)</span><br><span class="line">    <span class="keyword">case</span> _ =&gt; <span class="comment">// do nothing</span></span><br><span class="line">  &#125;</span><br><span class="line">  broadcastFactory.newBroadcast[<span class="type">T</span>](value_, isLocal, bid)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="TorrentBroadcastFactory-newBroadcast"><a href="#TorrentBroadcastFactory-newBroadcast" class="headerlink" title="TorrentBroadcastFactory.newBroadcast"></a>TorrentBroadcastFactory.newBroadcast</h3><p>最终返回一个TorrentBroadcast对象，创建对象的同时已经将需要广播的数据写入到BlockManager中了</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">newBroadcast</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](value_ : <span class="type">T</span>, isLocal: <span class="type">Boolean</span>, id: <span class="type">Long</span>): <span class="type">Broadcast</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">TorrentBroadcast</span>[<span class="type">T</span>](value_, id)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="TorrentBroadcast-writeBlocks"><a href="#TorrentBroadcast-writeBlocks" class="headerlink" title="TorrentBroadcast.writeBlocks"></a>TorrentBroadcast.writeBlocks</h3><p>将整个广播变量在作为副本写入driver，将数据分为多个block写入到本地的BlockManager中</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 此广播变量包含的块总数</span></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> numBlocks: <span class="type">Int</span> = writeBlocks(obj)</span><br><span class="line"><span class="comment">// 将对象分为多个block并写入BlockManager中</span></span><br><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">writeBlocks</span></span>(value: <span class="type">T</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="keyword">import</span> <span class="type">StorageLevel</span>._</span><br><span class="line">  <span class="comment">// 将广播变量在driver存储一份副本</span></span><br><span class="line">  <span class="keyword">val</span> blockManager = <span class="type">SparkEnv</span>.get.blockManager</span><br><span class="line">  <span class="keyword">if</span> (!blockManager.putSingle(broadcastId, value, <span class="type">MEMORY_AND_DISK</span>, tellMaster = <span class="literal">false</span>)) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s&quot;Failed to store <span class="subst">$broadcastId</span> in BlockManager&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 将数据分割为指定大小的block，然后对每个block进行序列化，并进行压缩</span></span><br><span class="line">    <span class="keyword">val</span> blocks =</span><br><span class="line">      <span class="type">TorrentBroadcast</span>.blockifyObject(value, blockSize, <span class="type">SparkEnv</span>.get.serializer, compressionCodec)</span><br><span class="line">    <span class="keyword">if</span> (checksumEnabled) &#123;</span><br><span class="line">      checksums = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](blocks.length)</span><br><span class="line">    &#125;</span><br><span class="line">    blocks.zipWithIndex.foreach &#123; <span class="keyword">case</span> (block, i) =&gt;</span><br><span class="line">      <span class="keyword">if</span> (checksumEnabled) &#123;</span><br><span class="line">        checksums(i) = calcChecksum(block)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">val</span> pieceId = <span class="type">BroadcastBlockId</span>(id, <span class="string">&quot;piece&quot;</span> + i)</span><br><span class="line">      <span class="keyword">val</span> bytes = <span class="keyword">new</span> <span class="type">ChunkedByteBuffer</span>(block.duplicate())</span><br><span class="line">      <span class="comment">// 将每个block写入BlockManager</span></span><br><span class="line">      <span class="keyword">if</span> (!blockManager.putBytes(pieceId, bytes, <span class="type">MEMORY_AND_DISK_SER</span>, tellMaster = <span class="literal">true</span>)) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s&quot;Failed to store <span class="subst">$pieceId</span> of <span class="subst">$broadcastId</span> &quot;</span> +</span><br><span class="line">          <span class="string">s&quot;in local BlockManager&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    blocks.length</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt;</span><br><span class="line">      logError(<span class="string">s&quot;Store broadcast <span class="subst">$broadcastId</span> fail, remove all pieces of the broadcast&quot;</span>)</span><br><span class="line">      blockManager.removeBroadcast(id, tellMaster = <span class="literal">true</span>)</span><br><span class="line">      <span class="keyword">throw</span> t</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="读取Broadcast的数据"><a href="#读取Broadcast的数据" class="headerlink" title="读取Broadcast的数据"></a>读取Broadcast的数据</h2><p>获取广播变量数据就是从BlockManager获取之前存储的多个block，但是最开始只在driver端存储了数据，所以就需要远程获取，当然为了减少driver的请求压力，每个executor获取到block后就会在自己那存一份，当其他executor要获取同样的数据时会优先从同机架的executor获取</p><div align=center><img src="broadcast-read.png"></div><h3 id="Broadcast-value"><a href="#Broadcast-value" class="headerlink" title="Broadcast.value"></a>Broadcast.value</h3><p>Broadcast.value是获取广播变量的入口方法，它调用自身的getValue方法，此时需要去看Broadcast的实现类TorrentBroadcast中的getValue方法</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">getValue</span></span>() = synchronized &#123;</span><br><span class="line">  <span class="keyword">val</span> memoized: <span class="type">T</span> = <span class="keyword">if</span> (_value == <span class="literal">null</span>) <span class="literal">null</span>.asInstanceOf[<span class="type">T</span>] <span class="keyword">else</span> _value.get</span><br><span class="line">  <span class="keyword">if</span> (memoized != <span class="literal">null</span>) &#123;</span><br><span class="line">    memoized</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 重点看这个方法</span></span><br><span class="line">    <span class="keyword">val</span> newlyRead = readBroadcastBlock()</span><br><span class="line">    _value = <span class="keyword">new</span> <span class="type">SoftReference</span>[<span class="type">T</span>](newlyRead)</span><br><span class="line">    newlyRead</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="TorrentBroadcast-readBroadcastBlock"><a href="#TorrentBroadcast-readBroadcastBlock" class="headerlink" title="TorrentBroadcast.readBroadcastBlock"></a>TorrentBroadcast.readBroadcastBlock</h3><p>读取广播变量的block，先从本地BlockManager获取，找到后缓存并返回，找不到就去其他BlockManager找，找到了就保存到本地BlockManager并返回</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">readBroadcastBlock</span></span>(): <span class="type">T</span> = <span class="type">Utils</span>.tryOrIOException &#123;</span><br><span class="line">  <span class="type">TorrentBroadcast</span>.torrentBroadcastLock.withLock(broadcastId) &#123;</span><br><span class="line">    <span class="comment">// 每个BlockManager都有一个缓存map来存储常用的block数据</span></span><br><span class="line">    <span class="keyword">val</span> broadcastCache = <span class="type">SparkEnv</span>.get.broadcastManager.cachedValues</span><br><span class="line"></span><br><span class="line">    <span class="type">Option</span>(broadcastCache.get(broadcastId)).map(_.asInstanceOf[<span class="type">T</span>]).getOrElse &#123;</span><br><span class="line">      setConf(<span class="type">SparkEnv</span>.get.conf)</span><br><span class="line">      <span class="keyword">val</span> blockManager = <span class="type">SparkEnv</span>.get.blockManager</span><br><span class="line">      <span class="comment">// 获取本地的BlockManager，然后获取指定id的广播变量数据</span></span><br><span class="line">      blockManager.getLocalValues(broadcastId) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="comment">// 如果在本地找到了那么把它加入到缓存map中并返回</span></span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(blockResult) =&gt;</span><br><span class="line">          <span class="keyword">if</span> (blockResult.data.hasNext) &#123;</span><br><span class="line">            <span class="keyword">val</span> x = blockResult.data.next().asInstanceOf[<span class="type">T</span>]</span><br><span class="line">            releaseBlockManagerLock(broadcastId)</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (x != <span class="literal">null</span>) &#123;</span><br><span class="line">              broadcastCache.put(broadcastId, x)</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            x</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s&quot;Failed to get locally stored broadcast data: <span class="subst">$broadcastId</span>&quot;</span>)</span><br><span class="line">          &#125;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">          <span class="keyword">val</span> estimatedTotalSize = <span class="type">Utils</span>.bytesToString(numBlocks * blockSize)</span><br><span class="line">          logInfo(<span class="string">s&quot;Started reading broadcast variable <span class="subst">$id</span> with <span class="subst">$numBlocks</span> pieces &quot;</span> +</span><br><span class="line">            <span class="string">s&quot;(estimated total size <span class="subst">$estimatedTotalSize</span>)&quot;</span>)</span><br><span class="line">          <span class="keyword">val</span> startTimeNs = <span class="type">System</span>.nanoTime()</span><br><span class="line">          <span class="comment">// 如果没找到就会走到这个方法，去其他BlockManager去找对应的block，如果找到了则把数据读取过来</span></span><br><span class="line">          <span class="keyword">val</span> blocks = readBlocks()</span><br><span class="line">          logInfo(<span class="string">s&quot;Reading broadcast variable <span class="subst">$id</span> took <span class="subst">$&#123;Utils.getUsedTimeNs(startTimeNs)&#125;</span>&quot;</span>)</span><br><span class="line"></span><br><span class="line">          <span class="keyword">try</span> &#123;</span><br><span class="line">            <span class="comment">// 如有需要解压数据并将block数据转为最终的object数据</span></span><br><span class="line">            <span class="keyword">val</span> obj = <span class="type">TorrentBroadcast</span>.unBlockifyObject[<span class="type">T</span>](</span><br><span class="line">              blocks.map(_.toInputStream()), <span class="type">SparkEnv</span>.get.serializer, compressionCodec)</span><br><span class="line">            <span class="keyword">val</span> storageLevel = <span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK</span></span><br><span class="line">            <span class="comment">// 将数据缓存到当前的BlockManager</span></span><br><span class="line">            <span class="keyword">if</span> (!blockManager.putSingle(broadcastId, obj, storageLevel, tellMaster = <span class="literal">false</span>)) &#123;</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s&quot;Failed to store <span class="subst">$broadcastId</span> in BlockManager&quot;</span>)</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (obj != <span class="literal">null</span>) &#123;</span><br><span class="line">              broadcastCache.put(broadcastId, obj)</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            obj</span><br><span class="line">          &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            blocks.foreach(_.dispose())</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="BlockManager-getLocalValues"><a href="#BlockManager-getLocalValues" class="headerlink" title="BlockManager.getLocalValues"></a>BlockManager.getLocalValues</h4><p>从本地BlockManager获取指定block的数据</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getLocalValues</span></span>(blockId: <span class="type">BlockId</span>): <span class="type">Option</span>[<span class="type">BlockResult</span>] = &#123;</span><br><span class="line">  logDebug(<span class="string">s&quot;Getting local block <span class="subst">$blockId</span>&quot;</span>)</span><br><span class="line">  <span class="comment">// 从本地的BlockInfoManager查找对应BlockId的BlockInfo，lockForReading方法会在当前block未被加写入锁时获取对应的Block Info，找不到返回None</span></span><br><span class="line">  blockInfoManager.lockForReading(blockId) <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">      logDebug(<span class="string">s&quot;Block <span class="subst">$blockId</span> was not found&quot;</span>)</span><br><span class="line">      <span class="type">None</span></span><br><span class="line">    <span class="keyword">case</span> <span class="type">Some</span>(info) =&gt;</span><br><span class="line">      <span class="keyword">val</span> level = info.level</span><br><span class="line">      logDebug(<span class="string">s&quot;Level for block <span class="subst">$blockId</span> is <span class="subst">$level</span>&quot;</span>)</span><br><span class="line">      <span class="keyword">val</span> taskContext = <span class="type">Option</span>(<span class="type">TaskContext</span>.get())</span><br><span class="line">      <span class="comment">// 如果存储等级用到了内存那么先在内存中找，找到后封装并返回</span></span><br><span class="line">      <span class="keyword">if</span> (level.useMemory &amp;&amp; memoryStore.contains(blockId)) &#123;</span><br><span class="line">        <span class="keyword">val</span> iter: <span class="type">Iterator</span>[<span class="type">Any</span>] = <span class="keyword">if</span> (level.deserialized) &#123;</span><br><span class="line">          memoryStore.getValues(blockId).get</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          serializerManager.dataDeserializeStream(</span><br><span class="line">            blockId, memoryStore.getBytes(blockId).get.toInputStream())(info.classTag)</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> ci = <span class="type">CompletionIterator</span>[<span class="type">Any</span>, <span class="type">Iterator</span>[<span class="type">Any</span>]](iter, &#123;</span><br><span class="line">          releaseLock(blockId, taskContext)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">BlockResult</span>(ci, <span class="type">DataReadMethod</span>.<span class="type">Memory</span>, info.size))</span><br><span class="line">      <span class="comment">// 内存中没有就去磁盘找</span></span><br><span class="line">      &#125; <span class="keyword">else</span> <span class="keyword">if</span> (level.useDisk &amp;&amp; diskStore.contains(blockId)) &#123;</span><br><span class="line">        <span class="keyword">val</span> diskData = diskStore.getBytes(blockId)</span><br><span class="line">        <span class="keyword">val</span> iterToReturn: <span class="type">Iterator</span>[<span class="type">Any</span>] = &#123;</span><br><span class="line">          <span class="keyword">if</span> (level.deserialized) &#123;</span><br><span class="line">            <span class="keyword">val</span> diskValues = serializerManager.dataDeserializeStream(</span><br><span class="line">              blockId,</span><br><span class="line">              diskData.toInputStream())(info.classTag)</span><br><span class="line">            <span class="comment">// 如果存储等级包含内存那么尝试将数据缓存到内存中加速后续读取</span></span><br><span class="line">            maybeCacheDiskValuesInMemory(info, blockId, level, diskValues)</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">val</span> stream = maybeCacheDiskBytesInMemory(info, blockId, level, diskData)</span><br><span class="line">              .map &#123; _.toInputStream(dispose = <span class="literal">false</span>) &#125;</span><br><span class="line">              .getOrElse &#123; diskData.toInputStream() &#125;</span><br><span class="line">            serializerManager.dataDeserializeStream(blockId, stream)(info.classTag)</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">val</span> ci = <span class="type">CompletionIterator</span>[<span class="type">Any</span>, <span class="type">Iterator</span>[<span class="type">Any</span>]](iterToReturn, &#123;</span><br><span class="line">          releaseLockAndDispose(blockId, diskData, taskContext)</span><br><span class="line">        &#125;)</span><br><span class="line">        <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">BlockResult</span>(ci, <span class="type">DataReadMethod</span>.<span class="type">Disk</span>, info.size))</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        handleLocalReadFailure(blockId)  <span class="comment">// 没找到则抛出异常</span></span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="TorrentBroadcast-readBlocks"><a href="#TorrentBroadcast-readBlocks" class="headerlink" title="TorrentBroadcast.readBlocks"></a>TorrentBroadcast.readBlocks</h3><p>从driver或者executor获取block</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">readBlocks</span></span>(): <span class="type">Array</span>[<span class="type">BlockData</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> blocks = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">BlockData</span>](numBlocks)</span><br><span class="line">  <span class="keyword">val</span> bm = <span class="type">SparkEnv</span>.get.blockManager</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (pid &lt;- <span class="type">Random</span>.shuffle(<span class="type">Seq</span>.range(<span class="number">0</span>, numBlocks))) &#123;</span><br><span class="line">    <span class="keyword">val</span> pieceId = <span class="type">BroadcastBlockId</span>(id, <span class="string">&quot;piece&quot;</span> + pid)</span><br><span class="line">    logDebug(<span class="string">s&quot;Reading piece <span class="subst">$pieceId</span> of <span class="subst">$broadcastId</span>&quot;</span>)</span><br><span class="line">    <span class="comment">// 依然先从本地找，找到则返回</span></span><br><span class="line">    bm.getLocalBytes(pieceId) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(block) =&gt;</span><br><span class="line">        blocks(pid) = block</span><br><span class="line">        releaseBlockManagerLock(pieceId)</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        <span class="comment">// 找不到则远程获取，可以是driver，也可以是executor</span></span><br><span class="line">        bm.getRemoteBytes(pieceId) <span class="keyword">match</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="type">Some</span>(b) =&gt;</span><br><span class="line">            <span class="keyword">if</span> (checksumEnabled) &#123;</span><br><span class="line">              <span class="keyword">val</span> sum = calcChecksum(b.chunks(<span class="number">0</span>))</span><br><span class="line">              <span class="keyword">if</span> (sum != checksums(pid)) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s&quot;corrupt remote block <span class="subst">$pieceId</span> of <span class="subst">$broadcastId</span>:&quot;</span> +</span><br><span class="line">                  <span class="string">s&quot; <span class="subst">$sum</span> != <span class="subst">$&#123;checksums(pid)&#125;</span>&quot;</span>)</span><br><span class="line">              &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 如果远程找到了对应的block则将它放到本地的BlockManager</span></span><br><span class="line">            <span class="keyword">if</span> (!bm.putBytes(pieceId, b, <span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>, tellMaster = <span class="literal">true</span>)) &#123;</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(</span><br><span class="line">                <span class="string">s&quot;Failed to store <span class="subst">$pieceId</span> of <span class="subst">$broadcastId</span> in local BlockManager&quot;</span>)</span><br><span class="line">            &#125;</span><br><span class="line">            blocks(pid) = <span class="keyword">new</span> <span class="type">ByteBufferBlockData</span>(b, <span class="literal">true</span>)</span><br><span class="line">          <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s&quot;Failed to get <span class="subst">$pieceId</span> of <span class="subst">$broadcastId</span>&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  blocks</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="BlockManager-getRemoteBytes-gt-getRemoteBlock"><a href="#BlockManager-getRemoteBytes-gt-getRemoteBlock" class="headerlink" title="BlockManager.getRemoteBytes-&gt;getRemoteBlock"></a>BlockManager.getRemoteBytes-&gt;getRemoteBlock</h3><p>先从当前executor上运行的其他作业中找，找到则尝试读取并转换为最终的格式返回，没找到或者读取失败就从driver或者其他executor查找</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">getRemoteBlock</span></span>[<span class="type">T</span>](</span><br><span class="line">    blockId: <span class="type">BlockId</span>,</span><br><span class="line">    bufferTransformer: <span class="type">ManagedBuffer</span> =&gt; <span class="type">T</span>): <span class="type">Option</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">  logDebug(<span class="string">s&quot;Getting remote block <span class="subst">$blockId</span>&quot;</span>)</span><br><span class="line">  require(blockId != <span class="literal">null</span>, <span class="string">&quot;BlockId is null&quot;</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 因为所有的block都在driver注册，所以直接去driver查找block在本地运行的其他作业的location和status</span></span><br><span class="line">  <span class="keyword">val</span> locationsAndStatusOption = master.getLocationsAndStatus(blockId, blockManagerId.host)</span><br><span class="line">  <span class="keyword">if</span> (locationsAndStatusOption.isEmpty) &#123;</span><br><span class="line">    logDebug(<span class="string">s&quot;Block <span class="subst">$blockId</span> is unknown by block manager master&quot;</span>)</span><br><span class="line">    <span class="type">None</span></span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> locationsAndStatus = locationsAndStatusOption.get</span><br><span class="line">    <span class="keyword">val</span> blockSize = locationsAndStatus.status.diskSize.max(locationsAndStatus.status.memSize)</span><br><span class="line"></span><br><span class="line">    locationsAndStatus.localDirs.flatMap &#123; localDirs =&gt;</span><br><span class="line">      <span class="comment">// 从在同一主机上运行的其他作业的本地目录中读取block数据，随后转换为ManagedBuffer</span></span><br><span class="line">      <span class="keyword">val</span> blockDataOption =</span><br><span class="line">        readDiskBlockFromSameHostExecutor(blockId, localDirs, locationsAndStatus.status.diskSize)</span><br><span class="line">      <span class="keyword">val</span> res = blockDataOption.flatMap &#123; blockData =&gt;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="type">Some</span>(bufferTransformer(blockData))</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">            logDebug(<span class="string">&quot;Block from the same host executor cannot be opened: &quot;</span>, e)</span><br><span class="line">            <span class="type">None</span></span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">      logInfo(<span class="string">s&quot;Read <span class="subst">$blockId</span> from the disk of a same host executor is &quot;</span> +</span><br><span class="line">        (<span class="keyword">if</span> (res.isDefined) <span class="string">&quot;successful.&quot;</span> <span class="keyword">else</span> <span class="string">&quot;failed.&quot;</span>))</span><br><span class="line">      res</span><br><span class="line">    &#125;.orElse &#123;</span><br><span class="line">      <span class="comment">// 如果为空就去driver或者其他executor查找，它从locationsAndStatus拿到block所在的host、port以及executorId获取对应的blockId数据，后续涉及到netty就不再深入了</span></span><br><span class="line">      fetchRemoteManagedBuffer(blockId, blockSize, locationsAndStatus).map(bufferTransformer)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="BlockManagerMasterEndpoint-getLocationsAndStatus"><a href="#BlockManagerMasterEndpoint-getLocationsAndStatus" class="headerlink" title="BlockManagerMasterEndpoint.getLocationsAndStatus"></a>BlockManagerMasterEndpoint.getLocationsAndStatus</h3><p>getLocationsAndStatus方法从driver获取block在本地运行的其他作业的location和status，它向DriverEndpoint发送了一个GetLocationsAndStatus消息，返回BlockLocationsAndStatus。最终调用BlockManagerMasterEndpoint的getLocationsAndStatus方法</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getLocationsAndStatus</span></span>(</span><br><span class="line">    blockId: <span class="type">BlockId</span>,</span><br><span class="line">    requesterHost: <span class="type">String</span>): <span class="type">Option</span>[<span class="type">BlockLocationsAndStatus</span>] = &#123;</span><br><span class="line">  <span class="comment">// blockLocations是一个map，维护所有的blockId对应它的BalckManagerId集合</span></span><br><span class="line">  <span class="keyword">val</span> locations = <span class="type">Option</span>(blockLocations.get(blockId)).map(_.toSeq).getOrElse(<span class="type">Seq</span>.empty)</span><br><span class="line">  <span class="comment">// 如果存在那么获取对应的BlockStatus</span></span><br><span class="line">  <span class="keyword">val</span> status = locations.headOption.flatMap &#123; bmId =&gt;</span><br><span class="line">    <span class="keyword">if</span> (externalShuffleServiceRddFetchEnabled &amp;&amp; bmId.port == externalShuffleServicePort) &#123;</span><br><span class="line">      <span class="type">Option</span>(blockStatusByShuffleService(bmId).get(blockId))</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      blockManagerInfo.get(bmId).flatMap(_.getStatus(blockId))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (locations.nonEmpty &amp;&amp; status.isDefined) &#123;</span><br><span class="line">    <span class="keyword">val</span> localDirs = locations.find &#123; loc =&gt;</span><br><span class="line">      <span class="comment">// 找到blockId对应的locations集合并返回</span></span><br><span class="line">      loc.host == requesterHost &amp;&amp;</span><br><span class="line">        (loc.port == externalShuffleServicePort ||</span><br><span class="line">          blockManagerInfo</span><br><span class="line">            .get(loc)</span><br><span class="line">            .flatMap(_.getStatus(blockId).map(_.storageLevel.useDisk))</span><br><span class="line">            .getOrElse(<span class="literal">false</span>))</span><br><span class="line">    &#125;.flatMap &#123; bmId =&gt; <span class="type">Option</span>(executorIdToLocalDirs.getIfPresent(bmId.executorId)) &#125;</span><br><span class="line">    <span class="type">Some</span>(<span class="type">BlockLocationsAndStatus</span>(locations, status.get, localDirs))</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="type">None</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="删除Broadcast数据"><a href="#删除Broadcast数据" class="headerlink" title="删除Broadcast数据"></a>删除Broadcast数据</h2><p>unpersist流程：BlockManagerMaster发送RemoveBroadcast消息-&gt;DriverEndpoint发送RemoveBroadcast消息-&gt;BlockManagerMasterEndpoint发送RemoveBroadcast消息-&gt;BlockManagerStorageEndpoint-&gt;每个executor对应的BlockManager.removeBroadcast</p><p>destroy流程和unpersist类似，只是removeFromDriver参数为true，表示在最后一步removeBroadcast时会将driver的相关block也删掉，此时广播变量就无法使用了，而unpersist仅仅是删除executor上的block数据，仍可以继续使用。</p>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark源码-collect流程</title>
      <link href="/2022/10/18/Software/Spark%E6%BA%90%E7%A0%81-collect%E6%B5%81%E7%A8%8B/"/>
      <url>/2022/10/18/Software/Spark%E6%BA%90%E7%A0%81-collect%E6%B5%81%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p>Spark collect操作流程刨析，从代码中理解Spark执行流程</p><span id="more"></span><p>没有通过debug而是以collect为入口向下点，中间有几次RPC调用，了解基础的Netty即可找到它实际的调用方法。Spark版本为3.1.2</p><div align=center><img src="collect.png"></div><h2 id="SparkContext-collect"><a href="#SparkContext-collect" class="headerlink" title="SparkContext.collect"></a>SparkContext.collect</h2><p>点进collect方法，它调用了sc的重载方法runJob，需要传入两个参数分别是当前rdd和一个处理函数，这个处理函数将运行在每个partition对数据进行处理</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">collect</span></span>(): <span class="type">Array</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> results = sc.runJob(<span class="keyword">this</span>, (iter: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; iter.toArray)</span><br><span class="line">  <span class="type">Array</span>.concat(results: _*)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="SparkContext-runJob-T-U-ClassTag-rdd-RDD-T-func-Iterator-T-x3D-gt-U"><a href="#SparkContext-runJob-T-U-ClassTag-rdd-RDD-T-func-Iterator-T-x3D-gt-U" class="headerlink" title="SparkContext.runJob[T, U: ClassTag](rdd: RDD[T], func: Iterator[T] &#x3D;&gt; U)"></a>SparkContext.runJob[T, U: ClassTag](rdd: RDD[T], func: Iterator[T] &#x3D;&gt; U)</h2><p>点击runJob函数它继续调用自身的重载方法，第三个参数partitions表示处理函数作用在所有的partition，并不是所有的函数都会作用在所有的partition，例如first</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](rdd: <span class="type">RDD</span>[<span class="type">T</span>], func: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">U</span>): <span class="type">Array</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">  runJob(rdd, func, <span class="number">0</span> until rdd.partitions.length)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="SparkContext-runJob-T-U-ClassTag-rdd-RDD-T-func-Iterator-T-x3D-gt-U-partitions-Seq-Int"><a href="#SparkContext-runJob-T-U-ClassTag-rdd-RDD-T-func-Iterator-T-x3D-gt-U-partitions-Seq-Int" class="headerlink" title="SparkContext.runJob[T, U: ClassTag](rdd: RDD[T],func: Iterator[T] &#x3D;&gt; U,partitions: Seq[Int])"></a>SparkContext.runJob[T, U: ClassTag](rdd: RDD[T],func: Iterator[T] &#x3D;&gt; U,partitions: Seq[Int])</h2><p>继续点runJob会调用clean函数对处理函数进行闭包清理，有关闭包清理查看另一篇文章 <a href="">ClosureCleaner</a></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">    func: <span class="type">Iterator</span>[<span class="type">T</span>] =&gt; <span class="type">U</span>,</span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>]): <span class="type">Array</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanedFunc = clean(func)</span><br><span class="line">  runJob(rdd, (ctx: <span class="type">TaskContext</span>, it: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; cleanedFunc(it), partitions)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="SparkContext-runJob-T-U-ClassTag-rdd-RDD-T-func-TaskContext-Iterator-T-x3D-gt-U-partitions-Seq-Int-resultHandler-Int-U-x3D-gt-Unit"><a href="#SparkContext-runJob-T-U-ClassTag-rdd-RDD-T-func-TaskContext-Iterator-T-x3D-gt-U-partitions-Seq-Int-resultHandler-Int-U-x3D-gt-Unit" class="headerlink" title="SparkContext.runJob[T, U: ClassTag](rdd: RDD[T],func: (TaskContext, Iterator[T]) &#x3D;&gt; U,partitions: Seq[Int],resultHandler: (Int, U) &#x3D;&gt; Unit)"></a>SparkContext.runJob[T, U: ClassTag](rdd: RDD[T],func: (TaskContext, Iterator[T]) &#x3D;&gt; U,partitions: Seq[Int],resultHandler: (Int, U) &#x3D;&gt; Unit)</h2><p>继续点两次runJob会到一个比较重要的函数，它几乎是所有action的入口函数</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>: <span class="type">ClassTag</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">    <span class="comment">// resultHandler是一个回调函数，用来生成每一个结果</span></span><br><span class="line">    resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (stopped.get()) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalStateException</span>(<span class="string">&quot;SparkContext has been shutdown&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 标识调用当前方法的代码位置，例如在web界面看到的stage标识，分为短标识和长标识，短标识代表你写的代码中的具体位置和方法，长标识代表代码的调用路径</span></span><br><span class="line">  <span class="keyword">val</span> callSite = getCallSite</span><br><span class="line">  <span class="keyword">val</span> cleanedFunc = clean(func)</span><br><span class="line">  logInfo(<span class="string">&quot;Starting job: &quot;</span> + callSite.shortForm)</span><br><span class="line">  <span class="keyword">if</span> (conf.getBoolean(<span class="string">&quot;spark.logLineage&quot;</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">    logInfo(<span class="string">&quot;RDD&#x27;s recursive dependencies:\n&quot;</span> + rdd.toDebugString)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 在给定的rdd上运行action函数，并将结果放入resultHandler中</span></span><br><span class="line">  dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, resultHandler, localProperties.get)</span><br><span class="line">  <span class="comment">// 进度标识，不重要</span></span><br><span class="line">  progressBar.foreach(_.finishAll())</span><br><span class="line">  rdd.doCheckpoint()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="doCheckpoint"><a href="#doCheckpoint" class="headerlink" title="doCheckpoint"></a>doCheckpoint</h3><h2 id="DAGScheduler-runJob"><a href="#DAGScheduler-runJob" class="headerlink" title="DAGScheduler.runJob"></a>DAGScheduler.runJob</h2><p>向DAGScheduler提交一个action作业并等待作业完成后打印失败或成功日志</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">runJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">    callSite: <span class="type">CallSite</span>,</span><br><span class="line">    resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,</span><br><span class="line">    properties: <span class="type">Properties</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> start = <span class="type">System</span>.nanoTime</span><br><span class="line">  <span class="comment">// 向DAGScheduler提交一个action作业，重要入口</span></span><br><span class="line">  <span class="keyword">val</span> waiter = submitJob(rdd, func, partitions, callSite, resultHandler, properties)</span><br><span class="line">  <span class="comment">// 等待作业执行完成</span></span><br><span class="line">  <span class="type">ThreadUtils</span>.awaitReady(waiter.completionFuture, <span class="type">Duration</span>.<span class="type">Inf</span>)</span><br><span class="line">  <span class="comment">// 作业成功打印日志，失败则打印错误</span></span><br><span class="line">  waiter.completionFuture.value.get <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> scala.util.<span class="type">Success</span>(_) =&gt;</span><br><span class="line">      logInfo(<span class="string">&quot;Job %d finished: %s, took %f s&quot;</span>.format</span><br><span class="line">        (waiter.jobId, callSite.shortForm, (<span class="type">System</span>.nanoTime - start) / <span class="number">1e9</span>))</span><br><span class="line">    <span class="keyword">case</span> scala.util.<span class="type">Failure</span>(exception) =&gt;</span><br><span class="line">      logInfo(<span class="string">&quot;Job %d failed: %s, took %f s&quot;</span>.format</span><br><span class="line">        (waiter.jobId, callSite.shortForm, (<span class="type">System</span>.nanoTime - start) / <span class="number">1e9</span>))</span><br><span class="line">      <span class="comment">// SPARK-8644: Include user stack trace in exceptions coming from DAGScheduler.</span></span><br><span class="line">      <span class="keyword">val</span> callerStackTrace = <span class="type">Thread</span>.currentThread().getStackTrace.tail</span><br><span class="line">      exception.setStackTrace(exception.getStackTrace ++ callerStackTrace)</span><br><span class="line">      <span class="keyword">throw</span> exception</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="DAGScheduler-submitJob"><a href="#DAGScheduler-submitJob" class="headerlink" title="DAGScheduler.submitJob"></a>DAGScheduler.submitJob</h2><p>先对partitions进行处理，然后将作业JobSubmitted放入eventProcessLoop</p><p>SparkListenerJobStart继承自SparkListenerEvent，它有许多子类用来监控不同事件的运行状态，详情查看另一篇文章 <a href="">SparkListenerEvent</a></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">submitJob</span></span>[<span class="type">T</span>, <span class="type">U</span>](</span><br><span class="line">    rdd: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">U</span>,</span><br><span class="line">    partitions: <span class="type">Seq</span>[<span class="type">Int</span>],</span><br><span class="line">    callSite: <span class="type">CallSite</span>,</span><br><span class="line">    resultHandler: (<span class="type">Int</span>, <span class="type">U</span>) =&gt; <span class="type">Unit</span>,</span><br><span class="line">    properties: <span class="type">Properties</span>): <span class="type">JobWaiter</span>[<span class="type">U</span>] = &#123;</span><br><span class="line">  <span class="comment">// 如果存在给定的partition大于分区长度或者小于0则抛出异常</span></span><br><span class="line">  <span class="keyword">val</span> maxPartitions = rdd.partitions.length</span><br><span class="line">  partitions.find(p =&gt; p &gt;= maxPartitions || p &lt; <span class="number">0</span>).foreach &#123; p =&gt;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(</span><br><span class="line">      <span class="string">&quot;Attempting to access a non-existent partition: &quot;</span> + p + <span class="string">&quot;. &quot;</span> +</span><br><span class="line">        <span class="string">&quot;Total number of partitions: &quot;</span> + maxPartitions)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 提交一次作业可以有多个job，给每个job分配递增ID</span></span><br><span class="line">  <span class="keyword">val</span> jobId = nextJobId.getAndIncrement()</span><br><span class="line">  <span class="comment">// 处理partitions为空的情况，没有分区供你处理，直接标识任务开始任务结束</span></span><br><span class="line">  <span class="keyword">if</span> (partitions.isEmpty) &#123;</span><br><span class="line">    <span class="keyword">val</span> clonedProperties = <span class="type">Utils</span>.cloneProperties(properties)</span><br><span class="line">    <span class="keyword">if</span> (sc.getLocalProperty(<span class="type">SparkContext</span>.<span class="type">SPARK_JOB_DESCRIPTION</span>) == <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="comment">// 标识调用的代码位置</span></span><br><span class="line">      clonedProperties.setProperty(<span class="type">SparkContext</span>.<span class="type">SPARK_JOB_DESCRIPTION</span>, callSite.shortForm)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> time = clock.getTimeMillis()</span><br><span class="line">    listenerBus.post(</span><br><span class="line">      <span class="type">SparkListenerJobStart</span>(jobId, time, <span class="type">Seq</span>.empty, clonedProperties))</span><br><span class="line">    listenerBus.post(</span><br><span class="line">      <span class="type">SparkListenerJobEnd</span>(jobId, time, <span class="type">JobSucceeded</span>))</span><br><span class="line">    <span class="comment">// Return immediately if the job is running 0 tasks</span></span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="type">JobWaiter</span>[<span class="type">U</span>](<span class="keyword">this</span>, jobId, <span class="number">0</span>, resultHandler)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  assert(partitions.nonEmpty)</span><br><span class="line">  <span class="keyword">val</span> func2 = func.asInstanceOf[(<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _]</span><br><span class="line">  <span class="keyword">val</span> waiter = <span class="keyword">new</span> <span class="type">JobWaiter</span>[<span class="type">U</span>](<span class="keyword">this</span>, jobId, partitions.size, resultHandler)</span><br><span class="line">  <span class="comment">// 将作业JobSubmitted放入eventProcessLoop，着重介绍DAGSchedulerEventProcessLoop</span></span><br><span class="line">  eventProcessLoop.post(<span class="type">JobSubmitted</span>(</span><br><span class="line">    jobId, rdd, func2, partitions.toArray, callSite, waiter,</span><br><span class="line">    <span class="type">Utils</span>.cloneProperties(properties)))</span><br><span class="line">  waiter</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="DAGScheduler-eventProcessLoop"><a href="#DAGScheduler-eventProcessLoop" class="headerlink" title="DAGScheduler.eventProcessLoop"></a>DAGScheduler.eventProcessLoop</h2><p>DAGSchedulerEventProcessLoop继承自EventLoop，它提供了一个onReceive用来执行提交给它的事件，包括作业提交JobSubmitted、作业取消JobCancelled、map stage提交MapStageSubmitted等事件，它们统一继承自DAGSchedulerEvent</p><p>关于EventLoop它是一个事件循环，用于接收来自调用者（提交任务）的事件并处理事件线程中的所有事件。它将启动一个独占事件线程来处理所有事件，有兴趣的可以看一下源码，实现并不复杂</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[scheduler] <span class="class"><span class="keyword">class</span> <span class="title">DAGSchedulerEventProcessLoop</span>(<span class="params">dagScheduler: <span class="type">DAGScheduler</span></span>)</span></span><br><span class="line">  <span class="keyword">extends</span> <span class="type">EventLoop</span>[<span class="type">DAGSchedulerEvent</span>](<span class="string">&quot;dag-scheduler-event-loop&quot;</span>) <span class="keyword">with</span> <span class="type">Logging</span> &#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> timer = dagScheduler.metricsSource.messageProcessingTimer</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 从事件队列轮询事件时在事件线程中调用</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> timerContext = timer.time()</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      doOnReceive(event)</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      timerContext.stop()</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">doOnReceive</span></span>(event: <span class="type">DAGSchedulerEvent</span>): <span class="type">Unit</span> = event <span class="keyword">match</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">JobSubmitted</span>(jobId, rdd, func, partitions, callSite, listener, properties) =&gt;</span><br><span class="line">      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, callSite, listener, properties)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> <span class="type">MapStageSubmitted</span>(jobId, dependency, callSite, listener, properties) =&gt;</span><br><span class="line">      dagScheduler.handleMapStageSubmitted(jobId, dependency, callSite, listener, properties)</span><br><span class="line">    <span class="comment">// 后面还有许多</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onError</span></span>(e: <span class="type">Throwable</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    logError(<span class="string">&quot;DAGSchedulerEventProcessLoop failed; shutting down SparkContext&quot;</span>, e)</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      dagScheduler.doCancelAllJobs()</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> t: <span class="type">Throwable</span> =&gt; logError(<span class="string">&quot;DAGScheduler failed to cancel all jobs.&quot;</span>, t)</span><br><span class="line">    &#125;</span><br><span class="line">    dagScheduler.sc.stopInNewThread()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onStop</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="comment">// Cancel any active jobs in postStop hook</span></span><br><span class="line">    dagScheduler.cleanUpAfterSchedulerStop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="DAGScheduler-handleJobSubmitted"><a href="#DAGScheduler-handleJobSubmitted" class="headerlink" title="DAGScheduler.handleJobSubmitted"></a>DAGScheduler.handleJobSubmitted</h2><p>提交一个job的操作，该函数中主要获取到了父级依赖，父级stage和ResultStage，标识job运行并提交ResultStage</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[scheduler] <span class="function"><span class="keyword">def</span> <span class="title">handleJobSubmitted</span></span>(jobId: <span class="type">Int</span>,</span><br><span class="line">    finalRDD: <span class="type">RDD</span>[_],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</span><br><span class="line">    partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">    callSite: <span class="type">CallSite</span>,</span><br><span class="line">    listener: <span class="type">JobListener</span>,</span><br><span class="line">    properties: <span class="type">Properties</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> finalStage: <span class="type">ResultStage</span> = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="comment">// 获取ResultStage，每碰到一个action算子就产生一个ResultStage</span></span><br><span class="line">    finalStage = createResultStage(finalRDD, func, partitions, jobId, callSite)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">BarrierJobSlotsNumberCheckFailed</span> =&gt;</span><br><span class="line">      <span class="keyword">val</span> numCheckFailures = barrierJobIdToNumTasksCheckFailures.compute(jobId,</span><br><span class="line">        (_: <span class="type">Int</span>, value: <span class="type">Int</span>) =&gt; value + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">      logWarning(<span class="string">s&quot;Barrier stage in job <span class="subst">$jobId</span> requires <span class="subst">$&#123;e.requiredConcurrentTasks&#125;</span> slots, &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;but only <span class="subst">$&#123;e.maxConcurrentTasks&#125;</span> are available. &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;Will retry up to <span class="subst">$&#123;maxFailureNumTasksCheck - numCheckFailures + 1&#125;</span> more times&quot;</span>)</span><br><span class="line">      <span class="comment">// 未达到最大失败次数时在一个新的线程中重新提交该任务</span></span><br><span class="line">      <span class="keyword">if</span> (numCheckFailures &lt;= maxFailureNumTasksCheck) &#123;</span><br><span class="line">        messageScheduler.schedule(</span><br><span class="line">          <span class="keyword">new</span> <span class="type">Runnable</span> &#123;</span><br><span class="line">            <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = eventProcessLoop.post(<span class="type">JobSubmitted</span>(jobId, finalRDD, func,</span><br><span class="line">              partitions, callSite, listener, properties))</span><br><span class="line">          &#125;,</span><br><span class="line">          timeIntervalNumTasksCheck,</span><br><span class="line">          <span class="type">TimeUnit</span>.<span class="type">SECONDS</span></span><br><span class="line">        )</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        barrierJobIdToNumTasksCheckFailures.remove(jobId)</span><br><span class="line">        listener.jobFailed(e)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">Exception</span> =&gt;</span><br><span class="line">      logWarning(<span class="string">&quot;Creating new stage failed due to exception - job: &quot;</span> + jobId, e)</span><br><span class="line">      listener.jobFailed(e)</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Job submitted, clear internal data.</span></span><br><span class="line">  barrierJobIdToNumTasksCheckFailures.remove(jobId)</span><br><span class="line">  <span class="comment">// 新建一个ActiveJob，每一个action算子是一个job</span></span><br><span class="line">  <span class="keyword">val</span> job = <span class="keyword">new</span> <span class="type">ActiveJob</span>(jobId, finalStage, callSite, listener, properties)</span><br><span class="line">  clearCacheLocs()</span><br><span class="line">  logInfo(<span class="string">&quot;Got job %s (%s) with %d output partitions&quot;</span>.format(</span><br><span class="line">    job.jobId, callSite.shortForm, partitions.length))</span><br><span class="line">  logInfo(<span class="string">&quot;Final stage: &quot;</span> + finalStage + <span class="string">&quot; (&quot;</span> + finalStage.name + <span class="string">&quot;)&quot;</span>)</span><br><span class="line">  logInfo(<span class="string">&quot;Parents of final stage: &quot;</span> + finalStage.parents)</span><br><span class="line">  logInfo(<span class="string">&quot;Missing parents: &quot;</span> + getMissingParentStages(finalStage))</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> jobSubmissionTime = clock.getTimeMillis()</span><br><span class="line">  <span class="comment">// 将此ActiveJob放入jobIdToActiveJob和activeJobs</span></span><br><span class="line">  jobIdToActiveJob(jobId) = job</span><br><span class="line">  activeJobs += job</span><br><span class="line">  <span class="comment">// 将ActiveJob绑定到ResultStage</span></span><br><span class="line">  finalStage.setActiveJob(job)</span><br><span class="line">  <span class="keyword">val</span> stageIds = jobIdToStageIds(jobId).toArray</span><br><span class="line">  <span class="keyword">val</span> stageInfos = stageIds.flatMap(id =&gt; stageIdToStage.get(id).map(_.latestInfo))</span><br><span class="line">  listenerBus.post(</span><br><span class="line">    <span class="type">SparkListenerJobStart</span>(job.jobId, jobSubmissionTime, stageInfos,</span><br><span class="line">      <span class="type">Utils</span>.cloneProperties(properties)))</span><br><span class="line">  <span class="comment">// 提交ResultStage</span></span><br><span class="line">  submitStage(finalStage)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="DAGScheduler-createResultStage"><a href="#DAGScheduler-createResultStage" class="headerlink" title="DAGScheduler.createResultStage"></a>DAGScheduler.createResultStage</h3><p>ResultStage继承自Stage，Stage有两个子类分别是ResultStage和ShuffleMapStage，ShuffleMapStage存在于两个shuffle依赖之间，而ResultStage则是整个job的最后一个依赖</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">createResultStage</span></span>(</span><br><span class="line">    rdd: <span class="type">RDD</span>[_],</span><br><span class="line">    func: (<span class="type">TaskContext</span>, <span class="type">Iterator</span>[_]) =&gt; _,</span><br><span class="line">    partitions: <span class="type">Array</span>[<span class="type">Int</span>],</span><br><span class="line">    jobId: <span class="type">Int</span>,</span><br><span class="line">    callSite: <span class="type">CallSite</span>): <span class="type">ResultStage</span> = &#123;</span><br><span class="line">  <span class="comment">// 返回给定rdd的直接父级的shuffle依赖以及与此stage的rdd关联的ResourceProfiles（ResourceProfile支持对指定Stage的资源如内存、磁盘等进行干预）</span></span><br><span class="line">  <span class="keyword">val</span> (shuffleDeps, resourceProfiles) = getShuffleDependenciesAndResourceProfiles(rdd)</span><br><span class="line">  <span class="keyword">val</span> resourceProfile = mergeResourceProfilesForStage(resourceProfiles)</span><br><span class="line">  checkBarrierStageWithDynamicAllocation(rdd)</span><br><span class="line">  checkBarrierStageWithNumSlots(rdd, resourceProfile)</span><br><span class="line">  checkBarrierStageWithRDDChainPattern(rdd, partitions.toSet.size)</span><br><span class="line">  <span class="comment">// 根据当前rdd的父级shuffle依赖获取或者创建它的ShuffleMapStage</span></span><br><span class="line">  <span class="keyword">val</span> parents = getOrCreateParentStages(shuffleDeps, jobId)</span><br><span class="line">  <span class="comment">// stageId在同一个job中递增</span></span><br><span class="line">  <span class="keyword">val</span> id = nextStageId.getAndIncrement()</span><br><span class="line">  <span class="keyword">val</span> stage = <span class="keyword">new</span> <span class="type">ResultStage</span>(id, rdd, func, partitions, parents, jobId,</span><br><span class="line">    callSite, resourceProfile.id)</span><br><span class="line">  stageIdToStage(id) = stage  <span class="comment">// 更新stageId和stage的map</span></span><br><span class="line">  updateJobIdStageIdMaps(jobId, stage)  <span class="comment">// 更新jobId和stageId的map</span></span><br><span class="line">  stage</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="DAGScheduler-submitStage"><a href="#DAGScheduler-submitStage" class="headerlink" title="DAGScheduler.submitStage"></a>DAGScheduler.submitStage</h2><p>提交当前ResultStage，但首先提交它的父级未提交的ShuffleMapStage，这个方法会从ResultStage向上迭代寻找父级ShuffleMapStage并从最开始向后提交stage</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitStage</span></span>(stage: <span class="type">Stage</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// 找到最早创建该stage的jobId</span></span><br><span class="line">  <span class="keyword">val</span> jobId = activeJobForStage(stage)</span><br><span class="line">  <span class="keyword">if</span> (jobId.isDefined) &#123;</span><br><span class="line">    logDebug(<span class="string">s&quot;submitStage(<span class="subst">$stage</span> (name=<span class="subst">$&#123;stage.name&#125;</span>;&quot;</span> +</span><br><span class="line">      <span class="string">s&quot;jobs=<span class="subst">$&#123;stage.jobIds.toSeq.sorted.mkString(&quot;,&quot;)&#125;</span>))&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> (!waitingStages(stage) &amp;&amp; !runningStages(stage) &amp;&amp; !failedStages(stage)) &#123;</span><br><span class="line">      <span class="comment">// 寻找它的父级未提交的ShuffleMapStage</span></span><br><span class="line">      <span class="keyword">val</span> missing = getMissingParentStages(stage).sortBy(_.id)</span><br><span class="line">      logDebug(<span class="string">&quot;missing: &quot;</span> + missing)</span><br><span class="line">      <span class="keyword">if</span> (missing.isEmpty) &#123;</span><br><span class="line">        logInfo(<span class="string">&quot;Submitting &quot;</span> + stage + <span class="string">&quot; (&quot;</span> + stage.rdd + <span class="string">&quot;), which has no missing parents&quot;</span>)</span><br><span class="line">        <span class="comment">// 如果没找到则代表已经找到起始的ShuffleMapStage，从起始stage开始提交</span></span><br><span class="line">        submitMissingTasks(stage, jobId.get)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">// 如果找到了则将遍历继续寻找父级ShuffleMapStage的ShuffleMapStage</span></span><br><span class="line">        <span class="keyword">for</span> (parent &lt;- missing) &#123;</span><br><span class="line">          submitStage(parent)</span><br><span class="line">        &#125;</span><br><span class="line">        waitingStages += stage</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    abortStage(stage, <span class="string">&quot;No active job for stage &quot;</span> + stage.id, <span class="type">None</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="DAGScheduler-submitMissingTasks"><a href="#DAGScheduler-submitMissingTasks" class="headerlink" title="DAGScheduler.submitMissingTasks"></a>DAGScheduler.submitMissingTasks</h2><p>查找stage对应的需要运行的taskID和位置等信息，由TaskScheduler提交并运行</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">submitMissingTasks</span></span>(stage: <span class="type">Stage</span>, jobId: <span class="type">Int</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 返回需要计算的taskID</span></span><br><span class="line">  <span class="keyword">val</span> partitionsToCompute: <span class="type">Seq</span>[<span class="type">Int</span>] = stage.findMissingPartitions()</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 省略</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">// 获取taskID对应的运行位置</span></span><br><span class="line">  <span class="keyword">val</span> taskIdToLocations: <span class="type">Map</span>[<span class="type">Int</span>, <span class="type">Seq</span>[<span class="type">TaskLocation</span>]] = <span class="keyword">try</span> &#123;</span><br><span class="line">    stage <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> s: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt; (id, getPreferredLocs(stage.rdd, id))&#125;.toMap</span><br><span class="line">      <span class="keyword">case</span> s: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">          <span class="keyword">val</span> p = s.partitions(id)</span><br><span class="line">          (id, getPreferredLocs(stage.rdd, p))</span><br><span class="line">        &#125;.toMap</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">      stage.makeNewStageAttempt(partitionsToCompute.size)</span><br><span class="line">      listenerBus.post(<span class="type">SparkListenerStageSubmitted</span>(stage.latestInfo,</span><br><span class="line">        <span class="type">Utils</span>.cloneProperties(properties)))</span><br><span class="line">      abortStage(stage, <span class="string">s&quot;Task creation failed: <span class="subst">$e</span>\n<span class="subst">$&#123;Utils.exceptionString(e)&#125;</span>&quot;</span>, <span class="type">Some</span>(e))</span><br><span class="line">      runningStages -= stage</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 创建一个新的StageInfo并将_latestInfo指向最新的StageInfo</span></span><br><span class="line">  stage.makeNewStageAttempt(partitionsToCompute.size, taskIdToLocations.values.toSeq)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 更新stage从DAGScheduler提交到TaskScheduler的时间并标识该stage一提交</span></span><br><span class="line">  <span class="keyword">if</span> (partitionsToCompute.nonEmpty) &#123;</span><br><span class="line">    stage.latestInfo.submissionTime = <span class="type">Some</span>(clock.getTimeMillis())</span><br><span class="line">  &#125;</span><br><span class="line">  listenerBus.post(<span class="type">SparkListenerStageSubmitted</span>(stage.latestInfo,</span><br><span class="line">    <span class="type">Utils</span>.cloneProperties(properties)))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 将stage序列化并作为广播变量</span></span><br><span class="line">  <span class="keyword">var</span> taskBinary: <span class="type">Broadcast</span>[<span class="type">Array</span>[<span class="type">Byte</span>]] = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">var</span> partitions: <span class="type">Array</span>[<span class="type">Partition</span>] = <span class="literal">null</span></span><br><span class="line">  <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">var</span> taskBinaryBytes: <span class="type">Array</span>[<span class="type">Byte</span>] = <span class="literal">null</span></span><br><span class="line">    <span class="type">RDDCheckpointData</span>.synchronized &#123;</span><br><span class="line">      taskBinaryBytes = stage <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">          <span class="type">JavaUtils</span>.bufferToArray(</span><br><span class="line">            closureSerializer.serialize((stage.rdd, stage.shuffleDep): <span class="type">AnyRef</span>))</span><br><span class="line">        <span class="keyword">case</span> stage: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">          <span class="type">JavaUtils</span>.bufferToArray(closureSerializer.serialize((stage.rdd, stage.func): <span class="type">AnyRef</span>))</span><br><span class="line">      &#125;</span><br><span class="line">      partitions = stage.rdd.partitions</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (taskBinaryBytes.length &gt; <span class="type">TaskSetManager</span>.<span class="type">TASK_SIZE_TO_WARN_KIB</span> * <span class="number">1024</span>) &#123;</span><br><span class="line">      logWarning(<span class="string">s&quot;Broadcasting large task binary with size &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;<span class="subst">$&#123;Utils.bytesToString(taskBinaryBytes.length)&#125;</span>&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 将taskBinaryBytes广播出去</span></span><br><span class="line">    taskBinary = sc.broadcast(taskBinaryBytes)</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> e: <span class="type">NotSerializableException</span> =&gt;</span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 创建对应的ShuffleMapTask或者ResultTask，它们是Task的子类，对应ShuffleMapStage和ResultStage。一个Stage可以有多个task并行处理</span></span><br><span class="line">  <span class="keyword">val</span> tasks: <span class="type">Seq</span>[<span class="type">Task</span>[_]] = <span class="keyword">try</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> serializedTaskMetrics = closureSerializer.serialize(stage.latestInfo.taskMetrics).array()</span><br><span class="line">    stage <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> stage: <span class="type">ShuffleMapStage</span> =&gt;</span><br><span class="line">        stage.pendingPartitions.clear()</span><br><span class="line">        <span class="comment">// 根据需要运行的taskId创建多个task</span></span><br><span class="line">        partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">          <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">          <span class="keyword">val</span> part = partitions(id)</span><br><span class="line">          stage.pendingPartitions += id</span><br><span class="line">          <span class="keyword">new</span> <span class="type">ShuffleMapTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class="line">            taskBinary, part, locs, properties, serializedTaskMetrics, <span class="type">Option</span>(jobId),</span><br><span class="line">            <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId, stage.rdd.isBarrier())</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> stage: <span class="type">ResultStage</span> =&gt;</span><br><span class="line">        partitionsToCompute.map &#123; id =&gt;</span><br><span class="line">          <span class="keyword">val</span> p: <span class="type">Int</span> = stage.partitions(id)</span><br><span class="line">          <span class="keyword">val</span> part = partitions(p)</span><br><span class="line">          <span class="keyword">val</span> locs = taskIdToLocations(id)</span><br><span class="line">          <span class="keyword">new</span> <span class="type">ResultTask</span>(stage.id, stage.latestInfo.attemptNumber,</span><br><span class="line">            taskBinary, part, locs, id, properties, serializedTaskMetrics,</span><br><span class="line">            <span class="type">Option</span>(jobId), <span class="type">Option</span>(sc.applicationId), sc.applicationAttemptId,</span><br><span class="line">            stage.rdd.isBarrier())</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">    <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt;</span><br><span class="line">      abortStage(stage, <span class="string">s&quot;Task creation failed: <span class="subst">$e</span>\n<span class="subst">$&#123;Utils.exceptionString(e)&#125;</span>&quot;</span>, <span class="type">Some</span>(e))</span><br><span class="line">      runningStages -= stage</span><br><span class="line">      <span class="keyword">return</span></span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 接下来就由TaskScheduler提交task去运行了，它的实现类是TaskSchedulerImpl，可以被其他scheduler实现例如YARN</span></span><br><span class="line">  <span class="keyword">if</span> (tasks.nonEmpty) &#123;</span><br><span class="line">    logInfo(<span class="string">s&quot;Submitting <span class="subst">$&#123;tasks.size&#125;</span> missing tasks from <span class="subst">$stage</span> (<span class="subst">$&#123;stage.rdd&#125;</span>) (first 15 &quot;</span> +</span><br><span class="line">      <span class="string">s&quot;tasks are for partitions <span class="subst">$&#123;tasks.take(15).map(_.partitionId)&#125;</span>)&quot;</span>)</span><br><span class="line">    taskScheduler.submitTasks(<span class="keyword">new</span> <span class="type">TaskSet</span>(</span><br><span class="line">      tasks.toArray, stage.id, stage.latestInfo.attemptNumber, jobId, properties,</span><br><span class="line">      stage.resourceProfileId))</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    markStageAsFinished(stage, <span class="type">None</span>)</span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="TaskScheduler-submitTasks"><a href="#TaskScheduler-submitTasks" class="headerlink" title="TaskScheduler.submitTasks"></a>TaskScheduler.submitTasks</h2><p>提交一个需要运行的task序列到Pool，最终由SchedulerBackend更新并启动它们</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">submitTasks</span></span>(taskSet: <span class="type">TaskSet</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> tasks = taskSet.tasks</span><br><span class="line">  logInfo(<span class="string">&quot;Adding task set &quot;</span> + taskSet.id + <span class="string">&quot; with &quot;</span> + tasks.length + <span class="string">&quot; tasks &quot;</span></span><br><span class="line">    + <span class="string">&quot;resource profile &quot;</span> + taskSet.resourceProfileId)</span><br><span class="line">  <span class="keyword">this</span>.synchronized &#123;</span><br><span class="line">    <span class="comment">// 创建TaskSetManager，它在TaskSchedulerImpl中的单个TaskSet中调度任务。此类跟踪每个task，如果任务task失败（最多失败次数以内）重试任务</span></span><br><span class="line">    <span class="keyword">val</span> manager = createTaskSetManager(taskSet, maxTaskFailures)</span><br><span class="line">    <span class="keyword">val</span> stage = taskSet.stageId</span><br><span class="line">    <span class="keyword">val</span> stageTaskSets =</span><br><span class="line">      taskSetsByStageIdAndAttempt.getOrElseUpdate(stage, <span class="keyword">new</span> <span class="type">HashMap</span>[<span class="type">Int</span>, <span class="type">TaskSetManager</span>])</span><br><span class="line"></span><br><span class="line">    stageTaskSets.foreach &#123; <span class="keyword">case</span> (_, ts) =&gt;</span><br><span class="line">      ts.isZombie = <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">    stageTaskSets(taskSet.stageAttemptId) = manager</span><br><span class="line">    <span class="comment">// 将TaskSetManager添加到Pool队列中，由它来根据Fair或者FIFO算法调度TaskSetManager</span></span><br><span class="line">    schedulableBuilder.addTaskSetManager(manager, manager.taskSet.properties)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!isLocal &amp;&amp; !hasReceivedTask) &#123;</span><br><span class="line">      starvationTimer.scheduleAtFixedRate(<span class="keyword">new</span> <span class="type">TimerTask</span>() &#123;</span><br><span class="line">        <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">          <span class="keyword">if</span> (!hasLaunchedTask) &#123;</span><br><span class="line">            logWarning(<span class="string">&quot;Initial job has not accepted any resources; &quot;</span> +</span><br><span class="line">              <span class="string">&quot;check your cluster UI to ensure that workers are registered &quot;</span> +</span><br><span class="line">              <span class="string">&quot;and have sufficient resources&quot;</span>)</span><br><span class="line">          &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">this</span>.cancel()</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;, <span class="type">STARVATION_TIMEOUT_MS</span>, <span class="type">STARVATION_TIMEOUT_MS</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    hasReceivedTask = <span class="literal">true</span></span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 更新并安排运行task，向driver发送一条ReviveOffers消息，再往后就是RPC相关的点了</span></span><br><span class="line">  backend.reviveOffers()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="CoarseGrainedSchedulerBackend-reviveOffers"><a href="#CoarseGrainedSchedulerBackend-reviveOffers" class="headerlink" title="CoarseGrainedSchedulerBackend.reviveOffers"></a>CoarseGrainedSchedulerBackend.reviveOffers</h2><p>backend是SchedulerBackend，它是一个特质，由其他调度系统继承并重写，例如Mesos、Yarn等等。</p><p>这里以YarnSchedulerBackend为例，假设提交到Yarn上，它其实调用的是CoarseGrainedSchedulerBackend的reviveOffers方法，向driver发送一个ReviveOffers消息</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 创建DriverEndpoint</span></span><br><span class="line"><span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">createDriverEndpoint</span></span>(): <span class="type">DriverEndpoint</span> = <span class="keyword">new</span> <span class="type">DriverEndpoint</span>()</span><br><span class="line"><span class="comment">// 获取DriverEndpointRef，向谁发送消息就要获取谁的Ref</span></span><br><span class="line"><span class="keyword">val</span> driverEndpoint = rpcEnv.setupEndpoint(<span class="type">ENDPOINT_NAME</span>, createDriverEndpoint())</span><br><span class="line">  <span class="comment">// createDriverEndpoint方法被YarnSchedulerBackend重写了，所以返回的是YarnDriverEndpoint，但是YarnDriverEndpoint没重写receive方法，所以接收消息还是得看DriverEndpoint的receive</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">createDriverEndpoint</span></span>(): <span class="type">DriverEndpoint</span> = &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">YarnDriverEndpoint</span>()</span><br><span class="line">    &#125;</span><br><span class="line"><span class="comment">// ReviveOffers就是一个Spark内部的消息，像start或stop一样代表不同的含义</span></span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reviveOffers</span></span>(): <span class="type">Unit</span> = <span class="type">Utils</span>.tryLogNonFatalError &#123;</span><br><span class="line">  driverEndpoint.send(<span class="type">ReviveOffers</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="DriverEndpoint-receive-gt-ReviveOffers-gt-makeOffers"><a href="#DriverEndpoint-receive-gt-ReviveOffers-gt-makeOffers" class="headerlink" title="DriverEndpoint.receive-&gt;ReviveOffers-&gt;makeOffers"></a>DriverEndpoint.receive-&gt;ReviveOffers-&gt;makeOffers</h2><p>DriverEndpoint的receive的方法对ReviveOffers的处理方法就是调用makeOffers方法，这个方法目的是封装task的任务描述，然后在executor启动这些任务</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">makeOffers</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// 返回我们提交的task的任务描述，包含taskId，executorId、task内容、jar包和file等</span></span><br><span class="line">  <span class="keyword">val</span> taskDescs = withLock &#123;</span><br><span class="line">    <span class="comment">// 过滤掉killed的executor</span></span><br><span class="line">    <span class="keyword">val</span> activeExecutors = executorDataMap.filterKeys(isExecutorActive)</span><br><span class="line">    <span class="keyword">val</span> workOffers = activeExecutors.map &#123;</span><br><span class="line">      <span class="keyword">case</span> (id, executorData) =&gt;</span><br><span class="line">        <span class="comment">// 用来存放executor上可用的免费资源，暂且不管ExecutorData是如何管理它自身的资源的</span></span><br><span class="line">        <span class="keyword">new</span> <span class="type">WorkerOffer</span>(id, executorData.executorHost, executorData.freeCores,</span><br><span class="line">          <span class="type">Some</span>(executorData.executorAddress.hostPort),</span><br><span class="line">          executorData.resourcesInfo.map &#123; <span class="keyword">case</span> (rName, rInfo) =&gt;</span><br><span class="line">            (rName, rInfo.availableAddrs.toBuffer)</span><br><span class="line">          &#125;, executorData.resourceProfileId)</span><br><span class="line">    &#125;.toIndexedSeq</span><br><span class="line">    <span class="comment">// 获取task的任务描述，这个方法中会从上面的Pool对象中的队列里按FIFO或者Fair算法获取TaskSetManager列表，也就是我们刚才提交的task</span></span><br><span class="line">    <span class="comment">// 然后给这些TaskSetManager分配WorkerOffer资源，细节比较复杂</span></span><br><span class="line">    scheduler.resourceOffers(workOffers, <span class="literal">true</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (taskDescs.nonEmpty) &#123;</span><br><span class="line">    launchTasks(taskDescs)  <span class="comment">// 启动这些任务</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="DriverEndpoint-launchTasks"><a href="#DriverEndpoint-launchTasks" class="headerlink" title="DriverEndpoint.launchTasks"></a>DriverEndpoint.launchTasks</h2><p>启动这些任务，将task任务描述序列化后封装为LaunchTask消息发送到对应的executor去执行</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">launchTasks</span></span>(tasks: <span class="type">Seq</span>[<span class="type">Seq</span>[<span class="type">TaskDescription</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">for</span> (task &lt;- tasks.flatten) &#123;</span><br><span class="line">    <span class="comment">// 序列化TaskDescription</span></span><br><span class="line">    <span class="keyword">val</span> serializedTask = <span class="type">TaskDescription</span>.encode(task)</span><br><span class="line">    <span class="comment">// 省略</span></span><br><span class="line">    <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> executorData = executorDataMap(task.executorId)</span><br><span class="line">      <span class="comment">// 在这里分配task资源，这些资源在task运行完之后释放</span></span><br><span class="line">      <span class="keyword">val</span> rpId = executorData.resourceProfileId</span><br><span class="line">      <span class="keyword">val</span> prof = scheduler.sc.resourceProfileManager.resourceProfileFromId(rpId)</span><br><span class="line">      <span class="keyword">val</span> taskCpus = <span class="type">ResourceProfile</span>.getTaskCpusOrDefaultForProfile(prof, conf)</span><br><span class="line">      executorData.freeCores -= taskCpus</span><br><span class="line">      task.resources.foreach &#123; <span class="keyword">case</span> (rName, rInfo) =&gt;</span><br><span class="line">        assert(executorData.resourcesInfo.contains(rName))</span><br><span class="line">        executorData.resourcesInfo(rName).acquire(rInfo.addresses)</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      logDebug(<span class="string">s&quot;Launching task <span class="subst">$&#123;task.taskId&#125;</span> on executor id: <span class="subst">$&#123;task.executorId&#125;</span> hostname: &quot;</span> +</span><br><span class="line">        <span class="string">s&quot;<span class="subst">$&#123;executorData.executorHost&#125;</span>.&quot;</span>)</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 将LaunchTask消息发送到对应的executor去执行</span></span><br><span class="line">      executorData.executorEndpoint.send(<span class="type">LaunchTask</span>(<span class="keyword">new</span> <span class="type">SerializableBuffer</span>(serializedTask)))</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="ExecutorBackend-receive"><a href="#ExecutorBackend-receive" class="headerlink" title="ExecutorBackend.receive"></a>ExecutorBackend.receive</h2><p>上述LaunchTask消息最终发送给CoarseGrainedExecutorBackend，它实现了ExecutorBackend，所以直接去看它的receive方法</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">  <span class="comment">// 省略</span></span><br><span class="line">  <span class="keyword">case</span> <span class="type">LaunchTask</span>(data) =&gt;</span><br><span class="line">    <span class="keyword">if</span> (executor == <span class="literal">null</span>) &#123;</span><br><span class="line">      exitExecutor(<span class="number">1</span>, <span class="string">&quot;Received LaunchTask command but executor was null&quot;</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 反序列化TaskDescription</span></span><br><span class="line">      <span class="keyword">val</span> taskDesc = <span class="type">TaskDescription</span>.decode(data.value)</span><br><span class="line">      logInfo(<span class="string">&quot;Got assigned task &quot;</span> + taskDesc.taskId)</span><br><span class="line">      taskResources(taskDesc.taskId) = taskDesc.resources</span><br><span class="line">      <span class="comment">// 终于到了Executor</span></span><br><span class="line">      executor.launchTask(<span class="keyword">this</span>, taskDesc)</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="comment">// 省略</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="Executor-launchTask"><a href="#Executor-launchTask" class="headerlink" title="Executor.launchTask"></a>Executor.launchTask</h2><p>在该Executor执行task并将结果发送给driver</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">launchTask</span></span>(context: <span class="type">ExecutorBackend</span>, taskDescription: <span class="type">TaskDescription</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="comment">// 启动一个TaskRunner，它是一个单独的线程，去运行TaskDescription对应的task，执行逻辑在TaskRunner的run方法中</span></span><br><span class="line">  <span class="comment">// 最终还是调用Task的run方法，针对ShuffleMapTask和ResultTask有不同的执行方法，ShuffleMapTask则将中间结果写到磁盘中，ResultTask执行完之后发送StatusUpdate任务成功和结果消息给DriverEndpoint</span></span><br><span class="line">  <span class="keyword">val</span> tr = <span class="keyword">new</span> <span class="type">TaskRunner</span>(context, taskDescription, plugins)</span><br><span class="line">  <span class="comment">// 将该task标识为正在运行</span></span><br><span class="line">  runningTasks.put(taskDescription.taskId, tr)</span><br><span class="line">  <span class="comment">// 执行该task</span></span><br><span class="line">  threadPool.execute(tr)</span><br><span class="line">  <span class="keyword">if</span> (decommissioned) &#123;</span><br><span class="line">    log.error(<span class="string">s&quot;Launching a task while in decommissioned state.&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="DriverEndpoint-receive-gt-StatusUpdate"><a href="#DriverEndpoint-receive-gt-StatusUpdate" class="headerlink" title="DriverEndpoint.receive-&gt;StatusUpdate"></a>DriverEndpoint.receive-&gt;StatusUpdate</h2><p>driver接收到结果数据消息后更新task状态、释放资源、将结果数据传给JobWaiter中的resultHandler</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">receive</span></span>: <span class="type">PartialFunction</span>[<span class="type">Any</span>, <span class="type">Unit</span>] = &#123;</span><br><span class="line">  <span class="keyword">case</span> <span class="type">StatusUpdate</span>(executorId, taskId, state, data, resources) =&gt;</span><br><span class="line">    <span class="comment">// 更新task状态，并将最终的结果数据传给最开始的JobWaiter中的resultHandler，由此就形成了一个闭环</span></span><br><span class="line">    scheduler.statusUpdate(taskId, state, data.value)</span><br><span class="line">    <span class="comment">// 释放task申请的资源</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="type">TaskState</span>.isFinished(state)) &#123;</span><br><span class="line">      executorDataMap.get(executorId) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(executorInfo) =&gt;</span><br><span class="line">          <span class="keyword">val</span> rpId = executorInfo.resourceProfileId</span><br><span class="line">          <span class="keyword">val</span> prof = scheduler.sc.resourceProfileManager.resourceProfileFromId(rpId)</span><br><span class="line">          <span class="keyword">val</span> taskCpus = <span class="type">ResourceProfile</span>.getTaskCpusOrDefaultForProfile(prof, conf)</span><br><span class="line">          executorInfo.freeCores += taskCpus</span><br><span class="line">          resources.foreach &#123; <span class="keyword">case</span> (k, v) =&gt;</span><br><span class="line">            executorInfo.resourcesInfo.get(k).foreach &#123; r =&gt;</span><br><span class="line">              r.release(v.addresses)</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">          makeOffers(executorId)</span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">          <span class="comment">// Ignoring the update since we don&#x27;t know about the executor.</span></span><br><span class="line">          logWarning(<span class="string">s&quot;Ignored task status update (<span class="subst">$taskId</span> state <span class="subst">$state</span>) &quot;</span> +</span><br><span class="line">            <span class="string">s&quot;from unknown executor with ID <span class="subst">$executorId</span>&quot;</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="comment">// 省略</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Shell-If条件判断</title>
      <link href="/2022/09/16/Language/Shell-If%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/"/>
      <url>/2022/09/16/Language/Shell-If%E6%9D%A1%E4%BB%B6%E5%88%A4%E6%96%AD/</url>
      
        <content type="html"><![CDATA[<p>Shell 脚本文件中 if 中的常用判断</p><span id="more"></span><h2 id="If-基本语法"><a href="#If-基本语法" class="headerlink" title="If 基本语法"></a>If 基本语法</h2><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> [ <span class="built_in">command</span> ];<span class="keyword">then</span></span><br><span class="line">  <span class="comment"># 符合该条件执行的语句</span></span><br><span class="line"><span class="keyword">elif</span> [ <span class="built_in">command</span> ];<span class="keyword">then</span></span><br><span class="line">  <span class="comment"># 符合该条件执行的语句</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="comment"># 符合该条件执行的语句</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h2 id="文件-x2F-目录判断"><a href="#文件-x2F-目录判断" class="headerlink" title="文件&#x2F;目录判断"></a>文件&#x2F;目录判断</h2><table><thead><tr><th align="center">操作</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">[ -b FILE ]</td><td align="center">如果 FILE 存在且是一个块特殊文件则为真。</td></tr><tr><td align="center">[ -c FILE ]</td><td align="center">如果 FILE 存在且是一个字特殊文件则为真。</td></tr><tr><td align="center">[ -d DIR ]</td><td align="center">如果 FILE 存在且是一个目录则为真。</td></tr><tr><td align="center">[ -e FILE ]</td><td align="center">如果 FILE 存在则为真。</td></tr><tr><td align="center">[ -f FILE ]</td><td align="center">如果 FILE 存在且是一个普通文件则为真。</td></tr><tr><td align="center">[ -g FILE ]</td><td align="center">如果 FILE 存在且已经设置了 SGID 则为真。</td></tr><tr><td align="center">[ -k FILE ]</td><td align="center">如果 FILE 存在且已经设置了粘制位则为真。</td></tr><tr><td align="center">[ -p FILE ]</td><td align="center">如果 FILE 存在且是一个名字管道(F 如果 O)则为真。</td></tr><tr><td align="center">[ -r FILE ]</td><td align="center">如果 FILE 存在且是可读的则为真。</td></tr><tr><td align="center">[ -s FILE ]</td><td align="center">如果 FILE 存在且大小不为 0 则为真。</td></tr><tr><td align="center">[ -t FD ]</td><td align="center">如果文件描述符 FD 打开且指向一个终端则为真。</td></tr><tr><td align="center">[ -u FILE ]</td><td align="center">如果 FILE 存在且设置了 SUID (set user ID)则为真。</td></tr><tr><td align="center">[ -w FILE ]</td><td align="center">如果 FILE 存在且是可写的则为真。</td></tr><tr><td align="center">[ -x FILE ]</td><td align="center">如果 FILE 存在且是可执行的则为真。</td></tr><tr><td align="center">[ -O FILE ]</td><td align="center">如果 FILE 存在且属有效用户 ID 则为真。</td></tr><tr><td align="center">[ -G FILE ]</td><td align="center">如果 FILE 存在且属有效用户组则为真。</td></tr><tr><td align="center">[ -L FILE ]</td><td align="center">如果 FILE 存在且是一个符号连接则为真。</td></tr><tr><td align="center">[ -N FILE ]</td><td align="center">如果 FILE 存在 and has been mod 如果 ied since it was last read 则为真。</td></tr><tr><td align="center">[ -S FILE ]</td><td align="center">如果 FILE 存在且是一个套接字则为真。</td></tr><tr><td align="center">[ FILE1 -nt FILE2 ]</td><td align="center">如果 FILE1 has been changed more recently than FILE2, or 如果 FILE1 exists and FILE2 does not 则为真。</td></tr><tr><td align="center">[ FILE1 -ot FILE2 ]</td><td align="center">如果 FILE1 比 FILE2 要老, 或者 FILE2 存在且 FILE1 不存在则为真。</td></tr><tr><td align="center">[ FILE1 -ef FILE2 ]</td><td align="center">如果 FILE1 和 FILE2 指向相同的设备和节点号则为真。</td></tr></tbody></table><h2 id="字符串判断"><a href="#字符串判断" class="headerlink" title="字符串判断"></a>字符串判断</h2><table><thead><tr><th align="center">操作</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">[ -z STRING ]</td><td align="center">如果 STRING 的长度为零则为真 ，即判断是否为空，空即是真。</td></tr><tr><td align="center">[ -n STRING ]</td><td align="center">如果 STRING 的长度非零则为真 ，即判断是否为非空，非空即是真。</td></tr><tr><td align="center">[ STRING1 &#x3D; STRING2 ]</td><td align="center">如果两个字符串相同则为真 。</td></tr><tr><td align="center">[ STRING1 !&#x3D; STRING2 ]</td><td align="center">如果字符串不相同则为真 。</td></tr><tr><td align="center">[ STRING1 ]</td><td align="center">如果字符串不为空则为真,与-n 类似。</td></tr><tr><td align="center">&#x3D;</td><td align="center">等于,如: if [ “$a” &#x3D; “$b” ]</td></tr><tr><td align="center">&#x3D;&#x3D;</td><td align="center">等于,如: if [ “$a” &#x3D;&#x3D; “$b” ],与 if [ “$a” &#x3D; “$b” ]等价</td></tr><tr><td align="center">!&#x3D;</td><td align="center">不等于,如: if [ “$a” !&#x3D; “$b” ] . 这个操作符将在 [[]] 结构中使用模式匹配.</td></tr><tr><td align="center">&lt;</td><td align="center">小于,在 ASCII 字母顺序下.如: if [[“$a” &lt; “$b”]] if [ “$a” &lt; “$b” ] 注意:在 [] 结构中 &lt; 需要被转义.</td></tr><tr><td align="center">&gt;</td><td align="center">大于,在 ASCII 字母顺序下.如:if [[“$a” &gt; “$b”]] if [ “$a” &gt; “$b” ] 注意:在 [] 结构中 &gt; 需要被转义.</td></tr></tbody></table><p><strong>注意:</strong> &#x3D;&#x3D; 的功能在 [[]] 和 [] 中的行为是不同的,如下: [[ $a &#x3D;&#x3D; z*]] 如果 $a 以 z 开头(模式匹配)那么将为 true; [[$a &#x3D;&#x3D; “z*”]] 如果 $a 等于 z* (字符匹配), 那么结果为 true; [ $a &#x3D;&#x3D; z* ] File globbing 和 word splitting 将会发生; 关于 File globbing 是一种关于文件的速记法,比如 *.c 就是,再如 ~ 也是; 但是 file globbing 并不是严格的正则表达式,虽然绝大多数情况下结构比较像.</p><h2 id="数值判断"><a href="#数值判断" class="headerlink" title="数值判断"></a>数值判断</h2><table><thead><tr><th align="center">操作</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">INT1 -eq INT2</td><td align="center">INT1 和 INT2 两数相等为真 ,&#x3D;</td></tr><tr><td align="center">INT1 -ne INT2</td><td align="center">INT1 和 INT2 两数不等为真 ,&lt;&gt;</td></tr><tr><td align="center">INT1 -gt INT2</td><td align="center">INT1 大于 INT1 为真 ,&gt;</td></tr><tr><td align="center">INT1 -ge INT2</td><td align="center">INT1 大于等于 INT2 为真,&gt;&#x3D;</td></tr><tr><td align="center">INT1 -lt INT2</td><td align="center">INT1 小于 INT2 为真 ,&lt;</td></tr><tr><td align="center">INT1 -le INT2</td><td align="center">INT1 小于等于 INT2 为真,&lt;&#x3D;</td></tr><tr><td align="center">&lt;</td><td align="center">小于(需要双括号),如: ((“$a” &lt; “$b”))</td></tr><tr><td align="center">&lt;&#x3D;</td><td align="center">小于等于(需要双括号),如: ((“$a” &lt;&#x3D; “$b”))</td></tr><tr><td align="center">&gt;</td><td align="center">大于(需要双括号),如: ((“$a” &gt; “$b”))</td></tr><tr><td align="center">&gt;&#x3D;</td><td align="center">大于等于(需要双括号),如: ((“$a” &gt;&#x3D; “$b”))</td></tr></tbody></table><h2 id="逻辑判断"><a href="#逻辑判断" class="headerlink" title="逻辑判断"></a>逻辑判断</h2><table><thead><tr><th align="center">操作</th><th align="center">说明</th></tr></thead><tbody><tr><td align="center">-a</td><td align="center">与</td></tr><tr><td align="center">-o</td><td align="center">或</td></tr><tr><td align="center">!</td><td align="center">非</td></tr></tbody></table><h2 id="条件变量替换"><a href="#条件变量替换" class="headerlink" title="条件变量替换"></a>条件变量替换</h2><p>Bash Shell 可以进行变量的条件替换,既只有某种条件发生时才进行替换,替换条件放在 {} 中</p><ol><li><p><code>$&#123;value:-word&#125;</code></p><p>当变量未定义或者值为空时,返回值为 word 的内容,否则返回变量的值</p></li><li><p><code>$&#123;value:=word&#125;</code></p><p>与前者类似,只是若变量未定义或者值为空时,在返回 word 的值的同时将 word 赋值给 value</p></li><li><p><code>$&#123;value:?message&#125;</code></p><p>若变量以赋值的话,正常替换.否则将消息 message 送到标准错误输出(若此替换出现在 Shell 程序中,那么该程序将终止运行)</p></li><li><p><code>$&#123;value:+word&#125;</code></p><p>若变量已赋值的话,其值才用 word 替换,否则不进行任何替换</p></li><li><p><code>$&#123;value:offset&#125;, $&#123;value:offset:length&#125;</code></p><p>从变量中提取子串,这里 offset 和 length 可以是算术表达式</p></li><li><p><code>$&#123; #value&#125;</code></p><p>返回变量的字符个数</p></li><li><p><code>$&#123;value#pattern&#125;, $&#123;value##pattern&#125;</code></p><p>去掉 value 中与 pattern 相匹配的部分,条件是 value 的开头与 pattern 相匹配 , # 与 ## 的区别在于一个是最短匹配模式,一个是最长匹配模式</p></li><li><p><code>$&#123;value%pattern&#125; , $&#123;value%%pattern&#125;</code></p><p>与 7 类似,只是是从 value 的尾部于 pattern 相匹配, % 与 %% 的区别与 # 与 ## 一样</p></li><li><p><code>$&#123;value/pattern/string&#125;, $&#123;value//pattern/string&#125;</code></p><p>进行变量内容的替换,把与 pattern 匹配的部分替换为 string 的内容, &#x2F; 与 &#x2F;&#x2F; 的区别与上同</p></li></ol>]]></content>
      
      
      <categories>
          
          <category> Language </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>变化是趋势，不变是常态</title>
      <link href="/2022/08/19/Mind/%E5%8F%98%E5%8C%96%E6%98%AF%E8%B6%8B%E5%8A%BF%EF%BC%8C%E4%B8%8D%E5%8F%98%E6%98%AF%E5%B8%B8%E6%80%81/"/>
      <url>/2022/08/19/Mind/%E5%8F%98%E5%8C%96%E6%98%AF%E8%B6%8B%E5%8A%BF%EF%BC%8C%E4%B8%8D%E5%8F%98%E6%98%AF%E5%B8%B8%E6%80%81/</url>
      
        <content type="html"><![CDATA[<p>人这一生充满了变数，你的每一次选择，每一次犹豫，每一次冲动，都可能会带给你截然不同的人生。</p><span id="more"></span><p>人这一生充满了变数，你的每一次选择，每一次犹豫，每一次冲动，都可能会带给你截然不同的人生，看起来一切都是自然而然发生的，实则你的现状和每一个小小的抉择息息相关。在这些变数中仍有一些需要坚守的不变的东西。</p><p>在学生时代，你学的知识是变化的，每个时期所学的知识都不相同；不变的是最终的目的，无非是在有限的时间里提高自己的认知能力和价值。工作的时候，你所学的技能是变化的，从参加工作到现在短短几年时间每年所用到的技能都不尽相同，每年都会掌握新技能；不变的是你是否能完全发挥自己的价值，用自己的双手获得更多的收益。在生活中，你的生活标准，生活现状，所承受的压力时刻也在变化；不变的是你对生活的态度，我们必须坚守对自己和家庭负责的态度，坚守给自己和家庭创造幸福生活的态度。</p><p>最近在工作中遇到了一些问题，前几年的时候，自己的基础较弱，所以拼命学习新知识新技能，实则对每项技术的深入理解都有所欠缺，这也导致了自己虽然会的看似挺多但都不精。现在换了一个相对稳定的工作，打算在此沉淀几年，但由于之前总是挑新知识去学的心态导致不能说服自己好好巩固现有的老知识，提不起来兴趣。</p><p>可能刚步入工作的时候快速学习新知识是常态，但最终总是要选一个行业，选几个重点技术去深入理解，这样才能做某一方面的专家，斜杠青年并不是一个长远的状态，至少没听说过斜杠中年斜杠老年。私认为在这种状态下应该有一个明确的方向，相比方向来说计划倒没有那么重要了，若没有方向这个时候最容易迷茫，貌似你已经没什么技术可以学了或者说你认为你掌握的技术已经够多了，但是工资一直上不去，那么沉下心来找到你工作的重心巩固已有的知识就是你现在最需要做的。</p>]]></content>
      
      
      <categories>
          
          <category> Mind </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mind </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark进阶-SparkContext之SparnEnv</title>
      <link href="/2022/08/11/Software/Spark%E8%BF%9B%E9%98%B6-SparkContext%E4%B9%8BSparnEnv/"/>
      <url>/2022/08/11/Software/Spark%E8%BF%9B%E9%98%B6-SparkContext%E4%B9%8BSparnEnv/</url>
      
        <content type="html"><![CDATA[<p>主要介绍了SparkEnv的创建和主要组件的重要功能</p><span id="more"></span><p>Spark对任务的计算都依托于Executor的能力，所有的Executor都有自己的Spark执行环境SparkEnv。有了SparkEnv，就可以将数据存储在存储体系中；就能利用计算引擎对计算任务进行处理，就可以在节点间进行通信等。SparkEnv还提供了多种多样的内部组件，实现不同的功能</p><p>创建SparkEnv的主要代码是调用SparkEnv的create方法</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">create</span></span>(</span><br><span class="line">    conf: <span class="type">SparkConf</span>,</span><br><span class="line">    executorId: <span class="type">String</span>,</span><br><span class="line">    bindAddress: <span class="type">String</span>,</span><br><span class="line">    advertiseAddress: <span class="type">String</span>,</span><br><span class="line">    port: <span class="type">Int</span>,</span><br><span class="line">    isLocal: <span class="type">Boolean</span>,</span><br><span class="line">    numUsableCores: <span class="type">Int</span>,</span><br><span class="line">    ioEncryptionKey: <span class="type">Option</span>[<span class="type">Array</span>[<span class="type">Byte</span>]],</span><br><span class="line">    listenerBus: <span class="type">LiveListenerBus</span> = <span class="literal">null</span>,</span><br><span class="line">    mockOutputCommitCoordinator: <span class="type">Option</span>[<span class="type">OutputCommitCoordinator</span>] = <span class="type">None</span>): <span class="type">SparkEnv</span> = &#123;</span><br><span class="line">  <span class="comment">// 主要对账号、权限及身份认证进行设置和管理</span></span><br><span class="line">  <span class="keyword">val</span> securityManager = <span class="keyword">new</span> <span class="type">SecurityManager</span>(conf, ioEncryptionKey)</span><br><span class="line">  <span class="comment">// NettyRpcEnv RPC环境</span></span><br><span class="line">  <span class="keyword">val</span> rpcEnv = <span class="type">RpcEnv</span>.create(systemName, bindAddress, advertiseAddress, port, conf,</span><br><span class="line">    securityManager, clientMode = !isDriver)</span><br><span class="line">  <span class="keyword">val</span> serializer = instantiateClassFromConf[<span class="type">Serializer</span>](</span><br><span class="line">    <span class="string">&quot;spark.serializer&quot;</span>, <span class="string">&quot;org.apache.spark.serializer.JavaSerializer&quot;</span>)</span><br><span class="line">  <span class="comment">// 序列化管理器</span></span><br><span class="line">  <span class="keyword">val</span> serializerManager = <span class="keyword">new</span> <span class="type">SerializerManager</span>(serializer, conf, ioEncryptionKey)</span><br><span class="line">  <span class="keyword">val</span> closureSerializer = <span class="keyword">new</span> <span class="type">JavaSerializer</span>(conf)</span><br><span class="line">  <span class="comment">// 广播管理器</span></span><br><span class="line">  <span class="keyword">val</span> broadcastManager = <span class="keyword">new</span> <span class="type">BroadcastManager</span>(isDriver, conf, securityManager)</span><br><span class="line">  <span class="comment">// map信息跟踪处理</span></span><br><span class="line">  <span class="keyword">val</span> mapOutputTracker = <span class="keyword">if</span> (isDriver) &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapOutputTrackerMaster</span>(conf, broadcastManager, isLocal)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapOutputTrackerWorker</span>(conf)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// shuffle管理器</span></span><br><span class="line">  <span class="keyword">val</span> shuffleManager = instantiateClass[<span class="type">ShuffleManager</span>](shuffleMgrClass)</span><br><span class="line">      <span class="keyword">val</span> memoryManager: <span class="type">MemoryManager</span> =</span><br><span class="line">    <span class="keyword">if</span> (useLegacyMemoryManager) &#123;</span><br><span class="line">      <span class="keyword">new</span> <span class="type">StaticMemoryManager</span>(conf, numUsableCores)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="type">UnifiedMemoryManager</span>(conf, numUsableCores)</span><br><span class="line">    &#125;</span><br><span class="line">  <span class="comment">// 块管理器</span></span><br><span class="line">  <span class="keyword">val</span> blockManager = <span class="keyword">new</span> <span class="type">BlockManager</span>(executorId, rpcEnv, blockManagerMaster,</span><br><span class="line">    serializerManager, conf, memoryManager, mapOutputTracker, shuffleManager,</span><br><span class="line">    blockTransferService, securityManager, numUsableCores)</span><br><span class="line">  <span class="comment">// 度量系统</span></span><br><span class="line">  <span class="keyword">val</span> metricsSystem = <span class="keyword">if</span> (isDriver) &#123;</span><br><span class="line">    <span class="type">MetricsSystem</span>.createMetricsSystem(<span class="string">&quot;driver&quot;</span>, conf, securityManager)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    conf.set(<span class="string">&quot;spark.executor.id&quot;</span>, executorId)</span><br><span class="line">    <span class="keyword">val</span> ms = <span class="type">MetricsSystem</span>.createMetricsSystem(<span class="string">&quot;executor&quot;</span>, conf, securityManager)</span><br><span class="line">    ms.start()</span><br><span class="line">    ms</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 判断是否可以将任务提交到hdfs的权限</span></span><br><span class="line">  <span class="keyword">val</span> outputCommitCoordinator = mockOutputCommitCoordinator.getOrElse &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">OutputCommitCoordinator</span>(conf, isDriver)</span><br><span class="line">  &#125;</span><br><span class="line">  ...</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="SerializerManager"><a href="#SerializerManager" class="headerlink" title="SerializerManager"></a>SerializerManager</h3><p>Spark中很多对象在通过网络传输或者写入存储体系时，都需要序列化。SparkEnv中有两个序列化的组件，分别是SerializerManager和closureSerializer。</p><p>这里创建的serializer默认为org.apache.spark.serializer.JavaSerializer，用户可以通过spark.serializer属性配置其他的序列化实现，如org.apache.spark.serializer.Kryo-Serializer。closureSerializer的实际类型固定为org.apache.spark.serializer.JavaSerializer，用户不能够自己指定。</p><p>需要注意的是Kryo序列化器比Java默认的序列化器更好用，但是它并不支持所有的Serializable对象，使用时需要手动指定并注册需要序列化的类</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">        .setAppName(<span class="string">&quot;Test&quot;</span>)</span><br><span class="line">        .set(<span class="string">&quot;spark.serializer&quot;</span>, <span class="string">&quot;org.apache.spark.serializer.KryoSerializer&quot;</span>)</span><br><span class="line">        .registerKryoClasses(<span class="type">Array</span>(</span><br><span class="line">          classOf[<span class="type">String</span>],</span><br><span class="line">          classOf[<span class="type">Array</span>[<span class="type">String</span>]],</span><br><span class="line">          classOf[<span class="type">GeneralMatcher</span>],</span><br><span class="line">          classOf[<span class="type">Map</span>[<span class="type">Int</span>, <span class="type">String</span>]]</span><br><span class="line">        ))</span><br></pre></td></tr></table></figure><p>从源码中可以看到Spark默认对广播对象、Shuffle输出数据和溢出到磁盘的Shuffle的数据都是进行压缩的，而对RDD默认是不压缩的，默认的CompressionCodec是lz4</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Whether to compress broadcast variables that are stored</span></span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> compressBroadcast = conf.getBoolean(<span class="string">&quot;spark.broadcast.compress&quot;</span>, <span class="literal">true</span>)</span><br><span class="line"><span class="comment">// Whether to compress shuffle output that are stored</span></span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> compressShuffle = conf.getBoolean(<span class="string">&quot;spark.shuffle.compress&quot;</span>, <span class="literal">true</span>)</span><br><span class="line"><span class="comment">// Whether to compress RDD partitions that are stored serialized</span></span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> compressRdds = conf.getBoolean(<span class="string">&quot;spark.rdd.compress&quot;</span>, <span class="literal">false</span>)</span><br><span class="line"><span class="comment">// Whether to compress shuffle output temporarily spilled to disk</span></span><br><span class="line"><span class="keyword">private</span>[<span class="keyword">this</span>] <span class="keyword">val</span> compressShuffleSpill = conf.getBoolean(<span class="string">&quot;spark.shuffle.spill.compress&quot;</span>, <span class="literal">true</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">lazy</span> <span class="keyword">val</span> compressionCodec: <span class="type">CompressionCodec</span> = <span class="type">CompressionCodec</span>.createCodec(conf)  <span class="comment">// lz4</span></span><br></pre></td></tr></table></figure><p>它提供了对Block输入输出的压缩和加密，对Block输出流的序列化以及输入流的反序列化等方法</p><h3 id="BroadcastManager"><a href="#BroadcastManager" class="headerlink" title="BroadcastManager"></a>BroadcastManager</h3><p>Broadcast在实例化时就会调用自身的initialize方法，通过initialized属性判断TorrentBroadcastFactory是否已经实例化，另外两个成员方法分别是newBroadcast用来创建TorrentBroadcast实例，unbroadcast用来取消广播变量。</p><p>调用TorrentBroadcastFactory的newBroadcast方法时就会调用它的writeBlocks方法，它将需要广播的数据使用BlockManager写到Driver和Executor上。需要读取广播中的数据调用getValue方法，它调用私有的readBroadcastBlock方法。</p><p>writeBlocks</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">writeBlocks</span></span>(value: <span class="type">T</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">  <span class="keyword">import</span> <span class="type">StorageLevel</span>._</span><br><span class="line">  <span class="comment">// Store a copy of the broadcast variable in the driver so that tasks run on the driver</span></span><br><span class="line">  <span class="comment">// do not create a duplicate copy of the broadcast variable&#x27;s value.</span></span><br><span class="line">  <span class="keyword">val</span> blockManager = <span class="type">SparkEnv</span>.get.blockManager</span><br><span class="line">  <span class="comment">// 将广播对象写入本地存储体系</span></span><br><span class="line">  <span class="keyword">if</span> (!blockManager.putSingle(broadcastId, value, <span class="type">MEMORY_AND_DISK</span>, tellMaster = <span class="literal">false</span>)) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s&quot;Failed to store <span class="subst">$broadcastId</span> in BlockManager&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 将广播对象转换为多个块，块大小默认为4M</span></span><br><span class="line">  <span class="keyword">val</span> blocks =</span><br><span class="line">    <span class="type">TorrentBroadcast</span>.blockifyObject(value, blockSize, <span class="type">SparkEnv</span>.get.serializer, compressionCodec)</span><br><span class="line">  <span class="keyword">if</span> (checksumEnabled) &#123;</span><br><span class="line">    checksums = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](blocks.length)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 遍历每一个块</span></span><br><span class="line">  blocks.zipWithIndex.foreach &#123; <span class="keyword">case</span> (block, i) =&gt;</span><br><span class="line">    <span class="keyword">if</span> (checksumEnabled) &#123;</span><br><span class="line">      checksums(i) = calcChecksum(block)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> pieceId = <span class="type">BroadcastBlockId</span>(id, <span class="string">&quot;piece&quot;</span> + i)</span><br><span class="line">    <span class="keyword">val</span> bytes = <span class="keyword">new</span> <span class="type">ChunkedByteBuffer</span>(block.duplicate())</span><br><span class="line">    <span class="comment">// 以序列化的方式将块写入driver和executor</span></span><br><span class="line">    <span class="keyword">if</span> (!blockManager.putBytes(pieceId, bytes, <span class="type">MEMORY_AND_DISK_SER</span>, tellMaster = <span class="literal">true</span>)) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s&quot;Failed to store <span class="subst">$pieceId</span> of <span class="subst">$broadcastId</span> in local BlockManager&quot;</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  blocks.length</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>readBroadcastBlock</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">readBroadcastBlock</span></span>(): <span class="type">T</span> = <span class="type">Utils</span>.tryOrIOException &#123;</span><br><span class="line">  <span class="type">TorrentBroadcast</span>.synchronized &#123;</span><br><span class="line">    setConf(<span class="type">SparkEnv</span>.get.conf)</span><br><span class="line">    <span class="keyword">val</span> blockManager = <span class="type">SparkEnv</span>.get.blockManager</span><br><span class="line">    <span class="comment">// 首先从本地找广播变量</span></span><br><span class="line">    blockManager.getLocalValues(broadcastId).map(_.data.next()) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(x) =&gt;</span><br><span class="line">        <span class="comment">// 如果可以获取到则加锁并返回该对象</span></span><br><span class="line">        releaseLock(broadcastId)</span><br><span class="line">        x.asInstanceOf[<span class="type">T</span>]</span><br><span class="line"></span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        logInfo(<span class="string">&quot;Started reading broadcast variable &quot;</span> + id)</span><br><span class="line">        <span class="keyword">val</span> startTimeMs = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">        <span class="comment">// 本地获取不到则到driver和executor获取块</span></span><br><span class="line">        <span class="keyword">val</span> blocks = readBlocks().flatMap(_.getChunks())</span><br><span class="line">        logInfo(<span class="string">&quot;Reading broadcast variable &quot;</span> + id + <span class="string">&quot; took&quot;</span> + <span class="type">Utils</span>.getUsedTimeMs(startTimeMs))</span><br><span class="line">        <span class="comment">// 将获取到的块再转换回原来的对象</span></span><br><span class="line">        <span class="keyword">val</span> obj = <span class="type">TorrentBroadcast</span>.unBlockifyObject[<span class="type">T</span>](</span><br><span class="line">          blocks, <span class="type">SparkEnv</span>.get.serializer, compressionCodec)</span><br><span class="line">        <span class="comment">// Store the merged copy in BlockManager so other tasks on this executor don&#x27;t</span></span><br><span class="line">        <span class="comment">// need to re-fetch it.</span></span><br><span class="line">        <span class="keyword">val</span> storageLevel = <span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK</span></span><br><span class="line">        <span class="comment">// 最后再将原来的对象写入到本地的存储体系</span></span><br><span class="line">        <span class="keyword">if</span> (!blockManager.putSingle(broadcastId, obj, storageLevel, tellMaster = <span class="literal">false</span>)) &#123;</span><br><span class="line">          <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s&quot;Failed to store <span class="subst">$broadcastId</span> in BlockManager&quot;</span>)</span><br><span class="line">        &#125;</span><br><span class="line">        obj</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><div align=center><img src="readBroadcast.jpg"></div><h3 id="MapOutputTracker"><a href="#MapOutputTracker" class="headerlink" title="MapOutputTracker"></a>MapOutputTracker</h3><p>mapOutputTracker用于跟踪map任务的输出状态，此状态便于reduce任务定位map输出结果所在的节点地址，进而获取中间输出结果。每个map任务或者reduce任务都会有其唯一标识，分别为mapId和reduceId。每个reduce任务的输入可能是多个map任务的输出，reduce会到各个map任务所在的节点上拉取Block，这一过程叫做Shuffle。每次Shuffle都有唯一的标识shuffleId。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 注册或查找MapOutputTrackerEndpoint</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">registerOrLookupEndpoint</span></span>(</span><br><span class="line">    name: <span class="type">String</span>, endpointCreator: =&gt; <span class="type">RpcEndpoint</span>):</span><br><span class="line">  <span class="type">RpcEndpointRef</span> = &#123;</span><br><span class="line">  <span class="keyword">if</span> (isDriver) &#123;</span><br><span class="line">    logInfo(<span class="string">&quot;Registering &quot;</span> + name)</span><br><span class="line">    rpcEnv.setupEndpoint(name, endpointCreator)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="type">RpcUtils</span>.makeDriverRef(name, conf, rpcEnv)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// Driver则创建MapOutputTrackerMaster，Executor则创建MapOutputTrackerWorker</span></span><br><span class="line"><span class="keyword">val</span> mapOutputTracker = <span class="keyword">if</span> (isDriver) &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapOutputTrackerMaster</span>(conf, broadcastManager, isLocal)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapOutputTrackerWorker</span>(conf)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// trackerEndpoint属性持有MapOutputTrackerMasterEndpoint</span></span><br><span class="line">mapOutputTracker.trackerEndpoint = registerOrLookupEndpoint(<span class="type">MapOutputTracker</span>.<span class="type">ENDPOINT_NAME</span>,</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapOutputTrackerMasterEndpoint</span>(</span><br><span class="line">    rpcEnv, mapOutputTracker.asInstanceOf[<span class="type">MapOutputTrackerMaster</span>], conf))</span><br></pre></td></tr></table></figure><p>可以看到针对当前实例是Driver还是Executor，创建mapOutputTracker的方式有所不同。</p><ul><li>如果当前应用程序是Driver，则创建MapOutputTrackerMaster，然后创建MapOutputTrackerMasterEndpoint，并且注册到Dispatcher中，注册名为MapOutputTracker。</li><li>如果当前应用程序是Executor，则创建MapOutputTrackerWorker，并从远端Driver实例的NettyRpcEnv的Dispatcher中查找MapOutputTrackerMasterEndpoint的引用。<br>无论是Driver还是Executor，最后都由mapOutputTracker的属性trackerEndpoint持有MapOutputTrackerMasterEndpoint的引用</li></ul><p>重点看一下MapOutputTracker中获取map状态信息的方法getStatuses</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">getStatuses</span></span>(shuffleId: <span class="type">Int</span>): <span class="type">Array</span>[<span class="type">MapStatus</span>] = &#123;</span><br><span class="line">  <span class="keyword">val</span> statuses = mapStatuses.get(shuffleId).orNull</span><br><span class="line">  <span class="comment">// 从当前MapOutputTracker的mapstatuses缓存中获取MapStatus数组，有的话直接返回</span></span><br><span class="line">  <span class="keyword">if</span> (statuses == <span class="literal">null</span>) &#123;</span><br><span class="line">    logInfo(<span class="string">&quot;Don&#x27;t have map outputs for shuffle &quot;</span> + shuffleId + <span class="string">&quot;, fetching them&quot;</span>)</span><br><span class="line">    <span class="keyword">val</span> startTime = <span class="type">System</span>.currentTimeMillis</span><br><span class="line">    <span class="keyword">var</span> fetchedStatuses: <span class="type">Array</span>[<span class="type">MapStatus</span>] = <span class="literal">null</span></span><br><span class="line">    <span class="comment">// 因为可能有多个线程获取，所以必须设为线程安全的操作</span></span><br><span class="line">    fetching.synchronized &#123;</span><br><span class="line">      <span class="comment">// fetching存储正在获取map信息的shuffleId，说明其他线程正在获取，当前线程等待即可</span></span><br><span class="line">      <span class="keyword">while</span> (fetching.contains(shuffleId)) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          fetching.wait()</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> e: <span class="type">InterruptedException</span> =&gt;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line"></span><br><span class="line">      <span class="comment">// 这里有两种情况，第一种情况是第一个进来的线程走到这并把shuffleId放入fetching；</span></span><br><span class="line">      <span class="comment">// 第二种情况是后来的线程在线程解除等待后走到这，此时fetchedStatuses可能已被第一个线程放入对应的map信息</span></span><br><span class="line">      fetchedStatuses = mapStatuses.get(shuffleId).orNull</span><br><span class="line">      <span class="keyword">if</span> (fetchedStatuses == <span class="literal">null</span>) &#123;</span><br><span class="line">        fetching += shuffleId</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 第一个线程执行里面的操作</span></span><br><span class="line">    <span class="keyword">if</span> (fetchedStatuses == <span class="literal">null</span>) &#123;</span><br><span class="line">      logInfo(<span class="string">&quot;Doing the fetch; tracker endpoint = &quot;</span> + trackerEndpoint)</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        <span class="comment">// askTracker向MapOutputTrackerMasterEndpoint发送GetMapOutputStatuses消息，以获取map任务的状态信息</span></span><br><span class="line">        <span class="keyword">val</span> fetchedBytes = askTracker[<span class="type">Array</span>[<span class="type">Byte</span>]](<span class="type">GetMapOutputStatuses</span>(shuffleId))</span><br><span class="line">        <span class="comment">// 请求方接收到map任务状态信息后，调用MapOutputTracker的deserializeMapStatuses方法对map任务状态进行反序列化操作，然后放入本地的mapStatuses缓存中</span></span><br><span class="line">        fetchedStatuses = <span class="type">MapOutputTracker</span>.deserializeMapStatuses(fetchedBytes)</span><br><span class="line">        logInfo(<span class="string">&quot;Got the output locations&quot;</span>)</span><br><span class="line">        mapStatuses.put(shuffleId, fetchedStatuses)</span><br><span class="line">      &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">        <span class="comment">// 不管这次是否获取到map信息，都需要把shuffleId从fetching移除并唤醒其他正在等待的线程，如果获取失败后续线程会继续执行上面的操作</span></span><br><span class="line">        fetching.synchronized &#123;</span><br><span class="line">          fetching -= shuffleId</span><br><span class="line">          fetching.notifyAll()</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    logDebug(<span class="string">s&quot;Fetching map output statuses for shuffle <span class="subst">$shuffleId</span> took &quot;</span> +</span><br><span class="line">      <span class="string">s&quot;<span class="subst">$&#123;System.currentTimeMillis - startTime&#125;</span> ms&quot;</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 如果获取到的话就直接返回</span></span><br><span class="line">    <span class="keyword">if</span> (fetchedStatuses != <span class="literal">null</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> fetchedStatuses</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      logError(<span class="string">&quot;Missing all output locations for shuffle &quot;</span> + shuffleId)</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">MetadataFetchFailedException</span>(</span><br><span class="line">        shuffleId, <span class="number">-1</span>, <span class="string">&quot;Missing all output locations for shuffle &quot;</span> + shuffleId)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 后续线程直接从当前MapOuputTracker的mapStatuses缓存中获取对应的map信息</span></span><br><span class="line">    <span class="keyword">return</span> statuses</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="MapOutputTrackerMaster"><a href="#MapOutputTrackerMaster" class="headerlink" title="MapOutputTrackerMaster"></a>MapOutputTrackerMaster</h4><p>接下来重点看一下MapOutputTrackerMaster，它提供了获取map信息的具体实现方法，也就是<code>askTracker[Array[Byte]](GetMapOutputStatuses(shuffleId))</code>的具体实现</p><p>MapOutputTrackerMaster的几个重要属性：</p><ul><li>mapStatuses：用于存储shuffleId与Array[MapStatus]的映射关系。由于MapStatus维护了map输出Block的地址BlockManagerId，所以reduce任务知道从何处获取map任务的中间输出</li><li>cachedSerializedStatuses：用于存储shuffleId与序列化后的状态的映射关系。其中key对应shuffleId, value为对MapStatus序列化后的字节数组。</li><li>mapOutputRequests：使用阻塞队列来缓存GetMapOutputMessage（获取map任务输出）的请求</li><li>threadpool：用于获取map输出的固定大小的线程池。此线程池提交的线程都以后台线程运行，且线程名以map-output-dispatcher为前缀，线程池大小可以使用spark.shuffle.mapOutput.dispatcher.numThreads属性配置，默认大小为8</li></ul><p>在创建MapOutputTrackerMaster的最后，会创建对map输出请求进行处理的线程池threadpool，它里面有固定数量的线程处理Executor发送过来的获取map信息的请求</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">val</span> threadpool: <span class="type">ThreadPoolExecutor</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> numThreads = conf.getInt(<span class="string">&quot;spark.shuffle.mapOutput.dispatcher.numThreads&quot;</span>, <span class="number">8</span>)</span><br><span class="line">  <span class="keyword">val</span> pool = <span class="type">ThreadUtils</span>.newDaemonFixedThreadPool(numThreads, <span class="string">&quot;map-output-dispatcher&quot;</span>)</span><br><span class="line">  <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until numThreads) &#123;</span><br><span class="line">    <span class="comment">// 启动与线程池大小相同数量的线程，每个线程执行的任务都是MessageLoop</span></span><br><span class="line">    pool.execute(<span class="keyword">new</span> <span class="type">MessageLoop</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  pool</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// MessageLoop实现了Java的Runnable接口</span></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">class</span> <span class="title">MessageLoop</span> <span class="keyword">extends</span> <span class="title">Runnable</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">run</span></span>(): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">try</span> &#123;</span><br><span class="line">      <span class="keyword">while</span> (<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">          <span class="comment">// 从mapOutputRequests中获取GetMapOutputMessage</span></span><br><span class="line">          <span class="comment">// 由于mapOutputRequests是个阻塞队列，所以当mapOutputRequests中没有GetMapOutputMessage时，MessageLoop线程会被阻塞</span></span><br><span class="line">          <span class="comment">// GetMapOutputMessage是个样例类，包含了shuffleId和RpcCallContext两个属性</span></span><br><span class="line">          <span class="keyword">val</span> data = mapOutputRequests.take()</span><br><span class="line">          <span class="comment">// 如果取到的GetMapOutputMessage是PoisonPill（“毒药”），那么此MessageLoop线程将退出（通过return语句）</span></span><br><span class="line">          <span class="comment">// 随后将PoisonPill重新放入到mapOutput-Requests中，这是因为threadpool线程池极有可能不止一个MessageLoop线程，为了让大家都“毒发身亡”，还需要把“毒药”放回到receivers中，这样其他“活着”的线程就会再次误食“毒药”，达到所有MessageLoop线程都结束的效果</span></span><br><span class="line">           <span class="keyword">if</span> (data == <span class="type">PoisonPill</span>) &#123;</span><br><span class="line">            mapOutputRequests.offer(<span class="type">PoisonPill</span>)</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">          &#125;</span><br><span class="line">          <span class="keyword">val</span> context = data.context</span><br><span class="line">          <span class="keyword">val</span> shuffleId = data.shuffleId</span><br><span class="line">          <span class="keyword">val</span> hostPort = context.senderAddress.hostPort</span><br><span class="line">          logDebug(<span class="string">&quot;Handling request to send map output locations for shuffle &quot;</span> + shuffleId +</span><br><span class="line">            <span class="string">&quot; to &quot;</span> + hostPort)</span><br><span class="line">          <span class="comment">// 调用getSerializedMapOutputStatuses方法获取GetMapOutputMessage携带的shuffleId所对应的序列化任务状态信息</span></span><br><span class="line">          <span class="keyword">val</span> mapOutputStatuses = getSerializedMapOutputStatuses(shuffleId)</span><br><span class="line">          <span class="comment">// 调用RpcCallContext的回调方法reply，将序列化的map任务状态信息返回给客户端（即其他节点的Executor）</span></span><br><span class="line">          context.reply(mapOutputStatuses)</span><br><span class="line">        &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="type">NonFatal</span>(e) =&gt; logError(e.getMessage, e)</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> ie: <span class="type">InterruptedException</span> =&gt; <span class="comment">// exit</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>最后主要通过调用MapOutputTrackerMaster的post方法将GetMapOutputMessage放入mapOutputRequests</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">post</span></span>(message: <span class="type">GetMapOutputMessage</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  mapOutputRequests.offer(message)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>MapOutputTrackerMaster的运行原理</p><div align=center><img src="MapOutputTrackerMaster.jpg"></div><ol><li>表示某个Executor调用MapOutputTrackerWorker的getStatuses方法获取某个shuffle的map任务状态信息，当发现本地的mapStatuses没有相应的缓存，则调用askTracker方法发送GetMapOutputStatuses消息。askTracker实际是通过MapOutputTrackerMasterEndpoint的NettyRpcEndpointRef向远端发送GetMapOutputStatuses消息。发送实际依托于NettyRpcEndpointRef持有的TransportClient。MapOutputTrackerMasterEndpoint在接收到GetMapOutputStatuses消息后，将GetMapOutputMessage消息放入mapOutput Requests队尾</li><li>表示MessageLoop线程从mapOutputRequests队头取出GetMapOutputMessage</li><li>表示从shuffleIdLocks数组中取出与当前GetMapOutputMessage携带的shuffleId相对应的锁</li><li>表示首先从cachedSerializedStatuses缓存中获取shuffleId对应的序列化任务状态信息</li><li>表示当cachedSerializedStatuses中没有shuffleId对应的序列化任务状态信息，则获取mapStatuses中缓存的shuffleId对应的任务状态数组</li><li>表示将任务状态数组进行序列化，然后使用BroadcastManager对序列化的任务状态进行广播</li><li>表示将序列化的任务状态放入cachedSerializedStatuses缓存中</li><li>表示将广播对象放入cachedSerializedBroadcast缓存中</li><li>表示将获得的序列化任务状态信息，通过回调GetMapOutputMessage消息携带的RpcCallContext的reply方法回复客户端</li></ol><h4 id="Shuffle注册"><a href="#Shuffle注册" class="headerlink" title="Shuffle注册"></a>Shuffle注册</h4><p>DAGScheduler在创建ShuffleMapStage的时候，将调用Map-OutputTrackerMaster的containsShuffle方法，查看是否已经存在shuffleId对应的MapStatus。如果MapOutputTrackerMaster中未注册此shuffleId，那么调用MapOutput-TrackerMaster的registerShuffle方法注册shuffleId</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 查询ShuffleId是否被跟踪</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">containsShuffle</span></span>(shuffleId: <span class="type">Int</span>): <span class="type">Boolean</span> = &#123;</span><br><span class="line">  cachedSerializedStatuses.contains(shuffleId) || mapStatuses.contains(shuffleId)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 注册一个MapStatuses为空的ShuffleId</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">registerShuffle</span></span>(shuffleId: <span class="type">Int</span>, numMaps: <span class="type">Int</span>) &#123;</span><br><span class="line">  <span class="keyword">if</span> (mapStatuses.put(shuffleId, <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">MapStatus</span>](numMaps)).isDefined) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">IllegalArgumentException</span>(<span class="string">&quot;Shuffle ID &quot;</span> + shuffleId + <span class="string">&quot; registered twice&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// add in advance</span></span><br><span class="line">  shuffleIdLocks.putIfAbsent(shuffleId, <span class="keyword">new</span> <span class="type">Object</span>())</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// </span></span><br></pre></td></tr></table></figure><p>当ShuffleMapStage内的所有ShuffleMapTask运行成功后，将调用MapOutputTrackerMaster的registerMapOutputs方法。registerMapOutputs方法将把ShuffleMapStage中每个Shuffle-MapTask的MapStatus保存到shuffleId在mapStatuses中对应的数组中</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">registerMapOutputs</span></span>(shuffleId: <span class="type">Int</span>, statuses: <span class="type">Array</span>[<span class="type">MapStatus</span>], changeEpoch: <span class="type">Boolean</span> = <span class="literal">false</span>) &#123;</span><br><span class="line">  mapStatuses.put(shuffleId, statuses.clone())</span><br><span class="line">  <span class="keyword">if</span> (changeEpoch) &#123;</span><br><span class="line">    incrementEpoch()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="构建存储体系"><a href="#构建存储体系" class="headerlink" title="构建存储体系"></a>构建存储体系</h3><p>存储体系。存储体系中最重要的组件包括Shuffle管理器ShuffleManager、内存管理器MemoryManager、块传输服务BlockTransferService、对所有BlockManager进行管理的BlockManagerMaster、磁盘块管理器DiskBlockManager、块锁管理器BlockInfoManager及块管理器BlockManager，这里只看各个组件的实例化，详细的内容在其他文章中说明</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> shortShuffleMgrNames = <span class="type">Map</span>(</span><br><span class="line">  <span class="string">&quot;sort&quot;</span> -&gt; classOf[org.apache.spark.shuffle.sort.<span class="type">SortShuffleManager</span>].getName,</span><br><span class="line">  <span class="string">&quot;tungsten-sort&quot;</span> -&gt; classOf[org.apache.spark.shuffle.sort.<span class="type">SortShuffleManager</span>].getName)</span><br><span class="line"><span class="keyword">val</span> shuffleMgrName = conf.get(<span class="string">&quot;spark.shuffle.manager&quot;</span>, <span class="string">&quot;sort&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> shuffleMgrClass = shortShuffleMgrNames.getOrElse(shuffleMgrName.toLowerCase, shuffleMgrName)</span><br><span class="line"><span class="keyword">val</span> shuffleManager = instantiateClass[<span class="type">ShuffleManager</span>](shuffleMgrClass)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> useLegacyMemoryManager = conf.getBoolean(<span class="string">&quot;spark.memory.useLegacyMode&quot;</span>, <span class="literal">false</span>)</span><br><span class="line"><span class="keyword">val</span> memoryManager: <span class="type">MemoryManager</span> =</span><br><span class="line">  <span class="keyword">if</span> (useLegacyMemoryManager) &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">StaticMemoryManager</span>(conf, numUsableCores)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="type">UnifiedMemoryManager</span>(conf, numUsableCores)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> blockManagerPort = <span class="keyword">if</span> (isDriver) &#123;</span><br><span class="line">  conf.get(<span class="type">DRIVER_BLOCK_MANAGER_PORT</span>)</span><br><span class="line">&#125; <span class="keyword">else</span> &#123;</span><br><span class="line">  conf.get(<span class="type">BLOCK_MANAGER_PORT</span>)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> blockTransferService =</span><br><span class="line">  <span class="keyword">new</span> <span class="type">NettyBlockTransferService</span>(conf, securityManager, bindAddress, advertiseAddress,</span><br><span class="line">    blockManagerPort, numUsableCores)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> blockManagerMaster = <span class="keyword">new</span> <span class="type">BlockManagerMaster</span>(registerOrLookupEndpoint(</span><br><span class="line">  <span class="type">BlockManagerMaster</span>.<span class="type">DRIVER_ENDPOINT_NAME</span>,</span><br><span class="line">  <span class="keyword">new</span> <span class="type">BlockManagerMasterEndpoint</span>(rpcEnv, isLocal, conf, listenerBus)),</span><br><span class="line">  conf, isDriver)</span><br><span class="line"></span><br><span class="line"><span class="comment">// NB: blockManager is not valid until initialize() is called later.</span></span><br><span class="line"><span class="keyword">val</span> blockManager = <span class="keyword">new</span> <span class="type">BlockManager</span>(executorId, rpcEnv, blockManagerMaster,</span><br><span class="line">  serializerManager, conf, memoryManager, mapOutputTracker, shuffleManager,</span><br><span class="line">  blockTransferService, securityManager, numUsableCores)</span><br></pre></td></tr></table></figure><ol><li>根据spark.shuffle.manager属性，实例化ShuffleManager。Spark2.x.x版本提供了sort和tungsten-sort两种ShuffleManager的实现。无论是sort还是tungsten-sort，其实现类都是SortShuffleManager</li><li>MemoryManager的主要实现有StaticMemoryManager和UnifiedMemoryManager。StaticMemoryManager是Spark早期版本遗留下来的内存管理器实现，可以配置spark.memory.useLegacyMode属性来指定，该属性默认为false，因此默认的内存管理器是UnifiedMemoryManager</li><li>获取当前SparkEnv的块传输服务BlockTransferService对外提供的端口号。如果当前实例是Driver，则从SparkConf中获取由常量DRIVER_BLOCK_MANAGER_PORT指定的端口。如果当前实例是Executor，则从SparkConf中获取由常量BLOCK_MANAGER_PORT指定的端口</li><li>创建块传输服务BlockTransferService。这里使用的是BlockTransferService的子类NettyBlockTransferService, NettyBlockTransferService将提供对外的块传输服务。也正是因为MapOutputTracker与NettyBlockTransferService的配合，才实现了Spark的Shuffle</li><li>查找或注册BlockManagerMasterEndpoint，这里和MapOutputTracker处理方式相同</li></ol>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Oozie-入门基础</title>
      <link href="/2022/04/20/Software/Oozie-%E5%85%A5%E9%97%A8%E5%9F%BA%E7%A1%80/"/>
      <url>/2022/04/20/Software/Oozie-%E5%85%A5%E9%97%A8%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<p>Oozie基础命令和Workflow以及Coordinator的简单配置</p><span id="more"></span><h2 id="Command"><a href="#Command" class="headerlink" title="Command"></a>Command</h2><p>oozie常用命令</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">启动oozie</span></span><br><span class="line">bin/oozied.sh start</span><br><span class="line"><span class="meta"># </span><span class="language-bash">停止oozie</span></span><br><span class="line">bin/oozied.sh stop</span><br><span class="line"><span class="meta"># </span><span class="language-bash">访问页面</span></span><br><span class="line">curl http://localhost:11000/oozie/</span><br><span class="line"><span class="meta"># </span><span class="language-bash">提交并运行作业，job.properties在本地，全局配置文件，可在workflow和coordinator中使用</span></span><br><span class="line">oozie job -oozie http://localhost:11000/oozie -config job.properties -run</span><br><span class="line"><span class="meta"># </span><span class="language-bash">杀死作业，<span class="built_in">kill</span>后接jobID</span></span><br><span class="line">oozie job -oozie http://localhost:11000/oozie -kill 0000411-180116183039102-oozie-hado-W</span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看作业状态</span></span><br><span class="line">oozie job -oozie http://localhost:11000/oozie -info 0000411-180116183039102-oozie-hado-W</span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看日志</span></span><br><span class="line">oozie job -oozie http://localhost:11000/oozie -log 0000411-180116183039102-oozie-hado-W</span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看配置文件</span></span><br><span class="line">oozie job -oozie http://localhost:11000/oozie -configcontent 0000411-180116183039102-oozie-hado-W</span><br><span class="line"><span class="meta"># </span><span class="language-bash">重跑workflow任务</span></span><br><span class="line">oozie job -oozie http://localhost:11000/oozie -rerun 0000411-180116183039102-oozie-hado-W -config workflow.xml</span><br><span class="line"><span class="meta"># </span><span class="language-bash">重跑coordinator任务，action后跟coordinator每次任务<span class="built_in">id</span>中@后的数字</span></span><br><span class="line">oozie job -oozie http://localhost:11000/oozie -rerun 0000411-180116183039102-oozie-hado-C -refresh -action 1</span><br><span class="line"><span class="meta"># </span><span class="language-bash">检查workflow是否合规</span></span><br><span class="line">oozie validate -oozie http://localhost:11000/oozie myApp/workflow.xml</span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看所有任务</span></span><br><span class="line">oozie jobs -oozie http://localhost:11000/oozie</span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看coordinator任务</span></span><br><span class="line">oozie jobs -oozie http://localhost:11000/oozie -jobtype coordinator</span><br><span class="line"><span class="meta"># </span><span class="language-bash">列出所有的ShareLib信息</span></span><br><span class="line">oozie admin -oozie http://localhost:11000/oozie -shareliblist</span><br><span class="line"><span class="meta"># </span><span class="language-bash">列出指定的ShareLib服务信息</span></span><br><span class="line">oozie admin -oozie http://localhost:11000/oozie -shareliblist spark</span><br><span class="line"><span class="meta"># </span><span class="language-bash">自动切换到最新的ShareLib目录</span></span><br><span class="line">oozie admin -oozie http://localhost:11000/oozie -sharelibupdate</span><br></pre></td></tr></table></figure><h2 id="Workflow"><a href="#Workflow" class="headerlink" title="Workflow"></a>Workflow</h2><h3 id="Control-flow-nodes"><a href="#Control-flow-nodes" class="headerlink" title="Control flow nodes"></a>Control flow nodes</h3><p>流程控制节点分为两类，一类是start、kill、end等定义流程开始结束的节点；一类是decision、fork、join等提供控制流程执行路径的节点<br>配置文件所有的变量都可以通过命令行提交任务时指定<code>-D name_node=hdfs://localhost:8020</code>传入<br>job-tracker和name-node标签分别对应yarn的server地址和hdfs namenode的地址</p><ol><li>start、kill、end节点<br>start和end是必须指定的节点，end标识流程success，kill标识流程failed<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">name</span>=<span class="string">&quot;foo-wf&quot;</span> <span class="attr">xmlns</span>=<span class="string">&quot;uri:oozie:workflow:0.1&quot;</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 流程启动时执行的node名称 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">start</span> <span class="attr">to</span>=<span class="string">&quot;firstHadoopJob&quot;</span>/&gt;</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">&lt;!-- 流程执行失败时指向kill node，那么oozie就会标记流程失败，不会再执行后续的流程 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">kill</span> <span class="attr">name</span>=<span class="string">&quot;kill&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">message</span>&gt;</span>Action failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]<span class="tag">&lt;/<span class="name">message</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">kill</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指向end node表示流程执行成功从而执行下一个流程 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">end</span> <span class="attr">name</span>=<span class="string">&quot;end&quot;</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li>decision<br>相当于case when语法，条件返回true时执行对应指定的node，否则就会执行default默认的node<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">name</span>=<span class="string">&quot;foo-wf&quot;</span> <span class="attr">xmlns</span>=<span class="string">&quot;uri:oozie:workflow:0.1&quot;</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="tag">&lt;<span class="name">decision</span> <span class="attr">name</span>=<span class="string">&quot;mydecision&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">switch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">case</span> <span class="attr">to</span>=<span class="string">&quot;reconsolidatejob&quot;</span>&gt;</span></span><br><span class="line">              $&#123;fs:fileSize(secondjobOutputDir) gt 10 * GB&#125;</span><br><span class="line">            <span class="tag">&lt;/<span class="name">case</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">case</span> <span class="attr">to</span>=<span class="string">&quot;rexpandjob&quot;</span>&gt;</span></span><br><span class="line">              $&#123;fs:filSize(secondjobOutputDir) lt 100 * MB&#125;</span><br><span class="line">            <span class="tag">&lt;/<span class="name">case</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">case</span> <span class="attr">to</span>=<span class="string">&quot;recomputejob&quot;</span>&gt;</span></span><br><span class="line">              $&#123; hadoop:counters(&#x27;secondjob&#x27;)[RECORDS][REDUCE_OUT] lt 1000000 &#125;</span><br><span class="line">            <span class="tag">&lt;/<span class="name">case</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">default</span> <span class="attr">to</span>=<span class="string">&quot;end&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">switch</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">decision</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li>fork、join<br>fork和join必须同时使用，fork指定可以并行执行的流程，所有流程执行完成后汇聚到join才会执行下一个流程，可以多个fork和join嵌套使用。下面的例子就是firstparalleljob和secondparalleljob同时执行，都执行成功后汇聚到joining再执行nextaction<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">name</span>=<span class="string">&quot;sample-wf&quot;</span> <span class="attr">xmlns</span>=<span class="string">&quot;uri:oozie:workflow:0.1&quot;</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="tag">&lt;<span class="name">fork</span> <span class="attr">name</span>=<span class="string">&quot;forking&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">path</span> <span class="attr">start</span>=<span class="string">&quot;firstparalleljob&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">path</span> <span class="attr">start</span>=<span class="string">&quot;secondparalleljob&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">fork</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">&quot;firstparallejob&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">map-reduce</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">job-tracker</span>&gt;</span>foo:8021<span class="tag">&lt;/<span class="name">job-tracker</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name-node</span>&gt;</span>bar:8020<span class="tag">&lt;/<span class="name">name-node</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">job-xml</span>&gt;</span>job1.xml<span class="tag">&lt;/<span class="name">job-xml</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">map-reduce</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">&quot;joining&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">&quot;kill&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">&quot;secondparalleljob&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">map-reduce</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">job-tracker</span>&gt;</span>foo:8021<span class="tag">&lt;/<span class="name">job-tracker</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name-node</span>&gt;</span>bar:8020<span class="tag">&lt;/<span class="name">name-node</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">job-xml</span>&gt;</span>job2.xml<span class="tag">&lt;/<span class="name">job-xml</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">map-reduce</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">&quot;joining&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">&quot;kill&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">join</span> <span class="attr">name</span>=<span class="string">&quot;joining&quot;</span> <span class="attr">to</span>=<span class="string">&quot;nextaction&quot;</span>/&gt;</span></span><br><span class="line">    ...</span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><h3 id="Action-nodes"><a href="#Action-nodes" class="headerlink" title="Action nodes"></a>Action nodes</h3><p>action node是由oozie分发到不同的yarn节点去完成的，当执行成功返回到ok继续下个流程，当执行失败返回到error执行下个流程，可以指定到kill或者忽略错误正常指定到下个流程<br>oozie提供了失败重试的功能，它会在失败后一定时间内重新执行，间隔的时间和重试次数都需要在外部指定<br>下面选择几种常用的流程配置文件</p><ol><li>HDFS<br>oozie只提供了这几种简单的文件操作命令，configuration.property可以传入hdfs相关的配置，影响下面命令的执行<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">name</span>=<span class="string">&quot;sample-wf&quot;</span> <span class="attr">xmlns</span>=<span class="string">&quot;uri:oozie:workflow:0.5&quot;</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">&quot;hdfscommands&quot;</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">fs</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name-node</span>&gt;</span>hdfs://foo:8020<span class="tag">&lt;/<span class="name">name-node</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>some.property<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>some.value<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">              <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">delete</span> <span class="attr">path</span>=<span class="string">&#x27;/usr/tucu/temp-data&#x27;</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">mkdir</span> <span class="attr">path</span>=<span class="string">&#x27;archives/$&#123;wf:id()&#125;&#x27;</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">move</span> <span class="attr">source</span>=<span class="string">&#x27;$&#123;jobInput&#125;&#x27;</span> <span class="attr">target</span>=<span class="string">&#x27;archives/$&#123;wf:id()&#125;/processed-input&#x27;</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">chmod</span> <span class="attr">path</span>=<span class="string">&#x27;$&#123;jobOutput&#125;&#x27;</span> <span class="attr">permissions</span>=<span class="string">&#x27;-rwxrw-rw-&#x27;</span> <span class="attr">dir-files</span>=<span class="string">&#x27;true&#x27;</span>&gt;</span><span class="tag">&lt;<span class="name">recursive</span>/&gt;</span><span class="tag">&lt;/<span class="name">chmod</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">chgrp</span> <span class="attr">path</span>=<span class="string">&#x27;$&#123;jobOutput&#125;&#x27;</span> <span class="attr">group</span>=<span class="string">&#x27;testgroup&#x27;</span> <span class="attr">dir-files</span>=<span class="string">&#x27;true&#x27;</span>&gt;</span><span class="tag">&lt;<span class="name">recursive</span>/&gt;</span><span class="tag">&lt;/<span class="name">chgrp</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">fs</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">&quot;myotherjob&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">&quot;errorcleanup&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li>SSH<br>如果需要在指定的服务器运行命令那么ssh相对shell是个更好的选择，它会先登录目标服务器再执行命令，前提是可以免密登录<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">name</span>=<span class="string">&quot;sample-wf&quot;</span> <span class="attr">xmlns</span>=<span class="string">&quot;uri:oozie:workflow:0.1&quot;</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">&quot;myssjob&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">ssh</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">host</span>&gt;</span>localhost:22<span class="tag">&lt;<span class="name">host</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">command</span>&gt;</span>chmod<span class="tag">&lt;/<span class="name">command</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">args</span>&gt;</span>-R<span class="tag">&lt;/<span class="name">args</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">args</span>&gt;</span>777<span class="tag">&lt;/<span class="name">args</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">args</span>&gt;</span>/home/root/*<span class="tag">&lt;/<span class="name">args</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">ssh</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">&quot;myotherjob&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">&quot;errorcleanup&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li>Java<br>运行java程序相当于hadoop jar…命令，注意jar包要放在和workflow.xml相同路径的lib文件夹下，或者使用archive指定<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">name</span>=<span class="string">&quot;sample-wf&quot;</span> <span class="attr">xmlns</span>=<span class="string">&quot;uri:oozie:workflow:0.1&quot;</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">&quot;myfirstjavajob&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">java</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">job-tracker</span>&gt;</span>foo:8021<span class="tag">&lt;/<span class="name">job-tracker</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name-node</span>&gt;</span>bar:8020<span class="tag">&lt;/<span class="name">name-node</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">prepare</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">delete</span> <span class="attr">path</span>=<span class="string">&quot;$&#123;jobOutput&#125;&quot;</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">prepare</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.queue.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">value</span>&gt;</span>default<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">main-class</span>&gt;</span>org.apache.oozie.MyFirstMainClass<span class="tag">&lt;/<span class="name">main-class</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">java-opts</span>&gt;</span>-Dblah<span class="tag">&lt;/<span class="name">java-opts</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">arg</span>&gt;</span>argument1<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">arg</span>&gt;</span>argument2<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">java</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">&quot;myotherjob&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">&quot;errorcleanup&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li>Sqoop<br>直接在command标签内写入sqoop后的所有命令和参数即可<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">name</span>=<span class="string">&quot;sample-wf&quot;</span> <span class="attr">xmlns</span>=<span class="string">&quot;uri:oozie:workflow:0.1&quot;</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">&quot;myfirsthivejob&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">sqoop</span> <span class="attr">xmlns</span>=<span class="string">&quot;uri:oozie:sqoop-action:0.2&quot;</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">job-traker</span>&gt;</span>foo:8021<span class="tag">&lt;/<span class="name">job-tracker</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">name-node</span>&gt;</span>bar:8020<span class="tag">&lt;/<span class="name">name-node</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">prepare</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">delete</span> <span class="attr">path</span>=<span class="string">&quot;$&#123;jobOutput&#125;&quot;</span>/&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">prepare</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapred.compress.map.output<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">command</span>&gt;</span>import --connect jdbc:hsqldb:file:db.hsqldb --table TT --target-dir hdfs://localhost:8020/user/tucu/foo -m 1<span class="tag">&lt;/<span class="name">command</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">sqoop</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">&quot;myotherjob&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">&quot;errorcleanup&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li>Spark<br>除了master、node、name和class参数外其它参数都可以放到spark-opts中<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">name</span>=<span class="string">&quot;sample-wf&quot;</span> <span class="attr">xmlns</span>=<span class="string">&quot;uri:oozie:workflow:0.1&quot;</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line">    <span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">&quot;sparkJob&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">spark</span> <span class="attr">xmlns</span>=<span class="string">&quot;uri:oozie:spark-action:0.1&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">job-tracker</span>&gt;</span>$&#123;job_tracker&#125;<span class="tag">&lt;/<span class="name">job-tracker</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name-node</span>&gt;</span>$&#123;name_node&#125;<span class="tag">&lt;/<span class="name">name-node</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">prepare</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">delete</span> <span class="attr">path</span>=<span class="string">&quot;$&#123;output&#125;&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">prepare</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">master</span>&gt;</span>yarn-cluster<span class="tag">&lt;/<span class="name">master</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">mode</span>&gt;</span>cluster<span class="tag">&lt;/<span class="name">mode</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>spark-test<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">class</span>&gt;</span>com.hh.anta.analyzer.jobs.statreport.backbone.LargeIpAnalysis<span class="tag">&lt;/<span class="name">class</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">jar</span>&gt;</span>$&#123;jar_path&#125;<span class="tag">&lt;/<span class="name">jar</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">spark-opts</span>&gt;</span>--num-executors 4 --executor-memory 10g --executor-cores 2 --driver-memory 2g<span class="tag">&lt;/<span class="name">spark-opts</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">arg</span>&gt;</span>$&#123;input&#125;<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">arg</span>&gt;</span>$&#123;output&#125;<span class="tag">&lt;/<span class="name">arg</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">spark</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line">    ...</span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure></li></ol><h2 id="Coordinator"><a href="#Coordinator" class="headerlink" title="Coordinator"></a>Coordinator</h2><p>Coordinator是用来执行Workflow工作流的周期性调度工作，它和workflow一样采用xml配置文件的方式配置工作流，有关它的例子可以查看<a href="https://www.aboutyun.com//forum.php/?mod=viewthread&tid=7721&extra=page=1&page=1&">示例</a></p><h3 id="重要定义"><a href="#重要定义" class="headerlink" title="重要定义"></a>重要定义</h3><ul><li>Actual time实际时间：指某事实际发生的时间</li><li>Nominal time理论时间：指某事理论上发生的时间。理论上它和实际时间应该匹配，但在实际中由于延迟，实际时间可能会发生晚于理论时间</li><li>Dataset数据集：它为工作流提供数据目录，一般是HDFS上的路径。一个Datasets通常有几个Dataset实例，每个实例都可以被单独引用</li><li>Synchronous Dataset同步数据集：它以固定的时间间隔生成，每个时间间隔都有一个关联的数据集实例。同步数据集实例由它们的理论时间标识</li><li>Coordinator Action协调器操作：它是一种工作流，它在满足一组条件时启动(输入Dataset实例可用)</li><li>Coordinator Application协调器应用程序：它定义了创建Coordinator Action的条件(频率)以及启动action的时间。同时还定义了开始时间和结束时间</li><li>Coordinator Job协调作业：它是协调定义的可执行实例。作业提交是通过提交作业配置来完成的，该配置解析应用程序定义中的所有参数</li><li>Data pipeline数据管道：它是一组使用和产生相互依赖的Dataset的Coordinator Application的管道</li><li>Coordinator Definition Language协调器定义语言：用于描述Dataset和Coordinator Application的语言</li><li>Coordinator Engine协调器引擎：执行Coordinator Job的系统</li></ul><h3 id="Control"><a href="#Control" class="headerlink" title="Control"></a>Control</h3><p>定义了一个Coordinator Job的控制信息，主要包括如下三个配置元素</p><ol><li>timeout<br>超时时间，单位为分钟。当一个Coordinator Job启动的时候，会初始化多个Coordinator动作，timeout用来限制这个初始化过程。默认值为-1，表示永远不超时</li><li>concurrency<br>并发数，指多个Coordinator Job并发执行，默认值为1</li><li>execution<br>配置多个Coordinator Job并发执行的策略：默认是FIFO。另外还有两种：LIFO（最新的先执行）、LAST_ONLY（只执行最新的Coordinator Job，其它的全部丢弃）</li></ol><h3 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h3><p>定义了一个hdfs uri，通过频率和开始时间周期性的生成hdfs目录</p><ol><li>name<br>Dataset名称</li><li>frequency<br>频率用来指定同一工作流执行的时间间隔，oozie提供了几种内置的方法用来指定间隔<ul><li><code>$&#123;coord:minutes(int n)&#125;</code>，单位为分钟的时间间隔</li><li><code>$&#123;coord:hours(int n)&#125;</code>，单位为小时的时间间隔</li><li><code>$&#123;coord:days(int n)&#125;</code>，单位为天的时间间隔</li><li><code>$&#123;coord:months(int n)&#125;</code>，单位为月的时间间隔</li><li><code>$&#123;cron syntax&#125;</code>，使用crontab表达式，例如<code>$&#123;0,10 15 * * 2-6&#125;</code></li></ul></li><li>initial-instance<br>工作流启动的初始时间，它会根据初始时间和frequency周期性的执行工作流。注意格式需要跟timezone相匹配</li><li>timezone<br>Oozie的默认处理时区是UTC，一般要将它修改为当地的时区，oozie支持两种方式，分别是<code>GMT[+/-]##:## (eg: GMT+05:30)</code>和<code>Zoneinfo标识(eg: Asia/Shanghai)</code>。而且需要注意oozie中的datatime是要根据timezone来指定的（eg：timezone设为GMT+8:00，那么开始时间的格式就是2019-10-1T00:00+0800）</li><li>uri-template<br>定义的是hdfs上的路径模板，因为是周期性的存储数据，那么保存的目录就应该是动态生成的<br>路径模板由常量和变量组成，常量是你自己提供的，变量是oozie提供的几个时间度量值，用来获取当前的时间，分别是<code>YEAR、MONTH、DAY、HOUR、MINUTE</code>，对应输出的格式为<code>YYYYMMDDHHmm</code></li><li>done-flag<br>工作流完成标识符，用来告诉oozie什么情况下认为当前任务已完成，给它指定不同的值将代表不同的含义：不指定标签时当目录下出现_SUCCESS文件那么标记任务成功；指定标签但值为空时只要当前文件夹存在即为成功；指定标签并且指定值那么只有文件夹下出现指定值的文件时才会标记成功<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">datasets</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">include</span>&gt;</span>hdfs://foo:8020/app/dataset-definitions/globallogs.xml<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">dataset</span> <span class="attr">name</span>=<span class="string">&quot;logs&quot;</span> <span class="attr">frequency</span>=<span class="string">&quot;$&#123;coord:hours(12)&#125;&quot;</span></span></span><br><span class="line"><span class="tag">           <span class="attr">initial-instance</span>=<span class="string">&quot;2009-02-15T08:15Z&quot;</span> <span class="attr">timezone</span>=<span class="string">&quot;Americas/Los_Angeles&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">uri-template</span>&gt;</span></span><br><span class="line">    hdfs://foo:8020/app/logs/$&#123;market&#125;/$&#123;YEAR&#125;$&#123;MONTH&#125;/$&#123;DAY&#125;/$&#123;HOUR&#125;/$&#123;MINUTE&#125;/data</span><br><span class="line">    <span class="tag">&lt;/<span class="name">uri-template</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">done-flag</span>&gt;</span><span class="tag">&lt;/<span class="name">done-flag</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dataset</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">dataset</span> <span class="attr">name</span>=<span class="string">&quot;stats&quot;</span> <span class="attr">frequency</span>=<span class="string">&quot;$&#123;coord:months(1)&#125;&quot;</span></span></span><br><span class="line"><span class="tag">           <span class="attr">initial-instance</span>=<span class="string">&quot;2009-01-10T10:00Z&quot;</span> <span class="attr">timezone</span>=<span class="string">&quot;Americas/Los_Angeles&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">uri-template</span>&gt;</span>hdfs://foo:8020/usr/app/stats/$&#123;YEAR&#125;/$&#123;MONTH&#125;/data<span class="tag">&lt;/<span class="name">uri-template</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">done-flag</span>&gt;</span>over<span class="tag">&lt;/<span class="name">done-flag</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">dataset</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">datasets</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li>input-events<br>coordinator每次执行时的输入文件，可以是指定的一个文件或者多个时间点生成的多个文件<br>一个Coordinator应用的输入事件指定了要执行一个Coordinator动作必须满足的输入条件，只有当input-events对应的所有文件夹都已创建或者生成了done-flag文件，工作流才会进入running状态<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">input-events</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">data-in</span> <span class="attr">name</span>=<span class="string">&quot;BB_INPUT_PREFIX&quot;</span> <span class="attr">dataset</span>=<span class="string">&quot;BbInputPrefix&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">start-instance</span>&gt;</span>$&#123;coord:current(0)&#125;<span class="tag">&lt;/<span class="name">start-instance</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">end-instance</span>&gt;</span>$&#123;coord:current(5)&#125;<span class="tag">&lt;/<span class="name">end-instance</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">data-in</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">data-in</span> <span class="attr">name</span>=<span class="string">&quot;IDC_INPUT_PREFIX&quot;</span> <span class="attr">dataset</span>=<span class="string">&quot;IdcInputPrefix&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">instance</span>&gt;</span>$&#123;coord:current(5)&#125;<span class="tag">&lt;/<span class="name">instance</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">data-in</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">input-events</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li>output-events<br>一个Coordinator动作可能会生成一个或多个dataset实例，这些实例由output-events定义<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">output-events</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">data-out</span> <span class="attr">name</span>=<span class="string">&quot;BB_OUTPUT_PREFIX&quot;</span> <span class="attr">dataset</span>=<span class="string">&quot;BbOutputPrefix&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">instance</span>&gt;</span>$&#123;coord:current(5)&#125;<span class="tag">&lt;/<span class="name">instance</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">data-out</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">data-out</span> <span class="attr">name</span>=<span class="string">&quot;IDC_OUTPUT_PREFIX&quot;</span> <span class="attr">dataset</span>=<span class="string">&quot;IdcOutputPrefix&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">instance</span>&gt;</span>$&#123;coord:current(5)&#125;<span class="tag">&lt;/<span class="name">instance</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">data-out</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">output-events</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li>action<br>定义coordinator需要触发的workflow，其中定义的property可以在workflow中使用，因为是定时任务，所以输入输出目录就是从这里动态生成的<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">action</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">workflow</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">app-path</span>&gt;</span>$&#123;wf_app_path&#125;<span class="tag">&lt;/<span class="name">app-path</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>dayTime<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;coord:formatTime(coord:dateOffset(coord:nominalTime(), -1, &#x27;DAY&#x27;), &#x27;yyyy-MM-dd&#x27;)&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br></pre></td></tr></table></figure>有几个常见的函数需要掌握：<br>${coord:dataIn(String name)}：获取input-events下指定的dataIn对应的所有路径，dataOut作用相同<br>${coord:formatTime(String timeStamp, String format)}：格式化日期，format参数对应的模板是<code>yyyyMMddHHmmss</code>，详细的参考<a href="https://docs.oracle.com/javase/6/docs/api/java/text/SimpleDateFormat.html">官网</a><br>${coord:dateOffset(String baseDate, int instance, String timeUnit)}：偏移指定的时间间隔，baseDate需传入带时区标准的日期时间格式，timeUnit的参数可以是DAY、HOUR、MINUTE<br>${coord:nominalTime()}：Coordinator的开始时间加上整数倍的频率得到的时间，也就是应该运行当前工作流的时间，可以作为formatTime和dateOffset的输入参数<br>${coord:current(int n)}：返回日期时间：从一个Coordinator动作（Action）创建时开始计算，第n个dataset实例执行时间</li></ol><h2 id="Extra"><a href="#Extra" class="headerlink" title="Extra"></a>Extra</h2><h3 id="Expression-Language-Functions"><a href="#Expression-Language-Functions" class="headerlink" title="Expression Language Functions"></a>Expression Language Functions</h3><p>Oozie除了允许使用工作流作业属性来参数化工作流作业外，还提供了一组EL函数，这些函数支持对工作流动作节点和决策节点中的谓词进行更复杂的参数化，还有很多跟hadoop相关的常量和函数查看<a href="https://oozie.apache.org/docs/4.1.0/WorkflowFunctionalSpec.html#a4.2.4_Hadoop_EL_Constants">官网</a></p><ol><li>Basic EL Constants<br>oozie定义了一些常量，可以直接拿来用<ul><li>KB: 1024, one kilobyte.</li><li>MB: 1024 * KB, one megabyte.</li><li>GB: 1024 * MB, one gigabyte.</li><li>TB: 1024 * GB, one terabyte.</li><li>PB: 1024 * TG, one petabyte.</li></ul></li><li>Workflow EL Functions<br>这部分直接可以在workflow配置文件中使用${wf:id()}调用，下面列出了常见的操作，更多属性查看<a href="https://oozie.apache.org/docs/4.1.0/WorkflowFunctionalSpec.html#a4.2.3_Workflow_EL_Functions">官网</a><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">String wf:id()  # 返回当前工作流的ID</span><br><span class="line">String wf:name()  # 返回当前工作流的名称</span><br><span class="line">String wf:appPath()  # 返回当前工作流的路径</span><br><span class="line">String wf:conf(String name)  # 返回当前工作流配置属性的值，如果未定义则返回空字符串</span><br><span class="line">String wf:user()  # 返回启动当前工作流的用户名</span><br><span class="line">String wf:group()  # 返回当前工作流的组/ACL</span><br><span class="line">String wf:callback(String stateVar)  # 返回当前工作流操作节点的回调URL, stateVar可以是该操作的退出状态(=OK=或ERROR)，也可以是执行任务的远程系统用退出状态替换的令牌</span><br><span class="line">String wf:errorMessage(String message)  # 返回指定操作节点的错误消息，如果没有处于error状态的操作节点退出，则返回一个空字符串</span><br></pre></td></tr></table></figure></li></ol><h3 id="Workflow-Lifecycle"><a href="#Workflow-Lifecycle" class="headerlink" title="Workflow Lifecycle"></a>Workflow Lifecycle</h3><p>oozie针对工作流定义了不同的状态，它在运行过程中可以处于任何状态<br>    - PREP:当工作流作业第一次创建时，它将处于PREP状态，定义了工作流但还没有运行<br>    - RUNNING:当创建的工作流启动时，它会进入RUNNING状态，当它没有达到结束状态时就会一直保持在RUNNING状态，直到错误或它被挂起<br>    - SUSPENDED:正在运行的工作流可以挂起，它将一直处于SUSPENDED状态，直到该工作流作业恢复或被终止<br>    - SUCCEEDED:当一个正在运行的工作流任务到达结束节点时，最终状态为SUCCEEDED<br>    - KILLED:当管理员或所有者通过对Oozie的请求杀死一个已创建的、正在运行的或挂起的工作流作业时，工作流将进入KILLED状态<br>    - FAILED:当一个正在运行的工作流作业由于意外错误而失败时，它将以FAILED状态结束</p><h3 id="失败重试"><a href="#失败重试" class="headerlink" title="失败重试"></a>失败重试</h3><p>工作流失败后oozie可以提供失败重试机制，需要指定重试间隔和重试次数，注意开启失败重试必须做好之前失败工作流的数据清理工作<br>    1. 第一步需要修改一个配置文件，默认oozie只会在几个指定的错误发生时失败重试，需要将它修改为只要发生错误就进行失败重试：<code>oozie.service.LiteWorkflowStoreService.user.retry.error.code.ext=ALL</code><br>    2. 第二步是在workflow配置文件中指定重试间隔和重试次数</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">name</span>=<span class="string">&quot;etl_ds_hive2_action-$&#123;etl_name&#125;&quot;</span> <span class="attr">xmlns</span>=<span class="string">&quot;uri:oozie:workflow:0.5&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">start</span> <span class="attr">to</span>=<span class="string">&quot;hive2_action&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">&quot;hive2_action&quot;</span> <span class="attr">cred</span>=<span class="string">&quot;hive2&quot;</span> <span class="attr">retry-max</span>=<span class="string">&quot;3&quot;</span> <span class="attr">retry-interval</span>=<span class="string">&quot;2&quot;</span> &gt;</span>  <span class="comment">&lt;!--指定失败重试次数和失败间隔，单位是分钟--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">ssh</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">host</span>&gt;</span>localhost:22<span class="tag">&lt;<span class="name">host</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">command</span>&gt;</span>chmod<span class="tag">&lt;/<span class="name">command</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">args</span>&gt;</span>-R<span class="tag">&lt;/<span class="name">args</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">args</span>&gt;</span>777<span class="tag">&lt;/<span class="name">args</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">args</span>&gt;</span>/home/root/*<span class="tag">&lt;/<span class="name">args</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">ssh</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">&quot;end&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">&quot;Kill&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">kill</span> <span class="attr">name</span>=<span class="string">&quot;Kill&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">message</span>&gt;</span>Action failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]<span class="tag">&lt;/<span class="name">message</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">kill</span>&gt;</span> </span><br><span class="line">    <span class="tag">&lt;<span class="name">end</span> <span class="attr">name</span>=<span class="string">&quot;end&quot;</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br><span class="line"></span><br></pre></td></tr></table></figure><h3 id="配置可以解析UTC-8时区"><a href="#配置可以解析UTC-8时区" class="headerlink" title="配置可以解析UTC+8时区"></a>配置可以解析UTC+8时区</h3><p> 新安装的oozie默认只可以解析UTC格式的时间，配置后指定开始和结束时间时就可以使用东八区时间了。CDH配置高级中“oozie-site.xml 的 Oozie Server 高级配置代码段（安全阀）”这个选项内添加下面的代码后重启<br> <figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>oozie.processing.timezone<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>GMT+0800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></p><h2 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h2><p>一个小例子，每五分钟执行一次，检测输入路径内的最近三个事件（按dataset的frequency往前数三个事件，如果对应的文件夹存在则满足条件，否则一直等待），满足条件后在指定的输出路径新建文件夹，注意使用fs action时的权限<br>coordinator.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">coordinator-app</span> <span class="attr">name</span>=<span class="string">&quot;coord-test&quot;</span> <span class="attr">frequency</span>=<span class="string">&quot;$&#123;coord:minutes(5)&#125;&quot;</span></span></span><br><span class="line"><span class="tag">     <span class="attr">start</span>=<span class="string">&quot;$&#123;start_time&#125;&quot;</span> <span class="attr">end</span>=<span class="string">&quot;$&#123;end_time&#125;&quot;</span> <span class="attr">timezone</span>=<span class="string">&quot;GMT+08:00&quot;</span></span></span><br><span class="line"><span class="tag">     <span class="attr">xmlns</span>=<span class="string">&quot;uri:oozie:coordinator:0.1&quot;</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">datasets</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">dataset</span> <span class="attr">name</span>=<span class="string">&quot;logs-1&quot;</span> <span class="attr">frequency</span>=<span class="string">&quot;$&#123;coord:minutes(1)&#125;&quot;</span></span></span><br><span class="line"><span class="tag">               <span class="attr">initial-instance</span>=<span class="string">&quot;$&#123;log_start_time&#125;&quot;</span> <span class="attr">timezone</span>=<span class="string">&quot;GMT+08:00&quot;</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">uri-template</span>&gt;</span>$&#123;input_prefix&#125;/$&#123;HOUR&#125;$&#123;MINUTE&#125;<span class="tag">&lt;/<span class="name">uri-template</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">done-flag</span>&gt;</span><span class="tag">&lt;/<span class="name">done-flag</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">dataset</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">dataset</span> <span class="attr">name</span>=<span class="string">&quot;logs-2&quot;</span> <span class="attr">frequency</span>=<span class="string">&quot;$&#123;coord:minutes(5)&#125;&quot;</span></span></span><br><span class="line"><span class="tag">               <span class="attr">initial-instance</span>=<span class="string">&quot;$&#123;log_start_time&#125;&quot;</span> <span class="attr">timezone</span>=<span class="string">&quot;GMT+08:00&quot;</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">uri-template</span>&gt;</span>$&#123;output_prefix&#125;/$&#123;HOUR&#125;$&#123;MINUTE&#125;<span class="tag">&lt;/<span class="name">uri-template</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">done-flag</span>&gt;</span><span class="tag">&lt;/<span class="name">done-flag</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">dataset</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">datasets</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">input-events</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">data-in</span> <span class="attr">name</span>=<span class="string">&quot;input&quot;</span> <span class="attr">dataset</span>=<span class="string">&quot;logs-1&quot;</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">start-instance</span>&gt;</span>$&#123;coord:current(-3)&#125;<span class="tag">&lt;/<span class="name">start-instance</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">end-instance</span>&gt;</span>$&#123;coord:current(0)&#125;<span class="tag">&lt;/<span class="name">end-instance</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">data-in</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">input-events</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">output-events</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">data-out</span> <span class="attr">name</span>=<span class="string">&quot;output&quot;</span> <span class="attr">dataset</span>=<span class="string">&quot;logs-2&quot;</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">instance</span>&gt;</span>$&#123;coord:current(0)&#125;<span class="tag">&lt;/<span class="name">instance</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">data-out</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">output-events</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;<span class="name">action</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">workflow</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">app-path</span>&gt;</span>$&#123;wf_application_path&#125;<span class="tag">&lt;/<span class="name">app-path</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                         <span class="tag">&lt;<span class="name">name</span>&gt;</span>HDFS_INPUT<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                         <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;coord:dataIn(&#x27;input&#x27;)&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                         <span class="tag">&lt;<span class="name">name</span>&gt;</span>HDFS_OUTPUT<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                         <span class="tag">&lt;<span class="name">value</span>&gt;</span>$&#123;coord:dataOut(&#x27;output&#x27;)&#125;<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">                    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">               <span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;/<span class="name">workflow</span>&gt;</span></span><br><span class="line">     <span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">coordinator-app</span>&gt;</span></span><br></pre></td></tr></table></figure><p>workflow.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">workflow-app</span> <span class="attr">name</span>=<span class="string">&quot;sample-wf&quot;</span> <span class="attr">xmlns</span>=<span class="string">&quot;uri:oozie:workflow:0.1&quot;</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">start</span> <span class="attr">to</span>=<span class="string">&quot;myssjob&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">action</span> <span class="attr">name</span>=<span class="string">&quot;myssjob&quot;</span>&gt;</span></span><br><span class="line">         <span class="tag">&lt;<span class="name">fs</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 注意执行权限 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">mkdir</span> <span class="attr">path</span>=<span class="string">&#x27;$&#123;HDFS_OUTPUT&#125;/result&#x27;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">fs</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">ok</span> <span class="attr">to</span>=<span class="string">&quot;end&quot;</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">error</span> <span class="attr">to</span>=<span class="string">&quot;kill&quot;</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">action</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">kill</span> <span class="attr">name</span>=<span class="string">&quot;kill&quot;</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">message</span>&gt;</span>Action failed, error message[$&#123;wf:errorMessage(wf:lastErrorNode())&#125;]<span class="tag">&lt;/<span class="name">message</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">kill</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">end</span> <span class="attr">name</span>=<span class="string">&quot;end&quot;</span>/&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">workflow-app</span>&gt;</span></span><br></pre></td></tr></table></figure><p>start.sh</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line">start_time=2022-07-21T13:30+0800</span><br><span class="line">end_time=2022-07-21T13:40+0800</span><br><span class="line">log_start_time=2022-07-21T13:20+0800</span><br><span class="line">workflow_user=hdfs</span><br><span class="line">input_prefix=/user/wxk/oozie/hdfs/input</span><br><span class="line">output_prefix=/user/wxk/oozie/hdfs/output</span><br><span class="line">name_node=hdfs://namenode.haohan.com:8020</span><br><span class="line">wf_application_path=hdfs://namenode.haohan.com:8020/user/wxk/oozie/hdfs/config</span><br><span class="line"></span><br><span class="line">oozie job -oozie http://localhost:11000/oozie -run \</span><br><span class="line">    -D oozie.wf.validate.ForkJoin=<span class="literal">false</span> \</span><br><span class="line">    -D oozie.use.system.libpath=<span class="literal">true</span> \</span><br><span class="line">    -D oozie.coord.application.path=<span class="variable">$&#123;wf_application_path&#125;</span> \  <span class="comment"># 告知oozie配置所在位置，如果非定时任务则使用oozie.wf.application.path</span></span><br><span class="line">    -D useStrictFilterStrategy=<span class="literal">true</span> \</span><br><span class="line">    -D user.name=<span class="variable">$&#123;workflow_user&#125;</span> \</span><br><span class="line">    -D mapreduce.job.user.name=<span class="variable">$&#123;workflow_user&#125;</span> \</span><br><span class="line">    -D wf_application_path=<span class="variable">$&#123;wf_application_path&#125;</span> \</span><br><span class="line">    -D start_time=<span class="variable">$&#123;start_time&#125;</span> \</span><br><span class="line">    -D end_time=<span class="variable">$&#123;end_time&#125;</span> \</span><br><span class="line">    -D log_start_time=<span class="variable">$&#123;log_start_time&#125;</span> \</span><br><span class="line">    -D input_prefix=<span class="variable">$&#123;input_prefix&#125;</span> \</span><br><span class="line">    -D output_prefix=<span class="variable">$&#123;output_prefix&#125;</span> \</span><br><span class="line">    -D name_node=<span class="variable">$&#123;name_node&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Oozie </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase-05协处理器和过滤器</title>
      <link href="/2022/02/27/Software/HBase-05%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8%E5%92%8C%E8%BF%87%E6%BB%A4%E5%99%A8/"/>
      <url>/2022/02/27/Software/HBase-05%E5%8D%8F%E5%A4%84%E7%90%86%E5%99%A8%E5%92%8C%E8%BF%87%E6%BB%A4%E5%99%A8/</url>
      
        <content type="html"><![CDATA[<p>HBase 的协处理器 Coprocessor 和过滤器的使用方法介绍。</p><span id="more"></span>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase-04建表设计高级操作</title>
      <link href="/2022/02/21/Software/HBase-04%E5%BB%BA%E8%A1%A8%E8%AE%BE%E8%AE%A1%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C/"/>
      <url>/2022/02/21/Software/HBase-04%E5%BB%BA%E8%A1%A8%E8%AE%BE%E8%AE%A1%E9%AB%98%E7%BA%A7%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p>HBase 的常用建表属性介绍和表设计，以及数据热点问题等。</p><span id="more"></span><h3 id="建表属性"><a href="#建表属性" class="headerlink" title="建表属性"></a>建表属性</h3><p>在这讲 HBase 的建表属性其实是列簇的属性，HBase 支持对每个列簇设置不同的属性，默认的属性可以使用<code>create &#39;table&#39;, &#39;a&#39;</code>命令并使用<code>describe &#39;table&#39;</code>命令查看，一般常用的就是 <code>VERSIONS、TTL和COMPRESSION</code>参数。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  <span class="type">NAME</span> =&gt; &#x27;a&#x27;,  <span class="comment">// 列簇名称</span></span><br><span class="line">  <span class="comment">// 布隆过滤器类型，有ROW、ROWCOL和NONE三种选择，分别是按rowkey建索引、按rowkey和qualifier联合建索引以及不使用布隆过滤器</span></span><br><span class="line">  <span class="comment">// 将在每次插入行时将对应的rowkey哈希插入到布隆</span></span><br><span class="line">  <span class="type">BLOOMFILTER</span> =&gt; &#x27;<span class="type">ROW</span>&#x27;,  </span><br><span class="line">  <span class="type">VERSIONS</span> =&gt; &#x27;<span class="number">1</span>&#x27;,  <span class="comment">// 数据版本，若为3则最多保留三次版本数据</span></span><br><span class="line">  <span class="type">IN_MEMORY</span> =&gt; &#x27;<span class="literal">false</span>&#x27;,  <span class="comment">// 数据是否存储在内存</span></span><br><span class="line">  <span class="type">KEEP_DELETED_CELLS</span> =&gt; &#x27;<span class="type">FALSE</span>&#x27;, </span><br><span class="line">  <span class="type">DATA_BLOCK_ENCODING</span> =&gt; &#x27;<span class="type">NONE</span>&#x27;, </span><br><span class="line">  <span class="type">TTL</span> =&gt; &#x27;<span class="type">FOREVER</span>&#x27;,  <span class="comment">// 数据有效期，默认是2147483647s</span></span><br><span class="line">  <span class="type">COMPRESSION</span> =&gt; &#x27;<span class="type">NONE</span>&#x27;,  <span class="comment">// 压缩方式，默认不压缩，数据压缩有GZIP, SNAPPY，LZO</span></span><br><span class="line">  <span class="type">MIN_VERSIONS</span> =&gt; &#x27;<span class="number">0</span>&#x27;,  <span class="comment">// 数据过期后至少保存多少个版本数据，默认不保留任何版本数据，只有在设置了TTL的时候生效，compact操作的时候执行</span></span><br><span class="line">  <span class="type">BLOCKCACHE</span> =&gt; &#x27;<span class="literal">true</span>&#x27;,  <span class="comment">// 读数据时是否使用块缓存</span></span><br><span class="line">  <span class="type">BLOCKSIZE</span> =&gt; &#x27;<span class="number">65536</span>&#x27;,  <span class="comment">// 块缓存大小</span></span><br><span class="line">  <span class="type">REPLICATION_SCOPE</span> =&gt; &#x27;<span class="number">0</span>&#x27;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="HBase预分区"><a href="#HBase预分区" class="headerlink" title="HBase预分区"></a>HBase预分区</h3><p>默认情况下，创建 HBase 表时只有一个 Region，插入的所有数据都会被存到这个 Region 中，直到数据量达到默认值才会分割成两个 Region，HBase 提供了一种加快批量写入的方法就是预分区，通过预先创建一些空的 Region，这样数据写入 HBase 时，会写入不同的 Region 中，在集群内做数据的负载均衡。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">create table with specific <span class="built_in">split</span> points</span> </span><br><span class="line"><span class="meta">hbase&gt;</span><span class="language-bash">create <span class="string">&#x27;table1&#x27;</span>,<span class="string">&#x27;f1&#x27;</span>,SPLITS =&gt; [<span class="string">&#x27;\x10\x00&#x27;</span>, <span class="string">&#x27;\x20\x00&#x27;</span>, <span class="string">&#x27;\x30\x00&#x27;</span>, <span class="string">&#x27;\x40\x00&#x27;</span>]</span> </span><br><span class="line"><span class="meta"># </span><span class="language-bash">create table with four regions based on random bytes keys</span> </span><br><span class="line"><span class="meta">hbase&gt;</span><span class="language-bash">create <span class="string">&#x27;table2&#x27;</span>,<span class="string">&#x27;f1&#x27;</span>, &#123; NUMREGIONS =&gt; 8 , SPLITALGO =&gt; <span class="string">&#x27;UniformSplit&#x27;</span> &#125;</span> </span><br><span class="line"><span class="meta"># </span><span class="language-bash">create table with five regions based on hex keys</span> </span><br><span class="line"><span class="meta">hbase&gt;</span><span class="language-bash">create <span class="string">&#x27;table3&#x27;</span>,<span class="string">&#x27;f1&#x27;</span>, &#123; NUMREGIONS =&gt; 10, SPLITALGO =&gt; <span class="string">&#x27;HexStringSplit&#x27;</span> &#125;</span></span><br></pre></td></tr></table></figure><p>HBase 提供了几种预分区策略，可以使用 shell 或者 Java API 构建预分区。具体的 Region 区间可以在 HBase WebUI 查看。</p><h3 id="表设计"><a href="#表设计" class="headerlink" title="表设计"></a>表设计</h3><h4 id="列簇设计"><a href="#列簇设计" class="headerlink" title="列簇设计"></a>列簇设计</h4><p>追求的原则是：列簇长度尽量小，列簇数量尽量少。</p><p>列簇长度最好是一个字符，列簇数量最好在三个以下，因为扫描表的时候需要遍历所有列簇下的 qualifier，列簇越多遍历次数就越多，增加IO负载。</p><h4 id="RowKey-设计"><a href="#RowKey-设计" class="headerlink" title="RowKey 设计"></a>RowKey 设计</h4><p>HBase 的数据是按 RowKey 排序放在一个或多个 Region 当中，查询数据的时候也是根据 RowKey 检索数据并返回，所以 RowKey 设计就显得十分重要，设计的合适与否决定了数据的查询效率。</p><p>RowKey 的设计遵循三个原则：长度原则、散列原则和唯一原则。</p><ul><li><p>长度原则：一个 Cell 中除了存储了 一个 key-value 之外也存储了 rowKey 值（Client 查询出一条记录遍历 Result 的时候就可以取出来），底层存储数据的时候 RowKey 也会占用额外的空间，所以 RowKey 的设计要尽量短，建议10-100个字节，不过最好不要超过16个字节。</p></li><li><p>散列原则：如果 RowKey 是按时间戳方式递增，建议对高位加散列前缀，由程序循环生成，低位放时间戳，这样可以避免数据都存在一个 RegionServer 中形成热点现象，这样做可以使得数据存储在多个 RegionServer 中，查询时提高查询效率。</p></li><li><p>唯一原则：RowKey 必须保持唯一性，底层是按照 RowKey 顺序存储的，所以设计 RowKey 的时候最好将经常读取的数据放在一起，提升读取效率。</p></li></ul><h4 id="数据热点"><a href="#数据热点" class="headerlink" title="数据热点"></a>数据热点</h4><p>HBase 中的行是按 RowKey 字典顺序排序存储的，这种设计优化了 scan 操作，可以将相关的或者经常读取的数据放在一起便于 scan。但如果 RowKey 设计糟糕的话，就会造成数据热点现象，热点发生在大量的 client 直接访问集群中的一个 Region 或极少的 Region，大量访问会导致 热点 Region 所在的单个机器超出自身承受能力，引起性能下降甚至 Region 不可用。所以在设计 RowKey 的时候就要必要热点 Region 的产生。</p><p>避免数据热点有几种常见的方法：数据加盐、哈希和反转</p><ul><li><p>数据加盐：具体的做法就是在 RowKey 前增加随机数前缀，排序后就避免了数据存储在同一个 Region。</p></li><li><p>哈希：哈希和加盐的不同之处是哈希的前缀是确定的（前提当然是使用同一个哈希算法），使用哈希可以让客户端重构完整的 RowKey，可以使用 get 准确的获取到某一行数据，加随机数的话不便于实现准确的 get。</p></li><li><p>反转：反转并不是直接将 RowKey 反转，而是根据 RowKey 的特征，比如手机号可以将前七位挪到后面，将年月日中的年放到月日后面。</p></li></ul><p>在处理数据热点的时候不能太过散列，也不能太过集中，应该结合实际业务进行设计。例如频繁获取单个 RowKey 的话那么应该让它们尽量分散；范围查询大量数据的时候，数据如果很分散就需要去多个 Region 去查询，如果太过集中，所有的查询请求都会压在一个节点；全表扫描的话就不用考虑这么多了。</p>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase-03架构设计和读写过程</title>
      <link href="/2022/02/17/Software/HBase-03%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%92%8C%E8%AF%BB%E5%86%99%E8%BF%87%E7%A8%8B/"/>
      <url>/2022/02/17/Software/HBase-03%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%92%8C%E8%AF%BB%E5%86%99%E8%BF%87%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p>对 HBase 的物理架构和读写过程做了详细的介绍。</p><span id="more"></span><h3 id="Hbase-物理架构"><a href="#Hbase-物理架构" class="headerlink" title="Hbase 物理架构"></a>Hbase 物理架构</h3><div align=center><img src="HBase物理架构.png"></div><h4 id="StoreFile-amp-HFile"><a href="#StoreFile-amp-HFile" class="headerlink" title="StoreFile &amp; HFile"></a>StoreFile &amp; HFile</h4><p>StoreFile 以 HFile 格式保存在 HDFS 上  看下图 HFile 的数据组织格式：</p><div align=center><img src="HFile.png"></div><p>首先 HFile 文件是不定长的 长度固定的只有其中的两块 Trailer 和 FileInfo。正如图中所示：Trailer中有指 指向其他数据块的起始点；FileInfo中记录了文件的一些Meta信息，例如 AVG_KEY_LEN, AVG_VALUE_LEN, LAST_KEY, COMPARATOR, MAX_SEQ_ID_KEY等。</p><p>HFile分为六个部分： </p><ol><li>Data Block 段：保存表中的数据，这部分可以被压缩；</li><li>Meta Block 段(可选的)：保存用户自定义的key-value对，可以被压缩；</li><li>File Info 段：HFile的元信息，不压缩，用户也可以在这一部分添加自己的元信息；</li><li>Data Block Index 段：Data Block的索引，每条索引的key是被索引的block的第一条记录的key；</li><li>Meta Block Index 段(可选的)：Meta Block的索引；</li><li>Trailer段：这一段是定长的。保存了每一段的偏移量，读取一个HFile时，会先读取Trailer，Trailer保存了每个段的起始位置(段的Magic Number用来做安全check)，<br>然后 DataBlock Index会被读取到内存中，这样当检索某个key时不需要扫描整个HFile，只需从内存中找到key所在的block，通过一次磁盘IO将整个block读取到内存中，再找到需要的key。DataBlock Index采用LRU机制淘汰机制。</li></ol><p>HFile 的 Data Block，Meta Block 通常使用压缩方式存储，压缩之后可以大大减少网络IO和磁盘IO，随之而来的开销当然是花费cpu进行压缩和解压缩。目标HFile的压缩支持两种方式 Gzip 和 LZO。</p><p>Data Index 和 Meta Index 块记录了每个 Data 块和 Meta 块的起始点。</p><p>Data Block 是 HBase I&#x2F;O 的基本单元，为了提高效率，HRegionServer 中有基于 LRU 的 Block Cache 机制。每个 Data 块的大小可以在创建一个 Table 的时候通过参数指定，大号的 Block 有利于顺序 Scan，小号 Block 利于随机查询。 每个 Data 块除了开头的 Magic 以外就是一个个 KeyValue 对拼接而成，Magic 内容就是一些随机数字，目的是防止数据损坏。</p><p>HFile 里面的每个 KeyValue 对就是一个简单的 byte 数组。但是每个 byte 数组里面包含了很多项，并且有固定的结构。我们来看看里面的具体结构：</p><div align=center><img src="KeyValue.png"></div><p>开始是两个固定长度的数值，分别表示 Key 的长度和 Value 的长度。紧接着是 Key，开始是固定长度的数值，表示RowKey的长度，紧接着是 RowKey，然后是固定长度的数值，表示 Family 的长度 然后是 Family，接着是 Qualifier，然后是两个固定长度的数值，表示TimeStamp和KeyType（Put&#x2F;Delete）。Value 部分没有那么复杂的结构，就是纯粹的二进制数据了。</p><h4 id="WAL-amp-HLog"><a href="#WAL-amp-HLog" class="headerlink" title="WAL &amp; HLog"></a>WAL &amp; HLog</h4><p>它用来做灾难恢复使用，类似于 MySQL 中的 binlog，HLog 记录所有的数据变更操作。HBase 采用类 LSM 的架构操作，数据没有直接写入 HBase，而是先写到 MemStore，等满足一定条件的时候再异步刷写到磁盘，为防止刷写期间进程发生异常导致数据丢失，所以会把数据按顺序写入 HLog 中，如果 RegionServer 发生宕机或其他异常，会从 HLog 恢复数据，保证数据不丢失。</p><p>WAL（Write-Ahead Logging）是一种高效的日志算法，几乎是所有非内存数据库提升写性能的不二法门，基本原理是在数据写入之前首先顺序写入日志，然后再写入缓存，等到缓存写满之后统一落盘。之所以能够提升写性能，是因为 WAL 将一次随机写转化为了一次顺序写加一次内存写。提升写性能的同时，WAL 可以保证数据的可靠性，即在任何情况下数据不丢失。假如一次写入完成之后发生了宕机，即使所有缓存中的数据丢失，也可以通过恢复日志还原出丢失的数据。</p><p>每个 RegionServer 维护一个 HLog，而不是每一个 Region 一个，这样做不同 Region 的日志会混在一起，同时写入多个文件时可以减少磁盘寻址次数。HLog 就是一个普通的 Hadoop Sequence File，它的 key 是 HLogkey 对象，记录了写入数据的归属信息，除了 table 和 region 名字以外，还包括 sequence number 和 timestamp；HLog 的value 是 HBase 的 KeyValue 对象，即对应 HFile 的 keyvalue。</p><h4 id="MemStore-amp-StoreFile"><a href="#MemStore-amp-StoreFile" class="headerlink" title="MemStore &amp; StoreFile"></a>MemStore &amp; StoreFile</h4><p>一个 HRegion 由多个 Store 组成，每个 Store 包含一个列簇的所有数据。Store 包括位于内存的 MemStore 和位于磁盘的 StoreFile（也就是 HFile），写操作时会先写入 MemStore，当数据量达到一定阈值的时候，HRegionServer 会启动 flushcache 进程写刷写到 StoreFile，每次写入形成一个单独的 HFile 文件。当客户端检索数据时，会优先在 MemStore 查找，找不到才会去 StoreFile 检索。</p><h4 id="Flush"><a href="#Flush" class="headerlink" title="Flush"></a>Flush</h4><p>数据在更新时首先写入 HLog（WAL Log），再写入 MemStore 中，MemStore（存储结构 CocurrentSkipListMap，优点就是 增删改查 key-value 效率都很高）中的数据是排序的，当 MemStore 累计到一定阈值（默认是128M，局部控制）时，就会创建一个新的 MemStore，并且将老的 MemStore 添加到 flush 队列，由单独的线程 flush 到磁盘上，成为一个 StoreFile。与此同时，系统会在 ZooKeeper 中记录一个 redo point，表示这个时刻之前的变更已经持久化了。当系统出现意外时，可能导致内存 MemStore 中的数据丢失，此时使用 HLog（WAL Log）来恢复 checkpoint 之后的数据。Memstore 执行刷盘操作的的触发条件：</p><ol><li><p>全局内存控制：当所有 memstore 占整个 heap 的最大比例的时候，会触发刷盘的操作。这个参数是 hbase.regionserver.global.memstore.upperLimit，默认为整个heap 内存的40%。这个全局的参数是控制内存整体的使用情况，但这并不意味着全局内存触发的刷盘操作会将所有的 MemStore 都进行刷盘，而是通过另外一个参数 hbase.regionserver.global.memstore.lowerLimit 来控制，默认是整个 heap 内存的35%。当 flush 到所有 memstore 占整个 heap 内存的比率为35%的时候，就停止刷盘。这么做主要是为了减少刷盘对业务带来的影响，实现平滑系统负载的目的。</p></li><li><p>局部内存控制：当 MemStore 的大小达到 hbase.hregion.memstore.flush.size 大小的时候会触发刷盘，默认128M大小。</p></li><li><p>HLog 的数量：前面说到 HLog 为了保证 HBase 数据的一致性，那么如果 HLog 太多的话，会导致故障恢复的时间太长，因此 HBase 会对 HLog 的最大个数做限制。当达到 HLog 的最大个数的时候，会强制刷盘。这个参数是 hase.regionserver.max.logs，默认是32个。</p></li><li><p>手动操作：可以通过 HBase Shell 或者 Java API 手动触发 flush 的操作。</p></li></ol><h4 id="Split-amp-Compact"><a href="#Split-amp-Compact" class="headerlink" title="Split &amp; Compact"></a>Split &amp; Compact</h4><p>HBase 的三种默认的 Split 策略：<code>ConstantSizeRegionSplitPolicy（常数数量）、IncreasingToUpperBoundRegionSplitPolicy（递增上限）、SteppingSplitPolicy（步增上线）</code>。StoreFile 是只读的，一旦创建后就不可以再修改。因此 HBase 的更新&#x2F;修改其实是不断追加的操作。当一个 Store 中的 StoreFile 达到一定的阈值后，就会进行一次合并（minor_compact、major_compact），将对同一个 key 的修改合并到一起，形成一个大的 StoreFile，当 StoreFile 的大小达到一定阈值后，又会对 StoreFile 进行 split，等分为两个 StoreFile。由于对表的更新是不断追加的，compact 时，需要访问 Store 中全部的 StoreFile 和 MemStore，将他们按 rowkey 进行合并，由于都是经过排序的并且还有索引，合并的过程会比较快。</p><p>Minor_Compact 和 Major_Compact 的区别：Minor 操作只用来做部分文件的合并操作以及包括 minVersion&#x3D;0 并且设置 ttl 的过期版本清理，不做任何删除数据、多版本数据的清理工作；Major 操作是对 Region下的 HStore下的所有 StoreFile 执行合并操作，最终的结果是整理合并出一个文件。</p><p>Client 写入 -&gt; 存入 MemStore，一直到 MemStore 达到阈值 -&gt; Flush 成一个 StoreFile，直至增长到一定阈值 -&gt; 触发 Compact 合并操作 -&gt; 多个 StoreFile 合并成一个 StoreFile，同时进行版本合并和数据删除 -&gt; 当 StoreFiles Compact 后，逐步形成越来越大的 StoreFile -&gt; 单个 StoreFile 大小超过一定阈值（默认10G）后，触发 Split 操作，把当前 Region Split 成2个 Region，Region 会下线，新 Split 出的两个子 Region 会被 HMaster 分配到相应的 HRegionServer 上，使得原先一个 Region 的压力得以分流到两个 Region 上。由此过程可知，HBase 只是增加数据，所有的更新和删除操作，都是在 Compact 阶段做的，所以，用户写操作只需要进入到内存即可立即返回，从而保证I&#x2F;O高性能。</p><h3 id="RegionServer-工作机制"><a href="#RegionServer-工作机制" class="headerlink" title="RegionServer 工作机制"></a>RegionServer 工作机制</h3><h4 id="Region-分配"><a href="#Region-分配" class="headerlink" title="Region 分配"></a>Region 分配</h4><p>任何时刻，一个 Region 只能分配给一个 RegionServer。Master 记录了当前有哪些可用的 RegionServer。以及当前哪些 Region 分配给了哪些 RegionServer，哪些 Region 还没有分配。当需要分配的新的 Region，并且有一个 RegionServer 上有可用空间时，Master 就给这个 RegionServer 发送一个装载请求，把 Region 分配给这个 RegionServer。RegionServer 得到请求后，就开始对此 Region 提供服务，包括：该 Regoin 的 compact 和 split 以及该 Region 的IO读写。</p><h4 id="RegionServer-上线"><a href="#RegionServer-上线" class="headerlink" title="RegionServer 上线"></a>RegionServer 上线</h4><p>Master 使用 Zookeeper 来跟踪 RegionServer 状态。当某个 RegionServer 启动时，会首先在 ZooKeeper 上的 server 目录下建立代表自己的 znode。由于 Master 订阅了 server 目录上的变更消息，当 server 目录下的文件出现新增或删除操作时，Master 可以得到来自 ZooKeeper 的实时通知。因此一旦 RegionServer 上线，Master 能马上得到消息。</p><h4 id="RegionServer-下线"><a href="#RegionServer-下线" class="headerlink" title="RegionServer 下线"></a>RegionServer 下线</h4><p>当 RegionServer 下线时，它和 Zookeeper 的会话断开，ZooKeeper 会自动释放代表这台 server 的文件上的独占锁。Master 就可以确定 RegionServer 挂了。无论哪种情况，RegionServer 都无法继续为它的 Region 提供服务了，此时 Master 会删除 server 目录下代表这台 RegionServer 的 znode 数据，并将这台 RegionServer 的 Region 分配给其它还活着的同志。</p><h3 id="Master-工作机制"><a href="#Master-工作机制" class="headerlink" title="Master 工作机制"></a>Master 工作机制</h3><h4 id="Master上线"><a href="#Master上线" class="headerlink" title="Master上线"></a>Master上线</h4><p>Master启动进行以下步骤：</p><ol><li>从 ZooKeeper 上获取唯一一个代表 Active Master 的锁，用来阻止其它 Master 成为 Master。使用 zookeeper 实现了分布式独占锁；</li><li>扫描 ZooKeeper 上的 server 父节点，获得当前可用的 RegionServer 列表。 rs 节点下的 regionserver 列表；</li><li>和每个 RegionServer 通信，获得当前已分配的 Region 和 RegionServer 的对应关系。 每个表有多少个 regoin, 哪些 regionserver 保管了那些 region…</li><li>扫描 META 表中 Region 的集合，计算得到当前还未分配的 Region，将他们放入待分配 Region 列表。 有一些 regoin 是无人认领的。</li></ol><h4 id="Master-下线"><a href="#Master-下线" class="headerlink" title="Master 下线"></a>Master 下线</h4><p>由于 Master 只维护表和 Region 的元数据，而不参与表数据IO的过程，Master 下线仅导致所有元数据的修改被冻结(无法创建删除表，无法修改表的 schema，无法进行 Region 的负载均衡，无法处理 Region 上下线，无法进行 Region 的合并，唯一例外的是 Region 的 split 可以正常进行，因为只有 RegionServer 参与)，表的数据读写还可以正常进行。因此 Master 下线短时间内对整个 HBase 集群没有影响。</p><p>从上线过程可以看到，Master 保存的信息全是可以冗余信息（都可以从系统其它地方收集到或者计算出来）。因此，一般 HBase 集群中总是有一个 Master 在提提供服务，还有一个以上的 Master 在等待时机抢占它的位置。</p><h3 id="读写过程"><a href="#读写过程" class="headerlink" title="读写过程"></a>读写过程</h3><div align=center><img src="Get过程.png"></div><ol><li><p>客户端通过 ZooKeeper 查询到 META 表的 RegionServer 和 Region 所在主机位置，随后去 META 表查询目标 key 所在的 RegionServer 和 Region</p></li><li><p>联系 RegionServer 查询目标数据</p></li><li><p>RegionServer 定位到目标数据所在的 Region，发出查询数据请求</p></li><li><p>Region 先在 Memstore 中查找，查到则返回，查不到会去 BlockCache 查找</p></li><li><p>如果还找不到，则在 StoreFile 中扫描，因为 HFile 的特殊设计，所以扫描起来不会很慢。为了判断要查的数据在不在当前的 StoreFile 中，应用到了 BloomFilter。BloomFilter（布隆过滤器）可以判断一个元素是不是在一个庞大的集合内，但是他有一个弱点，它有一定的误判率</p></li></ol><div align=center><img src="Put过程.png"></div><ol><li><p>Client 先根据 RowKey 找到对应的 Region 所在的 RegionServer</p></li><li><p>Client 向 RegionServer 提交写请求</p></li><li><p>RegionServer 找到目标 Region</p></li><li><p>Region 检查数据是否与 Schema一致</p></li><li><p>如果客户端没有指定版本，则获取当前系统时间作为数据版本</p></li><li><p>将更新写入 WAL Log</p></li><li><p>将更新写入 Memstore</p></li><li><p>判断Memstore的是否需要 flush 为 StoreFile 文件</p></li></ol><p>写入数据的过程补充：</p><p>每个 HRegionServer 中都会有一个HLog对象，HLog 是一个实现 Write Ahead Log 的类，每次用户操作写入 Memstore 的同时，也会写一份数据到 HLog 文件，HLog 文件定期会滚动更新，并删除旧的文件(已持久化到 StoreFile 中的数据)。当 HRegionServer 意外终止后，HMaster 会通过 ZooKeeper 感知，HMaster 首先处理遗留的 HLog 文件，将不同 Region 的 log 数据拆分，分别放到相应 Region 目录下，然后再将失效的 Region（带有刚刚拆分的log）重新分配，领取到这些Region的 HRegionServer 在 load Region 的过程中，会发现有历史 HLog 需要处理，因此会 Replay HLog 中的数据到 MemStore 中，然后 flush 到 StoreFiles，完成数据恢复。</p>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase-02API操作</title>
      <link href="/2022/02/15/Software/HBase-02API%E6%93%8D%E4%BD%9C/"/>
      <url>/2022/02/15/Software/HBase-02API%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p>HBase 的基础 API 操作，批量读写主要整合 Spark 来做数据的导入导出。</p><span id="more"></span><h3 id="Client-API"><a href="#Client-API" class="headerlink" title="Client API"></a>Client API</h3><p>代码主要使用 Scala 编写，只列出了基本的操作方法。新的 API 有两个入口，分别是 Admin 和 Table，Admin 管理表，Table 操作表数据，命令行支持的所有方法基本都能在这两个接口下找到。</p><h4 id="HBase-Connection"><a href="#HBase-Connection" class="headerlink" title="HBase Connection"></a>HBase Connection</h4><p>需要新建一个 HBase 配置，可以直接 addResource 或者使用 set 方法指定连接所需配置信息，然后再使用 Conenction 工厂方法创建连接，connection 用完记得 close。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">conf.addResource(<span class="string">&quot;hbase-site.xml&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> connection = <span class="type">ConnectionFactory</span>.createConnection(conf)</span><br></pre></td></tr></table></figure><h4 id="Create"><a href="#Create" class="headerlink" title="Create"></a>Create</h4><p>调用 Connection 的 getAdmin 方法返回 Admin 对象，检查表不存在后再建表。建表需要传入 ColumnFamilyDescriptor 对象，使用 ColumnFamilyDescriptorBuilder.newBuilder 创建对象并在里面指定建表所需的参数，最后直接调用 Admin 对象的 createTable 方法，参数可以传入单个或多个 ColumnFamilyDescriptor 对象。若要指定 namespace 直接跟表名写在一起：space:table，Admin对象用完需要 close。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTable</span></span>(tableName: <span class="type">String</span>, columnFamily: <span class="type">List</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> admin = connection.getAdmin</span><br><span class="line">  <span class="keyword">if</span> (admin.tableExists(<span class="type">TableName</span>.valueOf(tableName))) &#123;</span><br><span class="line">    <span class="type">LOG</span>.warn(<span class="string">s&quot;表 <span class="subst">$&#123;tableName&#125;</span> 已存在&quot;</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> cfs = columnFamily.map(cf =&gt; <span class="type">ColumnFamilyDescriptorBuilder</span>.newBuilder(<span class="type">Bytes</span>.toBytes(cf)).build())</span><br><span class="line">    <span class="keyword">val</span> desc = <span class="type">TableDescriptorBuilder</span></span><br><span class="line">      .newBuilder(<span class="type">TableName</span>.valueOf(tableName))</span><br><span class="line">      .setColumnFamilies(cfs.asJavaCollection)</span><br><span class="line">      .build()</span><br><span class="line">    admin.createTable(desc)</span><br><span class="line">    <span class="type">LOG</span>.info(<span class="string">s&quot;表 <span class="subst">$&#123;tableName&#125;</span> 创建成功&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  admin.close()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">createTable(<span class="string">&quot;test&quot;</span>, <span class="string">&quot;info&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="Put"><a href="#Put" class="headerlink" title="Put"></a>Put</h4><p>put 需要在 Table 对象下操作，使用 getTable 并传入 TableName 对象得到 Table，然后新建一个 Put 对象，添加列簇、列限定符和数据等信息，再将它传给 Table 对象的 put 方法即可，最后再关闭 Table对象。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">putData</span></span>(tableName: <span class="type">String</span>, rowKey: <span class="type">String</span>, cf: <span class="type">String</span>, column: <span class="type">String</span>, value: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> table = connection.getTable(<span class="type">TableName</span>.valueOf(tableName))</span><br><span class="line">  <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>(<span class="type">Bytes</span>.toBytes(rowKey))</span><br><span class="line">  put.addColumn(<span class="type">Bytes</span>.toBytes(cf), <span class="type">Bytes</span>.toBytes(column), <span class="type">Bytes</span>.toBytes(value))</span><br><span class="line">  table.put(put)</span><br><span class="line">  <span class="type">LOG</span>.info(<span class="string">s&quot;table &#x27;<span class="subst">$&#123;tableName&#125;</span>&#x27; rowKey &#x27;<span class="subst">$&#123;rowKey&#125;</span>&#x27; insert complete&quot;</span>)</span><br><span class="line">  table.close()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">putData(<span class="string">&quot;test&quot;</span>, <span class="string">&quot;1&quot;</span>, <span class="string">&quot;info&quot;</span>, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;Bob&quot;</span>)</span><br></pre></td></tr></table></figure><h4 id="Get-x2F-Scan"><a href="#Get-x2F-Scan" class="headerlink" title="Get&#x2F;Scan"></a>Get&#x2F;Scan</h4><p>我把 get 和 scan 功能整合在一个方法中，使用 Scan 构造对象的时候可以将 Get 对象传进去转为 Scan 对象。</p><p>首先还是先构建 Table 对象，这里先对列簇和列限定符参数做了一些判断。不指定列簇则返回所有列簇内的所有字段，列簇和 cfColumn 不可以同时指定，指定列簇则返回列簇包含的所有字段。指定列限定符则是通过 Map 的方式传进去，然后调用 Scan 的 addColumn 方法逐个新增到对象中。</p><p>调用 getScanner 方法后即可返回 Result 对象组成的列表，Result 对象内包含着一条记录的所有内容，它是由多个 Cell 组成的，每个 Cell 包含了<code>rowKey, columnFamily, qualifier, value</code>等数据，新版本中则使用 CellUtil.cloneRow(cell) 等方法取出对应的数据，除过 getTimestamp 方法其他的 Cell 获取属性的成员方法已经弃用，应避免使用。</p><p>最后别忘记关闭 scanner 和 table。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getData</span></span>(tableName: <span class="type">String</span>,</span><br><span class="line">            rowKey: <span class="type">String</span> = <span class="string">&quot;&quot;</span>,</span><br><span class="line">            cf: <span class="type">String</span> = <span class="string">&quot;&quot;</span>,</span><br><span class="line">            cfColumn: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]()): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> scan = <span class="keyword">new</span> <span class="type">Scan</span>()</span><br><span class="line">  <span class="keyword">val</span> table = connection.getTable(<span class="type">TableName</span>.valueOf(tableName))</span><br><span class="line">  <span class="keyword">if</span> (cf.nonEmpty &amp;&amp; cfColumn.nonEmpty) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RuntimeException</span>(<span class="string">&quot;限定字段在cfColumn中指定，cf和cfColumn不可同时指定&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (rowKey.nonEmpty) &#123;</span><br><span class="line">    <span class="keyword">val</span> get = <span class="keyword">new</span> <span class="type">Get</span>(<span class="type">Bytes</span>.toBytes(rowKey))</span><br><span class="line">    scan = <span class="keyword">new</span> <span class="type">Scan</span>(get)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (cf.nonEmpty) &#123;</span><br><span class="line">    scan.addFamily(<span class="type">Bytes</span>.toBytes(cf))</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (cfColumn.nonEmpty) &#123;</span><br><span class="line">    cfColumn.foreach(col =&gt; scan.addColumn(<span class="type">Bytes</span>.toBytes(col._1), <span class="type">Bytes</span>.toBytes(col._2)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> scanner = table.getScanner(scan)</span><br><span class="line">  scanner.asScala.toList.foreach &#123;</span><br><span class="line">    result =&gt; result.rawCells().foreach &#123;</span><br><span class="line">      cell =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> data = <span class="type">MyCell</span>.builder(<span class="type">CellUtil</span>.cloneRow(cell),</span><br><span class="line">          <span class="type">CellUtil</span>.cloneFamily(cell),</span><br><span class="line">          <span class="type">CellUtil</span>.cloneQualifier(cell),</span><br><span class="line">          <span class="type">CellUtil</span>.cloneValue(cell),</span><br><span class="line">          cell.getTimestamp).toString</span><br><span class="line">        println(data)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  scanner.close()</span><br><span class="line">  table.close()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">getData(<span class="string">&quot;test&quot;</span>, <span class="string">&quot;1&quot;</span>)  <span class="comment">// get</span></span><br><span class="line">getData(<span class="string">&quot;test&quot;</span>, <span class="string">&quot;1&quot;</span>, cfColumn = <span class="type">Map</span>(<span class="string">&quot;info&quot;</span> -&gt; <span class="string">&quot;name&quot;</span>))  <span class="comment">// get指定列限定符</span></span><br><span class="line">getData(<span class="string">&quot;test&quot;</span>)  <span class="comment">// scan</span></span><br></pre></td></tr></table></figure><h3 id="Spark-读写"><a href="#Spark-读写" class="headerlink" title="Spark 读写"></a>Spark 读写</h3><h4 id="Read"><a href="#Read" class="headerlink" title="Read"></a>Read</h4><p>主要用到 SparkContext 的 newAPIHadoopRDD 将数据读为 RDD，再定义好 schema 转为 DataFrame，测试使用的是 Spark 2.3.2，中间会有些问题，但 debug 时数据确实可以取出来，生产中用到了再研究吧。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用Spark将HBase数据读取为DataFrame</span></span><br><span class="line"><span class="comment"> * @param connection Connection</span></span><br><span class="line"><span class="comment"> * @param session SparkSession</span></span><br><span class="line"><span class="comment"> * @param table table名字</span></span><br><span class="line"><span class="comment"> * @param cf 列簇</span></span><br><span class="line"><span class="comment"> * @param fields 字段</span></span><br><span class="line"><span class="comment"> * @return</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readAsDf</span></span>(connection: <span class="type">Connection</span>, session: <span class="type">SparkSession</span>, table: <span class="type">String</span>, cf: <span class="type">String</span>, fields: <span class="type">List</span>[<span class="type">String</span>]): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="comment">// 配置要读取的表和字段</span></span><br><span class="line">  <span class="keyword">val</span> hbaseConf: <span class="type">Configuration</span> = connection.getConfiguration</span><br><span class="line">  hbaseConf.set(<span class="type">TableInputFormat</span>.<span class="type">INPUT_TABLE</span>, table)</span><br><span class="line">  fields.filter(field =&gt; !field.equalsIgnoreCase(<span class="string">&quot;rowkey&quot;</span>))</span><br><span class="line">  hbaseConf.set(<span class="type">TableInputFormat</span>.<span class="type">SCAN_COLUMNS</span>,</span><br><span class="line">    fields</span><br><span class="line">      .filter(field =&gt; !field.equalsIgnoreCase(<span class="string">&quot;rowkey&quot;</span>))</span><br><span class="line">      .map(field =&gt; <span class="string">s&quot;<span class="subst">$&#123;cf&#125;</span>:<span class="subst">$&#123;field&#125;</span>&quot;</span>)</span><br><span class="line">      .mkString(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 将数据读为RDD</span></span><br><span class="line">  <span class="keyword">val</span> hbaseRDD: <span class="type">RDD</span>[(<span class="type">ImmutableBytesWritable</span>, <span class="type">Result</span>)] = session.sparkContext.newAPIHadoopRDD(</span><br><span class="line">    hbaseConf,</span><br><span class="line">    classOf[<span class="type">TableInputFormat</span>],</span><br><span class="line">    classOf[<span class="type">ImmutableBytesWritable</span>],</span><br><span class="line">    classOf[<span class="type">Result</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 解析读出来的数据为目标RDD</span></span><br><span class="line">  <span class="keyword">val</span> rowRDD = hbaseRDD.map&#123;row =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> values: <span class="type">ListBuffer</span>[<span class="type">String</span>] = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">String</span>]</span><br><span class="line">    <span class="keyword">val</span> result: <span class="type">Result</span>             = row._2</span><br><span class="line">    <span class="keyword">val</span> cells = result.rawCells()</span><br><span class="line">    <span class="keyword">for</span> (cell &lt;- cells) &#123;</span><br><span class="line">      println(<span class="type">Bytes</span>.toString(<span class="type">CellUtil</span>.cloneRow(cell)))</span><br><span class="line">      println(<span class="type">Bytes</span>.toString(<span class="type">CellUtil</span>.cloneValue(cell)))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- fields.indices) &#123;</span><br><span class="line">      <span class="keyword">if</span> (fields(i).equalsIgnoreCase(<span class="string">&quot;rowkey&quot;</span>)) &#123;</span><br><span class="line">        values += <span class="type">Bytes</span>.toString(result.getRow)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        values += <span class="type">Bytes</span>.toString(result.getValue(<span class="type">Bytes</span>.toBytes(cf), <span class="type">Bytes</span>.toBytes(fields(i))))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">Row</span>.fromSeq(values.toList)</span><br><span class="line">  &#125;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 构建DataFrame Schema并创建</span></span><br><span class="line">  <span class="keyword">val</span> schema = <span class="type">StructType</span>(</span><br><span class="line">    fields.map(field =&gt; <span class="type">DataTypes</span>.createStructField(field, <span class="type">DataTypes</span>.<span class="type">StringType</span>, <span class="literal">true</span>)))</span><br><span class="line">  <span class="keyword">val</span> dataFrame = session.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line">  session.close()</span><br><span class="line">  dataFrame</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="Write"><a href="#Write" class="headerlink" title="Write"></a>Write</h4><p>主要是用到 HBase 的 BulkLoad 功能，原理是直接将数据写入 HFile 再加载到 Region 中，好处是少了写入 MemStore 和 WAL，没了刷写的步骤，效率提升不少，但数据写入期间如果丢失无法恢复。在批量写入大量数据时可以选择这种方法。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 通过Spark写入DataFrame到HBase</span></span><br><span class="line"><span class="comment"> * @param connection Connection</span></span><br><span class="line"><span class="comment"> * @param session SparkSession</span></span><br><span class="line"><span class="comment"> * @param data DataFrame数据</span></span><br><span class="line"><span class="comment"> * @param tableName 表名</span></span><br><span class="line"><span class="comment"> * @param rowKey 以哪一列作为rowKey字段</span></span><br><span class="line"><span class="comment"> * @param cf 写入到哪个列簇</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">writeFromDf</span></span>(connection: <span class="type">Connection</span>, session: <span class="type">SparkSession</span>, data: <span class="type">DataFrame</span>, tableName: <span class="type">String</span>, rowKey: <span class="type">String</span>, cf: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> hbaseConf: <span class="type">Configuration</span> = connection.getConfiguration</span><br><span class="line">  <span class="keyword">var</span> fields = data.columns</span><br><span class="line">  <span class="keyword">val</span> table = <span class="type">TableName</span>.valueOf(tableName.getBytes())</span><br><span class="line">  <span class="keyword">val</span> stagingDir = <span class="string">&quot;/tmp/HBaseBulkLoad&quot;</span>  <span class="comment">// 不会覆盖，需要手动删除或每次用完后删除</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//去掉rowKey字段</span></span><br><span class="line">  fields = fields.dropWhile(_ == rowKey)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> hbaseContext = <span class="keyword">new</span> <span class="type">HBaseContext</span>(session.sparkContext, hbaseConf)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//将DataFrame转换bulkLoad需要的RDD格式</span></span><br><span class="line">  <span class="keyword">val</span> rddTmp: <span class="type">RDD</span>[<span class="type">Array</span>[(<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[(<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[<span class="type">Byte</span>])])]] = data.rdd.map(row =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> rk = row.getAs[<span class="type">String</span>](rowKey)</span><br><span class="line"></span><br><span class="line">    fields.map(field =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> value = row.getAs[<span class="type">String</span>](field)</span><br><span class="line">      (<span class="type">Bytes</span>.toBytes(rk), <span class="type">Array</span>((<span class="type">Bytes</span>.toBytes(cf), <span class="type">Bytes</span>.toBytes(field), <span class="type">Bytes</span>.toBytes(value))))</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;)</span><br><span class="line">  <span class="keyword">val</span> rddNew: <span class="type">RDD</span>[(<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[(<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[<span class="type">Byte</span>])])] = rddTmp.flatMap(array =&gt; array)</span><br><span class="line">  rddNew.hbaseBulkLoad(hbaseContext, table,</span><br><span class="line">    t =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> rowKey = t._1</span><br><span class="line">      <span class="keyword">val</span> family:<span class="type">Array</span>[<span class="type">Byte</span>] = t._2(<span class="number">0</span>)._1</span><br><span class="line">      <span class="keyword">val</span> qualifier = t._2(<span class="number">0</span>)._2</span><br><span class="line">      <span class="keyword">val</span> value = t._2(<span class="number">0</span>)._3</span><br><span class="line">      <span class="keyword">val</span> keyFamilyQualifier= <span class="keyword">new</span> <span class="type">KeyFamilyQualifier</span>(rowKey, family, qualifier)</span><br><span class="line"></span><br><span class="line">      <span class="type">Seq</span>((keyFamilyQualifier, value)).iterator</span><br><span class="line">    &#125;,</span><br><span class="line">    stagingDir)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// bulk load start</span></span><br><span class="line">  <span class="keyword">val</span> loader = <span class="keyword">new</span> <span class="type">LoadIncrementalHFiles</span>(hbaseConf)</span><br><span class="line">  loader.doBulkLoad(<span class="keyword">new</span> <span class="type">Path</span>(stagingDir), connection.getAdmin, connection.getTable(table),</span><br><span class="line">    connection.getRegionLocator(table))</span><br><span class="line"></span><br><span class="line">  session.close()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="整合-Spark-DataSource"><a href="#整合-Spark-DataSource" class="headerlink" title="整合 Spark DataSource"></a>整合 Spark DataSource</h4>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HBase-01基础概念</title>
      <link href="/2022/02/10/Software/HBase-01%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"/>
      <url>/2022/02/10/Software/HBase-01%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/</url>
      
        <content type="html"><![CDATA[<p>针对 HBase 的集群架构和表逻辑结构做了简单的介绍。</p><span id="more"></span><h3 id="HBase-介绍"><a href="#HBase-介绍" class="headerlink" title="HBase 介绍"></a>HBase 介绍</h3><ol><li><p>Apache HBase™ is the Hadoop database, a distributed, scalable, big data store.</p></li><li><p>Use Apache HBase™ when you need random, realtime read&#x2F;write access to your Big Data.</p></li><li><p>This project’s goal is the hosting of very large tables – billions of rows X millions of columns – atop clusters of commodity hardware.</p></li><li><p>Apache HBase is an open-source, distributed, versioned, non-relational database modeled after Google’s Bigtable: A Distributed Storage System for Structured Data by Chang et al. Just as Bigtable leverages the distributed data storage provided by the Google File System, Apache HBase provides Bigtable-like capabilities on top of Hadoop and HDFS.</p></li></ol><p>HBase 是 BigTable 的开源（源码使用Java编写）版本。是 Apache Hadoop 的数据库，是建立在HDFS 之上，被设计用来提供高可靠性、高性能、列存储、可伸缩、多版本的 NoSQL 的分布式数据存储系统，实现对大型数据的实时、随机的读写访问。详情查看 <a href="https://hbase.apache.org/">HBase 官网</a></p><h3 id="集群架构"><a href="#集群架构" class="headerlink" title="集群架构"></a>集群架构</h3><div align=center><img src="HBase集群架构.png"></div><ol><li><p>HBase 采用主从架构，HMaster 作为集群的管理节点，主要负责监视集群中的所有 RegionServer 实例，并且建表、修改表、删除表等原数据操作也是 HMaster 在负责，同时它支持部署多个 backup 机器防止 HMaster 意外挂掉接管它的工作。</p></li><li><p>RegionServer 管理所有的表数据操作和存储，对于数据的增删更新查都由它负责，在集群中有多个 RegionServer 节点。每个 RegionServer 中有多个 Store 负责数据的缓存和存储，最终数据以 HFile 的格式保存到 HDFS 的 DataNode 中。在数据写入 MemStore 之前会事先写入 WAL（HLog），以防数据在 MemStore 还未刷写到 HFile之前丢失。</p></li><li><p>Client 访问数据前首先需要访问 Zookeeper，找到 meta（记录了每个 HBase 表数据所在的 RegionServer 和 Region 等元数据信息）表的位置后访问 meta 表，把表数据缓存从本地，从表中找到目标 Region 再去与 Region 通信。</p></li><li><p>Zookeeper 负责维护集群的状态，HMaster 的选举以及 RegionServer 是否在线，存储 meta 表的位置等等。</p></li></ol><h3 id="核心物理概念"><a href="#核心物理概念" class="headerlink" title="核心物理概念"></a>核心物理概念</h3><h4 id="NameSpace（表命名空间）"><a href="#NameSpace（表命名空间）" class="headerlink" title="NameSpace（表命名空间）"></a>NameSpace（表命名空间）</h4><p>类似于 MySQL 中的数据库，表现在文件系统中一个表命名空间就是一个文件夹，和 MySQL不同的是它不是强制要求的，类似于 Hive，不指定的时候所有的表都存储在 default 目录下，一般用来隔离业务数据。</p><h4 id="Table"><a href="#Table" class="headerlink" title="Table"></a>Table</h4><p>逻辑上可类比为 Hive 中的表，HBase 表结构逻辑视图：</p><div align=center><img src="HBase表结构逻辑视图.png"></div><p>每条数据有唯一的 rowkey，它的字段可以存储在不同的 columnFamily 中，使用 columnFamily:qualifer 指定字段即可取出最新的 value（timestamp 最大）。定位一条数据的流程：<code>table -&gt; rowkey -&gt; columnFamily -&gt; qualifier(column) -&gt; timestamp -&gt; value</code>。</p><p>在一个 HBase 表中，存在多个列簇，每个列簇包含多个字段，每个字段对应的数据有多个版本，最小粒度的一条数据也就是一个 Cell。表内部 RowKey 是按顺序排列的，如果数据量太大，会按 RowKey 将数据切分为多个 Region，不同的 Region 是可以存储在不同的 RegionServer 中的，一个 Region 中的数据一定是在同一个 RegionServer 中。最后 Store 也是逻辑上的概念，客户端读写数据最终都由 Store 与底层磁盘存储的 HFile 文件联系，Store 中还有 MemStore 和 BlockCache 等内存数据的概念，MemStore 用来刷写 HFile 文件，BlockCache 则是读缓存，这部分后续再详细介绍。</p><h4 id="RowKey（行键）"><a href="#RowKey（行键）" class="headerlink" title="RowKey（行键）"></a>RowKey（行键）</h4><p>RowKey 是检索一条记录的主键，访问 HBase Table 的行，每个 RowKey 在表中都是唯一的。</p><p>检索数据只有三种方式：</p><ul><li><p>通过单个 rowKey 返回一条记录</p></li><li><p>通过 rowKey 的 range 范围返回对应的多条数据</p></li><li><p>scan 扫描全表的数据</p></li></ul><p>RowKey 可以是任意字符（最大长度为64KB），在 HBase 内部，rowKey 保存为字节数组，存储时会按照 rowKey 的字典序排序后存储。</p><h4 id="Region（区域）"><a href="#Region（区域）" class="headerlink" title="Region（区域）"></a>Region（区域）</h4><p>Table 按行分为多个 Region。Region是按数据量分割的，刚开始只有一个 Region，随着数据量的增加会自动分割为新的 Region，Region是 HBase 分布式存储和负载均衡的最小单元，分布在不同的 RegionServer中。</p><h4 id="Store"><a href="#Store" class="headerlink" title="Store"></a>Store</h4><p>每个 Region 中有一个或多个 Store，每个 Store 保存着一个 Column Family 中的数据。往 HBase 中写数据时，Store内部由 Memstore 刷写到 StoreFile（HFile）中，为保证数据的安全性，数据会先写入 WAL，再写入 MemStore。读数据时，如果是首次读操作，会先扫描 StoreFile，并把数据缓存到 BlockCache 中，下次读就直接从内存中把数据读出来，默认采用 LRUBlockCache 策略。</p><h4 id="Column-Family（列簇）"><a href="#Column-Family（列簇）" class="headerlink" title="Column Family（列簇）"></a>Column Family（列簇）</h4><p>HBase 表中的每个列都归属于某个列簇，列簇作为表的 Schema（列不是），建表的时候至少要指定一个列簇，列簇支持增加和修改，删除会连带它所包含的列以及数据全部删除。</p><p>查询数据时需要指定列簇 columnFamily:column。列簇也不是越多越好，假如要返回一个数据的所有字段，那么就需要遍历每个列簇取出它下面的所有字段，列簇越多 IO 次数就越多，搜寻文件就越多，效率就越低，官方推荐小于等于3，最好就一个。列簇在在文件系统中就是一个文件夹。</p><h4 id="Timestamp（时间戳）"><a href="#Timestamp（时间戳）" class="headerlink" title="Timestamp（时间戳）"></a>Timestamp（时间戳）</h4><p>HBase 通过 rowKey 和 column 确定一个存储单元 Cell，每个存储单元存储着同一份数据的不同版本，版本通过时间戳来索引。</p><p>支持版本索引以及最大版本数在建表时指定：<code>create &#39;Student&#39;, &#123;NAME =&gt; &#39;Stulnfo&#39;, VERSIONS =&gt; 3&#125;, &#123;NAME =&gt;&#39;Grades&#39;, BLOCKCACHE =&gt; true&#125;</code>，不同列簇的最大版本数可以不相同。</p><p>插入数据的时候可以显式指定时间戳，不指定的话系统会默认帮我们指定为当前时间戳。查询数据默认返回最近的时间戳（版本）对应的数据，若要返回不同版本的数据需指定时间戳。</p><h4 id="Cell（单元格）"><a href="#Cell（单元格）" class="headerlink" title="Cell（单元格）"></a>Cell（单元格）</h4><p>一个字段可以存储多个版本的数据，而每一个版本就称为一个 Cell，所以 HBase 中的最小数据粒度是一条数据的某个版本，也就是一个 Cell，而不是这一条数据。Cell 中的数据是没有类型的，底层全部使用字节码存储。</p><h1 id=""><a href="#" class="headerlink" title=""></a></h1>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> HBase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Zookeeper-介绍以及客户端操作</title>
      <link href="/2022/02/10/Software/Zookeeper-%E4%BB%8B%E7%BB%8D%E4%BB%A5%E5%8F%8A%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%93%8D%E4%BD%9C/"/>
      <url>/2022/02/10/Software/Zookeeper-%E4%BB%8B%E7%BB%8D%E4%BB%A5%E5%8F%8A%E5%AE%A2%E6%88%B7%E7%AB%AF%E6%93%8D%E4%BD%9C/</url>
      
        <content type="html"><![CDATA[<p>Zookeeper 基础概念介绍以及 Shell 客户端命令和 Curator API 操作。</p><span id="more"></span><h3 id="Begin"><a href="#Begin" class="headerlink" title="Begin"></a>Begin</h3>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Zookeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop-Yarn资源调度器了解</title>
      <link href="/2021/12/06/Software/Hadoop-Yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%99%A8%E4%BA%86%E8%A7%A3/"/>
      <url>/2021/12/06/Software/Hadoop-Yarn%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E5%99%A8%E4%BA%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p>简单了解了 YARN 资源调度器并针对容量调度器做了简单的介绍和配置。</p><span id="more"></span><h3 id="任务执行过程"><a href="#任务执行过程" class="headerlink" title="任务执行过程"></a>任务执行过程</h3><p>Application在Yarn中的执行过程，整个执行过程可以总结为三步：</p><ol><li>应用程序提交</li><li>启动应用的 ApplicationMaster 实例</li><li>ApplicationMaster 实例管理应用程序的执行</li></ol><div align=center><img src="Yarn任务提交流程.png"></div><p>具体提交过程为：</p><ol><li>客户端程序向 ResourceManager 提交应用并请求一个 ApplicationMaster 实例；</li><li>ResourceManager 找到一个可以运行一个 Container 的 NodeManager，并在这个 Container 中启动 ApplicationMaster 实例；</li><li>ApplicationMaster 向 ResourceManager 进行注册，注册之后客户端就可以查询 ResourceManager 获得自己 ApplicationMaster 的详细信息，以后就可以和自己的 ApplicationMaster 直接交互了（这个时候，客户端主动和 ApplicationMaster 交流，应用先向 ApplicationMaster 发送一个满足自己需求的资源请求）；</li><li>在平常的操作过程中，ApplicationMaster 根据 <code>resource-request协议</code> 向 ResourceManager 发送 <code>resource-request请求</code>；</li><li>当 Container 被成功分配后，ApplicationMaster 通过向 NodeManager 发送 <code>container-launch-specification信息</code> 来启动Container，<code>container-launch-specification信息</code>包含了能够让Container 和 ApplicationMaster 交流所需要的资料；</li><li>应用程序的代码以 task 形式在启动的 Container 中运行，并把运行的进度、状态等信息通过 <code>application-specific协议</code> 发送给ApplicationMaster；</li><li>在应用程序运行期间，提交应用的客户端主动和 ApplicationMaster 交流获得应用的运行状态、进度更新等信息，交流协议也是 <code>application-specific协议</code>；</li><li>一旦应用程序执行完成并且所有相关工作也已经完成，ApplicationMaster 向 ResourceManager 取消注册然后关闭，用到所有的 Container 也归还给系统。</li></ol><h3 id="调度器"><a href="#调度器" class="headerlink" title="调度器"></a>调度器</h3><p>Hadoop 作业调度器主要有三种：FIFO 调度器、容量调度器和公平调度器，Apache Hadoop 3.1.3 默认的调度器是Capacity Scheduler（容量调度器），CDH 默认的调度器是 Fair Scheduler（公平调度器）。详见 yarn-default.xml文件：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.class<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>配置Yarn使用的调度器插件类名<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="FIFO调度器"><a href="#FIFO调度器" class="headerlink" title="FIFO调度器"></a>FIFO调度器</h4><p>FIFO（First In First Out）：单队列，根据任务的先后顺序，先来的先处理，后来的后处理，后来的服务只能等待先来的服务运行完毕释放资源后才能按顺序执行，这种调度方式不推荐使用，会造成资源的浪费且效率低下。</p><h4 id="容量调度器"><a href="#容量调度器" class="headerlink" title="容量调度器"></a>容量调度器</h4><p>Capacity Scheduler 是多用户调度器，生产中常用这个调度器，它有以下几个特点：</p><ul><li>多队列：可以有多个队列，每个队列可配置一定的资源，每个队列采用 FIFO 调度策略</li><li>容量保证：可以为每个队列设置资源最低保障和资源使用上限，哪怕资源紧张也会保证单个队列内的最低资源使用</li><li>灵活性：如果一个队列资源空闲，可暂时共享给其他队列使用，一旦该队列有新的应用程序提交，其他队列借用的资源会归还给该队列</li><li>多用户：支持多用户共享集群和多应用同时运行，同一队列内若是有资源空闲也会运行后续程序，只是保证运行顺序是 FIFO；而且为防止同一个用户独占集群资源，会对一个用户下的所有应用所占用的资源进行限定</li></ul><p>容器调度器资源分配算法自上而下分为队列资源分配、作业资源分配和容器资源分配：</p><ul><li>队列资源分配：从 root 开始，使用深度优先算法，优先选择资源占用率较低的队列分配资源</li><li>作业资源分配：默认按照作业提交的优先级和提交时间顺序进行资源分配</li><li>容器资源分配：按容器的优先级进行分配；如果优先级相同，则按照数据本地性原则进行分配资源：任务和数据在同一节点 &gt; 任务和数据在同一机架 &gt; 任务和数据不在同一节点也不在同一机架</li></ul><h4 id="公平调度器"><a href="#公平调度器" class="headerlink" title="公平调度器"></a>公平调度器</h4><p>Fair Scheduler 公平调度器和容量调度器是 Facebook 开发的调度器</p><ul><li>与容量调度器的相同点：多队列、容量保证、灵活性、多租户</li><li>不同点：核心调度策略不同：容量调度器优先选择利用率低的队列，公平调度器优先选择对资源的缺额比例大的；每个队列可以设置的资源分配方式不同：容量调度器可选 FIFO 和 DRF，公平调度器可选 FIFO、FAIR 和 DRF</li></ul><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><p>详情见官网 <a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YarnCommands.html">Yarn 相关命令</a></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">yarn application -list  # 查看所有的application</span><br><span class="line">yarn application -list -appStates &lt;states&gt;  # 选择指定状态的application：ALL, NEW, NEW_SAVING, SUBMITTED, ACCEPTED, RUNNING, FINISHED, FAILED, KILLED</span><br><span class="line">yarn application -kill &lt;app ID&gt;  # 根据app ID杀死指定的application</span><br><span class="line">yarn applicationattempt -list &lt;ApplicationAttempt ID&gt;  # 查看尝试运行的application</span><br><span class="line"></span><br><span class="line">yarn logs -applicationId &lt;app ID&gt;  # 查看指定application的全部日志</span><br><span class="line">yarn logs -applicationId &lt;app ID&gt; -containerId &lt;container ID&gt;  # 查看指定container的日志</span><br><span class="line"></span><br><span class="line">yarn container -list &lt;ApplicationAttempt ID&gt;  # 查看正在运行的application下的所有container</span><br><span class="line">yarn container -status &lt;container ID&gt;  # 查看容器状态</span><br><span class="line"></span><br><span class="line">yarn node -list all  # 查看所有nodemannager</span><br><span class="line">yarn rmadmin -refreshQueues  # 更新队列相关的配置</span><br><span class="line">yarn queue -status &lt;queue name&gt;  # 查看队列状态</span><br></pre></td></tr></table></figure><h3 id="容量调度器配置案例"><a href="#容量调度器配置案例" class="headerlink" title="容量调度器配置案例"></a>容量调度器配置案例</h3><p>下方对<code>capacity-scheduler.xml</code>文件做了一些配置，除了 default 队列新建了一个 hive 队列，然后给 hive 队列分配所有资源的 80%，default 占 20%；<br>然后又在 hive 队列下新建了 operation 和 development 队列，分别占用 hive 资源的85%和15%；<br>随后又对 operation 资源进行用户分配，指定将 jason 用户映射到 operation 队列中，将 hadoop_admin 组中的用户映射到 development 队列，将 hive 用户映射到与 Linux 中主组名相同的队列。</p><p>配置完成后不用重启集群，使用<code>yarn rmadmin -refreshQueues</code>即可更新队列相关配置。</p><p>在使用的时候 mapreduce 任务直接在参数中指定队列名称即可：<code>hadoop jar hadoop-mapreduce-examples-3.1.3.jar wordcount -D mapreduce.job.queuename=operation /input /output</code>，Hive 操作中则使用<code>set mapreduce.job.queuename=operation </code>来指定要提交的队列。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">  Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span></span><br><span class="line"><span class="comment">  you may not use this file except in compliance with the License.</span></span><br><span class="line"><span class="comment">  You may obtain a copy of the License at</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    http://www.apache.org/licenses/LICENSE-2.0</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">  Unless required by applicable law or agreed to in writing, software</span></span><br><span class="line"><span class="comment">  distributed under the License is distributed on an &quot;AS IS&quot; BASIS,</span></span><br><span class="line"><span class="comment">  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.</span></span><br><span class="line"><span class="comment">  See the License for the specific language governing permissions and</span></span><br><span class="line"><span class="comment">  limitations under the License. See accompanying LICENSE file.</span></span><br><span class="line"><span class="comment">--&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- root始终是创建所有队列的顶级队列，因此我们现在顶级队列中创建2个子顶级队列。 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.queues<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>default,hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>这是为root顶级队列定义子队列，默认值为:&quot;default&quot;<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 注意哈，当我们定义好顶级队列的子队列后，我们接下来做为其设置队列容量，如果你没有做该步骤，那么启动RM将会失败。  --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>80<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>这里指定的是root顶队列下的hive这个子队列，该队列默认占用整个集群的80%的资源<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.default.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>20<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>这里指定的是root顶队列下的default这个子队列，该队列默认占用整个集群的20%的资源<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- </span></span><br><span class="line"><span class="comment">    我们可以为子顶队列继续分配子队列，比如我们将hive这个队列分为:&quot;operation&quot;和&quot;development&quot;这2个子队列。</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    下面配置的队列存在以下关系:</span></span><br><span class="line"><span class="comment">        (1)我们可以说&quot;hive&quot;这个队列是&quot;operation&quot;和&quot;development&quot;的父队列;</span></span><br><span class="line"><span class="comment">        (2)&quot;operation&quot;和&quot;development&quot;这2个队列是&quot;hive&quot;的子队列;</span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">    温馨提示:</span></span><br><span class="line"><span class="comment">        我们不能直接向父队列提交作业，只能向叶子队(就是没有子队列的队列)列提交作业。</span></span><br><span class="line"><span class="comment">    --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.queues<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>operation,development,testing<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>此处我在&quot;hive&quot;这个顶级队列中定义了三个子顶队列，分别为&quot;operation&quot;,&quot;development&quot;和&quot;testing&quot;<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!--</span></span><br><span class="line"><span class="comment">       按百分比为&quot;hive&quot;的2个子队列(即&quot;operation&quot;和&quot;development&quot;)分配容量，其容量之和为100%。  </span></span><br><span class="line"><span class="comment"></span></span><br><span class="line"><span class="comment">       需要注意的是:</span></span><br><span class="line"><span class="comment">       各个子队列容量之和为父队列的总容量,但其父队列的总容量又受顶队列资源限制;</span></span><br><span class="line"><span class="comment">       换句话说，&quot;operation&quot;,&quot;development&quot;这2个队列能使用的总容量只有集群总量的80%，因为&quot;hive&quot;这个队列容量配置的就是80%.</span></span><br><span class="line"><span class="comment">   --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.operation.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>85<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>这里指定的是一个&quot;operation&quot;队列占&quot;hive&quot;队列的百分比，即所有资源的85% * 80%的资源归该队列使用<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.development.capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>15<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>这里指定的是一个&quot;development&quot;队列占&quot;hive&quot;队列的百分比，即所有资源的15% * 80%的资源归该队列使用<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 配置限制用户容量的相关参数 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.operation.user-limit-factor<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    为支持叶子队列中特定用户设置最大容量。defalut队列用户将百分比限制在0.0到1.0之间。此参数的默认值为1，这意味着用户可以使用所有叶子队列的配置容量。</span><br><span class="line">    如果将此参数的值设置大于1，则用户可以使用超出叶子队列容量限制的资源。比如设置为2，则意味着用户最多可以使用2倍与配置容量的容量。</span><br><span class="line">    如果将其设置为0.25，则该用户仅可以使用队列配置容量的四分之一。</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.development.user-limit-factor<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.5<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.operation.maximum-capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>50<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    此参数用于设置容量的硬限制，此参数的默认值为100。如果要确保用户不能获取所有父队列的容量，则可以设置此参数。</span><br><span class="line">    此处我将向&quot;root.hive.operation&quot;叶子队列提交作业的用户不能占用&quot;root.hive&quot;队列容量的50%以上。</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.development.maximum-capacity<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>50<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.operation.minmum-user-limit-percent<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    假设配置了可以占用500GB RAM的叶子队列，如果20个用户象征队列提交作业怎么样？当然，你可以让所有20个用户的容器占用25GB的RAM，但那样太慢了。</span><br><span class="line">    我们可以通过配置该参数来控制分配给叶子队列用户的最小资源百分比。如果将此参数的值设置为10，则意味着通过此队列运行的应用程序的用户至少会被分配到</span><br><span class="line">    当前队列所有资源的10%。</span><br><span class="line">    综上所述，此参数可以限制用户的最小值资源百分比，最大值取决于集群中运行应用程序的用户数，它的工作流程如下:</span><br><span class="line">        (1)当第一个向这个叶子队列提交作业的用户可以使用100%的叶子队列的资源分配;</span><br><span class="line">        (2)当第二个向这个叶子队列提交作业的用户使用该队列的50%的资源;</span><br><span class="line">            (3)当第三个用户向队列提交应用程序时，所有用户被限制为该队列33%;</span><br><span class="line">            (4)随着其他用户开始将作业提交到此队列，最终每个用户可以稳定地使用队列10%的资源，但不会低于该值，这就是我们设置最小资源百分比的作用;</span><br><span class="line">        (5)需要注意的时，只有10个用户可以随时使用队列(因为10个用户已经占用完该队列资源)，而其他用户必须等待前10名用户任意一个用户释放资源。才能依次运行已提交的Job;</span><br><span class="line">   <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 限制应用程序数量 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.operation.maximum-applications<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>5000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    该参数可以对容量调度器提交的应用程序数量设置上限，即为在任何时候给定时间可以运行的最大应用程序数量设置硬限制。此参数root队列的默认值为10000。</span><br><span class="line">    对应的子顶队列以及叶子队列的最大应用上限也有对应的计算公式，比如我们要计算default队列的最大容器大小公式如下:</span><br><span class="line">        default_max_applications = root_max_applications * (100 - yarn.scheduler.capacity.root.hive.capacity)</span><br><span class="line">    最终算得default_max_applications的值为2000(带入上面的公式:&quot;10000 * (100 - 80)%&quot;,即:10000 * 0.2)</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.maximum-am-resource-percent<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>0.2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    该参数用于设置所有正在运行的ApplicationMasters可以使用的集群资源的百分比，即控制并发运行的应用程序的数量。此参数的默认值为10%。</span><br><span class="line">    当设置为0.2这意味着所有ApplicationMaster不能占用集群资源的20%以上(ApplicationMaster容器的RAM内存分配，这是为应用程序创建第一个容器)。</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 配置队列管理权限 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.operation.acl_administer_queue<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>hadoop_admin<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    指定谁可以管理root.hive.operation该叶子队列，其中&quot;*&quot;表示术语指定组的任何人都可以管理此队列。</span><br><span class="line">    可以配置容量调度器队列管理员来执行队列管理操作，例如将应用程序提交到队列，杀死应用程序，停止队列和查看队列信息等。</span><br><span class="line">    上面我配置的&quot;hadoop_admin&quot;，这意味着在hadoop_admin组的所有用户均可以管理&quot;root.hive.operation&quot;队列</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.operation.acl_submit_applications<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>jason,hive<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    此参数可以指定那些用户将应用程序提交到队列的ACL。如果不知定制，则从层次结果中的父队列派生ACL。根队列的默认值为&quot;*&quot;，即表示任何用户</span><br><span class="line">    常规用户无法查看或修改其他用户提交的应用程序，作为集群管理员，你可以对队列和作业执行以下操作:</span><br><span class="line">        (1)在运行时更改队列的定义和属性;</span><br><span class="line">        (2)停止队列以防止提交新的应用程序;</span><br><span class="line">        (3)启动停止的备份队列;</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 配置用户映射到队列 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.queue-mappings<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>u:jason:operation,g:hadoop_admin:development,u:hive:%primary_group<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    此参数可以将用户映射到指定队列，其中u表示用户，g表示组。</span><br><span class="line">    &quot;u:jason:operation&quot;:</span><br><span class="line">        表示将jason用户映射到operation队列中。</span><br><span class="line">    &quot;g:hadoop_admin:development&quot;:</span><br><span class="line">        表示将hadoop_admin组中的用户映射到development队列中。</span><br><span class="line">    &quot;u:hive:%primary_group&quot;:</span><br><span class="line">        表示将hive用户映射到与Linux中主组名相同的队列。</span><br><span class="line">    温馨提示:</span><br><span class="line">        YARN从左到右匹配此属性的映射，并使用其找到的第一个有效映射。</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">&lt;!-- 配置队列运行状态 --&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.state<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>RUNNING<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span></span><br><span class="line">    可以随时在跟对任意队列级别停止或启动队列，并使用&quot;yarn rmadmin -refreshQueues&quot;使得配置生效，无需重启整个YARN集群。</span><br><span class="line">    队列有两种状态，即STOPPED和RUNNING，默认均是RUNNING状态。</span><br><span class="line">    需要注意的是:</span><br><span class="line">        (1)如果停止root或者父队列，则叶子队列将变为非活动状态(即STOPPED状态)。</span><br><span class="line">            (2)如果停止运行中的队列，则当前正在运行的应用程序会继续运行直到完成，并且不会将新的的应用程序提交到此队列。</span><br><span class="line">        (3)若父队列为STOPPED，则子队列无法配置为RUNNING，若您真这样做，将会抛出异常哟。</span><br><span class="line">        温馨提示:</span><br><span class="line">        可以通过ResourceManager Web UI的Application页面中的Scheduler页面，来监视容量调度器队列的状态和设置。</span><br><span class="line">    <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.default.state<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>RUNNING<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>将&quot;root.default&quot;队列的状态设置为&quot;RUNNING&quot;状态<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.state<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>RUNNING<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>将&quot;root.hive&quot;队列的状态设置为&quot;RUNNING&quot;状态<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.operation.state<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>RUNNING<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>将&quot;root.hive.operation&quot;队列设置为&quot;RUNNING&quot;状态<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.development.state<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>RUNNING<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>将&quot;root.hive.development&quot;队列设置为&quot;RUNNING&quot;状态<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.capacity.root.hive.testing.state<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>RUNNING<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">description</span>&gt;</span>将&quot;root.hive.testing&quot;队列设置为&quot;RUNNING&quot;状态<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>读后感-南京大屠杀</title>
      <link href="/2021/11/25/Mind/%E8%AF%BB%E5%90%8E%E6%84%9F-%E5%8D%97%E4%BA%AC%E5%A4%A7%E5%B1%A0%E6%9D%80/"/>
      <url>/2021/11/25/Mind/%E8%AF%BB%E5%90%8E%E6%84%9F-%E5%8D%97%E4%BA%AC%E5%A4%A7%E5%B1%A0%E6%9D%80/</url>
      
        <content type="html"><![CDATA[<p>书名南京大屠杀，内容无需再介绍，细节也不再多言，它带给你的只有对战时日本的痛恨以及对南京同胞的惋惜同情，纵然不能因为几十年前的战争仇视如今的日本，但他们中很多人的做法仍然值得上一个评价——无耻至极！</p><span id="more"></span><p>前段时间淘书，将《毛泽东传》、《走进周恩来》等书籍尽数收入书单，随之推荐的《南京大屠杀》也被我注意到，应该说是作者张纯如是吸引我读这本书的最大因素。在书籍开头的中文版序中，我大概了解到了她写这本书的背景以及初衷，尤其是得知张纯如女士有很大可能是因为创作这本书的经历才导致的自杀更引起了我阅读它的兴趣。</p><p>我对南京大屠杀的印象仅仅停留在30万这个数字上，其他的比如起因，经过、结果以及因为什么会让日本人这么丧心病狂一概不知，在我读完这本书后更是感到羞愧，不仅是对发生在祖国大地上的这一残忍虐杀行为所知甚少，也包括了对于抗日战争以及新中国的崛起也知之甚少。我生活在全世界最安全的国家，过着如此幸福的生活，却不知道祖国的发展历程以及所经受的磨难，实在难以原谅。</p><p>文章的第一章第一段话就是“要理解日军的所作所为，必须首先弄清楚一系列显而易见的问题。在南京大屠杀期间，究竟为什么日本士兵的行为竟然完全脱离人类基本的行为规范”，作者先讲述了日本的历史以及在国际上所处的尴尬位置，从而促使他们产生了“大东亚共荣”这种所谓“圣战”的想法，这是第一个原因；第二个原因是日本的阶级制度，军方让所有人都以“天皇”为中心，把他当做心中的上帝，并且日本的教育体制从20世纪30年代开始就变得高度军事化，对于年轻士兵的训练异常严苛，教官会想尽一切办法打击他们的自信心，这也进一步促使了他们在面对手无缚鸡之力的南京民众，手握生杀大权时，扭曲的心理被放到最大，肆意释放着内心的压抑；第三个原因是日本曾经盲目的认为他们在三个月内就能征服全中国，然而仅仅是上海就花费了好几个月，上海沦陷后当时的国民政府南京自然成为了他们的征服和发泄目标，这从他们从未进入南京城之前就已经屠杀郊外的普通平民就可以看出来；第四个原因是蒋介石的无能、微操、临阵脱逃、唯唯诺诺，从最开始的下令死守，到明知败局却坚决不撤军，到放弃日本给出的停火协议，再到南京沦陷前一天通知唐生智等人撤退并且未将撤退的消息传递给下面的战士等等，所有的选项没有一个是正确的，看到这一段时气得我头皮发麻。</p><p>有些人认为留在南京的军队以及普通民众未发生反抗也是其中一个原因，且不论城中一大部分人是普普通通的老百姓，且不论留下的是被已经打懵的国民党军队，且不论日军在装备方面的优势，单单站在上帝角度对受害者加以评判就有问题，就跟现在有些人责怪那些遭到猥亵的妇女穿着太过暴露从而导致罪犯去犯罪是一个道理。和当时犹太人的遭遇一样，他们只是一群渴望生存的老百姓，现在去责怪他们没有勇气不反抗是无意义的，他们是受害者，应该受到惩罚和唾弃的只有小日本。</p><p>有关南京大屠杀的过程实在不愿提及，杀人比赛、折磨、肢解、强奸、烧死、毒死等等，再看一遍心头实在堵得慌，读到一半的时候买了一本纸质书，很长一段时间我是没有勇气再翻开了。在这一刻他们不再是日本人，说他们是狗东西都是侮辱了狗。作者在开头用了大量的篇幅描述日本的畸形文化和历史，在这个世界上恐怕也只有他们能做出这种恶行，相比于纳粹残害犹太人，这些猪狗不如的东西做出来的行为更加让人发自内心的恶心，尤其是大多数人逃脱了制裁安享晚年，实在是咽不下这口气。</p><p>南京大屠杀结束后大约有二三十万的生还难民，这是因为城内的诸多外国人在这期间与小日本斗智斗勇，不顾自己的安危保护中国难民免遭小日本杀害。有中国的辛德勒——约翰·拉贝，是南京城内国际安全区的主席，在他和其他外国友人的帮助下，保证了安全区内的平民尽可能的不被小日本杀害；有南京城内唯一的外科医生——罗伯特·威尔逊，随着局势日益恶化，医院的职员越来越少，但他却留了下来，救助了不少战斗伤员以及被小日本伤害的难民；有南京的活菩萨——威廉明娜·魏特琳，当时担任金陵女子文理学院教育系的主任和院长，她曾保护数千名中国妇女免遭小日本蹂躏。在他们的保护下南京城最终有二三十万难民得以生还，但他们的晚年生活都很凄惨，约翰·拉贝回国后将南京大屠杀的证据递交给希特勒，但却遭到纳粹的审问和威胁，最后更是丢了工作，食不果腹，有幸最终南京人民知道了这件事给他寄去了生活物资和金钱他们一家人才得以体面的生存下去。然后是罗伯特·威尔逊，他的后人有理由相信正是南京大屠杀时期不分昼夜救治伤员的劳累以及那段经历所造成的的阴影导致他过早的去世。最后是威廉明娜·魏特琳，在回到自己的国家后因饱受战争时期的折磨，最终身患抑郁症自杀身亡。</p><p>如此规模的大屠杀经历了六周，最终还是因为几名美国记者将此事发表到了报纸上，小日本迫于舆论的压力才想起来收拾烂摊子，这时小日本的无耻行径才刚刚开始。他们打扫了主要的街道，在墙上涂画进城后与老百姓和平共处的插画，鼓励日本旅游者去南京旅游，向外散布南京城内没有发生过大屠杀的谣言，这一系列操作可谓无耻至极，但南京城内的尸山血海以及众多生还者的证言并没有给他们说谎的机会。为了掩盖日本军方令人作呕的暴行，日本甚至阻挠他国外交使节重返南京。但事实证明，他们最终未能成功掩盖真相，消息还是被国际社会所获知，他们受到了多方的谴责，在南京战争罪行审判以及远东国际军事法庭上他们所残害我国同胞的骸骨就是最有力的犯罪证明。</p><p>但这场大屠杀并没有随着国际法庭的审判而结束，由于美国战后与苏联冷战，战败国的日本成为了美国拉拢的对象，在狼子野心的美国帮助下，战后的日本迅速恢复了经济，如今这父子俩狼狈为奸，在国际上不断给我们国家制造麻烦。再就是日本的态度，他们没有战后德国的觉悟，如果将德国政府战后支付的所有赔款都计算在内的话，包括个体受害者赔偿、财产损失赔偿、抚恤性赔偿、国家法定赔偿、个别案例的最后赔偿，以及根据与以色列和其他16个国家达成的战争赔款协定而做出的赔偿，总数将近1240亿德国马克，约合600亿美元。日本则几乎没有对其战争罪行支付过任何赔偿。甚至在政治上不承认南京大屠杀的存在，叫嚣着这都是中国的诬陷，学术界和教科书上也对南京大屠杀的死亡人数和所造成影响进行淡化，摇身一变将自己变为二战的受害者，原因是广岛和长崎遭受了原子弹的轰炸，对此我只想说活该。</p><p>相比于日本的无耻行径和美国的狼子野心，终究是当时的祖国不够强大。如今的中国已不是彼时的中国，我们强大繁荣、安居乐业，这们不该饱暖思淫欲，遗忘历史终将会导致历史再次重演。近几年国内涌现的日本文化思潮影响着每一个年轻人，有人甚至为了他们所热爱的文化和偶像去洗白靖国神社，而他们对抗日战争也所知甚少或者毫不感兴趣。也许是网络放大了极少数个体的极端化思想，也许是我对这些异己有所偏见，但愿可以增强下一代对历史的认知和了解，我们不是要一直活在仇恨之中，重视历史只是为了不让历史再次重现，认识到现如今的幸福生活是用烈士的血换来的，不容我们去践踏挥霍。</p>]]></content>
      
      
      <categories>
          
          <category> Mind </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mind </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据的尽头是哪里</title>
      <link href="/2021/11/06/Data/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%B0%BD%E5%A4%B4%E6%98%AF%E5%93%AA%E9%87%8C/"/>
      <url>/2021/11/06/Data/%E6%95%B0%E6%8D%AE%E7%9A%84%E5%B0%BD%E5%A4%B4%E6%98%AF%E5%93%AA%E9%87%8C/</url>
      
        <content type="html"><![CDATA[<p>这一篇文章并不是讨论技术意义上的数据尽头，因为我是一个做数据的，是想讨论一下一个数据人未来的尽头在哪里，顺便思考思考自己的职业规划。</p><span id="more"></span><p>初入社会到现在的工作其实一直跟数据有关，但大多拿不上台面，没那么高大上也没那么挣钱，可替代性也比较强，随着工作年限的增加自然不能什么都不考虑埋头苦干，一是为自己的出路，能更好的发挥自己的价值，二是为了工资的提升，做一些规划就很有必要。</p><p>以前不说了，拿我现在来说，在岗职位的技术含量也不是很高，主要是取数，出报表，其他数仓和 ETL 相关的职责没涉及多少，拿我的经验和掌握的技术来说，下一步还是要往离线数仓的方向发展。定这个目标主要有两方面的考虑，一是脱离单纯的取数，适当增加自己的不可替代性，虽然没有规定取数完了要往数仓转，但相比数据分析或者其他可以提升的方向我还是对数仓更感兴趣；二是为以后的数据开发、数据治理或者实时数仓打基础，因为身处北京一线城市，自然少不了互联网公司，有众多的 APP 和网页类产品，就少不了数据的存储计算，而且对时效和生态方面的要求只会越来越高，未来只会说是技术迭代更新，但思想仍不会变。同样我还定了一个标准，新公司所使用的技术不能过于单调，数据量不能太少，这对于自己的经验积累和锻炼尤其重要，公司规模也不能太小，首先得确保自己在公司内部有晋升渠道，至于公司不能给提供更好的平台需要换公司这是下下策。以上就是我对这次换工作的一些思考，当然，工资太少是事实，但这能说出来吗？</p><p>对于以后的发展方向有几个可供选择，一是转向业务，但也不是纯业务，做技术与业务之间的桥梁，重点并不关注底层技术的实现，而是优先考虑业务的逻辑与痛点，但这不是我感兴趣的，而且缺乏沟通能力也是我的一大弱点；二是数据治理和建模方面的，我也是从招聘岗位和看文章才知道有这类型岗位的存在，目的专注于公司层面的数据生态，搭建企业级的数据平台，当然这里面涉及到的就比较复杂全面了，远不是我现在所能达到的；三是数据开发、技术架构层面的，相对其他两个方向我对这个是最感兴趣的，注重于技术选型和架构，当然也包括底层实现，一般是部门或者小公司级别的，好处是不用绞尽脑汁的设计一些逻辑方面和生态方面的东西，这不是我擅长的，坏处是所要掌握的技术太多，学习之路太过漫长。而且另一方面又担心纯技术发展随着年龄的增长会将自己淘汰掉，35岁职业生涯分水岭不管在什么年龄都会考虑到，目前小公司或者国企这些公司限制的没那么严，打算三十多岁就慢慢往这方面转。</p><p>说完了大方向的规划，接下来是小方向和小目标的抉择了，怎样实现它也是个难点，前几天看了一篇文章把我的思绪引向了开源，确实，不管是自己搞开源还是贡献开源技术一方面能锻炼自己的技术，一方面也能积累人脉，为简历加分，所以我打算从现在开始就抽点时间放在这个上面，刚开始当然是各种技术的学习和提升，首先得把基础搭好啊，不然只有想法不知从何做起怎么行。还有一个小方向就是非全日制研究生，学历虽然对现在的我来说提升并没有多大，但我相信三十岁以后肯定用得上，毕竟我的第一学历有点过于糟糕了。最后，最重要最重要的小方向还没有说，当然是好好准备眼前的换工作，先把工作所需要的技术搞明白再好好实现自己的目标也不晚。</p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mind </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive入门-自定义函数（Scala）</title>
      <link href="/2021/10/31/Software/Hive%E5%85%A5%E9%97%A8-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0%EF%BC%88Scala%EF%BC%89/"/>
      <url>/2021/10/31/Software/Hive%E5%85%A5%E9%97%A8-%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0%EF%BC%88Scala%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<p>Hive 自定义函数的开发，使用 Scala 语言开发，目前只有 UDF 函数</p><span id="more"></span><p>首先需要导入相关的依赖，注意自己的 Hive 和 Hadoop 版本。其次是 @Description 装饰，name 是函数名，value 是函数及参数以及函数介绍，extended 则是函数的详细使用方法，这些信息在 Hive 中<code>desc function extended func_name</code>的时候会显示出来（但是我测试失败了）</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependencies</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--hadoop版本信息--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hadoop<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hadoop-client<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.3<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--hive版本信息--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.apache.hive<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>hive-exec<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>3.1.2<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependencies</span>&gt;</span></span><br></pre></td></tr></table></figure><p>Hive UDF函数需要继承<code>org.apache.hadoop.hive.ql.exec.udf</code>类，不需要重写函数，只需要在继承的类中定义 evaluate 方法即可，main 方法是为了测试函数。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> hive_scala</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.<span class="type">UDF</span></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hive.ql.exec.<span class="type">Description</span></span><br><span class="line"></span><br><span class="line"><span class="meta">@Description</span>(</span><br><span class="line">  name = <span class="string">&quot;concat_name&quot;</span>,</span><br><span class="line">  value = <span class="string">&quot;concat_name(str) - Concat &#x27;name:&#x27; to str\n&quot;</span>,</span><br><span class="line">  extended = <span class="string">&quot;    SELECT concat_name(&#x27;wxk&#x27;);\n&quot;</span> +</span><br><span class="line">    <span class="string">&quot;    &#x27;name:wxk&#x27;&quot;</span>)</span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 给输入的字符串加上Name：前缀</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">UDFConcatName</span> <span class="keyword">extends</span> <span class="title">UDF</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> name: <span class="type">String</span> = <span class="string">&quot;Name: &quot;</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(str: <span class="type">String</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    name + str</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(evaluate(<span class="string">&quot;wxk&quot;</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>然后是在 Hive 中注册使用，首先是将代码打包，然后可以在代码中直接使用命令将 jar 包添加到 classpath 中，再注册使用，不过仅限于当前会话，也可以将 jar 包传入 HDFS 创建永久函数，多个会话可以多次使用</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 当前会话中使用</span></span><br><span class="line"><span class="keyword">add</span> jar <span class="operator">/</span>home<span class="operator">/</span>warehouse<span class="operator">/</span>UDFConcatName<span class="number">-1.0</span>.jar;</span><br><span class="line">list jars;  <span class="comment">-- 查看当前job的classpath</span></span><br><span class="line"><span class="keyword">create</span> temporary <span class="keyword">function</span> concat_name <span class="keyword">as</span> <span class="string">&#x27;hive_scala.UDFConcatName&#x27;</span>;</span><br><span class="line"><span class="keyword">select</span> concat_name(<span class="string">&#x27;张三&#x27;</span>) <span class="keyword">as</span> concatName;</span><br><span class="line"><span class="keyword">drop</span> temporary <span class="keyword">function</span> if <span class="keyword">exists</span> concat_name;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 注册永久函数</span></span><br><span class="line">hadoop fs <span class="operator">-</span>put <span class="operator">/</span>home<span class="operator">/</span>warehouse<span class="operator">/</span>UDFConcatName<span class="number">-1.0</span>.jar <span class="operator">/</span>warehouse<span class="operator">/</span>functions<span class="operator">/</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">function</span> concat_name <span class="keyword">as</span> <span class="string">&#x27;hive_scala.UDFConcatName&#x27;</span> <span class="keyword">using</span> <span class="string">&#x27;hdfs://warehouse/functions/UDFConcatName-1.0.jar&#x27;</span>;</span><br><span class="line"><span class="keyword">select</span> concat_name(<span class="string">&#x27;张三&#x27;</span>) <span class="keyword">as</span> concatName;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">function</span> if <span class="keyword">exists</span> concat_name;</span><br><span class="line">reload functions;  <span class="comment">-- 其他会话可以使用使用reload更新函数</span></span><br></pre></td></tr></table></figure><p>UDAF 和 UDTF 看不懂，还是用 Spark 吧。。。</p>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive入门-Hive知识体系</title>
      <link href="/2021/10/28/Software/Hive%E5%85%A5%E9%97%A8-Hive%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/"/>
      <url>/2021/10/28/Software/Hive%E5%85%A5%E9%97%A8-Hive%E7%9F%A5%E8%AF%86%E4%BD%93%E7%B3%BB/</url>
      
        <content type="html"><![CDATA[<p>介绍了 Hive 数据仓库的特征、计算引擎、存储压缩、HQL、底层原理、性能优化以及常见面试题等知识。文章来源于<a href="https://mp.weixin.qq.com/s/bgsRLETej0KcZs-lQKNYKg#">五分钟学大数据</a> ，作者园陌</p><span id="more"></span><p>Hive涉及的知识点如下图所示，本文将逐一讲解：</p><div align=center><img src="Hive涉及的知识点.png"></div><p><strong>正文开始：</strong></p><h2 id="一-Hive概览"><a href="#一-Hive概览" class="headerlink" title="一. Hive概览"></a>一. Hive概览</h2><h3 id="1-1-hive的简介"><a href="#1-1-hive的简介" class="headerlink" title="1.1 hive的简介"></a>1.1 hive的简介</h3><p>Hive是基于Hadoop的一个数据仓库工具，可以将结构化的数据文件映射为一张数据库表，并提供类SQL查询功能。</p><p>其本质是将SQL转换为MapReduce&#x2F;Spark的任务进行运算，底层由HDFS来提供数据的存储，说白了hive可以理解为一个将SQL转换为MapReduce&#x2F;Spark的任务的工具，甚至更进一步可以说hive就是一个MapReduce&#x2F;Spark Sql的客户端</p><p>为什么要使用hive ?</p><p>主要的原因有以下几点:</p><ul><li>学习MapReduce的成本比较高, 项目周期要求太短, MapReduce如果要实现复杂的查询逻辑开发的难度是比较大的。</li><li>而如果使用hive, hive采用操作接口类似SQL语法, 提高快速开发的能力. 避免去书写MapReduce,减少学习成本, 而且提供了功能的扩展</li></ul><p>hive的特点:</p><ol><li>可扩展 :  Hive可以自由的扩展集群的规模，一般情况下不需要重启服务。</li><li>延展性 :  Hive支持用户自定义函数，用户可以根据自己的需求来实现自己的函数。</li><li>容错 :  良好的容错性，节点出现问题SQL仍可完成执行。</li></ol><h3 id="1-2-hive的架构"><a href="#1-2-hive的架构" class="headerlink" title="1.2 hive的架构"></a>1.2 hive的架构</h3><div align=center><img src="hive的架构.png"></div><p>基本组成:</p><p><strong>用户接口</strong>：包括CLI、JDBC&#x2F;ODBC、WebGUI。其中，CLI(command line interface)为shell命令行；JDBC&#x2F;ODBC是Hive的JAVA实现，与传统数据库JDBC类似；WebGUI是通过浏览器访问Hive。</p><p><strong>元数据存储</strong>：通常是存储在关系数据库如mysql&#x2F;derby中。Hive 将元数据存储在数据库中。Hive 中的元数据包括表的名字，表的列和分区及其属性，表的属性（是否为外部表等），表的数据所在目录等。</p><p><strong>解释器、编译器、优化器、执行器</strong>:完成HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在HDFS 中，并在随后有MapReduce 调用执行。</p><h3 id="1-3-hive与hadoop的关系"><a href="#1-3-hive与hadoop的关系" class="headerlink" title="1.3 hive与hadoop的关系"></a>1.3 hive与hadoop的关系</h3><p>Hive利用HDFS存储数据，利用MapReduce查询分析数据</p><div align=center><img src="hive与hadoop的关系.png"></div><h3 id="1-4-hive与传统数据库对比"><a href="#1-4-hive与传统数据库对比" class="headerlink" title="1.4 hive与传统数据库对比"></a>1.4 hive与传统数据库对比</h3><p>hive主要是用于海量数据的离线数据分析</p><div align=center><img src="hive与传统数据库对比.png"></div><ol><li><strong>查询语言</strong>。由于 SQL 被广泛的应用在数据仓库中，因此，专门针对 Hive 的特性设计了类 SQL 的查询语言 HQL。熟悉 SQL 开发的开发者可以很方便的使用 Hive 进行开发。</li><li><strong>数据存储位置</strong>。Hive 是建立在 Hadoop 之上的，所有 Hive 的数据都是存储在 HDFS 中的。而数据库则可以将数据保存在块设备或者本地文件系统中。</li><li><strong>数据格式</strong>。Hive 中没有定义专门的数据格式，数据格式可以由用户指定，用户定义数据格式需要指定三个属性：列分隔符（通常为空格、”\t”、”\x001″）、行分隔符（”\n”）以及读取文件数据的方法（Hive 中默认有三个文件格式 TextFile，SequenceFile 以及 RCFile）。由于在加载数据的过程中，不需要从用户数据格式到 Hive 定义的数据格式的转换，因此，Hive 在加载的过程中不会对数据本身进行任何修改，而只是将数据内容复制或者移动到相应的 HDFS 目录中。而在数据库中，不同的数据库有不同的存储引擎，定义了自己的数据格式。所有数据都会按照一定的组织存储，因此，数据库加载数据的过程会比较耗时。</li><li><strong>数据更新</strong>。由于 Hive 是针对数据仓库应用设计的，而数据仓库的内容是读多写少的。因此，Hive 中不支持对数据的改写和添加，所有的数据都是在加载的时候中确定好的。而数据库中的数据通常是需要经常进行修改的，因此可以使用 INSERT INTO …  VALUES 添加数据，使用 UPDATE … SET 修改数据。</li><li><strong>索引</strong>。之前已经说过，Hive 在加载数据的过程中不会对数据进行任何处理，甚至不会对数据进行扫描，因此也没有对数据中的某些 Key 建立索引。Hive 要访问数据中满足条件的特定值时，需要暴力扫描整个数据，因此访问延迟较高。由于 MapReduce 的引入， Hive 可以并行访问数据，因此即使没有索引，对于大数据量的访问，Hive 仍然可以体现出优势。数据库中，通常会针对一个或者几个列建立索引，因此对于少量的特定条件的数据的访问，数据库可以有很高的效率，较低的延迟。由于数据的访问延迟较高，决定了 Hive 不适合在线数据查询。</li><li><strong>执行</strong>。Hive 中大多数查询的执行是通过 Hadoop 提供的 MapReduce 来实现的，而数据库通常有自己的执行引擎。</li><li><strong>执行延迟</strong>。之前提到，Hive 在查询数据的时候，由于没有索引，需要扫描整个表，因此延迟较高。另外一个导致 Hive 执行延迟高的因素是 MapReduce 框架。由于 MapReduce 本身具有较高的延迟，因此在利用 MapReduce 执行 Hive 查询时，也会有较高的延迟。相对的，数据库的执行延迟较低。当然，这个低是有条件的，即数据规模较小，当数据规模大到超过数据库的处理能力的时候，Hive 的并行计算显然能体现出优势。</li><li><strong>可扩展性</strong>。由于 Hive 是建立在 Hadoop 之上的，因此 Hive 的可扩展性是和 Hadoop 的可扩展性是一致的（世界上最大的 Hadoop 集群在 Yahoo!，2009年的规模在 4000 台节点左右）。而数据库由于 ACID 语义的严格限制，扩展行非常有限。目前最先进的并行数据库 Oracle 在理论上的扩展能力也只有 100 台左右。</li><li><strong>数据规模</strong>。由于 Hive 建立在集群上并可以利用 MapReduce 进行并行计算，因此可以支持很大规模的数据；对应的，数据库可以支持的数据规模较小。</li></ol><p>总结：hive具有sql数据库的外表，但应用场景完全不同，hive只适合用来做批量数据统计分析。</p><h3 id="1-5-hive的数据存储"><a href="#1-5-hive的数据存储" class="headerlink" title="1.5 hive的数据存储"></a>1.5 hive的数据存储</h3><ol><li>Hive中所有的数据都存储在 HDFS 中，没有专门的数据存储格式（可支持Text，SequenceFile，ParquetFile，ORC格式RCFILE等）</li></ol><blockquote><p>SequenceFile是hadoop中的一种文件格式：文件内容是以序列化的kv对象来组织的</p></blockquote><ol><li>只需要在创建表的时候告诉 Hive 数据中的列分隔符和行分隔符，Hive 就可以解析数据。</li><li>Hive 中包含以下数据模型：DB、Table，External Table，Partition，Bucket。</li></ol><ul><li>db：在hdfs中表现为<code>hive.metastore.warehouse.dir</code>目录下一个文件夹。</li><li>table：在hdfs中表现所属db目录下一个文件夹。</li><li>external table：与table类似，不过其数据存放位置可以在任意指定路径。</li><li>partition：在hdfs中表现为table目录下的子目录。</li><li>bucket：在hdfs中表现为同一个表目录下根据hash散列之后的多个文件。</li></ul><h2 id="二、Hive表类型"><a href="#二、Hive表类型" class="headerlink" title="二、Hive表类型"></a>二、Hive表类型</h2><h3 id="2-1-Hive-数据类型"><a href="#2-1-Hive-数据类型" class="headerlink" title="2.1 Hive 数据类型"></a>2.1 Hive 数据类型</h3><p>Hive的基本数据类型有：<code>TINYINT，SAMLLINT，INT，BIGINT，BOOLEAN，FLOAT，DOUBLE，STRING，TIMESTAMP(V0.8.0+)和BINARY(V0.8.0+)</code>。</p><p>Hive的集合类型有：<code>STRUCT，MAP和ARRAY</code>。</p><p>Hive主要有四种数据模型(即表)：内部表、外部表、分区表和桶表。</p><p>表的元数据保存传统的数据库的表中，<strong>当前hive只支持Derby和MySQL数据库</strong>。</p><h3 id="2-2-Hive-内部表"><a href="#2-2-Hive-内部表" class="headerlink" title="2.2 Hive 内部表"></a>2.2 Hive 内部表</h3><p>Hive中的内部表和传统数据库中的表在概念上是类似的，Hive的每个表都有自己的存储目录，除了外部表外，所有的表数据都存放在配置在<code>hive-site.xml</code>文件的<code>$&#123;hive.metastore.warehouse.dir&#125;/table_name</code>目录下。</p><p>创建内部表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> students(user_no <span class="type">INT</span>,name STRING,sex STRING,  </span><br><span class="line">         grade STRING COMMOT <span class="string">&#x27;班级&#x27;</span>）COMMONT <span class="string">&#x27;学生表&#x27;</span>  </span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED </span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">STORE <span class="keyword">AS</span> TEXTFILE;      </span><br></pre></td></tr></table></figure><h3 id="2-3-Hive-外部表"><a href="#2-3-Hive-外部表" class="headerlink" title="2.3 Hive 外部表"></a>2.3 Hive 外部表</h3><p>被external修饰的为外部表（external table），外部表指向已经存在在Hadoop HDFS上的数据，除了在删除外部表时只删除元数据而不会删除表数据外，其他和内部表很像。</p><p>创建外部表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> students(user_no <span class="type">INT</span>,name STRING,sex STRING,  </span><br><span class="line">         class STRING COMMOT <span class="string">&#x27;班级&#x27;</span>）COMMONT <span class="string">&#x27;学生表&#x27;</span>  </span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED  </span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span>  </span><br><span class="line">STORE <span class="keyword">AS</span> SEQUENCEFILE </span><br><span class="line">LOCATION <span class="string">&#x27;/usr/test/data/students.txt&#x27;</span>;   </span><br></pre></td></tr></table></figure><h3 id="2-4-Hive-分区表"><a href="#2-4-Hive-分区表" class="headerlink" title="2.4 Hive 分区表"></a>2.4 Hive 分区表</h3><p>分区表的每一个分区都对应数据库中相应分区列的一个索引，但是其组织方式和传统的关系型数据库不同。在Hive中，分区表的每一个分区都对应表下的一个目录，所有的分区的数据都存储在对应的目录中。</p><p>比如说，分区表partitinTable有包含nation(国家)、ds(日期)和city(城市)3个分区，其中nation &#x3D; china，ds &#x3D; 20130506，city &#x3D; Shanghai则对应HDFS上的目录为：</p><p><code>/datawarehouse/partitinTable/nation=china/city=Shanghai/ds=20130506/</code>。</p><p><strong>分区中定义的变量名不能和表中的列相同</strong>。</p><p>创建分区表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> students(user_no <span class="type">INT</span>,name STRING,sex STRING,</span><br><span class="line">         class STRING COMMOT <span class="string">&#x27;班级&#x27;</span>）COMMONT <span class="string">&#x27;学生表&#x27;</span>  </span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (ds STRING,country STRING)  </span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED  </span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span>  </span><br><span class="line">STORE <span class="keyword">AS</span> SEQUENCEFILE;</span><br></pre></td></tr></table></figure><h3 id="2-5-Hive-分桶表"><a href="#2-5-Hive-分桶表" class="headerlink" title="2.5 Hive 分桶表"></a>2.5 Hive 分桶表</h3><p>桶表就是对指定列进行哈希(hash)计算，然后会根据hash值进行切分数据，将具有不同hash值的数据写到每个桶对应的文件中。</p><p>将数据按照指定的字段进行分成多个桶中去，说白了就是将数据按照字段进行划分，可以将数据按照字段划分到<strong>多个文件</strong>当中去。</p><p>创建分桶表：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span> students(user_no <span class="type">INT</span>,name STRING,sex STRING,  </span><br><span class="line">         class STRING COMMOT <span class="string">&#x27;班级&#x27;</span>,score <span class="type">SMALLINT</span> COMMOT <span class="string">&#x27;总分&#x27;</span>）COMMONT <span class="string">&#x27;学生表&#x27;</span>  </span><br><span class="line">PARTITIONED <span class="keyword">BY</span> (ds STRING,country STRING)  </span><br><span class="line">CLUSTERED <span class="keyword">BY</span>(user_no) SORTED <span class="keyword">BY</span>(score) <span class="keyword">INTO</span> <span class="number">32</span> BUCKETS  </span><br><span class="line"><span class="type">ROW</span> FORMAT DELIMITED  </span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span>  </span><br><span class="line">STORE <span class="keyword">AS</span> SEQUENCEFILE;      </span><br></pre></td></tr></table></figure><h3 id="2-6-Hive-视图"><a href="#2-6-Hive-视图" class="headerlink" title="2.6 Hive 视图"></a>2.6 Hive 视图</h3><p>在 Hive 中，视图是逻辑数据结构，可以通过隐藏复杂数据操作（Joins, 子查询, 过滤,数据扁平化）来于简化查询操作。</p><p>与关系数据库不同的是，Hive视图并不存储数据或者实例化。一旦创建 HIve 视图，它的 schema 也会立刻确定下来。对底层表后续的更改(如 增加新列)并不会影响视图的 schema。如果底层表被删除或者改变，之后对视图的查询将会 failed。基于以上 Hive view 的特性，我们在ETL和数据仓库中<strong>对于经常变化的表应慎重使用视图</strong>。</p><p>创建视图：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">VIEW</span> employee_skills</span><br><span class="line"> <span class="keyword">AS</span></span><br><span class="line"><span class="keyword">SELECT</span> name, skills_score[<span class="string">&#x27;DB&#x27;</span>] <span class="keyword">AS</span> DB,</span><br><span class="line">skills_score[<span class="string">&#x27;Perl&#x27;</span>] <span class="keyword">AS</span> Perl, </span><br><span class="line">skills_score[<span class="string">&#x27;Python&#x27;</span>] <span class="keyword">AS</span> Python,</span><br><span class="line">skills_score[<span class="string">&#x27;Sales&#x27;</span>] <span class="keyword">as</span> Sales, </span><br><span class="line">skills_score[<span class="string">&#x27;HR&#x27;</span>] <span class="keyword">as</span> HR </span><br><span class="line"><span class="keyword">FROM</span> employee;</span><br></pre></td></tr></table></figure><p>创建视图的时候是不会触发 MapReduce 的 Job，因为只存在元数据的改变。</p><p>但是，当对视图进行查询的时候依然会触发一个 MapReduce Job 进程：SHOW CREATE TABLE 或者 DESC FORMATTED TABLE 语句来显示通过  CREATE VIEW  语句创建的视图。以下是对Hive 视图的 DDL操作：</p><p>更改视图的属性：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">VIEW</span> employee_skills </span><br><span class="line"><span class="keyword">SET</span> TBLPROPERTIES (<span class="string">&#x27;comment&#x27;</span> <span class="operator">=</span> <span class="string">&#x27;This is a view&#x27;</span>);</span><br></pre></td></tr></table></figure><p>重新定义视图：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">ALTER</span> <span class="keyword">VIEW</span> employee_skills <span class="keyword">AS</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">from</span> employee ;</span><br></pre></td></tr></table></figure><p>删除视图：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">DROP</span> <span class="keyword">VIEW</span> employee_skills; </span><br></pre></td></tr></table></figure><h2 id="三、Hive数据抽样"><a href="#三、Hive数据抽样" class="headerlink" title="三、Hive数据抽样"></a>三、Hive数据抽样</h2><p>当数据规模不断膨胀时，我们需要找到一个数据的子集来加快数据分析效率。因此我们就需要通过筛选和分析数据集为了进行<strong>模式 &amp; 趋势识别</strong>。目前来说有三种方式来进行抽样：随机抽样，桶表抽样，和块抽样。</p><h3 id="3-1-随机抽样"><a href="#3-1-随机抽样" class="headerlink" title="3.1 随机抽样"></a>3.1 随机抽样</h3><p>关键词：<strong>rand()函数</strong>。</p><p>使用rand()函数进行随机抽样，limit关键字限制抽样返回的数据，其中rand函数前的distribute和sort关键字可以保证数据在mapper和reducer阶段是随机分布的。</p><p>案例如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> table_name </span><br><span class="line"><span class="keyword">where</span> col<span class="operator">=</span>xxx </span><br><span class="line">distribute <span class="keyword">by</span> rand() sort <span class="keyword">by</span> rand() </span><br><span class="line">limit num; </span><br></pre></td></tr></table></figure><p>使用order 关键词:</p><p>案例如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> table_name </span><br><span class="line"><span class="keyword">where</span> col<span class="operator">=</span>xxx </span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> rand() </span><br><span class="line">limit num; </span><br></pre></td></tr></table></figure><p>经测试对比，千万级数据中进行随机抽样 order by方式耗时更长，大约多30秒左右。</p><h3 id="3-2-块抽样"><a href="#3-2-块抽样" class="headerlink" title="3.2 块抽样"></a>3.2 块抽样</h3><p>关键词：<strong>tablesample()函数</strong>。</p><ol><li>tablesample(n percent) 根据hive表数据的大小按比例抽取数据，并保存到新的hive表中。如：抽取原hive表中10%的数据</li></ol><blockquote><p>注意：测试过程中发现，select语句不能带where条件且不支持子查询，可通过新建中间表或使用随机抽样解决。</p></blockquote><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> xxx <span class="keyword">tablesample</span>(<span class="number">10</span> <span class="keyword">percent</span>) 数字与<span class="keyword">percent</span>之间要有空格</span><br></pre></td></tr></table></figure><ol><li>tablesample(nM) 指定抽样数据的大小，单位为M。</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> xxx <span class="keyword">tablesample</span>(<span class="number">20</span>M) 数字与M之间不要有空格</span><br></pre></td></tr></table></figure><ol><li>tablesample(n rows) 指定抽样数据的行数，其中n代表每个map任务均取n行数据，map数量可通过hive表的简单查询语句确认（关键词：number of mappers: x)</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> xxx <span class="keyword">tablesample</span>(<span class="number">100</span> <span class="keyword">rows</span>) 数字与<span class="keyword">rows</span>之间要有空格</span><br></pre></td></tr></table></figure><h3 id="3-3-桶表抽样"><a href="#3-3-桶表抽样" class="headerlink" title="3.3 桶表抽样"></a>3.3 桶表抽样</h3><p>关键词：**tablesample (bucket x out of y [on colname])**。</p><p>其中x是要抽样的桶编号，桶编号从1开始，colname表示抽样的列，y表示桶的数量。</p><p>hive中分桶其实就是根据某一个字段Hash取模，放入指定数据的桶中，比如将表table_1按照ID分成100个桶，其算法是hash(id) % 100，这样，hash(id) % 100 &#x3D; 0的数据被放到第一个桶中，hash(id) % 100 &#x3D; 1的记录被放到第二个桶中。创建分桶表的关键语句为：CLUSTER BY语句。</p><p>例如：将表随机分成10组，抽取其中的第一个桶的数据：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> table_01 </span><br><span class="line"><span class="keyword">tablesample</span>(bucket <span class="number">1</span> <span class="keyword">out</span> <span class="keyword">of</span> <span class="number">10</span> <span class="keyword">on</span> rand())</span><br></pre></td></tr></table></figure><h2 id="四、Hive计算引擎"><a href="#四、Hive计算引擎" class="headerlink" title="四、Hive计算引擎"></a>四、Hive计算引擎</h2><p>目前Hive支持MapReduce、Tez和Spark 三种计算引擎。</p><h3 id="4-1-MR计算引擎"><a href="#4-1-MR计算引擎" class="headerlink" title="4.1 MR计算引擎"></a>4.1 MR计算引擎</h3><p>MR运行的完整过程：</p><p>Map在读取数据时，先将数据拆分成若干数据，并读取到Map方法中被处理。数据在输出的时候，被分成若干分区并写入内存缓存（buffer）中，内存缓存被数据填充到一定程度会溢出到磁盘并排序，当Map执行完后会将一个机器上输出的临时文件进行归并存入到HDFS中。</p><p>当Reduce启动时，会启动一个线程去读取Map输出的数据，并写入到启动Reduce机器的内存中，在数据溢出到磁盘时会对数据进行再次排序。当读取数据完成后会将临时文件进行合并，作为Reduce函数的数据源。</p><h3 id="4-2-Tez计算引擎"><a href="#4-2-Tez计算引擎" class="headerlink" title="4.2 Tez计算引擎"></a>4.2 Tez计算引擎</h3><p>Apache Tez是进行大规模数据处理且支持DAG作业的计算框架，它直接源于MapReduce框架，除了能够支持MapReduce特性，还支持新的作业形式，并允许不同类型的作业能够在一个集群中运行。</p><p>Tez将原有的Map和Reduce两个操作简化为一个概念——Vertex，并将原有的计算处理节点拆分成多个组成部分：Vertex Input、Vertex Output、Sorting、Shuffling和Merging。计算节点之间的数据通信被统称为Edge，这些分解后的元操作可以任意灵活组合，产生新的操作，这些操作经过一些控制程序组装后，可形成一个大的DAG作业。</p><p>通过允许Apache Hive运行复杂的DAG任务，Tez可以用来处理数据，之前需要多个MR jobs，现在一个Tez任务中。</p><div align=center><img src="Tez计算引擎.png"></div><p><strong>Tez和MapReduce作业的比较</strong>：</p><ul><li>Tez绕过了MapReduce很多不必要的中间的数据存储和读取的过程，直接在一个作业中表达了MapReduce需要多个作业共同协作才能完成的事情。</li><li>Tez和MapReduce一样都运行使用YARN作为资源调度和管理。但与MapReduce on YARN不同，Tez on YARN并不是将作业提交到ResourceManager，而是提交到AMPoolServer的服务上，AMPoolServer存放着若干已经预先启动ApplicationMaster的服务。</li><li>当用户提交一个作业上来后，AMPoolServer从中选择一个ApplicationMaster用于管理用户提交上来的作业，这样既可以节省ResourceManager创建ApplicationMaster的时间，而又能够重用每个ApplicationMaster的资源，节省了资源释放和创建时间。</li></ul><p><strong>Tez相比于MapReduce有几点重大改进</strong>：</p><ul><li>当查询需要有多个reduce逻辑时，Hive的MapReduce引擎会将计划分解，每个Redcue提交一个MR作业。这个链中的所有MR作业都需要逐个调度，每个作业都必须从HDFS中重新读取上一个作业的输出并重新洗牌。而在Tez中，几个reduce接收器可以直接连接，数据可以流水线传输，而不需要临时HDFS文件，这种模式称为MRR（Map-reduce-reduce*）。</li><li>Tez还允许一次发送整个查询计划，实现应用程序动态规划，从而使框架能够更智能地分配资源，并通过各个阶段流水线传输数据。对于更复杂的查询来说，这是一个巨大的改进，因为它消除了IO&#x2F;sync障碍和各个阶段之间的调度开销。</li><li>在MapReduce计算引擎中，无论数据大小，在洗牌阶段都以相同的方式执行，将数据序列化到磁盘，再由下游的程序去拉取，并反序列化。Tez可以允许小数据集完全在内存中处理，而MapReduce中没有这样的优化。仓库查询经常需要在处理完大量的数据后对小型数据集进行排序或聚合，Tez的优化也能极大地提升效率。</li></ul><h3 id="4-3-Spark计算引擎"><a href="#4-3-Spark计算引擎" class="headerlink" title="4.3 Spark计算引擎"></a>4.3 Spark计算引擎</h3><p>Apache Spark是专为大规模数据处理而设计的快速、通用支持DAG（有向无环图）作业的计算引擎，类似于Hadoop MapReduce的通用并行框架，可用来构建大型的、低延迟的数据分析应用程序。</p><p>Spark是用于<strong>大规模数据处理</strong>的统一分析引擎，基于内存计算，提高了在大数据环境下数据处理的实时性，同时保证了<strong>高容错性</strong>和<strong>高可伸缩性</strong>，允许用户将Spark部署在大量硬件之上，形成集群。</p><p><strong>Spark运行流程</strong></p><div align=center><img src="Spark运行流程.png"></div><p>Spark具有以下几个特性。</p><p>1．<strong>高效性</strong></p><p>Spark会将作业构成一个DAG，优化了大型作业一些重复且浪费资源的操作，对查询进行了优化，重新编写了物理执行引擎，如可以实现MRR模式。</p><p>2．<strong>易用性</strong></p><p>Spark不同于MapReducer只提供两种简单的编程接口，它提供了多种编程接口去操作数据，这些操作接口如果使用MapReduce去实现，需要更多的代码。Spark的操作接口可以分为两类：transformation（转换）和action（执行）。Transformation包含map、flatmap、distinct、reduceByKey和join等转换操作；Action包含reduce、collect、count和first等操作。</p><p>3．<strong>通用性</strong></p><p>Spark针对实时计算、批处理、交互式查询，提供了统一的解决方案。但在批处理方面相比于MapReduce处理同样的数据，Spark所要求的硬件设施更高，MapReduce在相同的设备下所能处理的数据量会比Spark多。所以在实际工作中，Spark在批处理方面只能算是MapReduce的一种补充。</p><p>4．<strong>兼容性</strong></p><p>Spark和MapReduce一样有丰富的产品生态做支撑。例如Spark可以使用YARN作为资源管理器，Spark也可以处理Hbase和HDFS上的数据。</p><h2 id="五、存储与压缩"><a href="#五、存储与压缩" class="headerlink" title="五、存储与压缩"></a>五、存储与压缩</h2><h3 id="5-1-Hive存储格式"><a href="#5-1-Hive存储格式" class="headerlink" title="5.1 Hive存储格式"></a>5.1 Hive存储格式</h3><p>Hive支持的存储数的格式主要有：TEXTFILE（行式存储） 、SEQUENCEFILE(行式存储)、ORC（列式存储）、PARQUET（列式存储）。</p><h4 id="5-1-1-行式存储和列式存储"><a href="#5-1-1-行式存储和列式存储" class="headerlink" title="5.1.1 行式存储和列式存储"></a>5.1.1 行式存储和列式存储</h4><div align=center><img src="行式存储和列式存储.png"></div><p>上图左边为逻辑表，右边第一个为行式存储，第二个为列式存储。</p><p><strong>行存储的特点：</strong> 查询满足条件的一整行数据的时候，列存储则需要去每个聚集的字段找到对应的每个列的值，行存储只需要找到其中一个值，其余的值都在相邻地方，所以此时行存储查询的速度更快。select  *</p><p><strong>列存储的特点：</strong> 因为每个字段的数据聚集存储，在查询只需要少数几个字段的时候，能大大减少读取的数据量；每个字段的数据类型一定是相同的，列式存储可以针对性的设计更好的设计压缩算法。select  某些字段效率更高。</p><h4 id="5-1-2-TEXTFILE"><a href="#5-1-2-TEXTFILE" class="headerlink" title="5.1.2 TEXTFILE"></a>5.1.2 TEXTFILE</h4><p>默认格式，数据不做压缩，磁盘开销大，数据解析开销大。可结合Gzip、Bzip2使用(系统自动检查，执行查询时自动解压)，但使用这种方式，hive不会对数据进行切分，从而无法对数据进行并行操作。</p><h4 id="5-1-3-ORC格式"><a href="#5-1-3-ORC格式" class="headerlink" title="5.1.3 ORC格式"></a>5.1.3 ORC格式</h4><p>Orc (Optimized Row Columnar)是hive 0.11版里引入的新的存储格式。</p><p>可以看到每个Orc文件由1个或多个stripe组成，每个stripe250MB大小，这个Stripe实际相当于RowGroup概念，不过大小由4MB-&gt;250MB，这样能提升顺序读的吞吐率。每个Stripe里有三部分组成，分别是Index Data,Row Data,Stripe Footer：</p><div align=center><img src="ORC格式.png"></div><ol><li>Index Data：一个轻量级的index，默认是每隔1W行做一个索引。这里做的索引只是记录某行的各字段在Row Data中的offset。</li><li>Row Data：存的是具体的数据，先取部分行，然后对这些行按列进行存储。对每个列进行了编码，分成多个Stream来存储。</li><li>Stripe Footer：存的是各个stripe的元数据信息</li></ol><p>每个文件有一个File Footer，这里面存的是每个Stripe的行数，每个Column的数据类型信息等；每个文件的尾部是一个PostScript，这里面记录了整个文件的压缩类型以及FileFooter的长度信息等。在读取文件时，会seek到文件尾部读PostScript，从里面解析到File Footer长度，再读FileFooter，从里面解析到各个Stripe信息，再读各个Stripe，即从后往前读。</p><h4 id="5-1-4-PARQUET格式"><a href="#5-1-4-PARQUET格式" class="headerlink" title="5.1.4 PARQUET格式"></a>5.1.4 PARQUET格式</h4><p>Parquet是面向分析型业务的列式存储格式，由Twitter和Cloudera合作开发，2015年5月从Apache的孵化器里毕业成为Apache顶级项目。</p><p>Parquet文件是以二进制方式存储的，所以是不可以直接读取的，文件中包括该文件的数据和元数据，因此Parquet格式文件是自解析的。</p><p>通常情况下，在存储Parquet数据的时候会按照Block大小设置行组的大小，由于一般情况下每一个Mapper任务处理数据的最小单位是一个Block，这样可以把每一个行组由一个Mapper任务处理，增大任务执行并行度。Parquet文件的格式如下图所示。</p><div align=center><img src="PARQUET格式.png"></div><p>上图展示了一个Parquet文件的内容，一个文件中可以存储多个行组，文件的首位都是该文件的Magic Code，用于校验它是否是一个Parquet文件，Footer length记录了文件元数据的大小，通过该值和文件长度可以计算出元数据的偏移量，文件的元数据中包括每一个行组的元数据信息和该文件存储数据的Schema信息。除了文件中每一个行组的元数据，每一页的开始都会存储该页的元数据，在Parquet中，有三种类型的页：数据页、字典页和索引页。数据页用于存储当前行组中该列的值，字典页存储该列值的编码字典，每一个列块中最多包含一个字典页，索引页用来存储当前行组下该列的索引，目前Parquet中还不支持索引页。</p><h3 id="5-2-Hive压缩格式"><a href="#5-2-Hive压缩格式" class="headerlink" title="5.2 Hive压缩格式"></a>5.2 Hive压缩格式</h3><p>在实际工作当中，hive当中处理的数据，一般都需要经过压缩，前期我们在学习hadoop的时候，已经配置过hadoop的压缩，我们这里的hive也是一样的可以使用压缩来节省我们的MR处理的网络带宽</p><p>mr支持的压缩格式:</p><table><thead><tr><th align="left">压缩格式</th><th align="left">工具</th><th align="left">算法</th><th align="left">文件扩展名</th><th align="left">是否可切分</th></tr></thead><tbody><tr><td align="left">DEFAULT</td><td align="left">无</td><td align="left">DEFAULT</td><td align="left">.deflate</td><td align="left">否</td></tr><tr><td align="left">Gzip</td><td align="left">gzip</td><td align="left">DEFAULT</td><td align="left">.gz</td><td align="left">否</td></tr><tr><td align="left">bzip2</td><td align="left">bzip2</td><td align="left">bzip2</td><td align="left">.bz2</td><td align="left">是</td></tr><tr><td align="left">LZO</td><td align="left">lzop</td><td align="left">LZO</td><td align="left">.lzo</td><td align="left">否</td></tr><tr><td align="left">LZ4</td><td align="left">无</td><td align="left">LZ4</td><td align="left">.lz4</td><td align="left">否</td></tr><tr><td align="left">Snappy</td><td align="left">无</td><td align="left">Snappy</td><td align="left">.snappy</td><td align="left">否</td></tr></tbody></table><p>hadoop支持的解压缩的类：</p><table><thead><tr><th align="left">压缩格式</th><th align="left">对应的编码&#x2F;解码器</th></tr></thead><tbody><tr><td align="left">DEFLATE</td><td align="left">org.apache.hadoop.io.compress.DefaultCodec</td></tr><tr><td align="left">gzip</td><td align="left">org.apache.hadoop.io.compress.GzipCodec</td></tr><tr><td align="left">bzip2</td><td align="left">org.apache.hadoop.io.compress.BZip2Codec</td></tr><tr><td align="left">LZO</td><td align="left">com.hadoop.compression.lzo.LzopCodec</td></tr><tr><td align="left">LZ4</td><td align="left">org.apache.hadoop.io.compress.Lz4Codec</td></tr><tr><td align="left">Snappy</td><td align="left">org.apache.hadoop.io.compress.SnappyCodec</td></tr></tbody></table><p>压缩性能的比较：</p><table><thead><tr><th align="left">压缩算法</th><th align="left">原始文件大小</th><th align="left">压缩文件大小</th><th align="left">压缩速度</th><th align="left">解压速度</th></tr></thead><tbody><tr><td align="left">gzip</td><td align="left">8.3GB</td><td align="left">1.8GB</td><td align="left">17.5MB&#x2F;s</td><td align="left">58MB&#x2F;s</td></tr><tr><td align="left">bzip2</td><td align="left">8.3GB</td><td align="left">1.1GB</td><td align="left">2.4MB&#x2F;s</td><td align="left">9.5MB&#x2F;s</td></tr><tr><td align="left">LZO</td><td align="left">8.3GB</td><td align="left">2.9GB</td><td align="left">49.3MB&#x2F;s</td><td align="left">74.6MB&#x2F;s</td></tr></tbody></table><p>Snappy生成的压缩文件要大20%到100%。在64位模式下的core i7处理器的单内核上，Snappy以250 MB&#x2F;秒或更多的速度压缩，并以500 MB&#x2F;秒或更多的速度解压。</p><p>实现压缩hadoop需要配置的压缩参数:</p><div align=center><img src="实现压缩hadoop需要配置的压缩参数.png"></div><p>hive配置压缩的方式:</p><ol><li>开启map端的压缩方式:</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.1</span>）开启hive中间传输数据压缩功能</span><br><span class="line"> hive (<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">set</span> hive.exec.compress.intermediate<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="number">1.2</span>）开启mapreduce中map输出压缩功能</span><br><span class="line"> hive (<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">set</span> mapreduce.map.output.compress<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="number">1.3</span>）设置mapreduce中map输出数据的压缩方式</span><br><span class="line"> hive (<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">set</span> mapreduce.map.output.compress.codec<span class="operator">=</span> org.apache.hadoop.io.compress.SnappyCodec;</span><br><span class="line"><span class="number">1.4</span>）执行查询语句</span><br><span class="line"> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="number">1</span>) <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure><ol><li>开启reduce端的压缩方式</li></ol><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>）开启hive最终输出数据压缩功能</span><br><span class="line"> hive (<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">set</span> hive.exec.compress.output<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="number">2</span>）开启mapreduce最终输出数据压缩</span><br><span class="line"> hive (<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="number">3</span>）设置mapreduce最终数据输出压缩方式</span><br><span class="line"> hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.codec <span class="operator">=</span> org.apache.hadoop.io.compress.SnappyCodec;</span><br><span class="line"><span class="number">4</span>）设置mapreduce最终数据输出压缩为块压缩</span><br><span class="line"> hive (<span class="keyword">default</span>)<span class="operator">&gt;</span><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.type<span class="operator">=</span>BLOCK;</span><br><span class="line"><span class="number">5</span>）测试一下输出结果是否是压缩文件</span><br><span class="line"> <span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/export/servers/snappy&#x27;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> score distribute <span class="keyword">by</span> s_id sort <span class="keyword">by</span> s_id <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><h3 id="5-3-存储和压缩相结合"><a href="#5-3-存储和压缩相结合" class="headerlink" title="5.3  存储和压缩相结合"></a>5.3  存储和压缩相结合</h3><p>ORC存储方式的压缩：</p><table><thead><tr><th align="left">Key</th><th align="left">Default</th><th align="left">Notes</th></tr></thead><tbody><tr><td align="left">orc.compress</td><td align="left">ZLIB</td><td align="left">高级压缩(可选: NONE, ZLIB, SNAPPY)</td></tr><tr><td align="left">orc.compress.size</td><td align="left">262,144</td><td align="left">每个压缩块中的字节数</td></tr><tr><td align="left">orc.stripe.size</td><td align="left">67,108,864</td><td align="left">每条stripe中的字节数</td></tr><tr><td align="left">orc.row.index.stride</td><td align="left">10,000</td><td align="left">索引条目之间的行数(必须是&gt;&#x3D; 1000)</td></tr><tr><td align="left">orc.create.index</td><td align="left">true</td><td align="left">是否创建行索引</td></tr><tr><td align="left">orc.bloom.filter.columns</td><td align="left">“”</td><td align="left">逗号分隔的列名列表，应该为其创建bloom过滤器</td></tr><tr><td align="left">orc.bloom.filter.fpp</td><td align="left">0.05</td><td align="left">bloom过滤器的假阳性概率(必须是&gt;0.0和&lt;1.0)</td></tr></tbody></table><p>创建一个非压缩的ORC存储方式：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>）建表语句</span><br><span class="line">    <span class="keyword">create</span> <span class="keyword">table</span> log_orc_none(</span><br><span class="line">    track_time string,</span><br><span class="line">    url string,</span><br><span class="line">    session_id string,</span><br><span class="line">    referer string,</span><br><span class="line">    ip string,</span><br><span class="line">    end_user_id string,</span><br><span class="line">    city_id string</span><br><span class="line">    )<span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span> STORED <span class="keyword">AS</span> orc tblproperties (&quot;orc.compress&quot;<span class="operator">=</span>&quot;NONE&quot;);</span><br><span class="line"><span class="number">2</span>）插入数据</span><br><span class="line"> <span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> log_orc_none <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log_text ;</span><br><span class="line"><span class="number">3</span>）查看插入后数据</span><br><span class="line"> dfs <span class="operator">-</span>du <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>myhive.db<span class="operator">/</span>log_orc_none;</span><br><span class="line"> 结果显示:</span><br><span class="line"> <span class="number">7.7</span> M  <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>log_orc_none<span class="operator">/</span><span class="number">123456</span>_0</span><br></pre></td></tr></table></figure><p>创建一个SNAPPY压缩的ORC存储方式：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>）建表语句</span><br><span class="line">    <span class="keyword">create</span> <span class="keyword">table</span> log_orc_snappy(</span><br><span class="line">    track_time string,</span><br><span class="line">    url string,</span><br><span class="line">    session_id string,</span><br><span class="line">    referer string,</span><br><span class="line">    ip string,</span><br><span class="line">    end_user_id string,</span><br><span class="line">    city_id string</span><br><span class="line">    )<span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span> STORED <span class="keyword">AS</span> orc tblproperties (&quot;orc.compress&quot;<span class="operator">=</span>&quot;SNAPPY&quot;);</span><br><span class="line"><span class="number">2</span>）插入数据</span><br><span class="line"> <span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> log_orc_snappy <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log_text ;</span><br><span class="line"><span class="number">3</span>）查看插入后数据</span><br><span class="line"> dfs <span class="operator">-</span>du <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>myhive.db<span class="operator">/</span>log_orc_snappy ;</span><br><span class="line"> 结果显示: </span><br><span class="line"> <span class="number">3.8</span> M  <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>log_orc_snappy<span class="operator">/</span><span class="number">123456</span>_0</span><br><span class="line"><span class="number">4</span>）上一节中默认创建的ORC存储方式，导入数据后的大小为</span><br><span class="line"> <span class="number">2.8</span> M  <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>log_orc<span class="operator">/</span><span class="number">123456</span>_0</span><br><span class="line"> 比Snappy压缩的还小。原因是orc存储文件默认采用ZLIB压缩。比snappy压缩的小。</span><br><span class="line"><span class="number">5</span>）存储方式和压缩总结：</span><br><span class="line"> 在实际的项目开发当中，hive表的数据存储格式一般选择：orc或parquet。压缩方式一般选择snappy。</span><br></pre></td></tr></table></figure><h3 id="5-4-主流存储文件性能对比"><a href="#5-4-主流存储文件性能对比" class="headerlink" title="5.4 主流存储文件性能对比"></a>5.4 主流存储文件性能对比</h3><p>从存储文件的压缩比和查询速度两个角度对比。</p><p>压缩比比较：</p><ul><li>TextFile</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">（<span class="number">1</span>）创建表，存储数据格式为TEXTFILE</span><br><span class="line">    <span class="keyword">create</span> <span class="keyword">table</span> log_text (</span><br><span class="line">    track_time string,</span><br><span class="line">    url string,</span><br><span class="line">    session_id string,</span><br><span class="line">    referer string,</span><br><span class="line">    ip string,</span><br><span class="line">    end_user_id string,</span><br><span class="line">    city_id string</span><br><span class="line">    )<span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span> STORED <span class="keyword">AS</span> TEXTFILE ;</span><br><span class="line"></span><br><span class="line">（<span class="number">2</span>）向表中加载数据</span><br><span class="line"> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/export/servers/hivedatas/log.data&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> log_text ;</span><br><span class="line"></span><br><span class="line">（<span class="number">3</span>）查看表中数据大小，大小为<span class="number">18.1</span>M</span><br><span class="line"> dfs <span class="operator">-</span>du <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>myhive.db<span class="operator">/</span>log_text;</span><br><span class="line"> 结果显示: </span><br><span class="line"> <span class="number">18.1</span> M  <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>log_text<span class="operator">/</span>log.data</span><br></pre></td></tr></table></figure><ul><li>ORC</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">（<span class="number">1</span>）创建表，存储数据格式为ORC</span><br><span class="line">    <span class="keyword">create</span> <span class="keyword">table</span> log_orc(</span><br><span class="line">    track_time string,</span><br><span class="line">    url string,</span><br><span class="line">    session_id string,</span><br><span class="line">    referer string,</span><br><span class="line">    ip string,</span><br><span class="line">    end_user_id string,</span><br><span class="line">    city_id string</span><br><span class="line">    )<span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span> STORED <span class="keyword">AS</span> orc ;</span><br><span class="line"></span><br><span class="line">（<span class="number">2</span>）向表中加载数据</span><br><span class="line"> <span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> log_orc <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log_text ;</span><br><span class="line"></span><br><span class="line">（<span class="number">3</span>）查看表中数据大小</span><br><span class="line"> dfs <span class="operator">-</span>du <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>myhive.db<span class="operator">/</span>log_orc;</span><br><span class="line"> 结果显示:</span><br><span class="line"> <span class="number">2.8</span> M  <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>log_orc<span class="operator">/</span><span class="number">123456</span>_0</span><br></pre></td></tr></table></figure><ul><li>Parquet</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>）创建表，存储数据格式为parquet</span><br><span class="line">    <span class="keyword">create</span> <span class="keyword">table</span> log_parquet(</span><br><span class="line">    track_time string,</span><br><span class="line">    url string,</span><br><span class="line">    session_id string,</span><br><span class="line">    referer string,</span><br><span class="line">    ip string,</span><br><span class="line">    end_user_id string,</span><br><span class="line">    city_id string</span><br><span class="line">    )<span class="type">ROW</span> FORMAT DELIMITED FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;\t&#x27;</span> STORED <span class="keyword">AS</span> PARQUET ; </span><br><span class="line"></span><br><span class="line"><span class="number">2</span>）向表中加载数据</span><br><span class="line"> <span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> log_parquet <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> log_text ;</span><br><span class="line"></span><br><span class="line"><span class="number">3</span>）查看表中数据大小</span><br><span class="line"> dfs <span class="operator">-</span>du <span class="operator">-</span>h <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>myhive.db<span class="operator">/</span>log_parquet;</span><br><span class="line"> 结果显示:</span><br><span class="line"> <span class="number">13.1</span> M  <span class="operator">/</span><span class="keyword">user</span><span class="operator">/</span>hive<span class="operator">/</span>warehouse<span class="operator">/</span>log_parquet<span class="operator">/</span><span class="number">123456</span>_0</span><br></pre></td></tr></table></figure><p>数据压缩比结论:</p><p><strong>ORC &gt;  Parquet &gt;  textFile</strong></p><p>存储文件的查询效率测试</p><ul><li>textFile</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> log_text;</span><br><span class="line">_c0</span><br><span class="line"><span class="number">100000</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">21.54</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)  </span><br></pre></td></tr></table></figure><ul><li>ORC</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> log_orc;</span><br><span class="line">_c0</span><br><span class="line"><span class="number">100000</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">20.867</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s) </span><br></pre></td></tr></table></figure><ul><li>Parquet</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> log_parquet; </span><br><span class="line">_c0</span><br><span class="line"><span class="number">100000</span></span><br><span class="line"><span class="type">Time</span> taken: <span class="number">22.922</span> seconds, Fetched: <span class="number">1</span> <span class="type">row</span>(s)</span><br></pre></td></tr></table></figure><p>存储文件的查询效率比较:</p><p><strong>ORC &gt; TextFile &gt; Parquet</strong></p><h2 id="六、Hive-Sql-大全"><a href="#六、Hive-Sql-大全" class="headerlink" title="六、Hive Sql 大全"></a>六、Hive Sql 大全</h2><blockquote><p>本节基本涵盖了Hive日常使用的所有SQL，因为SQL太多，所以将SQL进行了如下分类：一、DDL语句（数据定义语句）：<br>对数据库的操作：包含创建、修改数据库<br>对数据表的操作：分为内部表及外部表，分区表和分桶表<br>二、DQL语句（数据查询语句）：<br>单表查询、关联查询<br>hive函数：包含聚合函数，条件函数，日期函数，字符串函数等<br>行转列及列转行：lateral view 与 explode 以及 reflect<br>窗口函数与分析函数<br>其他一些窗口函数</p></blockquote><h3 id="hive的DDL语法"><a href="#hive的DDL语法" class="headerlink" title="hive的DDL语法"></a>hive的DDL语法</h3><h3 id="对数据库的操作"><a href="#对数据库的操作" class="headerlink" title="对数据库的操作"></a>对数据库的操作</h3><ul><li>创建数据库:</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> database if <span class="keyword">not</span> <span class="keyword">exists</span> myhive;</span><br><span class="line">说明：hive的表存放位置模式是由hive<span class="operator">-</span>site.xml当中的一个属性指定的 :hive.metastore.warehouse.dir</span><br><span class="line"></span><br><span class="line">创建数据库并指定hdfs存储位置 :</span><br><span class="line"><span class="keyword">create</span> database myhive2 location <span class="string">&#x27;/myhive2&#x27;</span>;</span><br></pre></td></tr></table></figure><ul><li>修改数据库:</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span>  database  myhive2  <span class="keyword">set</span>  dbproperties(<span class="string">&#x27;createtime&#x27;</span><span class="operator">=</span><span class="string">&#x27;20210329&#x27;</span>);</span><br></pre></td></tr></table></figure><blockquote><p>说明：可以使用alter  database 命令来修改数据库的一些属性。但是数据库的元数据信息是不可更改的，包括数据库的名称以及数据库所在的位置</p></blockquote><ul><li>查看数据库详细信息</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">查看数据库基本信息</span><br><span class="line">hive (myhive)<span class="operator">&gt;</span> <span class="keyword">desc</span>  database  myhive2;</span><br><span class="line"></span><br><span class="line">查看数据库更多详细信息</span><br><span class="line">hive (myhive)<span class="operator">&gt;</span> <span class="keyword">desc</span> database extended  myhive2;</span><br></pre></td></tr></table></figure><ul><li>删除数据库</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">删除一个空数据库，如果数据库下面有数据表，那么就会报错</span><br><span class="line"><span class="keyword">drop</span>  database  myhive2;</span><br><span class="line"></span><br><span class="line">强制删除数据库，包含数据库下面的表一起删除</span><br><span class="line"><span class="keyword">drop</span>  database  myhive  cascade; </span><br></pre></td></tr></table></figure><h3 id="对数据表的操作"><a href="#对数据表的操作" class="headerlink" title="对数据表的操作"></a>对数据表的操作</h3><h4 id="对管理表-内部表-的操作"><a href="#对管理表-内部表-的操作" class="headerlink" title="对管理表(内部表)的操作:"></a>对管理表(内部表)的操作:</h4><ul><li>建内部表:</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive (myhive)<span class="operator">&gt;</span> use myhive; <span class="comment">-- 使用myhive数据库</span></span><br><span class="line">hive (myhive)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> stu(id <span class="type">int</span>,name string);</span><br><span class="line">hive (myhive)<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> stu <span class="keyword">values</span> (<span class="number">1</span>,&quot;zhangsan&quot;);</span><br><span class="line">hive (myhive)<span class="operator">&gt;</span> <span class="keyword">insert</span> <span class="keyword">into</span> stu <span class="keyword">values</span> (<span class="number">1</span>,&quot;zhangsan&quot;),(<span class="number">2</span>,&quot;lisi&quot;);  <span class="comment">-- 一次插入多条数据</span></span><br><span class="line">hive (myhive)<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> stu;</span><br></pre></td></tr></table></figure><ul><li>hive建表时候的字段类型:</li></ul><table><thead><tr><th align="left"><strong>分类</strong></th><th align="left"><strong>类型</strong></th><th align="left"><strong>描述</strong></th><th align="left"><strong>字面量示例</strong></th></tr></thead><tbody><tr><td align="left">原始类型</td><td align="left">BOOLEAN</td><td align="left">true&#x2F;false</td><td align="left">TRUE</td></tr><tr><td align="left"></td><td align="left">TINYINT</td><td align="left">1字节的有符号整数 -128~127</td><td align="left">1Y</td></tr><tr><td align="left"></td><td align="left">SMALLINT</td><td align="left">2个字节的有符号整数，-32768~32767</td><td align="left">1S</td></tr><tr><td align="left"></td><td align="left"><strong>INT</strong></td><td align="left">4个字节的带符号整数</td><td align="left">1</td></tr><tr><td align="left"></td><td align="left">BIGINT</td><td align="left">8字节带符号整数</td><td align="left">1L</td></tr><tr><td align="left"></td><td align="left">FLOAT</td><td align="left">4字节单精度浮点数1.0</td><td align="left"></td></tr><tr><td align="left"></td><td align="left">DOUBLE</td><td align="left">8字节双精度浮点数</td><td align="left">1.0</td></tr><tr><td align="left"></td><td align="left">DEICIMAL</td><td align="left">任意精度的带符号小数</td><td align="left">1.0</td></tr><tr><td align="left"></td><td align="left"><strong>STRING</strong></td><td align="left">字符串，变长</td><td align="left">“a”,’b’</td></tr><tr><td align="left"></td><td align="left">VARCHAR</td><td align="left">变长字符串</td><td align="left">“a”,’b’</td></tr><tr><td align="left"></td><td align="left">CHAR</td><td align="left">固定长度字符串</td><td align="left">“a”,’b’</td></tr><tr><td align="left"></td><td align="left">BINARY</td><td align="left">字节数组</td><td align="left">无法表示</td></tr><tr><td align="left"></td><td align="left">TIMESTAMP</td><td align="left">时间戳，毫秒值精度</td><td align="left">122327493795</td></tr><tr><td align="left"></td><td align="left"><strong>DATE</strong></td><td align="left">日期</td><td align="left">‘2016-03-29’</td></tr><tr><td align="left"></td><td align="left">INTERVAL</td><td align="left">时间频率间隔</td><td align="left"></td></tr><tr><td align="left">复杂类型</td><td align="left">ARRAY</td><td align="left">有序的的同类型的集合</td><td align="left">array(1,2)</td></tr><tr><td align="left"></td><td align="left">MAP</td><td align="left">key-value,key必须为原始类型，value可以任意类型</td><td align="left">map(‘a’,1,’b’,2)</td></tr><tr><td align="left"></td><td align="left">STRUCT</td><td align="left">字段集合,类型可以不同</td><td align="left">struct(‘1’,1,1.0), named_stract(‘col1’,’1’,’col2’,1,’clo3’,1.0)</td></tr><tr><td align="left"></td><td align="left">UNION</td><td align="left">在有限取值范围内的一个值</td><td align="left">create_union(1,’a’,63)</td></tr></tbody></table><blockquote><p><strong>对decimal类型简单解释下</strong>：<br>用法：decimal(11,2) 代表最多有11位数字，其中后2位是小数，整数部分是9位；如果整数部分超过9位，则这个字段就会变成null；如果小数部分不足2位，则后面用0补齐两位，如果小数部分超过两位，则超出部分四舍五入<br>也可直接写 decimal，后面不指定位数，默认是 decimal(10,0)  整数10位，没有小数</p></blockquote><ul><li>创建表并指定字段之间的分隔符</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span>  <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> stu2(id <span class="type">int</span> ,name string) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span> stored <span class="keyword">as</span> textfile location <span class="string">&#x27;/user/stu2&#x27;</span>;</span><br></pre></td></tr></table></figure><blockquote><p>row format delimited fields terminated by ‘\t’  指定字段分隔符，默认分隔符为 ‘\001’<br>stored as 指定存储格式<br>location 指定存储位置</p></blockquote><ul><li>根据查询结果创建表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu3 <span class="keyword">as</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> stu2;</span><br></pre></td></tr></table></figure><ul><li>根据已经存在的表结构创建表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> stu4 <span class="keyword">like</span> stu2;</span><br></pre></td></tr></table></figure><ul><li>查询表的结构</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">只查询表内字段及属性</span><br><span class="line"><span class="keyword">desc</span> stu2;</span><br><span class="line"></span><br><span class="line">详细查询</span><br><span class="line"><span class="keyword">desc</span> formatted  stu2;</span><br></pre></td></tr></table></figure><ul><li>查询创建表的语句</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> <span class="keyword">create</span> <span class="keyword">table</span> stu2;</span><br></pre></td></tr></table></figure><h4 id="对外部表操作"><a href="#对外部表操作" class="headerlink" title="对外部表操作"></a>对外部表操作</h4><blockquote><p>外部表因为是指定其他的hdfs路径的数据加载到表当中来，所以hive表会认为自己不完全独占这份数据，所以删除hive表的时候，数据仍然存放在hdfs当中，不会删掉，只会删除表的元数据</p></blockquote><ul><li>构建外部表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> student (s_id string,s_name string) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>;</span><br></pre></td></tr></table></figure><ul><li>从本地文件系统向表中加载数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">追加操作</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/export/servers/hivedatas/student.csv&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> student;</span><br><span class="line"></span><br><span class="line">覆盖操作</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/export/servers/hivedatas/student.csv&#x27;</span> overwrite  <span class="keyword">into</span> <span class="keyword">table</span> student;</span><br></pre></td></tr></table></figure><ul><li>从hdfs文件系统向表中加载数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">load data inpath <span class="string">&#x27;/hivedatas/techer.csv&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> techer;</span><br><span class="line"></span><br><span class="line">加载数据到指定分区</span><br><span class="line">load data inpath <span class="string">&#x27;/hivedatas/techer.csv&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> techer <span class="keyword">partition</span>(cur_date<span class="operator">=</span><span class="number">20201210</span>);</span><br></pre></td></tr></table></figure><blockquote><ul><li><strong>注意</strong>：<br>1.使用 load data local 表示从本地文件系统加载，文件会拷贝到hdfs上<br>2.使用 load data 表示从hdfs文件系统加载，文件会直接移动到hive相关目录下，注意不是拷贝过去，因为hive认为hdfs文件已经有3副本了，没必要再次拷贝了<br>3.如果表是分区表，load 时不指定分区会报错<br>4.如果加载相同文件名的文件，会被自动重命名</li></ul></blockquote><h4 id="对分区表的操作"><a href="#对分区表的操作" class="headerlink" title="对分区表的操作"></a>对分区表的操作</h4><ul><li>创建分区表的语法</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score(s_id string, s_score <span class="type">int</span>) partitioned <span class="keyword">by</span> (<span class="keyword">month</span> string);</span><br></pre></td></tr></table></figure><ul><li>创建一个表带多个分区</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score2 (s_id string, s_score <span class="type">int</span>) partitioned <span class="keyword">by</span> (<span class="keyword">year</span> string,<span class="keyword">month</span> string,<span class="keyword">day</span> string);</span><br></pre></td></tr></table></figure><blockquote><p><strong>注意：<br>hive表创建的时候可以用 location 指定一个文件或者文件夹，当指定文件夹时，hive会加载文件夹下的所有文件，当表中无分区时，这个文件夹下不能再有文件夹，否则报错<br>当表是分区表时，比如 partitioned by (day string)， 则这个文件夹下的每一个文件夹就是一个分区，且文件夹名为 day&#x3D;20201123 这种格式，然后使用：msck  repair  table  score; 修复表结构，成功之后即可看到数据已经全部加载到表当中去了</strong></p></blockquote><ul><li>加载数据到一个分区的表中</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/export/servers/hivedatas/score.csv&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> score <span class="keyword">partition</span> (<span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;201806&#x27;</span>);</span><br></pre></td></tr></table></figure><ul><li>加载数据到一个多分区的表中去</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/export/servers/hivedatas/score.csv&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> score2 <span class="keyword">partition</span>(<span class="keyword">year</span><span class="operator">=</span><span class="string">&#x27;2018&#x27;</span>,<span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;06&#x27;</span>,<span class="keyword">day</span><span class="operator">=</span><span class="string">&#x27;01&#x27;</span>);</span><br></pre></td></tr></table></figure><ul><li>查看分区</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span>  partitions  score;</span><br></pre></td></tr></table></figure><ul><li>添加一个分区</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> score <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;201805&#x27;</span>);</span><br></pre></td></tr></table></figure><ul><li>同时添加多个分区</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> score <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;201804&#x27;</span>) <span class="keyword">partition</span>(<span class="keyword">month</span> <span class="operator">=</span> <span class="string">&#x27;201803&#x27;</span>);</span><br></pre></td></tr></table></figure><blockquote><p>注意：添加分区之后就可以在hdfs文件系统当中看到表下面多了一个文件夹</p></blockquote><ul><li>删除分区</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> score <span class="keyword">drop</span> <span class="keyword">partition</span>(<span class="keyword">month</span> <span class="operator">=</span> <span class="string">&#x27;201806&#x27;</span>);</span><br></pre></td></tr></table></figure><h4 id="对分桶表操作"><a href="#对分桶表操作" class="headerlink" title="对分桶表操作"></a>对分桶表操作</h4><blockquote><p>将数据按照指定的字段进行分成多个桶中去，就是按照分桶字段进行哈希划分到多个文件当中去<br>分区就是分文件夹，分桶就是分文件</p></blockquote><blockquote><p>分桶优点：<br>\1. 提高join查询效率<br>\2. 提高抽样效率</p></blockquote><ul><li>开启hive的捅表功能</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.enforce.bucketing<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure><ul><li>设置reduce的个数</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">3</span>;</span><br></pre></td></tr></table></figure><ul><li>创建桶表</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> course (c_id string,c_name string) clustered <span class="keyword">by</span>(c_id) <span class="keyword">into</span> <span class="number">3</span> buckets;</span><br></pre></td></tr></table></figure><blockquote><p>桶表的数据加载：由于桶表的数据加载通过hdfs  dfs  -put文件或者通过load  data均不可以，只能通过insert  overwrite 进行加载<br>所以把文件加载到桶表中，需要先创建普通表，并通过insert  overwrite的方式将普通表的数据通过查询的方式加载到桶表当中去</p></blockquote><ul><li>通过insert  overwrite给桶表中加载数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> course <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> course_common cluster <span class="keyword">by</span>(c_id);  <span class="comment">-- 最后指定桶字段</span></span><br></pre></td></tr></table></figure><h4 id="修改表和删除表"><a href="#修改表和删除表" class="headerlink" title="修改表和删除表"></a>修改表和删除表</h4><ul><li>修改表名称</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">alter</span>  <span class="keyword">table</span>  old_table_name  rename  <span class="keyword">to</span>  new_table_name;</span><br></pre></td></tr></table></figure><ul><li>增加&#x2F;修改列信息</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">查询表结构</span><br><span class="line"><span class="keyword">desc</span> score5;</span><br><span class="line"></span><br><span class="line">添加列</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> score5 <span class="keyword">add</span> columns (mycol string, mysco string);</span><br><span class="line"></span><br><span class="line">更新列</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> score5 change <span class="keyword">column</span> mysco mysconew <span class="type">int</span>;</span><br></pre></td></tr></table></figure><ul><li>删除表操作</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> score5;</span><br></pre></td></tr></table></figure><ul><li>清空表操作</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">truncate</span> <span class="keyword">table</span> score6;</span><br><span class="line"></span><br><span class="line">说明：只能清空管理表，也就是内部表；清空外部表，会产生错误</span><br></pre></td></tr></table></figure><blockquote><p><strong>注意：truncate 和 drop：<br>如果 hdfs 开启了回收站，drop 删除的表数据是可以从回收站恢复的，表结构恢复不了，需要自己重新创建；truncate 清空的表是不进回收站的，所以无法恢复truncate清空的表<br>所以 truncate 一定慎用，一旦清空将无力回天</strong></p></blockquote><h4 id="向hive表中加载数据"><a href="#向hive表中加载数据" class="headerlink" title="向hive表中加载数据"></a>向hive表中加载数据</h4><ul><li>直接向分区表中插入数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> score <span class="keyword">partition</span>(<span class="keyword">month</span> <span class="operator">=</span><span class="string">&#x27;201807&#x27;</span>) <span class="keyword">values</span> (<span class="string">&#x27;001&#x27;</span>,<span class="string">&#x27;002&#x27;</span>,<span class="string">&#x27;100&#x27;</span>);</span><br></pre></td></tr></table></figure><ul><li>通过load方式加载数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/export/servers/hivedatas/score.csv&#x27;</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> score <span class="keyword">partition</span>(<span class="keyword">month</span><span class="operator">=</span><span class="string">&#x27;201806&#x27;</span>);</span><br></pre></td></tr></table></figure><ul><li>通过查询方式加载数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> score2 <span class="keyword">partition</span>(<span class="keyword">month</span> <span class="operator">=</span> <span class="string">&#x27;201806&#x27;</span>) <span class="keyword">select</span> s_id,c_id,s_score <span class="keyword">from</span> score1;</span><br></pre></td></tr></table></figure><ul><li>查询语句中创建表并加载数据</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> score2 <span class="keyword">as</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> score1;</span><br></pre></td></tr></table></figure><ul><li>在创建表是通过location指定加载数据的路径</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> score6 (s_id string,c_id string,s_score <span class="type">int</span>) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span> location <span class="string">&#x27;/myscore&#x27;</span>;</span><br></pre></td></tr></table></figure><ul><li>export导出与import 导入 hive表数据（内部表操作）</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> techer2 <span class="keyword">like</span> techer; <span class="comment">--依据已有表结构创建表</span></span><br><span class="line"></span><br><span class="line">export <span class="keyword">table</span> techer <span class="keyword">to</span>  <span class="string">&#x27;/export/techer&#x27;</span>;</span><br><span class="line"></span><br><span class="line">import <span class="keyword">table</span> techer2 <span class="keyword">from</span> <span class="string">&#x27;/export/techer&#x27;</span>;</span><br></pre></td></tr></table></figure><h4 id="hive表中数据导出"><a href="#hive表中数据导出" class="headerlink" title="hive表中数据导出"></a>hive表中数据导出</h4><ul><li>insert导出</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">将查询的结果导出到本地</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/export/servers/exporthive&#x27;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> score;</span><br><span class="line"></span><br><span class="line">将查询的结果格式化导出到本地</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/export/servers/exporthive&#x27;</span> <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span> collection items terminated <span class="keyword">by</span> <span class="string">&#x27;#&#x27;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> student;</span><br><span class="line"></span><br><span class="line">将查询的结果导出到HDFS上(没有<span class="keyword">local</span>)</span><br><span class="line"><span class="keyword">insert</span> overwrite directory <span class="string">&#x27;/export/servers/exporthive&#x27;</span> <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span> collection items terminated <span class="keyword">by</span> <span class="string">&#x27;#&#x27;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> score;</span><br></pre></td></tr></table></figure><ul><li>Hadoop命令导出到本地</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dfs <span class="operator">-</span><span class="keyword">get</span> <span class="operator">/</span>export<span class="operator">/</span>servers<span class="operator">/</span>exporthive<span class="operator">/</span><span class="number">000000</span>_0 <span class="operator">/</span>export<span class="operator">/</span>servers<span class="operator">/</span>exporthive<span class="operator">/</span>local.txt;</span><br></pre></td></tr></table></figure><ul><li>hive shell 命令导出</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">基本语法：（hive <span class="operator">-</span>f<span class="operator">/</span><span class="operator">-</span>e 执行语句或者脚本 <span class="operator">&gt;</span> file）</span><br><span class="line"></span><br><span class="line">hive <span class="operator">-</span>e &quot;select * from myhive.score;&quot; <span class="operator">&gt;</span> <span class="operator">/</span>export<span class="operator">/</span>servers<span class="operator">/</span>exporthive<span class="operator">/</span>score.txt</span><br><span class="line"></span><br><span class="line">hive <span class="operator">-</span>f export.sh <span class="operator">&gt;</span> <span class="operator">/</span>export<span class="operator">/</span>servers<span class="operator">/</span>exporthive<span class="operator">/</span>score.txt</span><br></pre></td></tr></table></figure><ul><li>export导出到HDFS上</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export <span class="keyword">table</span> score <span class="keyword">to</span> <span class="string">&#x27;/export/exporthive/score&#x27;</span>;</span><br></pre></td></tr></table></figure><h3 id="hive的DQL查询语法"><a href="#hive的DQL查询语法" class="headerlink" title="hive的DQL查询语法"></a>hive的DQL查询语法</h3><h3 id="单表查询"><a href="#单表查询" class="headerlink" title="单表查询"></a>单表查询</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> [<span class="keyword">ALL</span> <span class="operator">|</span> <span class="keyword">DISTINCT</span>] select_expr, select_expr, ... </span><br><span class="line"><span class="keyword">FROM</span> table_reference</span><br><span class="line">[<span class="keyword">WHERE</span> where_condition] </span><br><span class="line">[<span class="keyword">GROUP</span> <span class="keyword">BY</span> col_list [<span class="keyword">HAVING</span> <span class="keyword">condition</span>]] </span><br><span class="line">[CLUSTER <span class="keyword">BY</span> col_list </span><br><span class="line">  <span class="operator">|</span> [DISTRIBUTE <span class="keyword">BY</span> col_list] [SORT <span class="keyword">BY</span><span class="operator">|</span> <span class="keyword">ORDER</span> <span class="keyword">BY</span> col_list] </span><br><span class="line">] </span><br><span class="line">[LIMIT number]</span><br></pre></td></tr></table></figure><blockquote><p>注意：<br>1、order by 会对输入做全局排序，因此只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。<br>2、sort by不是全局排序，其在数据进入reducer前完成排序。因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1，则sort by只保证每个reducer的输出有序，不保证全局有序。<br>3、distribute by(字段)根据指定的字段将数据分到不同的reducer，且分发算法是hash散列。<br>4、Cluster by(字段) 除了具有Distribute by的功能外，还会对该字段进行排序。<br>因此，如果分桶和sort字段是同一个时，此时，cluster by &#x3D; distribute by + sort by</p></blockquote><ul><li>WHERE语句</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> score <span class="keyword">where</span> s_score <span class="operator">&lt;</span> <span class="number">60</span>;</span><br></pre></td></tr></table></figure><blockquote><p>注意：<br>小于某个值是不包含null的，如上查询结果是把 s_score 为 null 的行剔除的</p></blockquote><ul><li>GROUP BY 分组</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_id ,<span class="built_in">avg</span>(s_score) <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id;</span><br><span class="line"></span><br><span class="line">分组后对数据进行筛选，使用<span class="keyword">having</span></span><br><span class="line"> <span class="keyword">select</span> s_id ,<span class="built_in">avg</span>(s_score) avgscore <span class="keyword">from</span> score <span class="keyword">group</span> <span class="keyword">by</span> s_id <span class="keyword">having</span> avgscore <span class="operator">&gt;</span> <span class="number">85</span>;</span><br></pre></td></tr></table></figure><blockquote><p>注意：<br>如果使用 group by 分组，则 select 后面只能写分组的字段或者聚合函数<br>where和having区别：<br>1 having是在 group by 分完组之后再对数据进行筛选，所以having 要筛选的字段只能是分组字段或者聚合函数<br>2 where 是从数据表中的字段直接进行的筛选的，所以不能跟在gruop by后面，也不能使用聚合函数</p></blockquote><ul><li>join 连接</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INNER</span> <span class="keyword">JOIN</span> 内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> techer t [<span class="keyword">inner</span>] <span class="keyword">join</span> course c <span class="keyword">on</span> t.t_id <span class="operator">=</span> c.t_id; <span class="comment">-- inner 可省略</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> 左外连接：左边所有数据会被返回，右边符合条件的被返回</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> techer t <span class="keyword">left</span> <span class="keyword">join</span> course c <span class="keyword">on</span> t.t_id <span class="operator">=</span> c.t_id; <span class="comment">-- outer可省略</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RIGHT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> 右外连接：右边所有数据会被返回，左边符合条件的被返回、</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> techer t <span class="keyword">right</span> <span class="keyword">join</span> course c <span class="keyword">on</span> t.t_id <span class="operator">=</span> c.t_id;</span><br><span class="line"></span><br><span class="line"><span class="keyword">FULL</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> 满外(全外)连接: 将会返回所有表中符合条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用<span class="keyword">NULL</span>值替代。</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> techer t <span class="keyword">FULL</span> <span class="keyword">JOIN</span> course c <span class="keyword">ON</span> t.t_id <span class="operator">=</span> c.t_id ;</span><br></pre></td></tr></table></figure><blockquote><p>注：1. hive2版本已经支持不等值连接，就是 join on条件后面可以使用大于小于符号了;并且也支持 join on 条件后跟or (早前版本 on 后只支持 &#x3D; 和 and，不支持 &gt; &lt; 和 or)<br>2.如hive执行引擎使用MapReduce，一个join就会启动一个job，一条sql语句中如有多个join，则会启动多个job</p></blockquote><blockquote><p>注意：表之间用逗号(,)连接和 inner join 是一样的<br>select * from table_a,table_b where table_a.id&#x3D;table_b.id;<br>它们的执行效率没有区别，只是书写方式不同，用逗号是sql 89标准，join 是sql 92标准。用逗号连接后面过滤条件用 where ，用 join 连接后面过滤条件是 on。</p></blockquote><ul><li>order by 排序</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">全局排序，只会有一个reduce</span><br><span class="line"><span class="keyword">ASC</span>（ascend）: 升序（默认） <span class="keyword">DESC</span>（descend）: 降序</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> student s <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> score sco <span class="keyword">ON</span> s.s_id <span class="operator">=</span> sco.s_id <span class="keyword">ORDER</span> <span class="keyword">BY</span> sco.s_score <span class="keyword">DESC</span>;</span><br></pre></td></tr></table></figure><blockquote><p>注意：order by 是全局排序，所以最后只有一个reduce，也就是在一个节点执行，如果数据量太大，就会耗费较长时间</p></blockquote><ul><li>sort by 局部排序</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">每个MapReduce内部进行排序，对全局结果集来说不是排序。</span><br><span class="line"></span><br><span class="line">设置reduce个数</span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">3</span>;</span><br><span class="line"></span><br><span class="line">查看设置reduce个数</span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces;</span><br><span class="line"></span><br><span class="line">查询成绩按照成绩降序排列</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> score sort <span class="keyword">by</span> s_score;</span><br><span class="line"></span><br><span class="line">将查询结果导入到文件中（按照成绩降序排列）</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">local</span> directory <span class="string">&#x27;/export/servers/hivedatas/sort&#x27;</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> score sort <span class="keyword">by</span> s_score;</span><br></pre></td></tr></table></figure><ul><li>distribute by  分区排序</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">distribute <span class="keyword">by</span>：类似MR中<span class="keyword">partition</span>，进行分区，结合sort <span class="keyword">by</span>使用</span><br><span class="line"></span><br><span class="line">设置reduce的个数，将我们对应的s_id划分到对应的reduce当中去</span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">7</span>;</span><br><span class="line"></span><br><span class="line">通过distribute <span class="keyword">by</span>  进行数据的分区</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> score distribute <span class="keyword">by</span> s_id sort <span class="keyword">by</span> s_score;</span><br></pre></td></tr></table></figure><blockquote><p>注意：Hive要求 distribute by 语句要写在 sort by 语句之前</p></blockquote><ul><li>cluster by</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">当distribute <span class="keyword">by</span>和sort <span class="keyword">by</span>字段相同时，可以使用cluster <span class="keyword">by</span>方式.</span><br><span class="line">cluster <span class="keyword">by</span>除了具有distribute <span class="keyword">by</span>的功能外还兼具sort <span class="keyword">by</span>的功能。但是排序只能是正序排序，不能指定排序规则为<span class="keyword">ASC</span>或者<span class="keyword">DESC</span>。</span><br><span class="line"></span><br><span class="line">以下两种写法等价</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> score cluster <span class="keyword">by</span> s_id;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> score distribute <span class="keyword">by</span> s_id sort <span class="keyword">by</span> s_id;</span><br></pre></td></tr></table></figure><h2 id="Hive函数"><a href="#Hive函数" class="headerlink" title="Hive函数"></a>Hive函数</h2><h3 id="聚合函数"><a href="#聚合函数" class="headerlink" title="聚合函数"></a>聚合函数</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive支持 <span class="built_in">count</span>(),<span class="built_in">max</span>(),<span class="built_in">min</span>(),<span class="built_in">sum</span>(),<span class="built_in">avg</span>() 等常用的聚合函数</span><br></pre></td></tr></table></figure><blockquote><p>注意：<br>聚合操作时要注意null值<br>count(*) 包含null值，统计所有行数<br>count(id) 不包含null值<br>min 求最小值是不包含null，除非所有值都是null<br>avg 求平均值也是不包含null</p></blockquote><ul><li>非空集合总体变量函数: var_pop</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="built_in">var_pop</span>(col)</span><br><span class="line">返回值: <span class="keyword">double</span></span><br><span class="line">说明: 统计结果集中col非空集合的总体变量（忽略<span class="keyword">null</span>）</span><br></pre></td></tr></table></figure><ul><li>非空集合样本变量函数: var_samp</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="built_in">var_samp</span> (col)</span><br><span class="line">返回值: <span class="keyword">double</span></span><br><span class="line">说明: 统计结果集中col非空集合的样本变量（忽略<span class="keyword">null</span>）</span><br></pre></td></tr></table></figure><ul><li>总体标准偏离函数: stddev_pop</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="built_in">stddev_pop</span>(col)</span><br><span class="line">返回值: <span class="keyword">double</span></span><br><span class="line">说明: 该函数计算总体标准偏离，并返回总体变量的平方根，其返回值与VAR_POP函数的平方根相同</span><br></pre></td></tr></table></figure><ul><li>中位数函数: percentile</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">语法: percentile(<span class="type">BIGINT</span> col, p)</span><br><span class="line">返回值: <span class="keyword">double</span></span><br><span class="line">说明: 求准确的第pth个百分位数，p必须介于<span class="number">0</span>和<span class="number">1</span>之间，但是col字段目前只支持整数，不支持浮点数类型</span><br></pre></td></tr></table></figure><h3 id="关系运算"><a href="#关系运算" class="headerlink" title="关系运算"></a>关系运算</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">支持：等值(<span class="operator">=</span>)、不等值(<span class="operator">!=</span> 或 <span class="operator">&lt;&gt;</span>)、小于(<span class="operator">&lt;</span>)、小于等于(<span class="operator">&lt;=</span>)、大于(<span class="operator">&gt;</span>)、大于等于(<span class="operator">&gt;=</span>)</span><br><span class="line"></span><br><span class="line">空值判断(<span class="keyword">is</span> <span class="keyword">null</span>)、非空判断(<span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">null</span>)</span><br></pre></td></tr></table></figure><ul><li>LIKE比较: LIKE</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">语法: A <span class="keyword">LIKE</span> B</span><br><span class="line">操作类型: strings</span><br><span class="line">描述: 如果字符串A或者字符串B为<span class="keyword">NULL</span>，则返回<span class="keyword">NULL</span>；如果字符串A符合表达式B 的正则语法，则为<span class="literal">TRUE</span>；否则为<span class="literal">FALSE</span>。B中字符”_”表示任意单个字符，而字符”<span class="operator">%</span>”表示任意数量的字符。</span><br></pre></td></tr></table></figure><ul><li>JAVA的LIKE操作: RLIKE</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">语法: A RLIKE B</span><br><span class="line">操作类型: strings</span><br><span class="line">描述: 如果字符串A或者字符串B为<span class="keyword">NULL</span>，则返回<span class="keyword">NULL</span>；如果字符串A符合JAVA正则表达式B的正则语法，则为<span class="literal">TRUE</span>；否则为<span class="literal">FALSE</span>。</span><br></pre></td></tr></table></figure><ul><li>REGEXP操作: REGEXP</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: A REGEXP B</span><br><span class="line">操作类型: strings</span><br><span class="line">描述: 功能与RLIKE相同</span><br><span class="line">示例：<span class="keyword">select</span> <span class="number">1</span> <span class="keyword">from</span> tableName <span class="keyword">where</span> <span class="string">&#x27;footbar&#x27;</span> REGEXP <span class="string">&#x27;^f.*r$&#x27;</span>;</span><br><span class="line">结果：<span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="数学运算"><a href="#数学运算" class="headerlink" title="数学运算"></a>数学运算</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">支持所有数值类型：加(<span class="operator">+</span>)、减(<span class="operator">-</span>)、乘(<span class="operator">*</span>)、除(<span class="operator">/</span>)、取余(<span class="operator">%</span>)、位与(<span class="operator">&amp;</span>)、位或(<span class="operator">|</span>)、位异或(<span class="operator">^</span>)、位取反(<span class="operator">~</span>)</span><br></pre></td></tr></table></figure><h3 id="逻辑运算"><a href="#逻辑运算" class="headerlink" title="逻辑运算"></a>逻辑运算</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">支持：逻辑与(<span class="keyword">and</span>)、逻辑或(<span class="keyword">or</span>)、逻辑非(<span class="keyword">not</span>)</span><br></pre></td></tr></table></figure><h3 id="数值运算"><a href="#数值运算" class="headerlink" title="数值运算"></a>数值运算</h3><ul><li>取整函数: round</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: round(<span class="keyword">double</span> a)</span><br><span class="line">返回值: <span class="type">BIGINT</span></span><br><span class="line">说明: 返回<span class="keyword">double</span>类型的整数值部分 （遵循四舍五入）</span><br><span class="line">示例：<span class="keyword">select</span> round(<span class="number">3.1415926</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">结果：<span class="number">3</span></span><br></pre></td></tr></table></figure><ul><li>指定精度取整函数: round</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: round(<span class="keyword">double</span> a, <span class="type">int</span> d)</span><br><span class="line">返回值: <span class="keyword">DOUBLE</span></span><br><span class="line">说明: 返回指定精度d的<span class="keyword">double</span>类型</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> round(<span class="number">3.1415926</span>,<span class="number">4</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">3.1416</span></span><br></pre></td></tr></table></figure><ul><li>向下取整函数: floor</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="built_in">floor</span>(<span class="keyword">double</span> a)</span><br><span class="line">返回值: <span class="type">BIGINT</span></span><br><span class="line">说明: 返回等于或者小于该<span class="keyword">double</span>变量的最大的整数</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">floor</span>(<span class="number">3.641</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure><ul><li>向上取整函数: ceil</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="built_in">ceil</span>(<span class="keyword">double</span> a)</span><br><span class="line">返回值: <span class="type">BIGINT</span></span><br><span class="line">说明: 返回等于或者大于该<span class="keyword">double</span>变量的最小的整数</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">ceil</span>(<span class="number">3.1415926</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">4</span></span><br></pre></td></tr></table></figure><ul><li>取随机数函数: rand</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">语法: rand(),rand(<span class="type">int</span> seed)</span><br><span class="line">返回值: <span class="keyword">double</span></span><br><span class="line">说明: 返回一个<span class="number">0</span>到<span class="number">1</span>范围内的随机数。如果指定种子seed，则会等到一个稳定的随机数序列</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> rand() <span class="keyword">from</span> tableName; <span class="comment">-- 每次执行此语句得到的结果都不同</span></span><br><span class="line"><span class="number">0.5577432776034763</span></span><br><span class="line"></span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> rand(<span class="number">100</span>) ;  <span class="comment">-- 只要指定种子，每次执行此语句得到的结果一样的</span></span><br><span class="line"><span class="number">0.7220096548596434</span></span><br></pre></td></tr></table></figure><ul><li>自然指数函数: exp</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="built_in">exp</span>(<span class="keyword">double</span> a)</span><br><span class="line">返回值: <span class="keyword">double</span></span><br><span class="line">说明: 返回自然对数e的a次方</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">exp</span>(<span class="number">2</span>) ;</span><br><span class="line"><span class="number">7.38905609893065</span></span><br></pre></td></tr></table></figure><ul><li>以10为底对数函数: log10</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="built_in">log10</span>(<span class="keyword">double</span> a)</span><br><span class="line">返回值: <span class="keyword">double</span></span><br><span class="line">说明: 返回以<span class="number">10</span>为底的a的对数</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">log10</span>(<span class="number">100</span>) ;</span><br><span class="line"><span class="number">2.0</span></span><br></pre></td></tr></table></figure><blockquote><p>此外还有：以2为底对数函数: log2()、对数函数: log()</p></blockquote><ul><li>幂运算函数: pow</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: pow(<span class="keyword">double</span> a, <span class="keyword">double</span> p)</span><br><span class="line">返回值: <span class="keyword">double</span></span><br><span class="line">说明: 返回a的p次幂</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> pow(<span class="number">2</span>,<span class="number">4</span>) ;</span><br><span class="line"><span class="number">16.0</span></span><br></pre></td></tr></table></figure><ul><li>开平方函数: sqrt</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="built_in">sqrt</span>(<span class="keyword">double</span> a)</span><br><span class="line">返回值: <span class="keyword">double</span></span><br><span class="line">说明: 返回a的平方根</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">sqrt</span>(<span class="number">16</span>) ;</span><br><span class="line"><span class="number">4.0</span></span><br></pre></td></tr></table></figure><ul><li>二进制函数: bin</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: bin(<span class="type">BIGINT</span> a)</span><br><span class="line">返回值: string</span><br><span class="line">说明: 返回a的二进制代码表示</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> bin(<span class="number">7</span>) ;</span><br><span class="line"><span class="number">111</span></span><br></pre></td></tr></table></figure><blockquote><p>十六进制函数: hex()、将十六进制转化为字符串函数: unhex()<br>进制转换函数: conv(bigint num, int from_base, int to_base) 说明: 将数值num从from_base进制转化到to_base进制</p></blockquote><blockquote><p>此外还有很多数学函数：绝对值函数: abs()、正取余函数: pmod()、正弦函数: sin()、反正弦函数: asin()、余弦函数: cos()、反余弦函数: acos()、positive函数: positive()、negative函数: negative()</p></blockquote><h3 id="条件函数"><a href="#条件函数" class="headerlink" title="条件函数"></a>条件函数</h3><ul><li>If函数: if</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">语法: if(<span class="type">boolean</span> testCondition, T valueTrue, T valueFalseOrNull)</span><br><span class="line">返回值: T</span><br><span class="line">说明: 当条件testCondition为<span class="literal">TRUE</span>时，返回valueTrue；否则返回valueFalseOrNull</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> if(<span class="number">1</span><span class="operator">=</span><span class="number">2</span>,<span class="number">100</span>,<span class="number">200</span>) ;</span><br><span class="line"><span class="number">200</span></span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> if(<span class="number">1</span><span class="operator">=</span><span class="number">1</span>,<span class="number">100</span>,<span class="number">200</span>) ;</span><br><span class="line"><span class="number">100</span></span><br></pre></td></tr></table></figure><ul><li>非空查找函数: coalesce</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="built_in">coalesce</span>(T v1, T v2, …)</span><br><span class="line">返回值: T</span><br><span class="line">说明: 返回参数中的第一个非空值；如果所有值都为<span class="keyword">NULL</span>，那么返回<span class="keyword">NULL</span></span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">coalesce</span>(<span class="keyword">null</span>,<span class="string">&#x27;100&#x27;</span>,<span class="string">&#x27;50&#x27;</span>) ;</span><br><span class="line"><span class="number">100</span></span><br></pre></td></tr></table></figure><ul><li>条件判断函数：case when (两种写法，其一)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="keyword">case</span> <span class="keyword">when</span> a <span class="keyword">then</span> b [<span class="keyword">when</span> c <span class="keyword">then</span> d]<span class="operator">*</span> [<span class="keyword">else</span> e] <span class="keyword">end</span></span><br><span class="line">返回值: T</span><br><span class="line">说明：如果a为<span class="literal">TRUE</span>,则返回b；如果c为<span class="literal">TRUE</span>，则返回d；否则返回e</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="keyword">case</span> <span class="keyword">when</span> <span class="number">1</span><span class="operator">=</span><span class="number">2</span> <span class="keyword">then</span> <span class="string">&#x27;tom&#x27;</span> <span class="keyword">when</span> <span class="number">2</span><span class="operator">=</span><span class="number">2</span> <span class="keyword">then</span> <span class="string">&#x27;mary&#x27;</span> <span class="keyword">else</span> <span class="string">&#x27;tim&#x27;</span> <span class="keyword">end</span> <span class="keyword">from</span> tableName;</span><br><span class="line">mary</span><br></pre></td></tr></table></figure><ul><li>条件判断函数：case when (两种写法，其二)</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="keyword">case</span> a <span class="keyword">when</span> b <span class="keyword">then</span> c [<span class="keyword">when</span> d <span class="keyword">then</span> e]<span class="operator">*</span> [<span class="keyword">else</span> f] <span class="keyword">end</span></span><br><span class="line">返回值: T</span><br><span class="line">说明：如果a等于b，那么返回c；如果a等于d，那么返回e；否则返回f</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">Select</span> <span class="keyword">case</span> <span class="number">100</span> <span class="keyword">when</span> <span class="number">50</span> <span class="keyword">then</span> <span class="string">&#x27;tom&#x27;</span> <span class="keyword">when</span> <span class="number">100</span> <span class="keyword">then</span> <span class="string">&#x27;mary&#x27;</span> <span class="keyword">else</span> <span class="string">&#x27;tim&#x27;</span> <span class="keyword">end</span> <span class="keyword">from</span> tableName;</span><br><span class="line">mary</span><br></pre></td></tr></table></figure><h3 id="日期函数"><a href="#日期函数" class="headerlink" title="日期函数"></a>日期函数</h3><blockquote><p>注：以下SQL语句中的 from tableName 可去掉，不影响查询结果</p></blockquote><p>- </p><ul><li><ol><li>获取当前UNIX时间戳函数: unix_timestamp</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: unix_timestamp()</span><br><span class="line">返回值: <span class="type">bigint</span></span><br><span class="line">说明: 获得当前时区的UNIX时间戳</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> unix_timestamp() <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">1616906976</span></span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>UNIX时间戳转日期函数: from_unixtime</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: from_unixtime(<span class="type">bigint</span> unixtime[, string format])</span><br><span class="line">返回值: string</span><br><span class="line">说明: 转化UNIX时间戳（从<span class="number">1970</span><span class="number">-01</span><span class="number">-01</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span> UTC到指定时间的秒数）到当前时区的时间格式</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> from_unixtime(<span class="number">1616906976</span>,<span class="string">&#x27;yyyyMMdd&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">20210328</span></span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>日期转UNIX时间戳函数: unix_timestamp</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: unix_timestamp(string <span class="type">date</span>)</span><br><span class="line">返回值: <span class="type">bigint</span></span><br><span class="line">说明: 转换格式为&quot;yyyy-MM-dd HH:mm:ss&quot;的日期到UNIX时间戳。如果转化失败，则返回<span class="number">0</span>。</span><br><span class="line">hive<span class="operator">&gt;</span>  <span class="keyword">select</span> unix_timestamp(<span class="string">&#x27;2021-03-08 14:21:15&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">1615184475</span></span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>指定格式日期转UNIX时间戳函数: unix_timestamp</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: unix_timestamp(string <span class="type">date</span>, string <span class="keyword">pattern</span>)</span><br><span class="line">返回值: <span class="type">bigint</span></span><br><span class="line">说明: 转换<span class="keyword">pattern</span>格式的日期到UNIX时间戳。如果转化失败，则返回<span class="number">0</span>。</span><br><span class="line">hive<span class="operator">&gt;</span>  <span class="keyword">select</span> unix_timestamp(<span class="string">&#x27;2021-03-08 14:21:15&#x27;</span>,<span class="string">&#x27;yyyyMMdd HH:mm:ss&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">1615184475</span></span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>日期时间转日期函数: to_date</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: to_date(string <span class="type">timestamp</span>)</span><br><span class="line">返回值: string</span><br><span class="line">说明: 返回日期时间字段中的日期部分。</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> to_date(<span class="string">&#x27;2021-03-28 14:03:01&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">2021</span><span class="number">-03</span><span class="number">-28</span></span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>日期转年函数: year</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="keyword">year</span>(string <span class="type">date</span>)</span><br><span class="line">返回值: <span class="type">int</span></span><br><span class="line">说明: 返回日期中的年。</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="keyword">year</span>(<span class="string">&#x27;2021-03-28 10:03:01&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">2021</span></span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="keyword">year</span>(<span class="string">&#x27;2021-03-28&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">2021</span></span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>日期转月函数: month</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="keyword">month</span> (string <span class="type">date</span>)</span><br><span class="line">返回值: <span class="type">int</span></span><br><span class="line">说明: 返回日期中的月份。</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="keyword">month</span>(<span class="string">&#x27;2020-12-28 12:03:01&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">12</span></span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="keyword">month</span>(<span class="string">&#x27;2021-03-08&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">8</span></span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>日期转天函数: day</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="keyword">day</span> (string <span class="type">date</span>)</span><br><span class="line">返回值: <span class="type">int</span></span><br><span class="line">说明: 返回日期中的天。</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="keyword">day</span>(<span class="string">&#x27;2020-12-08 10:03:01&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">8</span></span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="keyword">day</span>(<span class="string">&#x27;2020-12-24&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">24</span></span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>日期转小时函数: hour</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="keyword">hour</span> (string <span class="type">date</span>)</span><br><span class="line">返回值: <span class="type">int</span></span><br><span class="line">说明: 返回日期中的小时。</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="keyword">hour</span>(<span class="string">&#x27;2020-12-08 10:03:01&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">10</span></span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>日期转分钟函数: minute</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="keyword">minute</span> (string <span class="type">date</span>)</span><br><span class="line">返回值: <span class="type">int</span></span><br><span class="line">说明: 返回日期中的分钟。</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="keyword">minute</span>(<span class="string">&#x27;2020-12-08 10:03:01&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">3</span></span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>日期转秒函数: second</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="keyword">second</span> (string <span class="type">date</span>)</span><br><span class="line">返回值: <span class="type">int</span></span><br><span class="line">说明: 返回日期中的秒。</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="keyword">second</span>(<span class="string">&#x27;2020-12-08 10:03:01&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>日期转周函数: weekofyear</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: weekofyear (string <span class="type">date</span>)</span><br><span class="line">返回值: <span class="type">int</span></span><br><span class="line">说明: 返回日期在当前的周数。</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> weekofyear(<span class="string">&#x27;2020-12-08 10:03:01&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">49</span></span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>日期比较函数: datediff</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: datediff(string enddate, string startdate)</span><br><span class="line">返回值: <span class="type">int</span></span><br><span class="line">说明: 返回结束日期减去开始日期的天数。</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> datediff(<span class="string">&#x27;2020-12-08&#x27;</span>,<span class="string">&#x27;2012-05-09&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">213</span></span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>日期增加函数: date_add</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: date_add(string startdate, <span class="type">int</span> days)</span><br><span class="line">返回值: string</span><br><span class="line">说明: 返回开始日期startdate增加days天后的日期。</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> date_add(<span class="string">&#x27;2020-12-08&#x27;</span>,<span class="number">10</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">2020</span><span class="number">-12</span><span class="number">-18</span></span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>日期减少函数: date_sub</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: date_sub (string startdate, <span class="type">int</span> days)</span><br><span class="line">返回值: string</span><br><span class="line">说明: 返回开始日期startdate减少days天后的日期。</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> date_sub(<span class="string">&#x27;2020-12-08&#x27;</span>,<span class="number">10</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">2020</span><span class="number">-11</span><span class="number">-28</span></span><br></pre></td></tr></table></figure><h3 id="字符串函数"><a href="#字符串函数" class="headerlink" title="字符串函数"></a>字符串函数</h3><p>- </p><ul><li><ol><li>字符串长度函数：length</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: length(string A)</span><br><span class="line">返回值: <span class="type">int</span></span><br><span class="line">说明：返回字符串A的长度</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> length(<span class="string">&#x27;abcedfg&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">7</span></span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>字符串反转函数：reverse</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: reverse(string A)</span><br><span class="line">返回值: string</span><br><span class="line">说明：返回字符串A的反转结果</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> reverse(<span class="string">&#x27;abcedfg&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">gfdecba</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>字符串连接函数：concat</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: concat(string A, string B…)</span><br><span class="line">返回值: string</span><br><span class="line">说明：返回输入字符串连接后的结果，支持任意个输入字符串</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> concat(<span class="string">&#x27;abc&#x27;</span>,<span class="string">&#x27;def’,&#x27;</span>gh<span class="string">&#x27;)from tableName;</span></span><br><span class="line"><span class="string">abcdefgh</span></span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>带分隔符字符串连接函数：concat_ws</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: concat_ws(string SEP, string A, string B…)</span><br><span class="line">返回值: string</span><br><span class="line">说明：返回输入字符串连接后的结果，SEP表示各个字符串间的分隔符</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> concat_ws(<span class="string">&#x27;,&#x27;</span>,<span class="string">&#x27;abc&#x27;</span>,<span class="string">&#x27;def&#x27;</span>,<span class="string">&#x27;gh&#x27;</span>)<span class="keyword">from</span> tableName;</span><br><span class="line">abc,def,gh</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>字符串截取函数：substr,substring</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">语法: substr(string A, <span class="type">int</span> <span class="keyword">start</span>),<span class="built_in">substring</span>(string A, <span class="type">int</span> <span class="keyword">start</span>)</span><br><span class="line">返回值: string</span><br><span class="line">说明：返回字符串A从<span class="keyword">start</span>位置到结尾的字符串</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> substr(<span class="string">&#x27;abcde&#x27;</span>,<span class="number">3</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">cde</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">substring</span>(<span class="string">&#x27;abcde&#x27;</span>,<span class="number">3</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">cde</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> substr(<span class="string">&#x27;abcde&#x27;</span>,<span class="number">-1</span>) <span class="keyword">from</span> tableName; （和ORACLE相同）</span><br><span class="line">e</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>字符串截取函数：substr,substring</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">语法: substr(string A, <span class="type">int</span> <span class="keyword">start</span>, <span class="type">int</span> len),<span class="built_in">substring</span>(string A, <span class="type">int</span> <span class="keyword">start</span>, <span class="type">int</span> len)</span><br><span class="line">返回值: string</span><br><span class="line">说明：返回字符串A从<span class="keyword">start</span>位置开始，长度为len的字符串</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> substr(<span class="string">&#x27;abcde&#x27;</span>,<span class="number">3</span>,<span class="number">2</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">cd</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">substring</span>(<span class="string">&#x27;abcde&#x27;</span>,<span class="number">3</span>,<span class="number">2</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">cd</span><br><span class="line">hive<span class="operator">&gt;</span><span class="keyword">select</span> <span class="built_in">substring</span>(<span class="string">&#x27;abcde&#x27;</span>,<span class="number">-2</span>,<span class="number">2</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">de</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>字符串转大写函数：upper,ucase</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="built_in">upper</span>(string A) ucase(string A)</span><br><span class="line">返回值: string</span><br><span class="line">说明：返回字符串A的大写格式</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">upper</span>(<span class="string">&#x27;abSEd&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">ABSED</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> ucase(<span class="string">&#x27;abSEd&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">ABSED</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>字符串转小写函数：lower,lcase</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="built_in">lower</span>(string A) lcase(string A)</span><br><span class="line">返回值: string</span><br><span class="line">说明：返回字符串A的小写格式</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">lower</span>(<span class="string">&#x27;abSEd&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">absed</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> lcase(<span class="string">&#x27;abSEd&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">absed</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>去空格函数：trim</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="built_in">trim</span>(string A)</span><br><span class="line">返回值: string</span><br><span class="line">说明：去除字符串两边的空格</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">trim</span>(<span class="string">&#x27; abc &#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">abc</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>左边去空格函数：ltrim</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: ltrim(string A)</span><br><span class="line">返回值: string</span><br><span class="line">说明：去除字符串左边的空格</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> ltrim(<span class="string">&#x27; abc &#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">abc</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>右边去空格函数：rtrim</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: rtrim(string A)</span><br><span class="line">返回值: string</span><br><span class="line">说明：去除字符串右边的空格</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> rtrim(<span class="string">&#x27; abc &#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">abc</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>正则表达式替换函数：regexp_replace</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: regexp_replace(string A, string B, string C)</span><br><span class="line">返回值: string</span><br><span class="line">说明：将字符串A中的符合java正则表达式B的部分替换为C。注意，在有些情况下要使用转义字符,类似oracle中的regexp_replace函数。</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> regexp_replace(<span class="string">&#x27;foobar&#x27;</span>, <span class="string">&#x27;oo|ar&#x27;</span>, <span class="string">&#x27;&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">fb</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>正则表达式解析函数：regexp_extract</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">语法: regexp_extract(string subject, string <span class="keyword">pattern</span>, <span class="type">int</span> index)</span><br><span class="line">返回值: string</span><br><span class="line">说明：将字符串subject按照<span class="keyword">pattern</span>正则表达式的规则拆分，返回index指定的字符。</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> regexp_extract(<span class="string">&#x27;foothebar&#x27;</span>, <span class="string">&#x27;foo(.*?)(bar)&#x27;</span>, <span class="number">1</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">the</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> regexp_extract(<span class="string">&#x27;foothebar&#x27;</span>, <span class="string">&#x27;foo(.*?)(bar)&#x27;</span>, <span class="number">2</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">bar</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> regexp_extract(<span class="string">&#x27;foothebar&#x27;</span>, <span class="string">&#x27;foo(.*?)(bar)&#x27;</span>, <span class="number">0</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">foothebar</span><br><span class="line">strong<span class="operator">&gt;</span>注意，在有些情况下要使用转义字符，下面的等号要用双竖线转义，这是java正则表达式的规则。</span><br><span class="line"><span class="keyword">select</span> data_field,</span><br><span class="line">regexp_extract(data_field,<span class="string">&#x27;.*?bgStart\\=([^&amp;]+)&#x27;</span>,<span class="number">1</span>) <span class="keyword">as</span> aaa,</span><br><span class="line">regexp_extract(data_field,<span class="string">&#x27;.*?contentLoaded_headStart\\=([^&amp;]+)&#x27;</span>,<span class="number">1</span>) <span class="keyword">as</span> bbb,</span><br><span class="line">regexp_extract(data_field,<span class="string">&#x27;.*?AppLoad2Req\\=([^&amp;]+)&#x27;</span>,<span class="number">1</span>) <span class="keyword">as</span> ccc </span><br><span class="line"><span class="keyword">from</span> pt_nginx_loginlog_st </span><br><span class="line"><span class="keyword">where</span> pt <span class="operator">=</span> <span class="string">&#x27;2021-03-28&#x27;</span> limit <span class="number">2</span>;</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>URL解析函数：parse_url</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">语法: parse_url(string urlString, string partToExtract [, string keyToExtract])</span><br><span class="line">返回值: string</span><br><span class="line">说明：返回URL中指定的部分。partToExtract的有效值为：HOST, PATH, QUERY, <span class="keyword">REF</span>, PROTOCOL, AUTHORITY, FILE, <span class="keyword">and</span> USERINFO.</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> parse_url</span><br><span class="line">(<span class="string">&#x27;https://www.tableName.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#x27;</span>, <span class="string">&#x27;HOST&#x27;</span>) </span><br><span class="line"><span class="keyword">from</span> tableName;</span><br><span class="line">www.tableName.com </span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> parse_url</span><br><span class="line">(<span class="string">&#x27;https://www.tableName.com/path1/p.php?k1=v1&amp;k2=v2#Ref1&#x27;</span>, <span class="string">&#x27;QUERY&#x27;</span>, <span class="string">&#x27;k1&#x27;</span>)</span><br><span class="line"> <span class="keyword">from</span> tableName;</span><br><span class="line">v1</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>json解析函数：get_json_object</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">语法: get_json_object(string json_string, string path)</span><br><span class="line">返回值: string</span><br><span class="line">说明：解析json的字符串json_string,返回path指定的内容。如果输入的json字符串无效，那么返回<span class="keyword">NULL</span>。</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span>  get_json_object(<span class="string">&#x27;&#123;&quot;store&quot;:&#123;&quot;fruit&quot;:\[&#123;&quot;weight&quot;:8,&quot;type&quot;:&quot;apple&quot;&#125;,&#123;&quot;weight&quot;:9,&quot;type&quot;:&quot;pear&quot;&#125;], &quot;bicycle&quot;:&#123;&quot;price&quot;:19.95,&quot;color&quot;:&quot;red&quot;&#125; &#125;,&quot;email&quot;:&quot;amy@only_for_json_udf_test.net&quot;,&quot;owner&quot;:&quot;amy&quot;&#125;&#x27;</span>,<span class="string">&#x27;$.owner&#x27;</span>) <span class="keyword">from</span> tableName;</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>空格字符串函数：space</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">语法: space(<span class="type">int</span> n)</span><br><span class="line">返回值: string</span><br><span class="line">说明：返回长度为n的字符串</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> space(<span class="number">10</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> length(space(<span class="number">10</span>)) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">10</span></span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>重复字符串函数：repeat</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: repeat(string str, <span class="type">int</span> n)</span><br><span class="line">返回值: string</span><br><span class="line">说明：返回重复n次后的str字符串</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> repeat(<span class="string">&#x27;abc&#x27;</span>,<span class="number">5</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">abcabcabcabcabc</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>首字符ascii函数：ascii</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: ascii(string str)</span><br><span class="line">返回值: <span class="type">int</span></span><br><span class="line">说明：返回字符串str第一个字符的ascii码</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> ascii(<span class="string">&#x27;abcde&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">97</span></span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>左补足函数：lpad</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">语法: lpad(string str, <span class="type">int</span> len, string pad)</span><br><span class="line">返回值: string</span><br><span class="line">说明：将str进行用pad进行左补足到len位</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> lpad(<span class="string">&#x27;abc&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;td&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">tdtdtdtabc</span><br><span class="line">注意：与GP，ORACLE不同，pad 不能默认</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>右补足函数：rpad</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: rpad(string str, <span class="type">int</span> len, string pad)</span><br><span class="line">返回值: string</span><br><span class="line">说明：将str进行用pad进行右补足到len位</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> rpad(<span class="string">&#x27;abc&#x27;</span>,<span class="number">10</span>,<span class="string">&#x27;td&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">abctdtdtdt</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>分割字符串函数: split</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: split(string str, string pat)</span><br><span class="line">返回值: <span class="keyword">array</span></span><br><span class="line">说明: 按照pat字符串分割str，会返回分割后的字符串数组</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> split(<span class="string">&#x27;abtcdtef&#x27;</span>,<span class="string">&#x27;t&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line">[&quot;ab&quot;,&quot;cd&quot;,&quot;ef&quot;]</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>集合查找函数: find_in_set</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">语法: find_in_set(string str, string strList)</span><br><span class="line">返回值: <span class="type">int</span></span><br><span class="line">说明: 返回str在strlist第一次出现的位置，strlist是用逗号分割的字符串。如果没有找该str字符，则返回<span class="number">0</span></span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> find_in_set(<span class="string">&#x27;ab&#x27;</span>,<span class="string">&#x27;ef,ab,de&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">2</span></span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> find_in_set(<span class="string">&#x27;at&#x27;</span>,<span class="string">&#x27;ef,ab,de&#x27;</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">0</span></span><br></pre></td></tr></table></figure><h3 id="复合类型构建操作"><a href="#复合类型构建操作" class="headerlink" title="复合类型构建操作"></a>复合类型构建操作</h3><ul><li>Map类型构建: map</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">语法: map (key1, value1, key2, value2, …)</span><br><span class="line">说明：根据输入的key和<span class="keyword">value</span>对构建map类型</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">Create</span> <span class="keyword">table</span> mapTable <span class="keyword">as</span> <span class="keyword">select</span> map(<span class="string">&#x27;100&#x27;</span>,<span class="string">&#x27;tom&#x27;</span>,<span class="string">&#x27;200&#x27;</span>,<span class="string">&#x27;mary&#x27;</span>) <span class="keyword">as</span> t <span class="keyword">from</span> tableName;</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">describe</span> mapTable;</span><br><span class="line">t       map<span class="operator">&lt;</span>string ,string<span class="operator">&gt;</span></span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> t <span class="keyword">from</span> tableName;</span><br><span class="line">&#123;&quot;100&quot;:&quot;tom&quot;,&quot;200&quot;:&quot;mary&quot;&#125;</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>Struct类型构建: struct</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">语法: struct(val1, val2, val3, …)</span><br><span class="line">说明：根据输入的参数构建结构体struct类型</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> struct_table <span class="keyword">as</span> <span class="keyword">select</span> struct(<span class="string">&#x27;tom&#x27;</span>,<span class="string">&#x27;mary&#x27;</span>,<span class="string">&#x27;tim&#x27;</span>) <span class="keyword">as</span> t <span class="keyword">from</span> tableName;</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">describe</span> struct_table;</span><br><span class="line">t       struct<span class="operator">&lt;</span>col1:string ,col2:string,col3:string<span class="operator">&gt;</span></span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> t <span class="keyword">from</span> tableName;</span><br><span class="line">&#123;&quot;col1&quot;:&quot;tom&quot;,&quot;col2&quot;:&quot;mary&quot;,&quot;col3&quot;:&quot;tim&quot;&#125;</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>array类型构建: array</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">语法: <span class="keyword">array</span>(val1, val2, …)</span><br><span class="line">说明：根据输入的参数构建数组<span class="keyword">array</span>类型</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> arr_table <span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">array</span>(&quot;tom&quot;,&quot;mary&quot;,&quot;tim&quot;) <span class="keyword">as</span> t <span class="keyword">from</span> tableName;</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">describe</span> tableName;</span><br><span class="line">t       <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span></span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> t <span class="keyword">from</span> tableName;</span><br><span class="line">[&quot;tom&quot;,&quot;mary&quot;,&quot;tim&quot;]</span><br></pre></td></tr></table></figure><h3 id="复杂类型访问操作"><a href="#复杂类型访问操作" class="headerlink" title="复杂类型访问操作"></a>复杂类型访问操作</h3><p>- </p><ul><li><ol><li>array类型访问: A[n]</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">语法: A[n]</span><br><span class="line">操作类型: A为<span class="keyword">array</span>类型，n为<span class="type">int</span>类型</span><br><span class="line">说明：返回数组A中的第n个变量值。数组的起始下标为<span class="number">0</span>。比如，A是个值为[<span class="string">&#x27;foo&#x27;</span>, <span class="string">&#x27;bar&#x27;</span>]的数组类型，那么A[<span class="number">0</span>]将返回<span class="string">&#x27;foo&#x27;</span>,而A[<span class="number">1</span>]将返回<span class="string">&#x27;bar&#x27;</span></span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> arr_table2 <span class="keyword">as</span> <span class="keyword">select</span> <span class="keyword">array</span>(&quot;tom&quot;,&quot;mary&quot;,&quot;tim&quot;) <span class="keyword">as</span> t</span><br><span class="line"> <span class="keyword">from</span> tableName;</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> t[<span class="number">0</span>],t[<span class="number">1</span>] <span class="keyword">from</span> arr_table2;</span><br><span class="line">tom     mary    tim</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>map类型访问: M[key]</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">语法: M[key]</span><br><span class="line">操作类型: M为map类型，key为map中的key值</span><br><span class="line">说明：返回map类型M中，key值为指定值的<span class="keyword">value</span>值。比如，M是值为&#123;<span class="string">&#x27;f&#x27;</span> <span class="operator">-</span><span class="operator">&gt;</span> <span class="string">&#x27;foo&#x27;</span>, <span class="string">&#x27;b&#x27;</span> <span class="operator">-</span><span class="operator">&gt;</span> <span class="string">&#x27;bar&#x27;</span>, <span class="string">&#x27;all&#x27;</span> <span class="operator">-</span><span class="operator">&gt;</span> <span class="string">&#x27;foobar&#x27;</span>&#125;的map类型，那么M[<span class="string">&#x27;all&#x27;</span>]将会返回<span class="string">&#x27;foobar&#x27;</span></span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">Create</span> <span class="keyword">table</span> map_table2 <span class="keyword">as</span> <span class="keyword">select</span> map(<span class="string">&#x27;100&#x27;</span>,<span class="string">&#x27;tom&#x27;</span>,<span class="string">&#x27;200&#x27;</span>,<span class="string">&#x27;mary&#x27;</span>) <span class="keyword">as</span> t <span class="keyword">from</span> tableName;</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> t[<span class="string">&#x27;200&#x27;</span>],t[<span class="string">&#x27;100&#x27;</span>] <span class="keyword">from</span> map_table2;</span><br><span class="line">mary    tom</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>struct类型访问: S.x</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">语法: S.x</span><br><span class="line">操作类型: S为struct类型</span><br><span class="line">说明：返回结构体S中的x字段。比如，对于结构体struct foobar &#123;<span class="type">int</span> foo, <span class="type">int</span> bar&#125;，foobar.foo返回结构体中的foo字段</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> str_table2 <span class="keyword">as</span> <span class="keyword">select</span> struct(<span class="string">&#x27;tom&#x27;</span>,<span class="string">&#x27;mary&#x27;</span>,<span class="string">&#x27;tim&#x27;</span>) <span class="keyword">as</span> t <span class="keyword">from</span> tableName;</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">describe</span> tableName;</span><br><span class="line">t       struct<span class="operator">&lt;</span>col1:string ,col2:string,col3:string<span class="operator">&gt;</span></span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> t.col1,t.col3 <span class="keyword">from</span> str_table2;</span><br><span class="line">tom     tim</span><br></pre></td></tr></table></figure><h3 id="复杂类型长度统计函数"><a href="#复杂类型长度统计函数" class="headerlink" title="复杂类型长度统计函数"></a>复杂类型长度统计函数</h3><p>- </p><ul><li><ol><li>Map类型长度函数: size(Map&lt;k .V&gt;)</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: size(Map<span class="operator">&lt;</span>k .V<span class="operator">&gt;</span>)</span><br><span class="line">返回值: <span class="type">int</span></span><br><span class="line">说明: 返回map类型的长度</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> size(t) <span class="keyword">from</span> map_table2;</span><br><span class="line"><span class="number">2</span></span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>array类型长度函数: size(Array)</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">语法: size(<span class="keyword">Array</span><span class="operator">&lt;</span>T<span class="operator">&gt;</span>)</span><br><span class="line">返回值: <span class="type">int</span></span><br><span class="line">说明: 返回<span class="keyword">array</span>类型的长度</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> size(t) <span class="keyword">from</span> arr_table2;</span><br><span class="line"><span class="number">4</span></span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>类型转换函数  ***</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">类型转换函数: cast</span><br><span class="line">语法: <span class="built_in">cast</span>(expr <span class="keyword">as</span> <span class="operator">&lt;</span>type<span class="operator">&gt;</span>)</span><br><span class="line">返回值: Expected &quot;=&quot; <span class="keyword">to</span> follow &quot;type&quot;</span><br><span class="line">说明: 返回转换后的数据类型</span><br><span class="line">hive<span class="operator">&gt;</span> <span class="keyword">select</span> <span class="built_in">cast</span>(<span class="string">&#x27;1&#x27;</span> <span class="keyword">as</span> <span class="type">bigint</span>) <span class="keyword">from</span> tableName;</span><br><span class="line"><span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="hive当中的lateral-view-与-explode以及reflect和窗口函数"><a href="#hive当中的lateral-view-与-explode以及reflect和窗口函数" class="headerlink" title="hive当中的lateral view 与 explode以及reflect和窗口函数"></a>hive当中的lateral view 与 explode以及reflect和窗口函数</h2><h3 id="使用explode函数将hive表中的Map和Array字段数据进行拆分"><a href="#使用explode函数将hive表中的Map和Array字段数据进行拆分" class="headerlink" title="使用explode函数将hive表中的Map和Array字段数据进行拆分"></a>使用explode函数将hive表中的Map和Array字段数据进行拆分</h3><p>lateral view用于和split、explode等UDTF一起使用的，能将一行数据拆分成多行数据，在此基础上可以对拆分的数据进行聚合，lateral view首先为原始表的每行调用UDTF，UDTF会把一行拆分成一行或者多行，lateral view在把结果组合，产生一个支持别名表的虚拟表。</p><p>其中explode还可以用于将hive一列中复杂的array或者map结构拆分成多行</p><p>需求：现在有数据格式如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zhangsan child1,child2,child3,child4 k1:v1,k2:v2</span><br><span class="line"></span><br><span class="line">lisi child5,child6,child7,child8 k3:v3,k4:v4</span><br></pre></td></tr></table></figure><p>字段之间使用\t分割，需求将所有的child进行拆开成为一列</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">+----------+--+</span><br><span class="line">| mychild  |</span><br><span class="line">+----------+--+</span><br><span class="line">| child1   |</span><br><span class="line">| child2   |</span><br><span class="line">| child3   |</span><br><span class="line">| child4   |</span><br><span class="line">| child5   |</span><br><span class="line">| child6   |</span><br><span class="line">| child7   |</span><br><span class="line">| child8   |</span><br><span class="line">+----------+--+</span><br></pre></td></tr></table></figure><p>将map的key和value也进行拆开，成为如下结果</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">+-----------+-------------+--+</span><br><span class="line">| mymapkey  | mymapvalue  |</span><br><span class="line">+-----------+-------------+--+</span><br><span class="line">| k1        | v1          |</span><br><span class="line">| k2        | v2          |</span><br><span class="line">| k3        | v3          |</span><br><span class="line">| k4        | v4          |</span><br><span class="line">+-----------+-------------+--+</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>创建hive数据库</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">创建hive数据库</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> <span class="keyword">create</span> database hive_explode;</span><br><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> use hive_explode;</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>创建hive表，然后使用explode拆分map和array</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive_explode)<span class="operator">&gt;</span> <span class="keyword">create</span>  <span class="keyword">table</span> t3(name string,children <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span>,address Map<span class="operator">&lt;</span>string,string<span class="operator">&gt;</span>) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span>  collection items terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span> map keys terminated <span class="keyword">by</span> <span class="string">&#x27;:&#x27;</span> stored <span class="keyword">as</span> textFile;</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>加载数据</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">node03执行以下命令创建表数据文件</span><br><span class="line"> mkdir <span class="operator">-</span>p <span class="operator">/</span>export<span class="operator">/</span>servers<span class="operator">/</span>hivedatas<span class="operator">/</span></span><br><span class="line"> cd <span class="operator">/</span>export<span class="operator">/</span>servers<span class="operator">/</span>hivedatas<span class="operator">/</span></span><br><span class="line"> vim maparray</span><br><span class="line">内容如下:</span><br><span class="line">zhangsan child1,child2,child3,child4 k1:v1,k2:v2</span><br><span class="line">lisi child5,child6,child7,child8 k3:v3,k4:v4</span><br><span class="line"></span><br><span class="line">hive表当中加载数据</span><br><span class="line">hive (hive_explode)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/export/servers/hivedatas/maparray&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> t3;</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>使用explode将hive当中数据拆开</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">将<span class="keyword">array</span>当中的数据拆分开</span><br><span class="line">hive (hive_explode)<span class="operator">&gt;</span> <span class="keyword">SELECT</span> explode(children) <span class="keyword">AS</span> myChild <span class="keyword">FROM</span> t3;</span><br><span class="line"></span><br><span class="line">将map当中的数据拆分开</span><br><span class="line"></span><br><span class="line">hive (hive_explode)<span class="operator">&gt;</span> <span class="keyword">SELECT</span> explode(address) <span class="keyword">AS</span> (myMapKey, myMapValue) <span class="keyword">FROM</span> t3;</span><br></pre></td></tr></table></figure><h3 id="使用explode拆分json字符串"><a href="#使用explode拆分json字符串" class="headerlink" title="使用explode拆分json字符串"></a>使用explode拆分json字符串</h3><p>需求: 需求：现在有一些数据格式如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">a:shandong,b:beijing,c:hebei<span class="operator">|</span><span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span><span class="operator">|</span>[&#123;&quot;source&quot;:&quot;7fresh&quot;,&quot;monthSales&quot;:<span class="number">4900</span>,&quot;userCount&quot;:<span class="number">1900</span>,&quot;score&quot;:&quot;9.9&quot;&#125;,&#123;&quot;source&quot;:&quot;jd&quot;,&quot;monthSales&quot;:<span class="number">2090</span>,&quot;userCount&quot;:<span class="number">78981</span>,&quot;score&quot;:&quot;9.8&quot;&#125;,&#123;&quot;source&quot;:&quot;jdmart&quot;,&quot;monthSales&quot;:<span class="number">6987</span>,&quot;userCount&quot;:<span class="number">1600</span>,&quot;score&quot;:&quot;9.0&quot;&#125;]</span><br></pre></td></tr></table></figure><p>其中字段与字段之间的分隔符是 |</p><p>我们要解析得到所有的monthSales对应的值为以下这一列（行转列）</p><p>4900</p><p>2090</p><p>6987</p><p>- </p><ul><li><ol><li>创建hive表</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hive (hive_explode)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> explode_lateral_view</span><br><span class="line">                   <span class="operator">&gt;</span> (`area` string,</span><br><span class="line">                   <span class="operator">&gt;</span> `goods_id` string,</span><br><span class="line">                   <span class="operator">&gt;</span> `sale_info` string)</span><br><span class="line">                   <span class="operator">&gt;</span> <span class="type">ROW</span> FORMAT DELIMITED</span><br><span class="line">                   <span class="operator">&gt;</span> FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;|&#x27;</span></span><br><span class="line">                   <span class="operator">&gt;</span> STORED <span class="keyword">AS</span> textfile;</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>准备数据并加载数据</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">准备数据如下</span><br><span class="line">cd <span class="operator">/</span>export<span class="operator">/</span>servers<span class="operator">/</span>hivedatas</span><br><span class="line">vim explode_json</span><br><span class="line"></span><br><span class="line">a:shandong,b:beijing,c:hebei<span class="operator">|</span><span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">9</span><span class="operator">|</span>[&#123;&quot;source&quot;:&quot;7fresh&quot;,&quot;monthSales&quot;:<span class="number">4900</span>,&quot;userCount&quot;:<span class="number">1900</span>,&quot;score&quot;:&quot;9.9&quot;&#125;,&#123;&quot;source&quot;:&quot;jd&quot;,&quot;monthSales&quot;:<span class="number">2090</span>,&quot;userCount&quot;:<span class="number">78981</span>,&quot;score&quot;:&quot;9.8&quot;&#125;,&#123;&quot;source&quot;:&quot;jdmart&quot;,&quot;monthSales&quot;:<span class="number">6987</span>,&quot;userCount&quot;:<span class="number">1600</span>,&quot;score&quot;:&quot;9.0&quot;&#125;]</span><br><span class="line"></span><br><span class="line">加载数据到hive表当中去</span><br><span class="line">hive (hive_explode)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/export/servers/hivedatas/explode_json&#x27;</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> explode_lateral_view;</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>使用explode拆分Array</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive_explode)<span class="operator">&gt;</span> <span class="keyword">select</span> explode(split(goods_id,<span class="string">&#x27;,&#x27;</span>)) <span class="keyword">as</span> goods_id <span class="keyword">from</span> explode_lateral_view;</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>使用explode拆解Map</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive_explode)<span class="operator">&gt;</span> <span class="keyword">select</span> explode(split(area,<span class="string">&#x27;,&#x27;</span>)) <span class="keyword">as</span> area <span class="keyword">from</span> explode_lateral_view;</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>拆解json字段</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hive (hive_explode)<span class="operator">&gt;</span> <span class="keyword">select</span> explode(split(regexp_replace(regexp_replace(sale_info,<span class="string">&#x27;\\[\\&#123;&#x27;</span>,<span class="string">&#x27;&#x27;</span>),<span class="string">&#x27;&#125;]&#x27;</span>,<span class="string">&#x27;&#x27;</span>),<span class="string">&#x27;&#125;,\\&#123;&#x27;</span>)) <span class="keyword">as</span>  sale_info <span class="keyword">from</span> explode_lateral_view;</span><br><span class="line"></span><br><span class="line">然后我们想用get_json_object来获取key为monthSales的数据：</span><br><span class="line"></span><br><span class="line">hive (hive_explode)<span class="operator">&gt;</span> <span class="keyword">select</span> get_json_object(explode(split(regexp_replace(regexp_replace(sale_info,<span class="string">&#x27;\\[\\&#123;&#x27;</span>,<span class="string">&#x27;&#x27;</span>),<span class="string">&#x27;&#125;]&#x27;</span>,<span class="string">&#x27;&#x27;</span>),<span class="string">&#x27;&#125;,\\&#123;&#x27;</span>)),<span class="string">&#x27;$.monthSales&#x27;</span>) <span class="keyword">as</span>  sale_info <span class="keyword">from</span> explode_lateral_view;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">然后挂了FAILED: SemanticException [Error <span class="number">10081</span>]: UDTF<span class="string">&#x27;s are not supported outside the SELECT clause, nor nested in expressions</span></span><br><span class="line"><span class="string">UDTF explode不能写在别的函数内</span></span><br><span class="line"><span class="string">如果你这么写，想查两个字段，select explode(split(area,&#x27;</span>,<span class="string">&#x27;)) as area,good_id from explode_lateral_view;</span></span><br><span class="line"><span class="string">会报错FAILED: SemanticException 1:40 Only a single expression in the SELECT clause is supported with UDTF&#x27;</span>s. Error encountered near token <span class="string">&#x27;good_id&#x27;</span></span><br><span class="line">使用UDTF的时候，只支持一个字段，这时候就需要<span class="keyword">LATERAL</span> <span class="keyword">VIEW</span>出场了</span><br></pre></td></tr></table></figure><h3 id="配合LATERAL-VIEW使用"><a href="#配合LATERAL-VIEW使用" class="headerlink" title="配合LATERAL  VIEW使用"></a>配合LATERAL  VIEW使用</h3><p>配合lateral view查询多个字段</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive (hive_explode)<span class="operator">&gt;</span> <span class="keyword">select</span> goods_id2,sale_info <span class="keyword">from</span> explode_lateral_view <span class="keyword">LATERAL</span> <span class="keyword">VIEW</span> explode(split(goods_id,<span class="string">&#x27;,&#x27;</span>))goods <span class="keyword">as</span> goods_id2;</span><br><span class="line"></span><br><span class="line">其中<span class="keyword">LATERAL</span> <span class="keyword">VIEW</span> explode(split(goods_id,<span class="string">&#x27;,&#x27;</span>))goods相当于一个虚拟表，与原表explode_lateral_view笛卡尔积关联</span><br></pre></td></tr></table></figure><p>也可以多重使用</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hive (hive_explode)<span class="operator">&gt;</span> <span class="keyword">select</span> goods_id2,sale_info,area2</span><br><span class="line">                    <span class="keyword">from</span> explode_lateral_view </span><br><span class="line">                    <span class="keyword">LATERAL</span> <span class="keyword">VIEW</span> explode(split(goods_id,<span class="string">&#x27;,&#x27;</span>))goods <span class="keyword">as</span> goods_id2 </span><br><span class="line">                    <span class="keyword">LATERAL</span> <span class="keyword">VIEW</span> explode(split(area,<span class="string">&#x27;,&#x27;</span>))area <span class="keyword">as</span> area2;也是三个表笛卡尔积的结果</span><br></pre></td></tr></table></figure><p>最终，我们可以通过下面的句子，把这个json格式的一行数据，完全转换成二维表的方式展现</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive_explode)<span class="operator">&gt;</span> <span class="keyword">select</span> get_json_object(concat(<span class="string">&#x27;&#123;&#x27;</span>,sale_info_1,<span class="string">&#x27;&#125;&#x27;</span>),<span class="string">&#x27;$.source&#x27;</span>) <span class="keyword">as</span> source,get_json_object(concat(<span class="string">&#x27;&#123;&#x27;</span>,sale_info_1,<span class="string">&#x27;&#125;&#x27;</span>),<span class="string">&#x27;$.monthSales&#x27;</span>) <span class="keyword">as</span> monthSales,get_json_object(concat(<span class="string">&#x27;&#123;&#x27;</span>,sale_info_1,<span class="string">&#x27;&#125;&#x27;</span>),<span class="string">&#x27;$.userCount&#x27;</span>) <span class="keyword">as</span> monthSales,get_json_object(concat(<span class="string">&#x27;&#123;&#x27;</span>,sale_info_1,<span class="string">&#x27;&#125;&#x27;</span>),<span class="string">&#x27;$.score&#x27;</span>) <span class="keyword">as</span> monthSales <span class="keyword">from</span> explode_lateral_view <span class="keyword">LATERAL</span> <span class="keyword">VIEW</span> explode(split(regexp_replace(regexp_replace(sale_info,<span class="string">&#x27;\\[\\&#123;&#x27;</span>,<span class="string">&#x27;&#x27;</span>),<span class="string">&#x27;&#125;]&#x27;</span>,<span class="string">&#x27;&#x27;</span>),<span class="string">&#x27;&#125;,\\&#123;&#x27;</span>))sale_info <span class="keyword">as</span> sale_info_1;</span><br></pre></td></tr></table></figure><p>总结：</p><p>Lateral View通常和UDTF一起出现，为了解决UDTF不允许在select字段的问题。Multiple Lateral View可以实现类似笛卡尔乘积。Outer关键字可以把不输出的UDTF的空结果，输出成NULL，防止丢失数据。</p><h3 id="行转列"><a href="#行转列" class="headerlink" title="行转列"></a>行转列</h3><p>相关参数说明:</p><p>CONCAT(string A&#x2F;col, string B&#x2F;col…)：返回输入字符串连接后的结果，支持任意个输入字符串;</p><p>CONCAT_WS(separator, str1, str2,…)：它是一个特殊形式的 CONCAT()。第一个参数剩余参数间的分隔符。分隔符可以是与剩余参数一样的字符串。如果分隔符是 NULL，返回值也将为 NULL。这个函数会跳过分隔符参数后的任何 NULL 和空字符串。分隔符将被加到被连接的字符串之间;</p><p>COLLECT_SET(col)：函数只接受基本数据类型，它的主要作用是将某字段的值进行去重汇总，产生array类型字段。</p><p>数据准备:</p><table><thead><tr><th align="left">name</th><th align="left">constellation</th><th align="left">blood_type</th></tr></thead><tbody><tr><td align="left">孙悟空</td><td align="left">白羊座</td><td align="left">A</td></tr><tr><td align="left">老王</td><td align="left">射手座</td><td align="left">A</td></tr><tr><td align="left">宋宋</td><td align="left">白羊座</td><td align="left">B</td></tr><tr><td align="left">猪八戒</td><td align="left">白羊座</td><td align="left">A</td></tr><tr><td align="left">凤姐</td><td align="left">射手座</td><td align="left">A</td></tr></tbody></table><p>需求: 把星座和血型一样的人归类到一起。结果如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">射手座,A            老王<span class="operator">|</span>凤姐</span><br><span class="line">白羊座,A            孙悟空<span class="operator">|</span>猪八戒</span><br><span class="line">白羊座,B            宋宋</span><br></pre></td></tr></table></figure><p>实现步骤:</p><p>- </p><ul><li><ol><li>创建本地constellation.txt，导入数据</li></ol></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">node03服务器执行以下命令创建文件，注意数据使用\t进行分割</span><br><span class="line"><span class="built_in">cd</span> /export/servers/hivedatas</span><br><span class="line">vim constellation.txt</span><br><span class="line"></span><br><span class="line">数据如下: </span><br><span class="line">孙悟空 白羊座 A</span><br><span class="line">老王 射手座 A</span><br><span class="line">宋宋 白羊座 B       </span><br><span class="line">猪八戒 白羊座 A</span><br><span class="line">凤姐 射手座 A</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>创建hive表并导入数据</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">创建hive表并加载数据</span><br><span class="line">hive (hive_explode)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> person_info(</span><br><span class="line">                    name string, </span><br><span class="line">                    constellation string, </span><br><span class="line">                    blood_type string) </span><br><span class="line">                    <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> &quot;\t&quot;;</span><br><span class="line"></span><br><span class="line">加载数据</span><br><span class="line">hive (hive_explode)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/export/servers/hivedatas/constellation.txt&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> person_info;</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>按需求查询数据</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">hive (hive_explode)<span class="operator">&gt;</span> <span class="keyword">select</span></span><br><span class="line">                        t1.base,</span><br><span class="line">                        concat_ws(<span class="string">&#x27;|&#x27;</span>, collect_set(t1.name)) name</span><br><span class="line">                    <span class="keyword">from</span></span><br><span class="line">                        (<span class="keyword">select</span></span><br><span class="line">                            name,</span><br><span class="line">                            concat(constellation, &quot;,&quot; , blood_type) base</span><br><span class="line">                        <span class="keyword">from</span></span><br><span class="line">                            person_info) t1</span><br><span class="line">                    <span class="keyword">group</span> <span class="keyword">by</span></span><br><span class="line">                        t1.base;</span><br></pre></td></tr></table></figure><h3 id="列转行"><a href="#列转行" class="headerlink" title="列转行"></a>列转行</h3><p>所需函数:</p><p>EXPLODE(col)：将hive一列中复杂的array或者map结构拆分成多行。</p><p>LATERAL VIEW</p><p>用法：LATERAL VIEW udtf(expression) tableAlias AS columnAlias</p><p>解释：用于和split, explode等UDTF一起使用，它能够将一列数据拆成多行数据，在此基础上可以对拆分后的数据进行聚合。</p><p>数据准备:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/servers/hivedatas</span><br><span class="line">vim movie.txt</span><br><span class="line">文件内容如下:  数据字段之间使用\t进行分割</span><br><span class="line">《疑犯追踪》 悬疑,动作,科幻,剧情</span><br><span class="line">《Lie to me》 悬疑,警匪,动作,心理,剧情</span><br><span class="line">《战狼2》 战争,动作,灾难</span><br></pre></td></tr></table></figure><p>需求: 将电影分类中的数组数据展开。结果如下：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">《疑犯追踪》 悬疑</span><br><span class="line">《疑犯追踪》 动作</span><br><span class="line">《疑犯追踪》 科幻</span><br><span class="line">《疑犯追踪》 剧情</span><br><span class="line">《Lie to me》 悬疑</span><br><span class="line">《Lie to me》 警匪</span><br><span class="line">《Lie to me》 动作</span><br><span class="line">《Lie to me》 心理</span><br><span class="line">《Lie to me》 剧情</span><br><span class="line">《战狼2》 战争</span><br><span class="line">《战狼2》 动作</span><br><span class="line">《战狼2》 灾难</span><br></pre></td></tr></table></figure><p>实现步骤:</p><p>- </p><ul><li><ol><li>创建hive表</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> movie_info(</span><br><span class="line">    movie string, </span><br><span class="line">    category <span class="keyword">array</span><span class="operator">&lt;</span>string<span class="operator">&gt;</span>) </span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> &quot;\t&quot;</span><br><span class="line">collection items terminated <span class="keyword">by</span> &quot;,&quot;;</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>加载数据</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">load data <span class="keyword">local</span> inpath &quot;/export/servers/hivedatas/movie.txt&quot; <span class="keyword">into</span> <span class="keyword">table</span> movie_info;</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>按需求查询数据</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    movie,</span><br><span class="line">    category_name</span><br><span class="line"><span class="keyword">from</span> </span><br><span class="line">    movie_info <span class="keyword">lateral</span> <span class="keyword">view</span> explode(category) table_tmp <span class="keyword">as</span> category_name;</span><br></pre></td></tr></table></figure><h3 id="reflect函数"><a href="#reflect函数" class="headerlink" title="reflect函数"></a>reflect函数</h3><p>reflect函数可以支持在sql中调用java中的自带函数，秒杀一切udf函数。</p><p>需求1: 使用java.lang.Math当中的Max求两列中最大值</p><p>实现步骤:</p><p>- </p><ul><li><ol><li>创建hive表</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_udf(col1 <span class="type">int</span>,col2 <span class="type">int</span>) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>准备数据并加载数据</li></ol></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/servers/hivedatas</span><br><span class="line">vim test_udf </span><br><span class="line"></span><br><span class="line">文件内容如下:</span><br><span class="line">1,2</span><br><span class="line">4,3</span><br><span class="line">6,4</span><br><span class="line">7,5</span><br><span class="line">5,6</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>加载数据</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive_explode)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/export/servers/hivedatas/test_udf&#x27;</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> test_udf;</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>使用java.lang.Math当中的Max求两列当中的最大值</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive_explode)<span class="operator">&gt;</span> <span class="keyword">select</span> reflect(&quot;java.lang.Math&quot;,&quot;max&quot;,col1,col2) <span class="keyword">from</span> test_udf;</span><br></pre></td></tr></table></figure><p>需求2: 文件中不同的记录来执行不同的java的内置函数</p><p>实现步骤:</p><p>- </p><ul><li><ol><li>创建hive表</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive_explode)<span class="operator">&gt;</span> <span class="keyword">create</span> <span class="keyword">table</span> test_udf2(class_name string,method_name string,col1 <span class="type">int</span> , col2 <span class="type">int</span>) <span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>准备数据</li></ol></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /export/servers/hivedatas</span><br><span class="line">vim test_udf2</span><br><span class="line"></span><br><span class="line">文件内容如下:</span><br><span class="line">java.lang.Math,min,1,2</span><br><span class="line">java.lang.Math,max,2,3</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>加载数据</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive_explode)<span class="operator">&gt;</span> load data <span class="keyword">local</span> inpath <span class="string">&#x27;/export/servers/hivedatas/test_udf2&#x27;</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> test_udf2;</span><br></pre></td></tr></table></figure><p>- </p><ul><li><ol><li>执行查询</li></ol></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive (hive_explode)<span class="operator">&gt;</span> <span class="keyword">select</span> reflect(class_name,method_name,col1,col2) <span class="keyword">from</span> test_udf2;</span><br></pre></td></tr></table></figure><p>需求3: 判断是否为数字</p><p>实现方式:</p><p>使用apache commons中的函数，commons下的jar已经包含在hadoop的classpath中，所以可以直接使用。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> reflect(&quot;org.apache.commons.lang.math.NumberUtils&quot;,&quot;isNumber&quot;,&quot;123&quot;)</span><br></pre></td></tr></table></figure><h2 id="窗口函数与分析函数"><a href="#窗口函数与分析函数" class="headerlink" title="窗口函数与分析函数"></a>窗口函数与分析函数</h2><p>在sql中有一类函数叫做聚合函数,例如sum()、avg()、max()等等,这类函数可以将多行数据按照规则聚集为一行,一般来讲聚集后的行数是要少于聚集前的行数的。但是有时我们想要既显示聚集前的数据,又要显示聚集后的数据,这时我们便引入了窗口函数。窗口函数又叫OLAP函数&#x2F;分析函数，窗口函数兼具分组和排序功能。</p><p>窗口函数最重要的关键字是 <strong>partition by</strong> 和 <strong>order by。</strong></p><p>具体语法如下：<strong>over (partition by xxx order by xxx)</strong></p><h3 id="sum、avg、min、max"><a href="#sum、avg、min、max" class="headerlink" title="sum、avg、min、max"></a>sum、avg、min、max</h3><p>准备数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">建表语句:</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test_t1(</span><br><span class="line">cookieid string,</span><br><span class="line">createtime string,   <span class="comment">--day </span></span><br><span class="line">pv <span class="type">int</span></span><br><span class="line">) <span class="type">row</span> format delimited </span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;</span><br><span class="line"></span><br><span class="line">加载数据：</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/root/hivedata/test_t1.dat&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> test_t1;</span><br><span class="line"></span><br><span class="line">cookie1,<span class="number">2020</span><span class="number">-04</span><span class="number">-10</span>,<span class="number">1</span></span><br><span class="line">cookie1,<span class="number">2020</span><span class="number">-04</span><span class="number">-11</span>,<span class="number">5</span></span><br><span class="line">cookie1,<span class="number">2020</span><span class="number">-04</span><span class="number">-12</span>,<span class="number">7</span></span><br><span class="line">cookie1,<span class="number">2020</span><span class="number">-04</span><span class="number">-13</span>,<span class="number">3</span></span><br><span class="line">cookie1,<span class="number">2020</span><span class="number">-04</span><span class="number">-14</span>,<span class="number">2</span></span><br><span class="line">cookie1,<span class="number">2020</span><span class="number">-04</span><span class="number">-15</span>,<span class="number">4</span></span><br><span class="line">cookie1,<span class="number">2020</span><span class="number">-04</span><span class="number">-16</span>,<span class="number">4</span></span><br><span class="line"></span><br><span class="line">开启智能本地模式</span><br><span class="line"><span class="keyword">SET</span> hive.exec.mode.local.auto<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure><p>SUM函数和窗口函数的配合使用：结果和ORDER BY相关,默认为升序。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> cookieid,createtime,pv,</span><br><span class="line"><span class="built_in">sum</span>(pv) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime) <span class="keyword">as</span> pv1 </span><br><span class="line"><span class="keyword">from</span> test_t1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> cookieid,createtime,pv,</span><br><span class="line"><span class="built_in">sum</span>(pv) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">rows</span> <span class="keyword">between</span> unbounded preceding <span class="keyword">and</span> <span class="keyword">current</span> <span class="type">row</span>) <span class="keyword">as</span> pv2</span><br><span class="line"><span class="keyword">from</span> test_t1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> cookieid,createtime,pv,</span><br><span class="line"><span class="built_in">sum</span>(pv) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> cookieid) <span class="keyword">as</span> pv3</span><br><span class="line"><span class="keyword">from</span> test_t1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> cookieid,createtime,pv,</span><br><span class="line"><span class="built_in">sum</span>(pv) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">3</span> preceding <span class="keyword">and</span> <span class="keyword">current</span> <span class="type">row</span>) <span class="keyword">as</span> pv4</span><br><span class="line"><span class="keyword">from</span> test_t1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> cookieid,createtime,pv,</span><br><span class="line"><span class="built_in">sum</span>(pv) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">3</span> preceding <span class="keyword">and</span> <span class="number">1</span> following) <span class="keyword">as</span> pv5</span><br><span class="line"><span class="keyword">from</span> test_t1;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> cookieid,createtime,pv,</span><br><span class="line"><span class="built_in">sum</span>(pv) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> cookieid <span class="keyword">order</span> <span class="keyword">by</span> createtime <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="type">row</span> <span class="keyword">and</span> unbounded following) <span class="keyword">as</span> pv6</span><br><span class="line"><span class="keyword">from</span> test_t1;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">pv1: 分组内从起点到当前行的pv累积，如，<span class="number">11</span>号的pv1<span class="operator">=</span><span class="number">10</span>号的pv<span class="operator">+</span><span class="number">11</span>号的pv, <span class="number">12</span>号<span class="operator">=</span><span class="number">10</span>号<span class="operator">+</span><span class="number">11</span>号<span class="operator">+</span><span class="number">12</span>号</span><br><span class="line">pv2: 同pv1</span><br><span class="line">pv3: 分组内(cookie1)所有的pv累加</span><br><span class="line">pv4: 分组内当前行<span class="operator">+</span>往前<span class="number">3</span>行，如，<span class="number">11</span>号<span class="operator">=</span><span class="number">10</span>号<span class="operator">+</span><span class="number">11</span>号， <span class="number">12</span>号<span class="operator">=</span><span class="number">10</span>号<span class="operator">+</span><span class="number">11</span>号<span class="operator">+</span><span class="number">12</span>号，</span><br><span class="line">                        <span class="number">13</span>号<span class="operator">=</span><span class="number">10</span>号<span class="operator">+</span><span class="number">11</span>号<span class="operator">+</span><span class="number">12</span>号<span class="operator">+</span><span class="number">13</span>号， <span class="number">14</span>号<span class="operator">=</span><span class="number">11</span>号<span class="operator">+</span><span class="number">12</span>号<span class="operator">+</span><span class="number">13</span>号<span class="operator">+</span><span class="number">14</span>号</span><br><span class="line">pv5: 分组内当前行<span class="operator">+</span>往前<span class="number">3</span>行<span class="operator">+</span>往后<span class="number">1</span>行，如，<span class="number">14</span>号<span class="operator">=</span><span class="number">11</span>号<span class="operator">+</span><span class="number">12</span>号<span class="operator">+</span><span class="number">13</span>号<span class="operator">+</span><span class="number">14</span>号<span class="operator">+</span><span class="number">15</span>号<span class="operator">=</span><span class="number">5</span><span class="operator">+</span><span class="number">7</span><span class="operator">+</span><span class="number">3</span><span class="operator">+</span><span class="number">2</span><span class="operator">+</span><span class="number">4</span><span class="operator">=</span><span class="number">21</span></span><br><span class="line">pv6: 分组内当前行<span class="operator">+</span>往后所有行，如，<span class="number">13</span>号<span class="operator">=</span><span class="number">13</span>号<span class="operator">+</span><span class="number">14</span>号<span class="operator">+</span><span class="number">15</span>号<span class="operator">+</span><span class="number">16</span>号<span class="operator">=</span><span class="number">3</span><span class="operator">+</span><span class="number">2</span><span class="operator">+</span><span class="number">4</span><span class="operator">+</span><span class="number">4</span><span class="operator">=</span><span class="number">13</span>，</span><br><span class="line">        <span class="number">14</span>号<span class="operator">=</span><span class="number">14</span>号<span class="operator">+</span><span class="number">15</span>号<span class="operator">+</span><span class="number">16</span>号<span class="operator">=</span><span class="number">2</span><span class="operator">+</span><span class="number">4</span><span class="operator">+</span><span class="number">4</span><span class="operator">=</span><span class="number">10</span></span><br></pre></td></tr></table></figure><p>如果不指定rows between,默认为从起点到当前行;</p><p>如果不指定order by，则将分组内所有值累加;</p><p>关键是理解rows between含义,也叫做window子句：</p><p>preceding：往前</p><p>following：往后</p><p>current row：当前行</p><p>unbounded：起点</p><p>unbounded preceding 表示从前面的起点</p><p>unbounded following：表示到后面的终点</p><p>AVG，MIN，MAX，和SUM用法一样。</p><h3 id="row-number、rank、dense-rank、ntile"><a href="#row-number、rank、dense-rank、ntile" class="headerlink" title="row_number、rank、dense_rank、ntile"></a>row_number、rank、dense_rank、ntile</h3><p>准备数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">cookie1,<span class="number">2020</span><span class="number">-04</span><span class="number">-10</span>,<span class="number">1</span></span><br><span class="line">cookie1,<span class="number">2020</span><span class="number">-04</span><span class="number">-11</span>,<span class="number">5</span></span><br><span class="line">cookie1,<span class="number">2020</span><span class="number">-04</span><span class="number">-12</span>,<span class="number">7</span></span><br><span class="line">cookie1,<span class="number">2020</span><span class="number">-04</span><span class="number">-13</span>,<span class="number">3</span></span><br><span class="line">cookie1,<span class="number">2020</span><span class="number">-04</span><span class="number">-14</span>,<span class="number">2</span></span><br><span class="line">cookie1,<span class="number">2020</span><span class="number">-04</span><span class="number">-15</span>,<span class="number">4</span></span><br><span class="line">cookie1,<span class="number">2020</span><span class="number">-04</span><span class="number">-16</span>,<span class="number">4</span></span><br><span class="line">cookie2,<span class="number">2020</span><span class="number">-04</span><span class="number">-10</span>,<span class="number">2</span></span><br><span class="line">cookie2,<span class="number">2020</span><span class="number">-04</span><span class="number">-11</span>,<span class="number">3</span></span><br><span class="line">cookie2,<span class="number">2020</span><span class="number">-04</span><span class="number">-12</span>,<span class="number">5</span></span><br><span class="line">cookie2,<span class="number">2020</span><span class="number">-04</span><span class="number">-13</span>,<span class="number">6</span></span><br><span class="line">cookie2,<span class="number">2020</span><span class="number">-04</span><span class="number">-14</span>,<span class="number">3</span></span><br><span class="line">cookie2,<span class="number">2020</span><span class="number">-04</span><span class="number">-15</span>,<span class="number">9</span></span><br><span class="line">cookie2,<span class="number">2020</span><span class="number">-04</span><span class="number">-16</span>,<span class="number">7</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> test_t2 (</span><br><span class="line">cookieid string,</span><br><span class="line">createtime string,   <span class="comment">--day </span></span><br><span class="line">pv <span class="type">INT</span></span><br><span class="line">) <span class="type">ROW</span> FORMAT DELIMITED </span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span> </span><br><span class="line">stored <span class="keyword">as</span> textfile;</span><br><span class="line"></span><br><span class="line">加载数据：</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/root/hivedata/test_t2.dat&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> test_t2;</span><br></pre></td></tr></table></figure><ul><li><p>ROW_NUMBER()使用</p><p>ROW_NUMBER()从1开始，按照顺序，生成分组内记录的序列。</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">cookieid,</span><br><span class="line">createtime,</span><br><span class="line">pv,</span><br><span class="line"><span class="built_in">ROW_NUMBER</span>() <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> cookieid <span class="keyword">ORDER</span> <span class="keyword">BY</span> pv <span class="keyword">desc</span>) <span class="keyword">AS</span> rn </span><br><span class="line"><span class="keyword">FROM</span> test_t2;</span><br></pre></td></tr></table></figure><ul><li><p>RANK 和 DENSE_RANK使用</p><p>RANK() 生成数据项在分组中的排名，排名相等会在名次中留下空位 。</p><p>DENSE_RANK()生成数据项在分组中的排名，排名相等会在名次中不会留下空位。</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">cookieid,</span><br><span class="line">createtime,</span><br><span class="line">pv,</span><br><span class="line"><span class="built_in">RANK</span>() <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> cookieid <span class="keyword">ORDER</span> <span class="keyword">BY</span> pv <span class="keyword">desc</span>) <span class="keyword">AS</span> rn1,</span><br><span class="line"><span class="built_in">DENSE_RANK</span>() <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> cookieid <span class="keyword">ORDER</span> <span class="keyword">BY</span> pv <span class="keyword">desc</span>) <span class="keyword">AS</span> rn2,</span><br><span class="line"><span class="built_in">ROW_NUMBER</span>() <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> cookieid <span class="keyword">ORDER</span> <span class="keyword">BY</span> pv <span class="keyword">DESC</span>) <span class="keyword">AS</span> rn3 </span><br><span class="line"><span class="keyword">FROM</span> test_t2 </span><br><span class="line"><span class="keyword">WHERE</span> cookieid <span class="operator">=</span> <span class="string">&#x27;cookie1&#x27;</span>;</span><br></pre></td></tr></table></figure><ul><li><p>NTILE</p><p>有时会有这样的需求:如果数据排序后分为三部分，业务人员只关心其中的一部分，如何将这中间的三分之一数据拿出来呢?NTILE函数即可以满足。</p><p>ntile可以看成是：把有序的数据集合平均分配到指定的数量（num）个桶中, 将桶号分配给每一行。如果不能平均分配，则优先分配较小编号的桶，并且各个桶中能放的行数最多相差1。</p><p>然后可以根据桶号，选取前或后 n分之几的数据。数据会完整展示出来，只是给相应的数据打标签；具体要取几分之几的数据，需要再嵌套一层根据标签取出。</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">cookieid,</span><br><span class="line">createtime,</span><br><span class="line">pv,</span><br><span class="line"><span class="built_in">NTILE</span>(<span class="number">2</span>) <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> cookieid <span class="keyword">ORDER</span> <span class="keyword">BY</span> createtime) <span class="keyword">AS</span> rn1,</span><br><span class="line"><span class="built_in">NTILE</span>(<span class="number">3</span>) <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> cookieid <span class="keyword">ORDER</span> <span class="keyword">BY</span> createtime) <span class="keyword">AS</span> rn2,</span><br><span class="line"><span class="built_in">NTILE</span>(<span class="number">4</span>) <span class="keyword">OVER</span>(<span class="keyword">ORDER</span> <span class="keyword">BY</span> createtime) <span class="keyword">AS</span> rn3</span><br><span class="line"><span class="keyword">FROM</span> test_t2 </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> cookieid,createtime;</span><br></pre></td></tr></table></figure><h2 id="其他一些窗口函数"><a href="#其他一些窗口函数" class="headerlink" title="其他一些窗口函数"></a>其他一些窗口函数</h2><h3 id="lag-lead-first-value-last-value"><a href="#lag-lead-first-value-last-value" class="headerlink" title="lag,lead,first_value,last_value"></a>lag,lead,first_value,last_value</h3><ul><li>LAG<br><strong>LAG(col,n,DEFAULT) 用于统计窗口内往上第n行值</strong>第一个参数为列名，第二个参数为往上第n行（可选，默认为1），第三个参数为默认值（当往上第n行为NULL时候，取默认值，如不指定，则为NULL）</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> cookieid,</span><br><span class="line">createtime,</span><br><span class="line">url,</span><br><span class="line"><span class="built_in">ROW_NUMBER</span>() <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> cookieid <span class="keyword">ORDER</span> <span class="keyword">BY</span> createtime) <span class="keyword">AS</span> rn,</span><br><span class="line"><span class="built_in">LAG</span>(createtime,<span class="number">1</span>,<span class="string">&#x27;1970-01-01 00:00:00&#x27;</span>) <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> cookieid <span class="keyword">ORDER</span> <span class="keyword">BY</span> createtime) <span class="keyword">AS</span> last_1_time,</span><br><span class="line"><span class="built_in">LAG</span>(createtime,<span class="number">2</span>) <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> cookieid <span class="keyword">ORDER</span> <span class="keyword">BY</span> createtime) <span class="keyword">AS</span> last_2_time </span><br><span class="line"><span class="keyword">FROM</span> test_t4;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">last_1_time: 指定了往上第<span class="number">1</span>行的值，<span class="keyword">default</span>为<span class="string">&#x27;1970-01-01 00:00:00&#x27;</span>  </span><br><span class="line">                 cookie1第一行，往上<span class="number">1</span>行为<span class="keyword">NULL</span>,因此取默认值 <span class="number">1970</span><span class="number">-01</span><span class="number">-01</span> <span class="number">00</span>:<span class="number">00</span>:<span class="number">00</span></span><br><span class="line">                 cookie1第三行，往上<span class="number">1</span>行值为第二行值，<span class="number">2015</span><span class="number">-04</span><span class="number">-10</span> <span class="number">10</span>:<span class="number">00</span>:<span class="number">02</span></span><br><span class="line">                 cookie1第六行，往上<span class="number">1</span>行值为第五行值，<span class="number">2015</span><span class="number">-04</span><span class="number">-10</span> <span class="number">10</span>:<span class="number">50</span>:<span class="number">01</span></span><br><span class="line">last_2_time: 指定了往上第<span class="number">2</span>行的值，为指定默认值</span><br><span class="line">         cookie1第一行，往上<span class="number">2</span>行为<span class="keyword">NULL</span></span><br><span class="line">         cookie1第二行，往上<span class="number">2</span>行为<span class="keyword">NULL</span></span><br><span class="line">         cookie1第四行，往上<span class="number">2</span>行为第二行值，<span class="number">2015</span><span class="number">-04</span><span class="number">-10</span> <span class="number">10</span>:<span class="number">00</span>:<span class="number">02</span></span><br><span class="line">         cookie1第七行，往上<span class="number">2</span>行为第五行值，<span class="number">2015</span><span class="number">-04</span><span class="number">-10</span> <span class="number">10</span>:<span class="number">50</span>:<span class="number">01</span></span><br></pre></td></tr></table></figure><ul><li>LEAD</li></ul><p>与LAG相反<strong>LEAD(col,n,DEFAULT) 用于统计窗口内往下第n行值</strong>第一个参数为列名，第二个参数为往下第n行（可选，默认为1），第三个参数为默认值（当往下第n行为NULL时候，取默认值，如不指定，则为NULL）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> cookieid,</span><br><span class="line">createtime,</span><br><span class="line">url,</span><br><span class="line"><span class="built_in">ROW_NUMBER</span>() <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> cookieid <span class="keyword">ORDER</span> <span class="keyword">BY</span> createtime) <span class="keyword">AS</span> rn,</span><br><span class="line"><span class="built_in">LEAD</span>(createtime,<span class="number">1</span>,<span class="string">&#x27;1970-01-01 00:00:00&#x27;</span>) <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> cookieid <span class="keyword">ORDER</span> <span class="keyword">BY</span> createtime) <span class="keyword">AS</span> next_1_time,</span><br><span class="line"><span class="built_in">LEAD</span>(createtime,<span class="number">2</span>) <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> cookieid <span class="keyword">ORDER</span> <span class="keyword">BY</span> createtime) <span class="keyword">AS</span> next_2_time </span><br><span class="line"><span class="keyword">FROM</span> test_t4;</span><br></pre></td></tr></table></figure><ul><li><p>FIRST_VALUE</p><p>取分组内排序后，截止到当前行，第一个值</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> cookieid,</span><br><span class="line">createtime,</span><br><span class="line">url,</span><br><span class="line"><span class="built_in">ROW_NUMBER</span>() <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> cookieid <span class="keyword">ORDER</span> <span class="keyword">BY</span> createtime) <span class="keyword">AS</span> rn,</span><br><span class="line"><span class="built_in">FIRST_VALUE</span>(url) <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> cookieid <span class="keyword">ORDER</span> <span class="keyword">BY</span> createtime) <span class="keyword">AS</span> first1 </span><br><span class="line"><span class="keyword">FROM</span> test_t4;</span><br></pre></td></tr></table></figure><ul><li>LAST_VALUE</li></ul><p>取分组内排序后，截止到当前行，最后一个值</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> cookieid,</span><br><span class="line">createtime,</span><br><span class="line">url,</span><br><span class="line"><span class="built_in">ROW_NUMBER</span>() <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> cookieid <span class="keyword">ORDER</span> <span class="keyword">BY</span> createtime) <span class="keyword">AS</span> rn,</span><br><span class="line"><span class="built_in">LAST_VALUE</span>(url) <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> cookieid <span class="keyword">ORDER</span> <span class="keyword">BY</span> createtime) <span class="keyword">AS</span> last1 </span><br><span class="line"><span class="keyword">FROM</span> test_t4;</span><br></pre></td></tr></table></figure><p>如果想要取分组内排序后最后一个值，则需要变通一下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> cookieid,</span><br><span class="line">createtime,</span><br><span class="line">url,</span><br><span class="line"><span class="built_in">ROW_NUMBER</span>() <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> cookieid <span class="keyword">ORDER</span> <span class="keyword">BY</span> createtime) <span class="keyword">AS</span> rn,</span><br><span class="line"><span class="built_in">LAST_VALUE</span>(url) <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> cookieid <span class="keyword">ORDER</span> <span class="keyword">BY</span> createtime) <span class="keyword">AS</span> last1,</span><br><span class="line"><span class="built_in">FIRST_VALUE</span>(url) <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> cookieid <span class="keyword">ORDER</span> <span class="keyword">BY</span> createtime <span class="keyword">DESC</span>) <span class="keyword">AS</span> last2 </span><br><span class="line"><span class="keyword">FROM</span> test_t4 </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> cookieid,createtime;</span><br></pre></td></tr></table></figure><p><strong>特别注意order  by</strong></p><p>如果不指定ORDER BY，则进行排序混乱，会出现错误的结果</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> cookieid,</span><br><span class="line">createtime,</span><br><span class="line">url,</span><br><span class="line"><span class="built_in">FIRST_VALUE</span>(url) <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> cookieid) <span class="keyword">AS</span> first2  </span><br><span class="line"><span class="keyword">FROM</span> test_t4;</span><br></pre></td></tr></table></figure><h3 id="cume-dist-percent-rank"><a href="#cume-dist-percent-rank" class="headerlink" title="cume_dist,percent_rank"></a>cume_dist,percent_rank</h3><p>这两个序列分析函数不是很常用，<strong>注意：序列函数不支持WINDOW子句</strong></p><ul><li>数据准备</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">d1,user1,<span class="number">1000</span></span><br><span class="line">d1,user2,<span class="number">2000</span></span><br><span class="line">d1,user3,<span class="number">3000</span></span><br><span class="line">d2,user4,<span class="number">4000</span></span><br><span class="line">d2,user5,<span class="number">5000</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> test_t3 (</span><br><span class="line">dept STRING,</span><br><span class="line">userid string,</span><br><span class="line">sal <span class="type">INT</span></span><br><span class="line">) <span class="type">ROW</span> FORMAT DELIMITED </span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span> </span><br><span class="line">stored <span class="keyword">as</span> textfile;</span><br><span class="line"></span><br><span class="line">加载数据：</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/root/hivedata/test_t3.dat&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> test_t3;</span><br></pre></td></tr></table></figure><hr><ul><li><p>CUME_DIST  和order byd的排序顺序有关系</p><p>CUME_DIST 小于等于当前值的行数&#x2F;分组内总行数  order 默认顺序 正序 升序 比如，统计小于等于当前薪水的人数，所占总人数的比例</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">dept,</span><br><span class="line">userid,</span><br><span class="line">sal,</span><br><span class="line"><span class="built_in">CUME_DIST</span>() <span class="keyword">OVER</span>(<span class="keyword">ORDER</span> <span class="keyword">BY</span> sal) <span class="keyword">AS</span> rn1,</span><br><span class="line"><span class="built_in">CUME_DIST</span>() <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> dept <span class="keyword">ORDER</span> <span class="keyword">BY</span> sal) <span class="keyword">AS</span> rn2 </span><br><span class="line"><span class="keyword">FROM</span> test_t3;</span><br><span class="line"></span><br><span class="line">rn1: 没有<span class="keyword">partition</span>,所有数据均为<span class="number">1</span>组，总行数为<span class="number">5</span>，</span><br><span class="line">     第一行：小于等于<span class="number">1000</span>的行数为<span class="number">1</span>，因此，<span class="number">1</span><span class="operator">/</span><span class="number">5</span><span class="operator">=</span><span class="number">0.2</span></span><br><span class="line">     第三行：小于等于<span class="number">3000</span>的行数为<span class="number">3</span>，因此，<span class="number">3</span><span class="operator">/</span><span class="number">5</span><span class="operator">=</span><span class="number">0.6</span></span><br><span class="line">rn2: 按照部门分组，dpet<span class="operator">=</span>d1的行数为<span class="number">3</span>,</span><br><span class="line">     第二行：小于等于<span class="number">2000</span>的行数为<span class="number">2</span>，因此，<span class="number">2</span><span class="operator">/</span><span class="number">3</span><span class="operator">=</span><span class="number">0.6666666666666666</span></span><br></pre></td></tr></table></figure><ul><li><p>PERCENT_RANK</p><p>PERCENT_RANK 分组内当前行的RANK值-1&#x2F;分组内总行数-1</p><p>经调研 该函数显示现实意义不明朗 有待于继续考证</p></li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line">dept,</span><br><span class="line">userid,</span><br><span class="line">sal,</span><br><span class="line"><span class="built_in">PERCENT_RANK</span>() <span class="keyword">OVER</span>(<span class="keyword">ORDER</span> <span class="keyword">BY</span> sal) <span class="keyword">AS</span> rn1,   <span class="comment">--分组内</span></span><br><span class="line"><span class="built_in">RANK</span>() <span class="keyword">OVER</span>(<span class="keyword">ORDER</span> <span class="keyword">BY</span> sal) <span class="keyword">AS</span> rn11,          <span class="comment">--分组内RANK值</span></span><br><span class="line"><span class="built_in">SUM</span>(<span class="number">1</span>) <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> <span class="keyword">NULL</span>) <span class="keyword">AS</span> rn12,     <span class="comment">--分组内总行数</span></span><br><span class="line"><span class="built_in">PERCENT_RANK</span>() <span class="keyword">OVER</span>(<span class="keyword">PARTITION</span> <span class="keyword">BY</span> dept <span class="keyword">ORDER</span> <span class="keyword">BY</span> sal) <span class="keyword">AS</span> rn2 </span><br><span class="line"><span class="keyword">FROM</span> test_t3;</span><br><span class="line"></span><br><span class="line">rn1: rn1 <span class="operator">=</span> (rn11<span class="number">-1</span>) <span class="operator">/</span> (rn12<span class="number">-1</span>) </span><br><span class="line">    第一行,(<span class="number">1</span><span class="number">-1</span>)<span class="operator">/</span>(<span class="number">5</span><span class="number">-1</span>)<span class="operator">=</span><span class="number">0</span><span class="operator">/</span><span class="number">4</span><span class="operator">=</span><span class="number">0</span></span><br><span class="line">    第二行,(<span class="number">2</span><span class="number">-1</span>)<span class="operator">/</span>(<span class="number">5</span><span class="number">-1</span>)<span class="operator">=</span><span class="number">1</span><span class="operator">/</span><span class="number">4</span><span class="operator">=</span><span class="number">0.25</span></span><br><span class="line">    第四行,(<span class="number">4</span><span class="number">-1</span>)<span class="operator">/</span>(<span class="number">5</span><span class="number">-1</span>)<span class="operator">=</span><span class="number">3</span><span class="operator">/</span><span class="number">4</span><span class="operator">=</span><span class="number">0.75</span></span><br><span class="line">rn2: 按照dept分组，</span><br><span class="line">     dept<span class="operator">=</span>d1的总行数为<span class="number">3</span></span><br><span class="line">     第一行，(<span class="number">1</span><span class="number">-1</span>)<span class="operator">/</span>(<span class="number">3</span><span class="number">-1</span>)<span class="operator">=</span><span class="number">0</span></span><br><span class="line">     第三行，(<span class="number">3</span><span class="number">-1</span>)<span class="operator">/</span>(<span class="number">3</span><span class="number">-1</span>)<span class="operator">=</span><span class="number">1</span></span><br></pre></td></tr></table></figure><h3 id="grouping-sets-grouping-id-cube-rollup"><a href="#grouping-sets-grouping-id-cube-rollup" class="headerlink" title="grouping sets,grouping__id,cube,rollup"></a>grouping sets,grouping__id,cube,rollup</h3><p>这几个分析函数通常用于OLAP中，不能累加，而且需要根据不同维度上钻和下钻的指标统计，比如，分小时、天、月的UV数。</p><ul><li>数据准备</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2020</span><span class="number">-03</span>,<span class="number">2020</span><span class="number">-03</span><span class="number">-10</span>,cookie1</span><br><span class="line"><span class="number">2020</span><span class="number">-03</span>,<span class="number">2020</span><span class="number">-03</span><span class="number">-10</span>,cookie5</span><br><span class="line"><span class="number">2020</span><span class="number">-03</span>,<span class="number">2020</span><span class="number">-03</span><span class="number">-12</span>,cookie7</span><br><span class="line"><span class="number">2020</span><span class="number">-04</span>,<span class="number">2020</span><span class="number">-04</span><span class="number">-12</span>,cookie3</span><br><span class="line"><span class="number">2020</span><span class="number">-04</span>,<span class="number">2020</span><span class="number">-04</span><span class="number">-13</span>,cookie2</span><br><span class="line"><span class="number">2020</span><span class="number">-04</span>,<span class="number">2020</span><span class="number">-04</span><span class="number">-13</span>,cookie4</span><br><span class="line"><span class="number">2020</span><span class="number">-04</span>,<span class="number">2020</span><span class="number">-04</span><span class="number">-16</span>,cookie4</span><br><span class="line"><span class="number">2020</span><span class="number">-03</span>,<span class="number">2020</span><span class="number">-03</span><span class="number">-10</span>,cookie2</span><br><span class="line"><span class="number">2020</span><span class="number">-03</span>,<span class="number">2020</span><span class="number">-03</span><span class="number">-10</span>,cookie3</span><br><span class="line"><span class="number">2020</span><span class="number">-04</span>,<span class="number">2020</span><span class="number">-04</span><span class="number">-12</span>,cookie5</span><br><span class="line"><span class="number">2020</span><span class="number">-04</span>,<span class="number">2020</span><span class="number">-04</span><span class="number">-13</span>,cookie6</span><br><span class="line"><span class="number">2020</span><span class="number">-04</span>,<span class="number">2020</span><span class="number">-04</span><span class="number">-15</span>,cookie3</span><br><span class="line"><span class="number">2020</span><span class="number">-04</span>,<span class="number">2020</span><span class="number">-04</span><span class="number">-15</span>,cookie2</span><br><span class="line"><span class="number">2020</span><span class="number">-04</span>,<span class="number">2020</span><span class="number">-04</span><span class="number">-16</span>,cookie1</span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> test_t5 (</span><br><span class="line"><span class="keyword">month</span> STRING,</span><br><span class="line"><span class="keyword">day</span> STRING, </span><br><span class="line">cookieid STRING </span><br><span class="line">) <span class="type">ROW</span> FORMAT DELIMITED </span><br><span class="line">FIELDS TERMINATED <span class="keyword">BY</span> <span class="string">&#x27;,&#x27;</span> </span><br><span class="line">stored <span class="keyword">as</span> textfile;</span><br><span class="line"></span><br><span class="line">加载数据：</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/root/hivedata/test_t5.dat&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> test_t5;</span><br></pre></td></tr></table></figure><hr><ul><li>GROUPING SETS</li></ul><p>grouping sets是一种将多个group by 逻辑写在一个sql语句中的便利写法。</p><p>等价于将不同维度的GROUP BY结果集进行UNION ALL。</p><p><strong>GROUPING__ID</strong>，表示结果属于哪一个分组集合。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line"><span class="keyword">month</span>,</span><br><span class="line"><span class="keyword">day</span>,</span><br><span class="line"><span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,</span><br><span class="line">GROUPING__ID </span><br><span class="line"><span class="keyword">FROM</span> test_t5 </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span> </span><br><span class="line"><span class="keyword">GROUPING</span> SETS (<span class="keyword">month</span>,<span class="keyword">day</span>) </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br><span class="line"></span><br><span class="line">grouping_id表示这一组结果属于哪个分组集合，</span><br><span class="line">根据<span class="keyword">grouping</span> sets中的分组条件<span class="keyword">month</span>，<span class="keyword">day</span>，<span class="number">1</span>是代表<span class="keyword">month</span>，<span class="number">2</span>是代表<span class="keyword">day</span></span><br><span class="line"></span><br><span class="line">等价于 </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,<span class="keyword">NULL</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">1</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test_t5 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span> <span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">NULL</span> <span class="keyword">as</span> <span class="keyword">month</span>,<span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">2</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test_t5 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">day</span>;</span><br></pre></td></tr></table></figure><p>再如：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line"><span class="keyword">month</span>,</span><br><span class="line"><span class="keyword">day</span>,</span><br><span class="line"><span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,</span><br><span class="line">GROUPING__ID </span><br><span class="line"><span class="keyword">FROM</span> test_t5 </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span> </span><br><span class="line"><span class="keyword">GROUPING</span> SETS (<span class="keyword">month</span>,<span class="keyword">day</span>,(<span class="keyword">month</span>,<span class="keyword">day</span>)) </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br><span class="line"></span><br><span class="line">等价于</span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,<span class="keyword">NULL</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">1</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test_t5 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span> </span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">NULL</span>,<span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">2</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test_t5 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">day</span></span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,<span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">3</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test_t5 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span>;</span><br></pre></td></tr></table></figure><ul><li>CUBE</li></ul><p>根据GROUP BY的维度的所有组合进行聚合。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line"><span class="keyword">month</span>,</span><br><span class="line"><span class="keyword">day</span>,</span><br><span class="line"><span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,</span><br><span class="line">GROUPING__ID </span><br><span class="line"><span class="keyword">FROM</span> test_t5 </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span> </span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">CUBE</span> </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br><span class="line"></span><br><span class="line">等价于</span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">NULL</span>,<span class="keyword">NULL</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">0</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test_t5</span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,<span class="keyword">NULL</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">1</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test_t5 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span> </span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">NULL</span>,<span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">2</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test_t5 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">day</span></span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span> </span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">month</span>,<span class="keyword">day</span>,<span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,<span class="number">3</span> <span class="keyword">AS</span> GROUPING__ID <span class="keyword">FROM</span> test_t5 <span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span>;</span><br></pre></td></tr></table></figure><ul><li>ROLLUP</li></ul><p>是CUBE的子集，以最左侧的维度为主，从该维度进行层级聚合。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">比如，以<span class="keyword">month</span>维度进行层级聚合：</span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line"><span class="keyword">month</span>,</span><br><span class="line"><span class="keyword">day</span>,</span><br><span class="line"><span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,</span><br><span class="line">GROUPING__ID  </span><br><span class="line"><span class="keyword">FROM</span> test_t5 </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">month</span>,<span class="keyword">day</span></span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">ROLLUP</span> </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br><span class="line"></span><br><span class="line"><span class="comment">--把month和day调换顺序，则以day维度进行层级聚合：</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> </span><br><span class="line"><span class="keyword">day</span>,</span><br><span class="line"><span class="keyword">month</span>,</span><br><span class="line"><span class="built_in">COUNT</span>(<span class="keyword">DISTINCT</span> cookieid) <span class="keyword">AS</span> uv,</span><br><span class="line">GROUPING__ID  </span><br><span class="line"><span class="keyword">FROM</span> test_t5 </span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> <span class="keyword">day</span>,<span class="keyword">month</span> </span><br><span class="line"><span class="keyword">WITH</span> <span class="keyword">ROLLUP</span> </span><br><span class="line"><span class="keyword">ORDER</span> <span class="keyword">BY</span> GROUPING__ID;</span><br><span class="line">（这里，根据天和月进行聚合，和根据天聚合结果一样，因为有父子关系，如果是其他维度组合的话，就会不一样）</span><br></pre></td></tr></table></figure><h2 id="七、Hive执行计划"><a href="#七、Hive执行计划" class="headerlink" title="七、Hive执行计划"></a>七、Hive执行计划</h2><p>Hive SQL的执行计划描述SQL实际执行的整体轮廓，通过执行计划能了解SQL程序在转换成相应计算引擎的执行逻辑，掌握了执行逻辑也就能更好地把握程序出现的瓶颈点，从而能够实现更有针对性的优化。此外还能帮助开发者识别看似等价的SQL其实是不等价的，看似不等价的SQL其实是等价的SQL。<strong>可以说执行计划是打开SQL优化大门的一把钥匙</strong>。</p><p>要想学SQL执行计划，就需要学习查看执行计划的命令：<code>explain</code>，在查询语句的SQL前面加上关键字explain是查看执行计划的基本方法。</p><p>学会explain，能够给我们工作中使用hive带来极大的便利！</p><h3 id="查看SQL的执行计划"><a href="#查看SQL的执行计划" class="headerlink" title="查看SQL的执行计划"></a>查看SQL的执行计划</h3><p>Hive提供的执行计划目前可以查看的信息有以下几种：</p><ul><li><strong>explain</strong>：查看执行计划的基本信息；</li><li><strong>explain dependency</strong>：dependency在explain语句中使用会产生有关计划中输入的额外信息。它显示了输入的各种属性；</li><li><strong>explain authorization</strong>：查看SQL操作相关权限的信息；</li><li><strong>explain vectorization</strong>：查看SQL的向量化描述信息，显示为什么未对Map和Reduce进行矢量化。从 Hive 2.3.0 开始支持；</li><li><strong>explain analyze</strong>：用实际的行数注释计划。从 Hive 2.2.0 开始支持；</li><li><strong>explain cbo</strong>：输出由Calcite优化器生成的计划。CBO 从 Hive 4.0.0 版本开始支持；</li><li><strong>explain locks</strong>：这对于了解系统将获得哪些锁以运行指定的查询很有用。LOCKS 从 Hive 3.2.0 开始支持；</li><li><strong>explain ast</strong>：输出查询的抽象语法树。AST 在 Hive 2.1.0 版本删除了，存在bug，转储AST可能会导致OOM错误，将在4.0.0版本修复；</li><li><strong>explain extended</strong>：加上 extended 可以输出有关计划的额外信息。这通常是物理信息，例如文件名，这些额外信息对我们用处不大；</li></ul><h3 id="1-explain-的用法"><a href="#1-explain-的用法" class="headerlink" title="1.  explain 的用法"></a>1.  explain 的用法</h3><p><strong>Hive提供了explain命令来展示一个查询的执行计划</strong>，这个执行计划对于我们了解底层原理，Hive 调优，排查数据倾斜等很有帮助。</p><p>使用语法如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">explain query;</span><br></pre></td></tr></table></figure><p>在 hive cli 中输入以下命令(hive 2.3.7)：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">explain <span class="keyword">select</span> <span class="built_in">sum</span>(id) <span class="keyword">from</span> test1;</span><br></pre></td></tr></table></figure><p>得到结果：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage<span class="number">-1</span> <span class="keyword">is</span> a root stage</span><br><span class="line">  Stage<span class="number">-0</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-1</span></span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage<span class="number">-1</span></span><br><span class="line">    Map Reduce</span><br><span class="line">      Map Operator Tree:</span><br><span class="line">          TableScan</span><br><span class="line">            alias: test1</span><br><span class="line">            Statistics: Num <span class="keyword">rows</span>: <span class="number">6</span> Data size: <span class="number">75</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            <span class="keyword">Select</span> Operator</span><br><span class="line">              expressions: id (type: <span class="type">int</span>)</span><br><span class="line">              outputColumnNames: id</span><br><span class="line">              Statistics: Num <span class="keyword">rows</span>: <span class="number">6</span> Data size: <span class="number">75</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">              <span class="keyword">Group</span> <span class="keyword">By</span> Operator</span><br><span class="line">                aggregations: <span class="built_in">sum</span>(id)</span><br><span class="line">                mode: hash</span><br><span class="line">                outputColumnNames: _col0</span><br><span class="line">                Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">8</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                Reduce Output Operator</span><br><span class="line">                  sort <span class="keyword">order</span>:</span><br><span class="line">                  Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">8</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                  <span class="keyword">value</span> expressions: _col0 (type: <span class="type">bigint</span>)</span><br><span class="line">      Reduce Operator Tree:</span><br><span class="line">        <span class="keyword">Group</span> <span class="keyword">By</span> Operator</span><br><span class="line">          aggregations: <span class="built_in">sum</span>(VALUE._col0)</span><br><span class="line">          mode: mergepartial</span><br><span class="line">          outputColumnNames: _col0</span><br><span class="line">          Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">8</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">          File Output Operator</span><br><span class="line">            compressed: <span class="literal">false</span></span><br><span class="line">            Statistics: Num <span class="keyword">rows</span>: <span class="number">1</span> Data size: <span class="number">8</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            <span class="keyword">table</span>:</span><br><span class="line">                input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">                serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</span><br><span class="line"></span><br><span class="line">  Stage: Stage<span class="number">-0</span></span><br><span class="line">    <span class="keyword">Fetch</span> Operator</span><br><span class="line">      limit: <span class="number">-1</span></span><br><span class="line">      Processor Tree:</span><br><span class="line">        ListSink</span><br></pre></td></tr></table></figure><p>看完以上内容有什么感受，是不是感觉都看不懂，不要着急，下面将会详细讲解每个参数，相信你学完下面的内容之后再看 explain 的查询结果将游刃有余。</p><blockquote><p><strong>一个HIVE查询被转换为一个由一个或多个stage组成的序列（有向无环图DAG）。这些stage可以是MapReduce stage，也可以是负责元数据存储的stage，也可以是负责文件系统的操作（比如移动和重命名）的stage</strong>。</p></blockquote><p>我们将上述结果拆分看，先从最外层开始，包含两个大的部分：</p><ol><li>stage dependencies：各个stage之间的依赖性</li><li>stage plan：各个stage的执行计划</li></ol><p>先看第一部分 stage dependencies ，包含两个 stage，Stage-1 是根stage，说明这是开始的stage，Stage-0 依赖 Stage-1，Stage-1执行完成后执行Stage-0。</p><p>再看第二部分 stage plan，里面有一个 Map Reduce，一个MR的执行计划分为两个部分：</p><ol><li>Map Operator Tree：MAP端的执行计划树</li><li>Reduce Operator Tree：Reduce端的执行计划树</li></ol><p>这两个执行计划树里面包含这条sql语句的 operator：</p><ol><li><p><strong>TableScan：表扫描操作</strong>，map端第一个操作肯定是加载表，所以就是表扫描操作，常见的属性：</p></li><li><ul><li>alias：表名称</li><li>Statistics：表统计信息，包含表中数据条数，数据大小等</li></ul></li><li><p><strong>Select Operator：选取操作</strong>，常见的属性 ：</p></li><li><ul><li>expressions：需要的字段名称及字段类型</li><li>outputColumnNames：输出的列名称</li><li>Statistics：表统计信息，包含表中数据条数，数据大小等</li></ul></li><li><p><strong>Group By Operator：分组聚合操作</strong>，常见的属性：</p></li><li><ul><li>aggregations：显示聚合函数信息</li><li>mode：聚合模式，值有 hash：随机聚合，就是hash partition；partial：局部聚合；final：最终聚合</li><li>keys：分组的字段，如果没有分组，则没有此字段</li><li>outputColumnNames：聚合之后输出列名</li><li>Statistics：表统计信息，包含分组聚合之后的数据条数，数据大小等</li></ul></li><li><p><strong>Reduce Output Operator：输出到reduce操作</strong>，常见属性：</p></li><li><ul><li>sort order：值为空 不排序；值为 + 正序排序，值为 - 倒序排序；值为 +-  排序的列为两列，第一列为正序，第二列为倒序</li></ul></li><li><p><strong>Filter Operator：过滤操作</strong>，常见的属性：</p></li><li><ul><li>predicate：过滤条件，如sql语句中的where id&gt;&#x3D;1，则此处显示(id &gt;&#x3D; 1)</li></ul></li><li><p><strong>Map Join Operator：join 操作</strong>，常见的属性：</p></li><li><ul><li>condition map：join方式 ，如Inner Join 0 to 1 Left Outer Join0 to 2</li><li>keys: join 的条件字段</li><li>outputColumnNames：join 完成之后输出的字段</li><li>Statistics：join 完成之后生成的数据条数，大小等</li></ul></li><li><p><strong>File Output Operator：文件输出操作</strong>，常见的属性</p></li><li><ul><li>compressed：是否压缩</li><li>table：表的信息，包含输入输出文件格式化方式，序列化方式等</li></ul></li><li><p><strong>Fetch Operator 客户端获取数据操作</strong>，常见的属性：</p></li><li><ul><li>limit，值为 -1 表示不限制条数，其他值为限制的条数</li></ul></li></ol><h3 id="2-explain-的使用场景"><a href="#2-explain-的使用场景" class="headerlink" title="2. explain 的使用场景"></a>2. explain 的使用场景</h3><blockquote><p>本节介绍 explain 能够为我们在生产实践中带来哪些便利及解决我们哪些迷惑</p></blockquote><h4 id="案例一：join-语句会过滤-null-的值吗？"><a href="#案例一：join-语句会过滤-null-的值吗？" class="headerlink" title="案例一：join 语句会过滤 null 的值吗？"></a>案例一：join 语句会过滤 null 的值吗？</h4><p>现在，我们在hive cli 输入以下查询计划语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id,b.user_name <span class="keyword">from</span> test1 a <span class="keyword">join</span> test2 b <span class="keyword">on</span> a.id<span class="operator">=</span>b.id;</span><br></pre></td></tr></table></figure><p>问：<strong>上面这条 join 语句会过滤 id 为 null 的值吗</strong></p><p>执行下面语句：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">explain <span class="keyword">select</span> a.id,b.user_name <span class="keyword">from</span> test1 a <span class="keyword">join</span> test2 b <span class="keyword">on</span> a.id<span class="operator">=</span>b.id;</span><br></pre></td></tr></table></figure><p>我们来看结果 (为了适应页面展示，仅截取了部分输出信息)：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">TableScan</span><br><span class="line"> alias: a</span><br><span class="line"> Statistics: Num <span class="keyword">rows</span>: <span class="number">6</span> Data size: <span class="number">75</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line"> <span class="keyword">Filter</span> Operator</span><br><span class="line">    predicate: id <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">null</span> (type: <span class="type">boolean</span>)</span><br><span class="line">    Statistics: Num <span class="keyword">rows</span>: <span class="number">6</span> Data size: <span class="number">75</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">    <span class="keyword">Select</span> Operator</span><br><span class="line">        expressions: id (type: <span class="type">int</span>)</span><br><span class="line">        outputColumnNames: _col0</span><br><span class="line">        Statistics: Num <span class="keyword">rows</span>: <span class="number">6</span> Data size: <span class="number">75</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">        HashTable Sink Operator</span><br><span class="line">           keys:</span><br><span class="line">             <span class="number">0</span> _col0 (type: <span class="type">int</span>)</span><br><span class="line">             <span class="number">1</span> _col0 (type: <span class="type">int</span>)</span><br><span class="line"> ...</span><br></pre></td></tr></table></figure><p>从上述结果可以看到 <strong>predicate: id is not null</strong> 这样一行，<strong>说明 join 时会自动过滤掉关联字段为 null 值的情况，但 left join 或 full join 是不会自动过滤null值的</strong>，大家可以自行尝试下。</p><h4 id="案例二：group-by-分组语句会进行排序吗？"><a href="#案例二：group-by-分组语句会进行排序吗？" class="headerlink" title="案例二：group by 分组语句会进行排序吗？"></a>案例二：group by 分组语句会进行排序吗？</h4><p>看下面这条sql</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> id,<span class="built_in">max</span>(user_name) <span class="keyword">from</span> test1 <span class="keyword">group</span> <span class="keyword">by</span> id;</span><br></pre></td></tr></table></figure><p>问：<strong>group by 分组语句会进行排序吗</strong></p><p>直接来看 explain 之后结果 (为了适应页面展示，仅截取了部分输出信息)</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">TableScan</span><br><span class="line">   alias: test1</span><br><span class="line">   Statistics: Num <span class="keyword">rows</span>: <span class="number">9</span> Data size: <span class="number">108</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">   <span class="keyword">Select</span> Operator</span><br><span class="line">       expressions: id (type: <span class="type">int</span>), user_name (type: string)</span><br><span class="line">       outputColumnNames: id, user_name</span><br><span class="line">       Statistics: Num <span class="keyword">rows</span>: <span class="number">9</span> Data size: <span class="number">108</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">       <span class="keyword">Group</span> <span class="keyword">By</span> Operator</span><br><span class="line">          aggregations: <span class="built_in">max</span>(user_name)</span><br><span class="line">          keys: id (type: <span class="type">int</span>)</span><br><span class="line">          mode: hash</span><br><span class="line">          outputColumnNames: _col0, _col1</span><br><span class="line">          Statistics: Num <span class="keyword">rows</span>: <span class="number">9</span> Data size: <span class="number">108</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">          Reduce Output Operator</span><br><span class="line">            key expressions: _col0 (type: <span class="type">int</span>)</span><br><span class="line">            sort <span class="keyword">order</span>: <span class="operator">+</span></span><br><span class="line">            Map<span class="operator">-</span>reduce <span class="keyword">partition</span> columns: _col0 (type: <span class="type">int</span>)</span><br><span class="line">            Statistics: Num <span class="keyword">rows</span>: <span class="number">9</span> Data size: <span class="number">108</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            <span class="keyword">value</span> expressions: _col1 (type: string)</span><br><span class="line">...</span><br></pre></td></tr></table></figure><p>我们看 Group By Operator，里面有 keys: id (type: int) 说明按照 id 进行分组的，再往下看还有 sort order: + ，<strong>说明是按照 id 字段进行正序排序的</strong>。</p><h4 id="案例三：哪条sql执行效率高呢？"><a href="#案例三：哪条sql执行效率高呢？" class="headerlink" title="案例三：哪条sql执行效率高呢？"></a>案例三：哪条sql执行效率高呢？</h4><p>观察两条sql语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span></span><br><span class="line"> a.id,</span><br><span class="line"> b.user_name</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line"> test1 a</span><br><span class="line"><span class="keyword">JOIN</span> test2 b <span class="keyword">ON</span> a.id <span class="operator">=</span> b.id</span><br><span class="line"><span class="keyword">WHERE</span></span><br><span class="line"> a.id <span class="operator">&gt;</span> <span class="number">2</span>;</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line"> a.id,</span><br><span class="line"> b.user_name</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line"> (<span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> test1 <span class="keyword">WHERE</span> id <span class="operator">&gt;</span> <span class="number">2</span>) a</span><br><span class="line"><span class="keyword">JOIN</span> test2 b <span class="keyword">ON</span> a.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure><p><strong>这两条sql语句输出的结果是一样的，但是哪条sql执行效率高呢</strong>？</p><p>有人说第一条sql执行效率高，因为第二条sql有子查询，子查询会影响性能；</p><p>有人说第二条sql执行效率高，因为先过滤之后，在进行join时的条数减少了，所以执行效率就高了。</p><p>到底哪条sql效率高呢，我们直接在sql语句前面加上 explain，看下执行计划不就知道了嘛！</p><p>在第一条sql语句前加上 explain，得到如下结果</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain <span class="keyword">select</span> a.id,b.user_name <span class="keyword">from</span> test1 a <span class="keyword">join</span> test2 b <span class="keyword">on</span> a.id<span class="operator">=</span>b.id <span class="keyword">where</span> a.id <span class="operator">&gt;</span><span class="number">2</span>;</span><br><span class="line">OK</span><br><span class="line">Explain</span><br><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage<span class="number">-4</span> <span class="keyword">is</span> a root stage</span><br><span class="line">  Stage<span class="number">-3</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-4</span></span><br><span class="line">  Stage<span class="number">-0</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-3</span></span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage<span class="number">-4</span></span><br><span class="line">    Map Reduce <span class="keyword">Local</span> Work</span><br><span class="line">      Alias <span class="operator">-</span><span class="operator">&gt;</span> Map <span class="keyword">Local</span> Tables:</span><br><span class="line">        $hdt$_0:a</span><br><span class="line">          <span class="keyword">Fetch</span> Operator</span><br><span class="line">            limit: <span class="number">-1</span></span><br><span class="line">      Alias <span class="operator">-</span><span class="operator">&gt;</span> Map <span class="keyword">Local</span> Operator Tree:</span><br><span class="line">        $hdt$_0:a</span><br><span class="line">          TableScan</span><br><span class="line">            alias: a</span><br><span class="line">            Statistics: Num <span class="keyword">rows</span>: <span class="number">6</span> Data size: <span class="number">75</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            <span class="keyword">Filter</span> Operator</span><br><span class="line">              predicate: (id <span class="operator">&gt;</span> <span class="number">2</span>) (type: <span class="type">boolean</span>)</span><br><span class="line">              Statistics: Num <span class="keyword">rows</span>: <span class="number">2</span> Data size: <span class="number">25</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">              <span class="keyword">Select</span> Operator</span><br><span class="line">                expressions: id (type: <span class="type">int</span>)</span><br><span class="line">                outputColumnNames: _col0</span><br><span class="line">                Statistics: Num <span class="keyword">rows</span>: <span class="number">2</span> Data size: <span class="number">25</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                HashTable Sink Operator</span><br><span class="line">                  keys:</span><br><span class="line">                    <span class="number">0</span> _col0 (type: <span class="type">int</span>)</span><br><span class="line">                    <span class="number">1</span> _col0 (type: <span class="type">int</span>)</span><br><span class="line"></span><br><span class="line">  Stage: Stage<span class="number">-3</span></span><br><span class="line">    Map Reduce</span><br><span class="line">      Map Operator Tree:</span><br><span class="line">          TableScan</span><br><span class="line">            alias: b</span><br><span class="line">            Statistics: Num <span class="keyword">rows</span>: <span class="number">6</span> Data size: <span class="number">75</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            <span class="keyword">Filter</span> Operator</span><br><span class="line">              predicate: (id <span class="operator">&gt;</span> <span class="number">2</span>) (type: <span class="type">boolean</span>)</span><br><span class="line">              Statistics: Num <span class="keyword">rows</span>: <span class="number">2</span> Data size: <span class="number">25</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">              <span class="keyword">Select</span> Operator</span><br><span class="line">                expressions: id (type: <span class="type">int</span>), user_name (type: string)</span><br><span class="line">                outputColumnNames: _col0, _col1</span><br><span class="line">                Statistics: Num <span class="keyword">rows</span>: <span class="number">2</span> Data size: <span class="number">25</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                Map <span class="keyword">Join</span> Operator</span><br><span class="line">                  <span class="keyword">condition</span> map:</span><br><span class="line">                       <span class="keyword">Inner</span> <span class="keyword">Join</span> <span class="number">0</span> <span class="keyword">to</span> <span class="number">1</span></span><br><span class="line">                  keys:</span><br><span class="line">                    <span class="number">0</span> _col0 (type: <span class="type">int</span>)</span><br><span class="line">                    <span class="number">1</span> _col0 (type: <span class="type">int</span>)</span><br><span class="line">                  outputColumnNames: _col0, _col2</span><br><span class="line">                  Statistics: Num <span class="keyword">rows</span>: <span class="number">2</span> Data size: <span class="number">27</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                  <span class="keyword">Select</span> Operator</span><br><span class="line">                    expressions: _col0 (type: <span class="type">int</span>), _col2 (type: string)</span><br><span class="line">                    outputColumnNames: _col0, _col1</span><br><span class="line">                    Statistics: Num <span class="keyword">rows</span>: <span class="number">2</span> Data size: <span class="number">27</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                    File Output Operator</span><br><span class="line">                      compressed: <span class="literal">false</span></span><br><span class="line">                      Statistics: Num <span class="keyword">rows</span>: <span class="number">2</span> Data size: <span class="number">27</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                      <span class="keyword">table</span>:</span><br><span class="line">                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</span><br><span class="line">      <span class="keyword">Local</span> Work:</span><br><span class="line">        Map Reduce <span class="keyword">Local</span> Work</span><br><span class="line"></span><br><span class="line">  Stage: Stage<span class="number">-0</span></span><br><span class="line">    <span class="keyword">Fetch</span> Operator</span><br><span class="line">      limit: <span class="number">-1</span></span><br><span class="line">      Processor Tree:</span><br><span class="line">        ListSink</span><br></pre></td></tr></table></figure><p>在第二条sql语句前加上 explain，得到如下结果</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line">hive (<span class="keyword">default</span>)<span class="operator">&gt;</span> explain <span class="keyword">select</span> a.id,b.user_name <span class="keyword">from</span>(<span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span>  test1 <span class="keyword">where</span> id<span class="operator">&gt;</span><span class="number">2</span> ) a <span class="keyword">join</span> test2 b <span class="keyword">on</span> a.id<span class="operator">=</span>b.id;</span><br><span class="line">OK</span><br><span class="line">Explain</span><br><span class="line">STAGE DEPENDENCIES:</span><br><span class="line">  Stage<span class="number">-4</span> <span class="keyword">is</span> a root stage</span><br><span class="line">  Stage<span class="number">-3</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-4</span></span><br><span class="line">  Stage<span class="number">-0</span> depends <span class="keyword">on</span> stages: Stage<span class="number">-3</span></span><br><span class="line"></span><br><span class="line">STAGE PLANS:</span><br><span class="line">  Stage: Stage<span class="number">-4</span></span><br><span class="line">    Map Reduce <span class="keyword">Local</span> Work</span><br><span class="line">      Alias <span class="operator">-</span><span class="operator">&gt;</span> Map <span class="keyword">Local</span> Tables:</span><br><span class="line">        $hdt$_0:test1</span><br><span class="line">          <span class="keyword">Fetch</span> Operator</span><br><span class="line">            limit: <span class="number">-1</span></span><br><span class="line">      Alias <span class="operator">-</span><span class="operator">&gt;</span> Map <span class="keyword">Local</span> Operator Tree:</span><br><span class="line">        $hdt$_0:test1</span><br><span class="line">          TableScan</span><br><span class="line">            alias: test1</span><br><span class="line">            Statistics: Num <span class="keyword">rows</span>: <span class="number">6</span> Data size: <span class="number">75</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            <span class="keyword">Filter</span> Operator</span><br><span class="line">              predicate: (id <span class="operator">&gt;</span> <span class="number">2</span>) (type: <span class="type">boolean</span>)</span><br><span class="line">              Statistics: Num <span class="keyword">rows</span>: <span class="number">2</span> Data size: <span class="number">25</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">              <span class="keyword">Select</span> Operator</span><br><span class="line">                expressions: id (type: <span class="type">int</span>)</span><br><span class="line">                outputColumnNames: _col0</span><br><span class="line">                Statistics: Num <span class="keyword">rows</span>: <span class="number">2</span> Data size: <span class="number">25</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                HashTable Sink Operator</span><br><span class="line">                  keys:</span><br><span class="line">                    <span class="number">0</span> _col0 (type: <span class="type">int</span>)</span><br><span class="line">                    <span class="number">1</span> _col0 (type: <span class="type">int</span>)</span><br><span class="line"></span><br><span class="line">  Stage: Stage<span class="number">-3</span></span><br><span class="line">    Map Reduce</span><br><span class="line">      Map Operator Tree:</span><br><span class="line">          TableScan</span><br><span class="line">            alias: b</span><br><span class="line">            Statistics: Num <span class="keyword">rows</span>: <span class="number">6</span> Data size: <span class="number">75</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">            <span class="keyword">Filter</span> Operator</span><br><span class="line">              predicate: (id <span class="operator">&gt;</span> <span class="number">2</span>) (type: <span class="type">boolean</span>)</span><br><span class="line">              Statistics: Num <span class="keyword">rows</span>: <span class="number">2</span> Data size: <span class="number">25</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">              <span class="keyword">Select</span> Operator</span><br><span class="line">                expressions: id (type: <span class="type">int</span>), user_name (type: string)</span><br><span class="line">                outputColumnNames: _col0, _col1</span><br><span class="line">                Statistics: Num <span class="keyword">rows</span>: <span class="number">2</span> Data size: <span class="number">25</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                Map <span class="keyword">Join</span> Operator</span><br><span class="line">                  <span class="keyword">condition</span> map:</span><br><span class="line">                       <span class="keyword">Inner</span> <span class="keyword">Join</span> <span class="number">0</span> <span class="keyword">to</span> <span class="number">1</span></span><br><span class="line">                  keys:</span><br><span class="line">                    <span class="number">0</span> _col0 (type: <span class="type">int</span>)</span><br><span class="line">                    <span class="number">1</span> _col0 (type: <span class="type">int</span>)</span><br><span class="line">                  outputColumnNames: _col0, _col2</span><br><span class="line">                  Statistics: Num <span class="keyword">rows</span>: <span class="number">2</span> Data size: <span class="number">27</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                  <span class="keyword">Select</span> Operator</span><br><span class="line">                    expressions: _col0 (type: <span class="type">int</span>), _col2 (type: string)</span><br><span class="line">                    outputColumnNames: _col0, _col1</span><br><span class="line">                    Statistics: Num <span class="keyword">rows</span>: <span class="number">2</span> Data size: <span class="number">27</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                    File Output Operator</span><br><span class="line">                      compressed: <span class="literal">false</span></span><br><span class="line">                      Statistics: Num <span class="keyword">rows</span>: <span class="number">2</span> Data size: <span class="number">27</span> Basic stats: COMPLETE <span class="keyword">Column</span> stats: <span class="keyword">NONE</span></span><br><span class="line">                      <span class="keyword">table</span>:</span><br><span class="line">                          input format: org.apache.hadoop.mapred.SequenceFileInputFormat</span><br><span class="line">                          output format: org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat</span><br><span class="line">                          serde: org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe</span><br><span class="line">      <span class="keyword">Local</span> Work:</span><br><span class="line">        Map Reduce <span class="keyword">Local</span> Work</span><br><span class="line"></span><br><span class="line">  Stage: Stage<span class="number">-0</span></span><br><span class="line">    <span class="keyword">Fetch</span> Operator</span><br><span class="line">      limit: <span class="number">-1</span></span><br><span class="line">      Processor Tree:</span><br><span class="line">        ListSink</span><br></pre></td></tr></table></figure><p>大家有什么发现，除了表别名不一样，其他的执行计划完全一样，都是先进行 where 条件过滤，在进行 join 条件关联。<strong>说明 hive 底层会自动帮我们进行优化，所以这两条sql语句执行效率是一样的</strong>。</p><p>以上仅列举了3个我们生产中既熟悉又有点迷糊的例子，explain 还有很多其他的用途，如查看stage的依赖情况、排查数据倾斜、hive 调优等，小伙伴们可以自行尝试。</p><h3 id="2-explain-dependency的用法"><a href="#2-explain-dependency的用法" class="headerlink" title="2. explain dependency的用法"></a>2. explain dependency的用法</h3><p>explain dependency用于描述一段SQL需要的数据来源，输出是一个json格式的数据，里面包含以下两个部分的内容：</p><ul><li><strong>input_partitions</strong>：描述一段SQL依赖的数据来源表分区，里面存储的是分区名的列表，如果整段SQL包含的所有表都是非分区表，则显示为空。</li><li><strong>input_tables</strong>：描述一段SQL依赖的数据来源表，里面存储的是Hive表名的列表。</li></ul><p><strong>使用explain dependency查看SQL查询非分区普通表</strong>，在 hive cli 中输入以下命令：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">explain dependency <span class="keyword">select</span> s_age,<span class="built_in">count</span>(<span class="number">1</span>) num <span class="keyword">from</span> student_orc;</span><br></pre></td></tr></table></figure><p>得到结果：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;input_partitions&quot;:[],&quot;input_tables&quot;:[&#123;&quot;tablename&quot;:&quot;default@student_tb _orc&quot;,&quot;tabletype&quot;:&quot;MANAGED_TABLE&quot;&#125;]&#125;</span><br></pre></td></tr></table></figure><p><strong>使用explain dependency查看SQL查询分区表</strong>，在 hive cli 中输入以下命令：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">explain dependency <span class="keyword">select</span> s_age,<span class="built_in">count</span>(<span class="number">1</span>) num <span class="keyword">from</span> student_orc_partition;</span><br></pre></td></tr></table></figure><p>得到结果：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;input_partitions&quot;:[&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition@ part=0&quot;&#125;, </span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition@part=1&quot;&#125;, </span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition@part=2&quot;&#125;, </span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition@part=3&quot;&#125;,</span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition@part=4&quot;&#125;, </span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition@part=5&quot;&#125;,</span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition@part=6&quot;&#125;,</span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition@part=7&quot;&#125;,</span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition@part=8&quot;&#125;,</span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition@part=9&quot;&#125;], </span><br><span class="line">&quot;input_tables&quot;:[&#123;&quot;tablename&quot;:&quot;default@student_orc_partition&quot;, &quot;tabletype&quot;:&quot;MANAGED_TABLE&quot;&#125;]</span><br></pre></td></tr></table></figure><p>explain dependency的使用场景有两个：</p><ul><li><strong>场景一</strong>：快速排除。快速排除因为读取不到相应分区的数据而导致任务数据输出异常。例如，在一个以天分区的任务中，上游任务因为生产过程不可控因素出现异常或者空跑，导致下游任务引发异常。通过这种方式，可以快速查看SQL读取的分区是否出现异常。</li><li><strong>场景二</strong>：理清表的输入，帮助理解程序的运行，特别是有助于理解有多重子查询，多表连接的依赖输入。</li></ul><p>下面通过两个案例来看explain dependency的实际运用：</p><h4 id="案例一：识别看似等价的代码"><a href="#案例一：识别看似等价的代码" class="headerlink" title="案例一：识别看似等价的代码"></a>案例一：识别看似等价的代码</h4><p>对于刚接触SQL的程序员，很容易将</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> a <span class="keyword">inner</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.no<span class="operator">=</span>b.no <span class="keyword">and</span> a.f<span class="operator">&gt;</span><span class="number">1</span> <span class="keyword">and</span> a.f<span class="operator">&lt;</span><span class="number">3</span>;</span><br></pre></td></tr></table></figure><p>等价于</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> a <span class="keyword">inner</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.no<span class="operator">=</span>b.no <span class="keyword">where</span> a.f<span class="operator">&gt;</span><span class="number">1</span> <span class="keyword">and</span> a.f<span class="operator">&lt;</span><span class="number">3</span>;</span><br></pre></td></tr></table></figure><p>我们可以通过案例来查看下它们的区别：</p><p>代码1：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">a.s_no </span><br><span class="line"><span class="keyword">from</span> student_orc_partition a </span><br><span class="line"><span class="keyword">inner</span> <span class="keyword">join</span> </span><br><span class="line">student_orc_partition_only b </span><br><span class="line"><span class="keyword">on</span> a.s_no<span class="operator">=</span>b.s_no <span class="keyword">and</span> a.part<span class="operator">=</span>b.part <span class="keyword">and</span> a.part<span class="operator">&gt;=</span><span class="number">1</span> <span class="keyword">and</span> a.part<span class="operator">&lt;=</span><span class="number">2</span>;</span><br></pre></td></tr></table></figure><p>代码2：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> </span><br><span class="line">a.s_no </span><br><span class="line"><span class="keyword">from</span> student_orc_partition a </span><br><span class="line"><span class="keyword">inner</span> <span class="keyword">join</span> </span><br><span class="line">student_orc_partition_only b </span><br><span class="line"><span class="keyword">on</span> a.s_no<span class="operator">=</span>b.s_no <span class="keyword">and</span> a.part<span class="operator">=</span>b.part </span><br><span class="line"><span class="keyword">where</span> a.part<span class="operator">&gt;=</span><span class="number">1</span> <span class="keyword">and</span> a.part<span class="operator">&lt;=</span><span class="number">2</span>;</span><br></pre></td></tr></table></figure><p>我们看下上述两段代码explain dependency的输出结果：</p><p><strong>代码1的explain dependency结果</strong>：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;input_partitions&quot;: </span><br><span class="line">[&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition@part=0&quot;&#125;, </span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition@part=1&quot;&#125;, </span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition@part=2&quot;&#125;,</span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition_only@part=1&quot;&#125;, </span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition_only@part=2&quot;&#125;], </span><br><span class="line">&quot;input_tables&quot;: [&#123;&quot;tablename&quot;:&quot;default@student_orc_partition&quot;,&quot;tabletype&quot;:&quot;MANAGED_TABLE&quot;&#125;, &#123;&quot;tablename&quot;:&quot;default@student_orc_partition_only&quot;,&quot;tabletype&quot;:&quot;MANAGED_TABLE&quot;&#125;]&#125;</span><br></pre></td></tr></table></figure><p><strong>代码2的explain dependency结果</strong>：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;input_partitions&quot;: </span><br><span class="line">[&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition@part=1&quot;&#125;, </span><br><span class="line">&#123;&quot;partitionName&quot; : &quot;default@student_orc_partition@part=2&quot;&#125;,</span><br><span class="line">&#123;&quot;partitionName&quot; :&quot;default@student_orc_partition_only@part=1&quot;&#125;,</span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition_only@part=2&quot;&#125;], </span><br><span class="line">&quot;input_tables&quot;: [&#123;&quot;tablename&quot;:&quot;default@student_orc_partition&quot;,&quot;tabletype&quot;:&quot;MANAGED_TABLE&quot;&#125;, &#123;&quot;tablename&quot;:&quot;default@student_orc_partition_only&quot;,&quot;tabletype&quot;:&quot;MANAGED_TABLE&quot;&#125;]&#125;</span><br></pre></td></tr></table></figure><p>通过上面的输出结果可以看到，其实上述的两个SQL并不等价，代码1在内连接（inner join）中的连接条件（on）中加入非等值的过滤条件后，并没有将内连接的左右两个表按照过滤条件进行过滤，内连接在执行时会多读取part&#x3D;0的分区数据。而在代码2中，会过滤掉不符合条件的分区。</p><h4 id="案例二：识别SQL读取数据范围的差别"><a href="#案例二：识别SQL读取数据范围的差别" class="headerlink" title="案例二：识别SQL读取数据范围的差别"></a>案例二：识别SQL读取数据范围的差别</h4><p>代码1：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">explain dependency</span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">a.s_no </span><br><span class="line"><span class="keyword">from</span> student_orc_partition a </span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> </span><br><span class="line">student_orc_partition_only b </span><br><span class="line"><span class="keyword">on</span> a.s_no<span class="operator">=</span>b.s_no <span class="keyword">and</span> a.part<span class="operator">=</span>b.part <span class="keyword">and</span> b.part<span class="operator">&gt;=</span><span class="number">1</span> <span class="keyword">and</span> b.part<span class="operator">&lt;=</span><span class="number">2</span>;</span><br></pre></td></tr></table></figure><p>代码2：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">explain dependency </span><br><span class="line"><span class="keyword">select</span> </span><br><span class="line">a.s_no </span><br><span class="line"><span class="keyword">from</span> student_orc_partition a </span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> </span><br><span class="line">student_orc_partition_only b </span><br><span class="line"><span class="keyword">on</span> a.s_no<span class="operator">=</span>b.s_no <span class="keyword">and</span> a.part<span class="operator">=</span>b.part <span class="keyword">and</span> a.part<span class="operator">&gt;=</span><span class="number">1</span> <span class="keyword">and</span> a.part<span class="operator">&lt;=</span><span class="number">2</span>;</span><br></pre></td></tr></table></figure><p>以上两个代码的数据读取范围是一样的吗？答案是不一样，我们通过explain dependency来看下：</p><p><strong>代码1的explain dependency结果</strong>：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;input_partitions&quot;: </span><br><span class="line">[&#123;&quot;partitionName&quot;: &quot;default@student_orc_partition@part=0&quot;&#125;, </span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition@part=1&quot;&#125;, …中间省略<span class="number">7</span>个分区</span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition@part=9&quot;&#125;, </span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition_only@part=1&quot;&#125;, </span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition_only@part=2&quot;&#125;], </span><br><span class="line">&quot;input_tables&quot;: [&#123;&quot;tablename&quot;:&quot;default@student_orc_partition&quot;,&quot;tabletype&quot;:&quot;MANAGED_TABLE&quot;&#125;, &#123;&quot;tablename&quot;:&quot;default@student_orc_partition_only&quot;,&quot;tabletype&quot;:&quot;MANAGED_TABLE&quot;&#125;]&#125;</span><br></pre></td></tr></table></figure><p><strong>代码2的explain dependency结果</strong>：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&#123;&quot;input_partitions&quot;: </span><br><span class="line">[&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition@part=0&quot;&#125;, </span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition@part=1&quot;&#125;, …中间省略<span class="number">7</span>个分区 </span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition@part=9&quot;&#125;, </span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition_only@part=0&quot;&#125;, </span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition_only@part=1&quot;&#125;, …中间省略<span class="number">7</span>个分区 </span><br><span class="line">&#123;&quot;partitionName&quot;:&quot;default@student_orc_partition_only@part=9&quot;&#125;],</span><br><span class="line">&quot;input_tables&quot;: [&#123;&quot;tablename&quot;:&quot;default@student_orc_partition&quot;,&quot;tabletype&quot;:&quot;MANAGED_TABLE&quot;&#125;, &#123;&quot;tablename&quot;:&quot;default@student_orc_partition_only&quot;,&quot;tabletype&quot;:&quot;MANAGED_TABLE&quot;&#125;]&#125;</span><br></pre></td></tr></table></figure><p>可以看到，对左外连接在连接条件中加入非等值过滤的条件，<strong>如果过滤条件是作用于右表（b表）有起到过滤的效果，则右表只要扫描两个分区即可，但是左表（a表）会进行全表扫描。如果过滤条件是针对左表，则完全没有起到过滤的作用，那么两个表将进行全表扫描</strong>。这时的情况就如同全外连接一样都需要对两个数据进行全表扫描。</p><p>在使用过程中，容易认为代码片段2可以像代码片段1一样进行数据过滤，通过查看explain dependency的输出结果，可以知道不是如此。</p><h3 id="3-explain-authorization-的用法"><a href="#3-explain-authorization-的用法" class="headerlink" title="3. explain authorization 的用法"></a>3. explain authorization 的用法</h3><p>通过explain authorization可以知道当前SQL访问的数据来源（INPUTS） 和数据输出（OUTPUTS），以及当前Hive的访问用户 （CURRENT_USER）和操作（OPERATION）。</p><p>在 hive cli 中输入以下命令：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">explain <span class="keyword">authorization</span> </span><br><span class="line"><span class="keyword">select</span> variance(s_score) <span class="keyword">from</span> student_tb_orc;</span><br></pre></td></tr></table></figure><p>结果如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">INPUTS: </span><br><span class="line">  <span class="keyword">default</span><span class="variable">@student</span>_tb_orc </span><br><span class="line">OUTPUTS: </span><br><span class="line">  hdfs:<span class="operator">/</span><span class="operator">/</span>node01:<span class="number">8020</span><span class="operator">/</span>tmp<span class="operator">/</span>hive<span class="operator">/</span>hdfs<span class="operator">/</span>cbf182a5<span class="number">-8258</span><span class="number">-4157</span><span class="number">-9194</span><span class="operator">-</span> <span class="number">90</span>f1475a3ed5<span class="operator">/</span><span class="operator">-</span>mr<span class="number">-10000</span> </span><br><span class="line"><span class="built_in">CURRENT_USER</span>: </span><br><span class="line">  hdfs </span><br><span class="line">OPERATION: </span><br><span class="line">  QUERY </span><br><span class="line">AUTHORIZATION_FAILURES: </span><br><span class="line">  <span class="keyword">No</span> privilege <span class="string">&#x27;Select&#x27;</span> found <span class="keyword">for</span> inputs &#123; database:<span class="keyword">default</span>, <span class="keyword">table</span>:student_ tb_orc, columnName:s_score&#125;</span><br></pre></td></tr></table></figure><p>从上面的信息可知：</p><p>上面案例的数据来源是defalut数据库中的 student_tb_orc表；</p><p>数据的输出路径是hdfs:&#x2F;&#x2F;node01:8020&#x2F;tmp&#x2F;hive&#x2F;hdfs&#x2F;cbf182a5-8258-4157-9194-90f1475a3ed5&#x2F;-mr-10000；</p><p>当前的操作用户是hdfs，操作是查询；</p><p>观察上面的信息我们还会看到AUTHORIZATION_FAILURES信息，提示对当前的输入没有查询权限，但如果运行上面的SQL的话也能够正常运行。为什么会出现这种情况？<strong>Hive在默认不配置权限管理的情况下不进行权限验证，所有的用户在Hive里面都是超级管理员，即使不对特定的用户进行赋权，也能够正常查询</strong>。</p><h3 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h3><p>通过上面对explain的介绍，可以发现explain中有很多值得我们去研究的内容，读懂 explain 的执行计划有利于我们优化Hive SQL，同时也能提升我们对SQL的掌控力。</p><h2 id="八、Hive-SQL底层执行原理"><a href="#八、Hive-SQL底层执行原理" class="headerlink" title="八、Hive SQL底层执行原理"></a>八、Hive SQL底层执行原理</h2><blockquote><p>本节结构采用宏观着眼，微观入手，从整体到细节的方式剖析 Hive SQL 底层原理。第一节先介绍 Hive 底层的整体执行流程，然后第二节介绍执行流程中的 SQL 编译成 MapReduce 的过程，第三节剖析 SQL 编译成 MapReduce 的具体实现原理。</p></blockquote><h3 id="Hive-底层执行架构"><a href="#Hive-底层执行架构" class="headerlink" title="Hive 底层执行架构"></a>Hive 底层执行架构</h3><p>我们先来看下 Hive 的底层执行架构图， Hive 的主要组件与 Hadoop 交互的过程：</p><div align=center><img src="Hive底层执行架构.png"></div><p>在 Hive 这一侧，总共有五个组件：</p><ol><li>UI：用户界面。可看作我们提交SQL语句的命令行界面。</li><li>DRIVER：驱动程序。接收查询的组件。该组件实现了会话句柄的概念。</li><li>COMPILER：编译器。负责将 SQL 转化为平台可执行的执行计划。对不同的查询块和查询表达式进行语义分析，并最终借助表和从 metastore 查找的分区元数据来生成执行计划。</li><li>METASTORE：元数据库。存储 Hive 中各种表和分区的所有结构信息。</li><li>EXECUTION ENGINE：执行引擎。负责提交 COMPILER 阶段编译好的执行计划到不同的平台上。</li></ol><p>上图的基本流程是：</p><p><strong>步骤1</strong>：UI 调用 DRIVER 的接口；</p><p><strong>步骤2</strong>：DRIVER 为查询创建会话句柄，并将查询发送到 COMPILER(编译器)生成执行计划；</p><p><strong>步骤3和4</strong>：编译器从元数据存储中获取本次查询所需要的元数据，该元数据用于对查询树中的表达式进行类型检查，以及基于查询谓词修建分区；</p><p><strong>步骤5</strong>：编译器生成的计划是分阶段的DAG，每个阶段要么是 map&#x2F;reduce 作业，要么是一个元数据或者HDFS上的操作。将生成的计划发给 DRIVER。</p><p>如果是 map&#x2F;reduce 作业，该计划包括 map operator trees 和一个  reduce operator tree，执行引擎将会把这些作业发送给 MapReduce ：</p><p><strong>步骤6、6.1、6.2和6.3</strong>：执行引擎将这些阶段提交给适当的组件。在每个 task(mapper&#x2F;reducer) 中，从HDFS文件中读取与表或中间输出相关联的数据，并通过相关算子树传递这些数据。最终这些数据通过序列化器写入到一个临时HDFS文件中（如果不需要 reduce 阶段，则在 map 中操作）。临时文件用于向计划中后面的 map&#x2F;reduce 阶段提供数据。</p><p><strong>步骤7、8和9</strong>：最终的临时文件将移动到表的位置，确保不读取脏数据(文件重命名在HDFS中是原子操作)。对于用户的查询，临时文件的内容由执行引擎直接从HDFS读取，然后通过Driver发送到UI。</p><h3 id="Hive-SQL-编译成-MapReduce-过程"><a href="#Hive-SQL-编译成-MapReduce-过程" class="headerlink" title="Hive SQL 编译成 MapReduce 过程"></a>Hive SQL 编译成 MapReduce 过程</h3><p>编译 SQL 的任务是在上节中介绍的 COMPILER（编译器组件）中完成的。Hive将SQL转化为MapReduce任务，整个编译过程分为六个阶段：</p><div align=center><img src="Hive SQL 编译成 MapReduce 过程.png"></div><ol><li><strong>词法、语法解析</strong>: Antlr 定义 SQL 的语法规则，完成 SQL 词法，语法解析，将 SQL 转化为抽象语法树 AST Tree；</li></ol><blockquote><p>Antlr是一种语言识别的工具，可以用来构造领域语言。使用Antlr构造特定的语言只需要编写一个语法文件，定义词法和语法替换规则即可，Antlr完成了词法分析、语法分析、语义分析、中间代码生成的过程。</p></blockquote><ol><li><strong>语义解析</strong>: 遍历 AST Tree，抽象出查询的基本组成单元 QueryBlock；</li><li><strong>生成逻辑执行计划</strong>: 遍历 QueryBlock，翻译为执行操作树 OperatorTree；</li><li><strong>优化逻辑执行计划</strong>: 逻辑层优化器进行 OperatorTree 变换，合并 Operator，达到减少 MapReduce Job，减少数据传输及 shuffle 数据量；</li><li><strong>生成物理执行计划</strong>: 遍历 OperatorTree，翻译为 MapReduce 任务；</li><li><strong>优化物理执行计划</strong>: 物理层优化器进行 MapReduce 任务的变换，生成最终的执行计划。</li></ol><h5 id="下面对这六个阶段详细解析："><a href="#下面对这六个阶段详细解析：" class="headerlink" title="下面对这六个阶段详细解析："></a>下面对这六个阶段详细解析：</h5><p>为便于理解，我们拿一个简单的查询语句进行展示，对5月23号的地区维表进行查询：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dim.dim_region <span class="keyword">where</span> dt <span class="operator">=</span> <span class="string">&#x27;2021-05-23&#x27;</span>;</span><br></pre></td></tr></table></figure><p><strong>阶段一</strong>：词法、语法解析</p><p>根据Antlr定义的sql语法规则，将相关sql进行词法、语法解析，转化为抽象语法树AST Tree：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">ABSTRACT SYNTAX TREE:</span><br><span class="line">TOK_QUERY</span><br><span class="line">    TOK_FROM </span><br><span class="line">    TOK_TABREF</span><br><span class="line">           TOK_TABNAME</span><br><span class="line">               dim</span><br><span class="line">                 dim_region</span><br><span class="line">    TOK_INSERT</span><br><span class="line">      TOK_DESTINATION</span><br><span class="line">          TOK_DIR</span><br><span class="line">              TOK_TMP_FILE</span><br><span class="line">        TOK_SELECT</span><br><span class="line">          TOK_SELEXPR</span><br><span class="line">              TOK_ALLCOLREF</span><br><span class="line">        TOK_WHERE</span><br><span class="line">          <span class="operator">=</span></span><br><span class="line">              TOK_TABLE_OR_COL</span><br><span class="line">                  dt</span><br><span class="line">                    <span class="string">&#x27;2021-05-23&#x27;</span></span><br></pre></td></tr></table></figure><p><strong>阶段二</strong>：语义解析</p><p>遍历AST Tree，抽象出查询的基本组成单元QueryBlock：</p><p>AST Tree生成后由于其复杂度依旧较高，不便于翻译为mapreduce程序，需要进行进一步抽象和结构化，形成QueryBlock。</p><p>QueryBlock是一条SQL最基本的组成单元，包括三个部分：输入源，计算过程，输出。简单来讲一个QueryBlock就是一个子查询。</p><p>QueryBlock的生成过程为一个递归过程，先序遍历 AST Tree ，遇到不同的 Token 节点(理解为特殊标记)，保存到相应的属性中。</p><p><strong>阶段三</strong>：生成逻辑执行计划</p><p>遍历QueryBlock，翻译为执行操作树OperatorTree：</p><p>Hive最终生成的MapReduce任务，Map阶段和Reduce阶段均由OperatorTree组成。</p><p>基本的操作符包括：</p><ul><li>TableScanOperator</li><li>SelectOperator</li><li>FilterOperator</li><li>JoinOperator</li><li>GroupByOperator</li><li>ReduceSinkOperator&#96;</li></ul><p>Operator在Map Reduce阶段之间的数据传递都是一个流式的过程。每一个Operator对一行数据完成操作后之后将数据传递给childOperator计算。</p><p>由于Join&#x2F;GroupBy&#x2F;OrderBy均需要在Reduce阶段完成，所以在生成相应操作的Operator之前都会先生成一个ReduceSinkOperator，将字段组合并序列化为Reduce Key&#x2F;value, Partition Key。</p><p><strong>阶段四</strong>：优化逻辑执行计划</p><p>Hive中的逻辑查询优化可以大致分为以下几类：</p><ul><li>投影修剪</li><li>推导传递谓词</li><li>谓词下推</li><li>将Select-Select，Filter-Filter合并为单个操作</li><li>多路 Join</li><li>查询重写以适应某些列值的Join倾斜</li></ul><p><strong>阶段五</strong>：生成物理执行计划</p><p>生成物理执行计划即是将逻辑执行计划生成的OperatorTree转化为MapReduce Job的过程，主要分为下面几个阶段：</p><ol><li>对输出表生成MoveTask</li><li>从OperatorTree的其中一个根节点向下深度优先遍历</li><li>ReduceSinkOperator标示Map&#x2F;Reduce的界限，多个Job间的界限</li><li>遍历其他根节点，遇过碰到JoinOperator合并MapReduceTask</li><li>生成StatTask更新元数据</li><li>剪断Map与Reduce间的Operator的关系</li></ol><p><strong>阶段六</strong>：优化物理执行计划</p><p>Hive中的物理优化可以大致分为以下几类：</p><ul><li>分区修剪(Partition Pruning)</li><li>基于分区和桶的扫描修剪(Scan pruning)</li><li>如果查询基于抽样，则扫描修剪</li><li>在某些情况下，在 map 端应用 Group By</li><li>在 mapper 上执行 Join</li><li>优化 Union，使Union只在 map 端执行</li><li>在多路 Join 中，根据用户提示决定最后流哪个表</li><li>删除不必要的 ReduceSinkOperators</li><li>对于带有Limit子句的查询，减少需要为该表扫描的文件数</li><li>对于带有Limit子句的查询，通过限制 ReduceSinkOperator 生成的内容来限制来自 mapper 的输出</li><li>减少用户提交的SQL查询所需的Tez作业数量</li><li>如果是简单的提取查询，避免使用MapReduce作业</li><li>对于带有聚合的简单获取查询，执行不带 MapReduce 任务的聚合</li><li>重写 Group By 查询使用索引表代替原来的表</li><li>当表扫描之上的谓词是相等谓词且谓词中的列具有索引时，使用索引扫描</li></ul><hr><p>经过以上六个阶段，SQL 就被解析映射成了集群上的 MapReduce 任务。</p><h3 id="SQL编译成MapReduce具体原理"><a href="#SQL编译成MapReduce具体原理" class="headerlink" title="SQL编译成MapReduce具体原理"></a>SQL编译成MapReduce具体原理</h3><p>在阶段五-生成物理执行计划，即遍历 OperatorTree，翻译为 MapReduce 任务，这个过程具体是怎么转化的呢</p><p>我们接下来举几个常用 SQL 语句转化为 MapReduce 的具体步骤：</p><h5 id="Join的实现原理"><a href="#Join的实现原理" class="headerlink" title="Join的实现原理"></a>Join的实现原理</h5><p>以下面这个SQL为例，讲解 join 的实现：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> u.name, o.orderid <span class="keyword">from</span> <span class="keyword">order</span> o <span class="keyword">join</span> <span class="keyword">user</span> u <span class="keyword">on</span> o.uid <span class="operator">=</span> u.uid;</span><br></pre></td></tr></table></figure><p>在map的输出value中为不同表的数据打上tag标记，在reduce阶段根据tag判断数据来源。MapReduce的过程如下：</p><div align=center><img src="MapReduce CommonJoin的实现.png"></div><h5 id="Group-By的实现原理"><a href="#Group-By的实现原理" class="headerlink" title="Group By的实现原理"></a>Group By的实现原理</h5><p>以下面这个SQL为例，讲解 group by 的实现：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> rank, isonline, <span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">from</span> city <span class="keyword">group</span> <span class="keyword">by</span> rank, isonline;</span><br></pre></td></tr></table></figure><p>将GroupBy的字段组合为map的输出key值，利用MapReduce的排序，在reduce阶段保存LastKey区分不同的key。MapReduce的过程如下:</p><div align=center><img src="Group By的实现原理.png"></div><h5 id="Distinct的实现原理"><a href="#Distinct的实现原理" class="headerlink" title="Distinct的实现原理"></a>Distinct的实现原理</h5><p>以下面这个SQL为例，讲解 distinct 的实现：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> dealid, <span class="built_in">count</span>(<span class="keyword">distinct</span> uid) num <span class="keyword">from</span> <span class="keyword">order</span> <span class="keyword">group</span> <span class="keyword">by</span> dealid;</span><br></pre></td></tr></table></figure><p>当只有一个distinct字段时，如果不考虑Map阶段的Hash GroupBy，只需要将GroupBy字段和Distinct字段组合为map输出key，利用mapreduce的排序，同时将GroupBy字段作为reduce的key，在reduce阶段保存LastKey即可完成去重:</p><div align=center><img src="Distinct的实现原理.png"></div><h2 id="九、Hive千亿级数据倾斜"><a href="#九、Hive千亿级数据倾斜" class="headerlink" title="九、Hive千亿级数据倾斜"></a>九、Hive千亿级数据倾斜</h2><h3 id="数据倾斜问题剖析"><a href="#数据倾斜问题剖析" class="headerlink" title="数据倾斜问题剖析"></a>数据倾斜问题剖析</h3><p>数据倾斜是分布式系统不可避免的问题，任何分布式系统都有几率发生数据倾斜，但有些小伙伴在平时工作中感知不是很明显，这里要注意本篇文章的标题—“千亿级数据”，<strong>为什么说千亿级</strong>，因为如果一个任务的数据量只有几百万，它即使发生了数据倾斜，所有数据都跑到一台机器去执行，对于几百万的数据量，一台机器执行起来还是毫无压力的，这时数据倾斜对我们感知不大，只有数据达到一个量级时，一台机器应付不了这么多的数据，这时如果发生数据倾斜，那么最后就很难算出结果。</p><p>所以就需要我们对数据倾斜的问题进行优化，尽量避免或减轻数据倾斜带来的影响。</p><blockquote><p>在解决数据倾斜问题之前，还要再提一句：没有瓶颈时谈论优化，都是自寻烦恼。</p></blockquote><p>大家想想，在map和reduce两个阶段中，最容易出现数据倾斜的就是reduce阶段，因为map到reduce会经过shuffle阶段，在shuffle中默认会按照key进行hash，<strong>如果相同的key过多，那么hash的结果就是大量相同的key进入到同一个reduce中</strong>，导致数据倾斜。</p><p>那么有没有可能在map阶段就发生数据倾斜呢，是有这种可能的。</p><p>一个任务中，数据文件在进入map阶段之前会进行切分，默认是128M一个数据块，但是如果<strong>当对文件使用GZIP压缩等不支持文件分割操作的压缩方式</strong>时，MR任务读取压缩后的文件时，是对它切分不了的，该压缩文件只会被一个任务所读取，如果有一个超大的不可切分的压缩文件被一个map读取时，就会发生map阶段的数据倾斜。</p><p>所以，从本质上来说，<strong>发生数据倾斜的原因有两种：一是任务中需要处理大量相同的key的数据。二是任务读取不可分割的大文件</strong>。</p><h3 id="数据倾斜解决方案"><a href="#数据倾斜解决方案" class="headerlink" title="数据倾斜解决方案"></a>数据倾斜解决方案</h3><p>MapReduce和Spark中的数据倾斜解决方案原理都是类似的，以下讨论Hive使用MapReduce引擎引发的数据倾斜，Spark数据倾斜也可以此为参照。</p><h3 id="1-空值引发的数据倾斜"><a href="#1-空值引发的数据倾斜" class="headerlink" title="1. 空值引发的数据倾斜"></a>1. 空值引发的数据倾斜</h3><p>实际业务中有些大量的null值或者一些无意义的数据参与到计算作业中，表中有大量的null值，如果表之间进行join操作，就会有shuffle产生，这样所有的null值都会被分配到一个reduce中，必然产生数据倾斜。</p><p>之前有小伙伴问，如果A、B两表join操作，假如A表中需要join的字段为null，但是B表中需要join的字段不为null，这两个字段根本就join不上啊，为什么还会放到一个reduce中呢？</p><p>这里我们需要明确一个概念，数据放到同一个reduce中的原因不是因为字段能不能join上，而是因为shuffle阶段的hash操作，只要key的hash结果是一样的，它们就会被拉到同一个reduce中。</p><p><strong>解决方案</strong>：</p><p>第一种：可以直接不让null值参与join操作，即不让null值有shuffle阶段</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> log a</span><br><span class="line"> <span class="keyword">JOIN</span> users b</span><br><span class="line"> <span class="keyword">ON</span> a.user_id <span class="keyword">IS</span> <span class="keyword">NOT</span> <span class="keyword">NULL</span></span><br><span class="line">  <span class="keyword">AND</span> a.user_id <span class="operator">=</span> b.user_id</span><br><span class="line"><span class="keyword">UNION</span> <span class="keyword">ALL</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> log a</span><br><span class="line"><span class="keyword">WHERE</span> a.user_id <span class="keyword">IS</span> <span class="keyword">NULL</span>;</span><br></pre></td></tr></table></figure><p>第二种：因为null值参与shuffle时的hash结果是一样的，那么我们可以给null值随机赋值，这样它们的hash结果就不一样，就会进到不同的reduce中：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> log a</span><br><span class="line"> <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> users b <span class="keyword">ON</span> <span class="keyword">CASE</span> </span><br><span class="line">   <span class="keyword">WHEN</span> a.user_id <span class="keyword">IS</span> <span class="keyword">NULL</span> <span class="keyword">THEN</span> concat(<span class="string">&#x27;hive_&#x27;</span>, rand())</span><br><span class="line">   <span class="keyword">ELSE</span> a.user_id</span><br><span class="line">  <span class="keyword">END</span> <span class="operator">=</span> b.user_id;</span><br></pre></td></tr></table></figure><h3 id="2-不同数据类型引发的数据倾斜"><a href="#2-不同数据类型引发的数据倾斜" class="headerlink" title="2. 不同数据类型引发的数据倾斜"></a>2. 不同数据类型引发的数据倾斜</h3><p>对于两个表join，表a中需要join的字段key为int，表b中key字段既有string类型也有int类型。当按照key进行两个表的join操作时，默认的Hash操作会按int型的id来进行分配，这样所有的string类型都被分配成同一个id，结果就是所有的string类型的字段进入到一个reduce中，引发数据倾斜。</p><p><strong>解决方案</strong>：</p><p>如果key字段既有string类型也有int类型，默认的hash就都会按int类型来分配，那我们直接把int类型都转为string就好了，这样key字段都为string，hash时就按照string类型分配了：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span></span><br><span class="line"><span class="keyword">FROM</span> users a</span><br><span class="line"> <span class="keyword">LEFT</span> <span class="keyword">JOIN</span> logs b <span class="keyword">ON</span> a.usr_id <span class="operator">=</span> <span class="built_in">CAST</span>(b.user_id <span class="keyword">AS</span> string);</span><br></pre></td></tr></table></figure><h3 id="3-不可拆分大文件引发的数据倾斜"><a href="#3-不可拆分大文件引发的数据倾斜" class="headerlink" title="3. 不可拆分大文件引发的数据倾斜"></a>3. 不可拆分大文件引发的数据倾斜</h3><p>当集群的数据量增长到一定规模，有些数据需要归档或者转储，这时候往往会对数据进行压缩；<strong>当对文件使用GZIP压缩等不支持文件分割操作的压缩方式，在日后有作业涉及读取压缩后的文件时，该压缩文件只会被一个任务所读取</strong>。如果该压缩文件很大，则处理该文件的Map需要花费的时间会远多于读取普通文件的Map时间，该Map任务会成为作业运行的瓶颈。这种情况也就是Map读取文件的数据倾斜。</p><p><strong>解决方案：</strong></p><p>这种数据倾斜问题没有什么好的解决方案，只能将使用GZIP压缩等不支持文件分割的文件转为bzip和zip等支持文件分割的压缩方式。</p><p>所以，<strong>我们在对文件进行压缩时，为避免因不可拆分大文件而引发数据读取的倾斜，在数据压缩的时候可以采用bzip2和Zip等支持文件分割的压缩算法</strong>。</p><h3 id="4-数据膨胀引发的数据倾斜"><a href="#4-数据膨胀引发的数据倾斜" class="headerlink" title="4. 数据膨胀引发的数据倾斜"></a>4. 数据膨胀引发的数据倾斜</h3><p>在多维聚合计算时，如果进行分组聚合的字段过多，如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a，b，c，count（<span class="number">1</span>）<span class="keyword">from</span> log <span class="keyword">group</span> <span class="keyword">by</span> a，b，c <span class="keyword">with</span> <span class="keyword">rollup</span>;</span><br></pre></td></tr></table></figure><blockquote><p>注：对于最后的<code>with rollup</code>关键字不知道大家用过没，with rollup是用来在分组统计数据的基础上再进行统计汇总，即用来得到group by的汇总信息。</p></blockquote><p>如果上面的log表的数据量很大，并且Map端的聚合不能很好地起到数据压缩的情况下，会导致Map端产出的数据急速膨胀，这种情况容易导致作业内存溢出的异常。如果log表含有数据倾斜key，会加剧Shuffle过程的数据倾斜。</p><p><strong>解决方案</strong>：</p><p>可以拆分上面的sql，将<code>with rollup</code>拆分成如下几个sql：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> a, b, c, <span class="built_in">COUNT</span>(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">FROM</span> log</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> a, b, c;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> a, b, <span class="keyword">NULL</span>, <span class="built_in">COUNT</span>(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">FROM</span> log</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> a, b;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> a, <span class="keyword">NULL</span>, <span class="keyword">NULL</span>, <span class="built_in">COUNT</span>(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">FROM</span> log</span><br><span class="line"><span class="keyword">GROUP</span> <span class="keyword">BY</span> a;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> <span class="keyword">NULL</span>, <span class="keyword">NULL</span>, <span class="keyword">NULL</span>, <span class="built_in">COUNT</span>(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">FROM</span> log;</span><br></pre></td></tr></table></figure><p>但是，上面这种方式不太好，因为现在是对3个字段进行分组聚合，那如果是5个或者10个字段呢，那么需要拆解的SQL语句会更多。</p><p>在Hive中可以通过参数 <code>hive.new.job.grouping.set.cardinality</code> 配置的方式自动控制作业的拆解，该参数默认值是30。表示针对grouping sets&#x2F;rollups&#x2F;cubes这类多维聚合的操作，如果最后拆解的键组合大于该值，会启用新的任务去处理大于该值之外的组合。如果在处理数据时，某个分组聚合的列有较大的倾斜，可以适当调小该值。</p><h3 id="5-表连接时引发的数据倾斜"><a href="#5-表连接时引发的数据倾斜" class="headerlink" title="5. 表连接时引发的数据倾斜"></a>5. 表连接时引发的数据倾斜</h3><p>两表进行普通的repartition join时，如果表连接的键存在倾斜，那么在 Shuffle 阶段必然会引起数据倾斜。</p><p><strong>解决方案</strong>：</p><p>通常做法是将倾斜的数据存到分布式缓存中，分发到各个 Map任务所在节点。在Map阶段完成join操作，即MapJoin，这避免了 Shuffle，从而避免了数据倾斜。</p><blockquote><p>MapJoin是Hive的一种优化操作，<strong>其适用于小表JOIN大表的场景</strong>，由于表的JOIN操作是在Map端且在内存进行的，所以其并不需要启动Reduce任务也就不需要经过shuffle阶段，从而能在一定程度上节省资源提高JOIN效率。</p></blockquote><p>在Hive 0.11版本之前，如果想在Map阶段完成join操作，必须使用MAPJOIN来标记显示地启动该优化操作，<strong>由于其需要将小表加载进内存所以要注意小表的大小</strong>。</p><p>如将a表放到Map端内存中执行，在Hive 0.11版本之前需要这样写：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="comment">/* +mapjoin(a) */</span> a.id , a.name, b.age </span><br><span class="line"><span class="keyword">from</span> a <span class="keyword">join</span> b </span><br><span class="line"><span class="keyword">on</span> a.id <span class="operator">=</span> b.id;</span><br></pre></td></tr></table></figure><p>如果想将多个表放到Map端内存中，只需在mapjoin()中写多个表名称即可，用逗号分隔，如将a表和c表放到Map端内存中，则 <code>/* +mapjoin(a,c) */</code> 。</p><p>在Hive 0.11版本及之后，Hive默认启动该优化，也就是不在需要显示的使用MAPJOIN标记，其会在必要的时候触发该优化操作将普通JOIN转换成MapJoin，可以通过以下两个属性来设置该优化的触发时机：</p><p><code>hive.auto.convert.join=true</code> 默认值为true，自动开启MAPJOIN优化。</p><p><code>hive.mapjoin.smalltable.filesize=2500000</code> 默认值为2500000(25M)，通过配置该属性来确定使用该优化的表的大小，如果表的大小小于此值就会被加载进内存中。</p><p><strong>注意</strong>：使用默认启动该优化的方式如果出现莫名其妙的BUG(比如MAPJOIN并不起作用)，就将以下两个属性置为fase手动使用MAPJOIN标记来启动该优化:</p><p><code>hive.auto.convert.join=false</code> (关闭自动MAPJOIN转换操作)</p><p><code>hive.ignore.mapjoin.hint=false</code> (不忽略MAPJOIN标记)</p><p>再提一句：将表放到Map端内存时，如果节点的内存很大，但还是出现内存溢出的情况，我们可以通过这个参数 <code>mapreduce.map.memory.mb</code> 调节Map端内存的大小。</p><h3 id="6-确实无法减少数据量引发的数据倾斜"><a href="#6-确实无法减少数据量引发的数据倾斜" class="headerlink" title="6. 确实无法减少数据量引发的数据倾斜"></a>6. 确实无法减少数据量引发的数据倾斜</h3><p>在一些操作中，我们没有办法减少数据量，如在使用 collect_list 函数时：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> s_age,collect_list(s_score) list_score</span><br><span class="line"><span class="keyword">from</span> student</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> s_age</span><br></pre></td></tr></table></figure><blockquote><p>collect_list：将分组中的某列转为一个数组返回。</p></blockquote><p>在上述sql中，s_age有数据倾斜，但如果数据量大到一定的数量，会导致处理倾斜的Reduce任务产生内存溢出的异常。</p><blockquote><p>collect_list输出一个数组，中间结果会放到内存中，所以如果collect_list聚合太多数据，会导致内存溢出。</p></blockquote><p>有小伙伴说这是 group by 分组引起的数据倾斜，可以开启<code>hive.groupby.skewindata</code>参数来优化。我们接下来分析下：</p><p>开启该配置会将作业拆解成两个作业，第一个作业会尽可能将Map的数据平均分配到Reduce阶段，并在这个阶段实现数据的预聚合，以减少第二个作业处理的数据量；第二个作业在第一个作业处理的数据基础上进行结果的聚合。</p><p><code>hive.groupby.skewindata</code>的核心作用在于生成的第一个作业能够有效减少数量。但是对于collect_list这类要求全量操作所有数据的中间结果的函数来说，明显起不到作用，反而因为引入新的作业增加了磁盘和网络I&#x2F;O的负担，而导致性能变得更为低下。</p><p><strong>解决方案</strong>：</p><p>这类问题最直接的方式就是调整reduce所执行的内存大小。</p><p>调整reduce的内存大小使用<code>mapreduce.reduce.memory.mb</code>这个配置。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>通过上面的内容我们发现，<strong>shuffle阶段堪称性能的杀手</strong>，为什么这么说，一方面shuffle阶段是最容易引起数据倾斜的；另一方面shuffle的过程中会产生大量的磁盘I&#x2F;O、网络I&#x2F;O 以及压缩、解压缩、序列化和反序列化等。这些操作都是严重影响性能的。</p><p>所以围绕shuffle和数据倾斜有很多的调优点：</p><ul><li>Mapper 端的Buffer 设置为多大？Buffer 设置得大，可提升性能，减少磁盘I&#x2F;O ，但 是对内存有要求，对GC 有压力；Buffer 设置得小，可能不占用那么多内存， 但是可能频繁的磁盘I&#x2F;O 、频繁的网络I&#x2F;O 。</li></ul><h2 id="十、Hive企业级性能优化"><a href="#十、Hive企业级性能优化" class="headerlink" title="十、Hive企业级性能优化"></a>十、Hive企业级性能优化</h2><h2 id="Hive性能问题排查的方式"><a href="#Hive性能问题排查的方式" class="headerlink" title="Hive性能问题排查的方式"></a>Hive性能问题排查的方式</h2><p>当我们发现一条SQL语句执行时间过长或者不合理时，我们就要考虑对SQL进行优化，优化首先得进行问题排查，那么我们可以通过哪些方式进行排查呢。</p><p>经常使用关系型数据库的同学可能知道关系型数据库的优化的诀窍-<strong>看执行计划</strong>。如Oracle数据库，它有多种类型的执行计划，通过多种执行计划的配合使用，可以看到根据统计信息推演的执行计划，即Oracle推断出来的未真正运行的执行计划；还可以看到实际执行任务的执行计划；能够观察到从数据读取到最终呈现的主要过程和中间的量化数据。可以说，在Oracle开发领域，掌握合适的环节，选用不同的执行计划，SQL调优就不是一件难事。</p><p>Hive中也有执行计划，但是Hive的执行计划都是预测的，这点不像Oracle和SQL Server有真实的计划，可以看到每个阶段的处理数据、消耗的资源和处理的时间等量化数据。Hive提供的执行计划没有这些数据，这意味着虽然Hive的使用者知道整个SQL的执行逻辑，但是各阶段耗用的资源状况和整个SQL的执行瓶颈在哪里是不清楚的。</p><p>想要知道HiveSQL所有阶段的运行信息，可以查看<strong>YARN提供的日志</strong>。查看日志的链接，可以在每个作业执行后，在控制台打印的信息中找到。如下图所示：</p><div align=center><img src="YARN提供的日志.png"></div><p><strong>Hive提供的执行计划目前可以查看的信息有以下几种</strong>：</p><ol><li>查看执行计划的基本信息，即explain；</li><li>查看执行计划的扩展信息，即explain extended；</li><li>查看SQL数据输入依赖的信息，即explain dependency；</li><li>查看SQL操作相关权限的信息，即explain authorization；</li><li>查看SQL的向量化描述信息，即explain vectorization。</li></ol><p>在查询语句的SQL前面加上关键字explain是查看执行计划的基本方法。用explain打开的执行计划包含以下两部分：</p><ul><li>作业的依赖关系图，即STAGE DEPENDENCIES；</li><li>每个作业的详细信息，即STAGE PLANS。</li></ul><p><strong>Hive中的explain执行计划详解可看我之前写的这篇文章</strong>：</p><p><a href="https://mp.weixin.qq.com/s?__biz=Mzg2MzU2MDYzOA==&mid=2247484152&idx=1&sn=7e48aa4a9650481f960c6cac234977a4&scene=21#wechat_redirect">Hive底层原理：explain执行计划详解</a></p><blockquote><p>注：使用explain查看执行计划是Hive性能调优中非常重要的一种方式，请务必掌握！</p></blockquote><p><strong>总结：Hive对SQL语句性能问题排查的方式</strong>：</p><ol><li>使用explain查看执行计划；</li><li>查看YARN提供的日志。</li></ol><h2 id="Hive性能调优的方式"><a href="#Hive性能调优的方式" class="headerlink" title="Hive性能调优的方式"></a>Hive性能调优的方式</h2><p>为什么都说性能优化这项工作是比较难的，因为一项技术的优化，必然是一项综合性的工作，它是多门技术的结合。我们如果只局限于一种技术，那么肯定做不好优化的。</p><p>下面将从多个完全不同的角度来介绍Hive优化的多样性，我们先来一起感受下。</p><h3 id="1-SQL语句优化"><a href="#1-SQL语句优化" class="headerlink" title="1. SQL语句优化"></a>1. SQL语句优化</h3><p>SQL语句优化涉及到的内容太多，因篇幅有限，不能一一介绍到，所以就拿几个典型举例，让大家学到这种思想，以后遇到类似调优问题可以往这几个方面多思考下。</p><h4 id="1-union-all"><a href="#1-union-all" class="headerlink" title="1. union all"></a>1. union all</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu <span class="keyword">partition</span>(tp) </span><br><span class="line"><span class="keyword">select</span> s_age,<span class="built_in">max</span>(s_birth) stat,<span class="string">&#x27;max&#x27;</span> tp </span><br><span class="line"><span class="keyword">from</span> stu_ori</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> s_age</span><br><span class="line"></span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu <span class="keyword">partition</span>(tp) </span><br><span class="line"><span class="keyword">select</span> s_age,<span class="built_in">min</span>(s_birth) stat,<span class="string">&#x27;min&#x27;</span> tp </span><br><span class="line"><span class="keyword">from</span> stu_ori</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> s_age;</span><br></pre></td></tr></table></figure><p>我们简单分析上面的SQl语句，就是将每个年龄的最大和最小的生日获取出来放到同一张表中，union all 前后的两个语句都是对同一张表按照s_age进行分组，然后分别取最大值和最小值。对同一张表相同的字段进行两次分组，这造成了极大浪费，我们能不能改造下呢，当然是可以的，为大家介绍一个语法：<code>from ... insert into ...</code> ，这个语法将from前置，作用就是使用一张表，可以进行多次插入操作：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">--开启动态分区 </span></span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition<span class="operator">=</span><span class="literal">true</span>; </span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode<span class="operator">=</span>nonstrict; </span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> stu_ori </span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu <span class="keyword">partition</span>(tp) </span><br><span class="line"><span class="keyword">select</span> s_age,<span class="built_in">max</span>(s_birth) stat,<span class="string">&#x27;max&#x27;</span> tp </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> s_age</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> stu <span class="keyword">partition</span>(tp) </span><br><span class="line"><span class="keyword">select</span> s_age,<span class="built_in">min</span>(s_birth) stat,<span class="string">&#x27;min&#x27;</span> tp </span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> s_age;</span><br></pre></td></tr></table></figure><p>上面的SQL就可以对stu_ori表的s_age字段分组一次而进行两次不同的插入操作。</p><p><strong>这个例子告诉我们一定要多了解SQL语句，如果我们不知道这种语法，一定不会想到这种方式的</strong>。</p><h4 id="2-distinct"><a href="#2-distinct" class="headerlink" title="2. distinct"></a>2. distinct</h4><p>先看一个SQL，去重计数：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="number">1</span>) </span><br><span class="line"><span class="keyword">from</span>( </span><br><span class="line">  <span class="keyword">select</span> s_age </span><br><span class="line">  <span class="keyword">from</span> stu </span><br><span class="line">  <span class="keyword">group</span> <span class="keyword">by</span> s_age </span><br><span class="line">) b;</span><br></pre></td></tr></table></figure><p>这是简单统计年龄的枚举值个数，为什么不用distinct？</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="keyword">distinct</span> s_age) </span><br><span class="line"><span class="keyword">from</span> stu;</span><br></pre></td></tr></table></figure><p>有人说因为在数据量特别大的情况下使用第一种方式能够有效避免Reduce端的数据倾斜，但是事实如此吗？</p><p>我们先不管数据量特别大这个问题，<strong>就当前的业务和环境下使用distinct一定会比上面那种子查询的方式效率高</strong>。原因有以下几点：</p><ol><li>上面进行去重的字段是年龄字段，要知道年龄的枚举值是非常有限的，就算计算1岁到100岁之间的年龄，s_age的最大枚举值才是100，如果转化成MapReduce来解释的话，在Map阶段，每个Map会对s_age去重。由于s_age枚举值有限，因而每个Map得到的s_age也有限，最终得到reduce的数据量也就是map数量*s_age枚举值的个数。</li><li>distinct的命令会在内存中构建一个hashtable，查找去重的时间复杂度是O(1)；group by在不同版本间变动比较大，有的版本会用构建hashtable的形式去重，有的版本会通过排序的方式， 排序最优时间复杂度无法到O(1)。另外，第一种方式(group by)去重会转化为两个任务，会消耗更多的磁盘网络I&#x2F;O资源。</li><li>最新的Hive 3.0中新增了 count(distinct ) 优化，通过配置 <code>hive.optimize.countdistinct</code>，即使真的出现数据倾斜也可以自动优化，自动改变SQL执行的逻辑。</li><li>第二种方式(distinct)比第一种方式(group by)代码简洁，表达的意思简单明了，如果没有特殊的问题，代码简洁就是优！</li></ol><p><strong>这个例子告诉我们，有时候我们不要过度优化，调优讲究适时调优，过早进行调优有可能做的是无用功甚至产生负效应，在调优上投入的工作成本和回报不成正比。调优需要遵循一定的原则</strong>。</p><h3 id="2-数据格式优化"><a href="#2-数据格式优化" class="headerlink" title="2. 数据格式优化"></a>2. 数据格式优化</h3><p>Hive提供了多种数据存储组织格式，不同格式对程序的运行效率也会有极大的影响。</p><p>Hive提供的格式有TEXT、SequenceFile、RCFile、ORC和Parquet等。</p><p>SequenceFile是一个二进制key&#x2F;value对结构的平面文件，在早期的Hadoop平台上被广泛用于MapReduce输出&#x2F;输出格式，以及作为数据存储格式。</p><p>Parquet是一种列式数据存储格式，可以兼容多种计算引擎，如MapRedcue和Spark等，对多层嵌套的数据结构提供了良好的性能支持，是目前Hive生产环境中数据存储的主流选择之一。</p><p>ORC优化是对RCFile的一种优化，它提供了一种高效的方式来存储Hive数据，同时也能够提高Hive的读取、写入和处理数据的性能，能够兼容多种计算引擎。事实上，在实际的生产环境中，ORC已经成为了Hive在数据存储上的主流选择之一。</p><p>我们使用同样数据及SQL语句，只是数据存储格式不同，得到如下执行时长：</p><table><thead><tr><th align="center">数据格式</th><th align="center">CPU时间</th><th align="center">用户等待耗时</th></tr></thead><tbody><tr><td align="center">TextFile</td><td align="center">33分</td><td align="center">171秒</td></tr><tr><td align="center">SequenceFile</td><td align="center">38分</td><td align="center">162秒</td></tr><tr><td align="center">Parquet</td><td align="center">2分22秒</td><td align="center">50秒</td></tr><tr><td align="center">ORC</td><td align="center">1分52秒</td><td align="center">56秒</td></tr></tbody></table><blockquote><p>注：CPU时间：表示运行程序所占用服务器CPU资源的时间。<br>用户等待耗时：记录的是用户从提交作业到返回结果期间用户等待的所有时间。</p></blockquote><p><strong>查询TextFile类型的数据表耗时33分钟， 查询ORC类型的表耗时1分52秒，时间得以极大缩短，可见不同的数据存储格式也能给HiveSQL性能带来极大的影响。</strong></p><h3 id="3-小文件过多优化"><a href="#3-小文件过多优化" class="headerlink" title="3. 小文件过多优化"></a>3. 小文件过多优化</h3><p>小文件如果过多，对 hive 来说，在进行查询时，每个小文件都会当成一个块，启动一个Map任务来完成，而一个Map任务启动和初始化的时间远远大于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的Map数量是受限的。</p><p>所以我们有必要对小文件过多进行优化，关于小文件过多的解决的办法，我之前专门写了一篇文章讲解，具体可查看：</p><p><a href="https://mp.weixin.qq.com/s?__biz=Mzg2MzU2MDYzOA==&mid=2247483683&idx=1&sn=14b25010032bdf0d375080e48de36d7f&scene=21#wechat_redirect">解决hive小文件过多问题</a></p><h3 id="4-并行执行优化"><a href="#4-并行执行优化" class="headerlink" title="4. 并行执行优化"></a>4. 并行执行优化</h3><p>Hive会将一个查询转化成一个或者多个阶段。这样的阶段可以是MapReduce阶段、抽样阶段、合并阶段、limit阶段。或者Hive执行过程中可能需要的其他阶段。默认情况下，Hive一次只会执行一个阶段。不过，某个特定的job可能包含众多的阶段，而这些阶段可能并非完全互相依赖的，也就是说有些阶段是可以并行执行的，这样可能使得整个job的执行时间缩短。如果有更多的阶段可以并行执行，那么job可能就越快完成。</p><p>通过设置参数hive.exec.parallel值为true，就可以开启并发执行。在共享集群中，需要注意下，如果job中并行阶段增多，那么集群利用率就会增加。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.exec.parallel<span class="operator">=</span><span class="literal">true</span>; <span class="operator">/</span><span class="operator">/</span>打开任务并行执行</span><br><span class="line"><span class="keyword">set</span> hive.exec.parallel.thread.number<span class="operator">=</span><span class="number">16</span>; <span class="operator">/</span><span class="operator">/</span>同一个<span class="keyword">sql</span>允许最大并行度，默认为<span class="number">8</span>。</span><br></pre></td></tr></table></figure><p>当然得是在系统资源比较空闲的时候才有优势，否则没资源，并行也起不来。</p><h3 id="5-JVM优化"><a href="#5-JVM优化" class="headerlink" title="5. JVM优化"></a>5. JVM优化</h3><p>JVM重用是Hadoop调优参数的内容，其对Hive的性能具有非常大的影响，特别是对于很难避免小文件的场景或task特别多的场景，这类场景大多数执行时间都很短。</p><p>Hadoop的默认配置通常是使用派生JVM来执行map和Reduce任务的。这时JVM的启动过程可能会造成相当大的开销，尤其是执行的job包含有成百上千task任务的情况。JVM重用可以使得JVM实例在同一个job中重新使用N次。N的值可以在Hadoop的<code>mapred-site.xml</code>文件中进行配置。通常在10-20之间，具体多少需要根据具体业务场景测试得出。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.job.jvm.numtasks<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>10<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>How many tasks to run per jvm. If set to -1, there is</span><br><span class="line">  no limit. </span><br><span class="line">  <span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>我们也可以在hive中设置</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span>  mapred.job.reuse.jvm.num.tasks<span class="operator">=</span><span class="number">10</span>; <span class="operator">/</span><span class="operator">/</span>这个设置来设置我们的jvm重用</span><br></pre></td></tr></table></figure><p>这个功能的缺点是，开启JVM重用将一直占用使用到的task插槽，以便进行重用，直到任务完成后才能释放。如果某个“不平衡的”job中有某几个reduce task执行的时间要比其他Reduce task消耗的时间多的多的话，那么保留的插槽就会一直空闲着却无法被其他的job使用，直到所有的task都结束了才会释放。</p><h3 id="6-推测执行优化"><a href="#6-推测执行优化" class="headerlink" title="6. 推测执行优化"></a>6. 推测执行优化</h3><p>在分布式集群环境下，因为程序Bug（包括Hadoop本身的bug），负载不均衡或者资源分布不均等原因，会造成同一个作业的多个任务之间运行速度不一致，有些任务的运行速度可能明显慢于其他任务（比如一个作业的某个任务进度只有50%，而其他所有任务已经运行完毕），则这些任务会拖慢作业的整体执行进度。为了避免这种情况发生，Hadoop采用了推测执行（Speculative Execution）机制，它根据一定的法则推测出“拖后腿”的任务，并为这样的任务启动一个备份任务，让该任务与原始任务同时处理同一份数据，并最终选用最先成功运行完成任务的计算结果作为最终结果。</p><p>设置开启推测执行参数：Hadoop的<code>mapred-site.xml</code>文件中进行配置：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some map tasks </span><br><span class="line">               may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.speculative<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>If true, then multiple instances of some reduce tasks </span><br><span class="line">               may be executed in parallel.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>hive本身也提供了配置项来控制reduce-side的推测执行:</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.mapred.reduce.tasks.speculative.execution<span class="operator">=</span><span class="literal">true</span></span><br></pre></td></tr></table></figure><p>关于调优这些推测执行变量，还很难给一个具体的建议。如果用户对于运行时的偏差非常敏感的话，那么可以将这些功能关闭掉。如果用户因为输入数据量很大而需要执行长时间的map或者Reduce task的话，那么启动推测执行造成的浪费是非常巨大大。</p><h4 id="最后-1"><a href="#最后-1" class="headerlink" title="最后"></a>最后</h4><p>代码优化原则：</p><ul><li>理透需求原则，这是优化的根本；</li><li>把握数据全链路原则，这是优化的脉络；</li><li>坚持代码的简洁原则，这让优化更加简单；</li><li>没有瓶颈时谈论优化，这是自寻烦恼。</li></ul><h2 id="十一、Hive大厂面试真题"><a href="#十一、Hive大厂面试真题" class="headerlink" title="十一、Hive大厂面试真题"></a>十一、Hive大厂面试真题</h2><h3 id="1-hive内部表和外部表的区别"><a href="#1-hive内部表和外部表的区别" class="headerlink" title="1. hive内部表和外部表的区别"></a>1. hive内部表和外部表的区别</h3><p>未被external修饰的是内部表，被external修饰的为外部表。</p><blockquote><p>本文首发于公众号【五分钟学大数据】，关注公众号，获取最新大数据技术文章</p></blockquote><p><strong>区别</strong>：</p><ol><li>内部表数据由Hive自身管理，外部表数据由HDFS管理；</li><li>内部表数据存储的位置是<code>hive.metastore.warehouse.dir</code>（默认：<code>/user/hive/warehouse</code>），外部表数据的存储位置由自己制定（如果没有LOCATION，Hive将在HDFS上的<code>/user/hive/warehouse</code>文件夹下以外部表的表名创建一个文件夹，并将属于这个表的数据存放在这里）；</li><li><strong>删除内部表会直接删除元数据（metadata）及存储数据；删除外部表仅仅会删除元数据，HDFS上的文件并不会被删除</strong>。</li></ol><blockquote><p>本文首发于公众号【五分钟学大数据】</p></blockquote><h3 id="2-Hive有索引吗"><a href="#2-Hive有索引吗" class="headerlink" title="2. Hive有索引吗"></a>2. Hive有索引吗</h3><p>Hive支持索引（3.0版本之前），但是Hive的索引与关系型数据库中的索引并不相同，比如，Hive不支持主键或者外键。并且Hive索引提供的功能很有限，效率也并不高，因此Hive索引很少使用。</p><ul><li>索引适用的场景：</li></ul><p>适用于不更新的静态字段。以免总是重建索引数据。每次建立、更新数据后，都要重建索引以构建索引表。</p><ul><li>Hive索引的机制如下：</li></ul><p>hive在指定列上建立索引，会产生一张索引表（Hive的一张物理表），里面的字段包括：索引列的值、该值对应的HDFS文件路径、该值在文件中的偏移量。</p><p>Hive 0.8版本后引入bitmap索引处理器，这个处理器适用于去重后，值较少的列（例如，某字段的取值只可能是几个枚举值） 因为索引是用空间换时间，索引列的取值过多会导致建立bitmap索引表过大。</p><p><strong>注意</strong>：Hive中每次有数据时需要及时更新索引，相当于重建一个新表，否则会影响数据查询的效率和准确性，<strong>Hive官方文档已经明确表示Hive的索引不推荐被使用，在新版本的Hive中已经被废弃了</strong>。</p><p><strong>扩展</strong>：Hive是在0.7版本之后支持索引的，在0.8版本后引入bitmap索引处理器，在3.0版本开始移除索引的功能，取而代之的是2.3版本开始的物化视图，自动重写的物化视图替代了索引的功能。</p><h3 id="3-运维如何对hive进行调度"><a href="#3-运维如何对hive进行调度" class="headerlink" title="3. 运维如何对hive进行调度"></a>3. 运维如何对hive进行调度</h3><ol><li>将hive的sql定义在脚本当中；</li><li>使用azkaban或者oozie进行任务的调度；</li><li>监控任务调度页面。</li></ol><h3 id="4-ORC、Parquet等列式存储的优点"><a href="#4-ORC、Parquet等列式存储的优点" class="headerlink" title="4. ORC、Parquet等列式存储的优点"></a>4. ORC、Parquet等列式存储的优点</h3><p>ORC和Parquet都是高性能的存储方式，这两种存储格式总会带来存储和性能上的提升。</p><p><strong>Parquet</strong>:</p><ol><li>Parquet支持嵌套的数据模型，类似于Protocol Buffers，每一个数据模型的schema包含多个字段，每一个字段有三个属性：重复次数、数据类型和字段名。<br>重复次数可以是以下三种：required(只出现1次)，repeated(出现0次或多次)，optional(出现0次或1次)。每一个字段的数据类型可以分成两种：group(复杂类型)和primitive(基本类型)。</li><li>Parquet中没有Map、Array这样的复杂数据结构，但是可以通过repeated和group组合来实现的。</li><li>由于Parquet支持的数据模型比较松散，可能一条记录中存在比较深的嵌套关系，如果为每一条记录都维护一个类似的树状结可能会占用较大的存储空间，因此Dremel论文中提出了一种高效的对于嵌套数据格式的压缩算法：Striping&#x2F;Assembly算法。通过Striping&#x2F;Assembly算法，parquet可以使用较少的存储空间表示复杂的嵌套格式，并且通常Repetition level和Definition level都是较小的整数值，可以通过RLE算法对其进行压缩，进一步降低存储空间。</li><li>Parquet文件是以二进制方式存储的，是不可以直接读取和修改的，Parquet文件是自解析的，文件中包括该文件的数据和元数据。</li></ol><p><strong>ORC</strong>:</p><ol><li>ORC文件是自描述的，它的元数据使用Protocol Buffers序列化，并且文件中的数据尽可能的压缩以降低存储空间的消耗。</li><li>和Parquet类似，ORC文件也是以二进制方式存储的，所以是不可以直接读取，ORC文件也是自解析的，它包含许多的元数据，这些元数据都是同构ProtoBuffer进行序列化的。</li><li>ORC会尽可能合并多个离散的区间尽可能的减少I&#x2F;O次数。</li><li>ORC中使用了更加精确的索引信息，使得在读取数据时可以指定从任意一行开始读取，更细粒度的统计信息使得读取ORC文件跳过整个row group，ORC默认会对任何一块数据和索引信息使用ZLIB压缩，因此ORC文件占用的存储空间也更小。</li><li>在新版本的ORC中也加入了对Bloom Filter的支持，它可以进一 步提升谓词下推的效率，在Hive 1.2.0版本以后也加入了对此的支 持。</li></ol><h3 id="5-数据建模用的哪些模型？"><a href="#5-数据建模用的哪些模型？" class="headerlink" title="5. 数据建模用的哪些模型？"></a>5. 数据建模用的哪些模型？</h3><h5 id="1-星型模型"><a href="#1-星型模型" class="headerlink" title="1. 星型模型"></a>1. 星型模型</h5><div align=center><img src="星型模型.png"></div><p>星形模式(Star Schema)是最常用的维度建模方式。星型模式是以事实表为中心，所有的维度表直接连接在事实表上，像星星一样。星形模式的维度建模由一个事实表和一组维表成，且具有以下特点：</p><p>a. 维表只和事实表关联，维表之间没有关联；</p><p>b. 每个维表主键为单列，且该主键放置在事实表中，作为两边连接的外键；</p><p>c. 以事实表为核心，维表围绕核心呈星形分布。</p><h5 id="2-雪花模型"><a href="#2-雪花模型" class="headerlink" title="2. 雪花模型"></a>2. 雪花模型</h5><div align=center><img src="雪花模型.png"></div><p>雪花模式(Snowflake Schema)是对星形模式的扩展。<strong>雪花模式的维度表可以拥有其他维度表的</strong>，虽然这种模型相比星型更规范一些，但是由于这种模型不太容易理解，维护成本比较高，而且性能方面需要关联多层维表，性能比星型模型要低。</p><h5 id="3-星座模型"><a href="#3-星座模型" class="headerlink" title="3. 星座模型"></a>3. 星座模型</h5><div align=center><img src="星座模型.png"></div><p>星座模式是星型模式延伸而来，星型模式是基于一张事实表的，而<strong>星座模式是基于多张事实表的，而且共享维度信息</strong>。前面介绍的两种维度建模方法都是多维表对应单事实表，但在很多时候维度空间内的事实表不止一个，而一个维表也可能被多个事实表用到。在业务发展后期，绝大部分维度建模都采用的是星座模式。</p><p>数仓建模详细介绍可查看：<a href="https://mp.weixin.qq.com/s?__biz=Mzg2MzU2MDYzOA==&mid=2247485022&idx=1&sn=92890bec74a7ee61ea1b2dbf89f19ada&scene=21#wechat_redirect">通俗易懂数仓建模</a></p><h3 id="6-为什么要对数据仓库分层？"><a href="#6-为什么要对数据仓库分层？" class="headerlink" title="6. 为什么要对数据仓库分层？"></a>6. 为什么要对数据仓库分层？</h3><ul><li><strong>用空间换时间</strong>，通过大量的预处理来提升应用系统的用户体验（效率），因此数据仓库会存在大量冗余的数据。</li><li>如果不分层的话，如果源业务系统的业务规则发生变化将会影响整个数据清洗过程，工作量巨大。</li><li><strong>通过数据分层管理可以简化数据清洗的过程</strong>，因为把原来一步的工作分到了多个步骤去完成，相当于把一个复杂的工作拆成了多个简单的工作，把一个大的黑盒变成了一个白盒，每一层的处理逻辑都相对简单和容易理解，这样我们比较容易保证每一个步骤的正确性，当数据发生错误的时候，往往我们只需要局部调整某个步骤即可。</li></ul><p>数据仓库详细介绍可查看：<a href="https://mp.weixin.qq.com/s?__biz=Mzg2MzU2MDYzOA==&mid=2247484692&idx=1&sn=f624672e62ba6cd4cc69bdb6db28756a&scene=21#wechat_redirect">万字详解整个数据仓库建设体系</a></p><h3 id="7-使用过Hive解析JSON串吗"><a href="#7-使用过Hive解析JSON串吗" class="headerlink" title="7. 使用过Hive解析JSON串吗"></a>7. 使用过Hive解析JSON串吗</h3><p><strong>Hive处理json数据总体来说有两个方向的路走</strong>：</p><ol><li>将json以字符串的方式整个入Hive表，然后通过使用UDF函数解析已经导入到hive中的数据，比如使用<code>LATERAL VIEW json_tuple</code>的方法，获取所需要的列名。</li><li>在导入之前将json拆成各个字段，导入Hive表的数据是已经解析过的。这将需要使用第三方的 SerDe。</li></ol><p>详细介绍可查看：<a href="https://mp.weixin.qq.com/s?__biz=Mzg2MzU2MDYzOA==&mid=2247485175&idx=1&sn=63f58dc2946678d4e50eb6e7bb3ff745&scene=21#wechat_redirect">Hive解析Json数组超全讲解</a></p><h3 id="8-sort-by-和-order-by-的区别"><a href="#8-sort-by-和-order-by-的区别" class="headerlink" title="8. sort by 和 order by 的区别"></a>8. sort by 和 order by 的区别</h3><p><strong>order by 会对输入做全局排序，因此只有一个reducer</strong>（多个reducer无法保证全局有序）只有一个reducer，会导致当输入规模较大时，需要较长的计算时间。</p><p>sort by不是全局排序，其在数据进入reducer前完成排序. 因此，如果用sort by进行排序，并且设置mapred.reduce.tasks&gt;1， 则<strong>sort by只保证每个reducer的输出有序，不保证全局有序</strong>。</p><h3 id="9-数据倾斜怎么解决"><a href="#9-数据倾斜怎么解决" class="headerlink" title="9. 数据倾斜怎么解决"></a>9. 数据倾斜怎么解决</h3><p>数据倾斜问题主要有以下几种：</p><ol><li>空值引发的数据倾斜</li><li>不同数据类型引发的数据倾斜</li><li>不可拆分大文件引发的数据倾斜</li><li>数据膨胀引发的数据倾斜</li><li>表连接时引发的数据倾斜</li><li>确实无法减少数据量引发的数据倾斜</li></ol><p>以上倾斜问题的具体解决方案可查看：<a href="https://mp.weixin.qq.com/s?__biz=Mzg2MzU2MDYzOA==&mid=2247485154&idx=1&sn=cd7129544497c1a621e49dbc1d7ed5c3&scene=21#wechat_redirect">Hive千亿级数据倾斜解决方案</a></p><p><strong>注意</strong>：对于 left join 或者 right join 来说，不会对关联的字段自动去除null值，对于 inner join 来说，会对关联的字段自动去除null值。</p><p>小伙伴们在阅读时注意下，在上面的文章（Hive千亿级数据倾斜解决方案）中，有一处sql出现了上述问题（举例的时候原本是想使用left join的，结果手误写成了join）。此问题由公众号读者发现，感谢这位读者指正。</p><h3 id="10-Hive-小文件过多怎么解决"><a href="#10-Hive-小文件过多怎么解决" class="headerlink" title="10. Hive 小文件过多怎么解决"></a>10. Hive 小文件过多怎么解决</h3><h5 id="1-使用-hive-自带的-concatenate-命令，自动合并小文件"><a href="#1-使用-hive-自带的-concatenate-命令，自动合并小文件" class="headerlink" title="1. 使用 hive 自带的 concatenate 命令，自动合并小文件"></a>1. 使用 hive 自带的 concatenate 命令，自动合并小文件</h5><p>使用方法：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#对于非分区表</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> A concatenate;</span><br><span class="line"></span><br><span class="line">#对于分区表</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> B <span class="keyword">partition</span>(<span class="keyword">day</span><span class="operator">=</span><span class="number">20201224</span>) concatenate;</span><br></pre></td></tr></table></figure><blockquote><p>注意：<br>1、concatenate 命令只支持 RCFILE 和 ORC 文件类型。<br>2、使用concatenate命令合并小文件时不能指定合并后的文件数量，但可以多次执行该命令。<br>3、当多次使用concatenate后文件数量不在变化，这个跟参数 mapreduce.input.fileinputformat.split.minsize&#x3D;256mb 的设置有关，可设定每个文件的最小size。</p></blockquote><h5 id="2-调整参数减少Map数量"><a href="#2-调整参数减少Map数量" class="headerlink" title="2. 调整参数减少Map数量"></a>2. 调整参数减少Map数量</h5><p>设置map输入合并小文件的相关参数（执行Map前进行小文件合并）：</p><p>在mapper中将多个文件合成一个split作为输入（<code>CombineHiveInputFormat</code>底层是Hadoop的<code>CombineFileInputFormat</code>方法）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; <span class="comment">-- 默认</span></span><br></pre></td></tr></table></figure><p>每个Map最大输入大小（这个值决定了合并后文件的数量）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.max.split.size<span class="operator">=</span><span class="number">256000000</span>;   <span class="comment">-- 256M</span></span><br></pre></td></tr></table></figure><p>一个节点上split的至少大小（这个值决定了多个DataNode上的文件是否需要合并）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node<span class="operator">=</span><span class="number">100000000</span>;  <span class="comment">-- 100M</span></span><br></pre></td></tr></table></figure><p>一个交换机下split的至少大小(这个值决定了多个交换机上的文件是否需要合并)：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack<span class="operator">=</span><span class="number">100000000</span>;  <span class="comment">-- 100M</span></span><br></pre></td></tr></table></figure><h5 id="3-减少Reduce的数量"><a href="#3-减少Reduce的数量" class="headerlink" title="3. 减少Reduce的数量"></a>3. 减少Reduce的数量</h5><p>reduce 的个数决定了输出的文件的个数，所以可以调整reduce的个数控制hive表的文件数量。</p><p>hive中的分区函数 distribute by 正好是控制MR中partition分区的，可以通过设置reduce的数量，结合分区函数让数据均衡的进入每个reduce即可：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#设置reduce的数量有两种方式，第一种是直接设置reduce个数</span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">10</span>;</span><br><span class="line"></span><br><span class="line">#第二种是设置每个reduce的大小，Hive会根据数据总大小猜测确定一个reduce个数</span><br><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer<span class="operator">=</span><span class="number">5120000000</span>; <span class="comment">-- 默认是1G，设置为5G</span></span><br><span class="line"></span><br><span class="line">#执行以下语句，将数据均衡的分配到reduce中</span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">10</span>;</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> A <span class="keyword">partition</span>(dt)</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> B</span><br><span class="line">distribute <span class="keyword">by</span> rand();</span><br></pre></td></tr></table></figure><p>对于上述语句解释：如设置reduce数量为10，使用 rand()， 随机生成一个数 <code>x % 10</code> ， 这样数据就会随机进入 reduce 中，防止出现有的文件过大或过小。</p><h5 id="4-使用hadoop的archive将小文件归档"><a href="#4-使用hadoop的archive将小文件归档" class="headerlink" title="4. 使用hadoop的archive将小文件归档"></a>4. 使用hadoop的archive将小文件归档</h5><p>Hadoop Archive简称HAR，是一个高效地将小文件放入HDFS块中的文件存档工具，它能够将多个小文件打包成一个HAR文件，这样在减少namenode内存使用的同时，仍然允许对文件进行透明的访问。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#用来控制归档是否可用</span><br><span class="line"><span class="keyword">set</span> hive.archive.enabled<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line">#通知Hive在创建归档时是否可以设置父目录</span><br><span class="line"><span class="keyword">set</span> hive.archive.har.parentdir.settable<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line">#控制需要归档文件的大小</span><br><span class="line"><span class="keyword">set</span> har.partfile.size<span class="operator">=</span><span class="number">1099511627776</span>;</span><br><span class="line"></span><br><span class="line">使用以下命令进行归档：</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> A ARCHIVE <span class="keyword">PARTITION</span>(dt<span class="operator">=</span><span class="string">&#x27;2021-05-07&#x27;</span>, hr<span class="operator">=</span><span class="string">&#x27;12&#x27;</span>);</span><br><span class="line"></span><br><span class="line">对已归档的分区恢复为原文件：</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> A UNARCHIVE <span class="keyword">PARTITION</span>(dt<span class="operator">=</span><span class="string">&#x27;2021-05-07&#x27;</span>, hr<span class="operator">=</span><span class="string">&#x27;12&#x27;</span>);</span><br></pre></td></tr></table></figure><blockquote><p>注意:<br><strong>归档的分区可以查看不能 insert overwrite，必须先 unarchive</strong></p></blockquote><p>Hive 小文件问题具体可查看：<a href="https://mp.weixin.qq.com/s?__biz=Mzg2MzU2MDYzOA==&mid=2247483683&idx=1&sn=14b25010032bdf0d375080e48de36d7f&chksm=ce77f7f2f9007ee49d773367f2d7abbf708a2e0e18a55794fcd797944ac73ccc88a41d253d6b&token=1679639512&lang=zh_CN&scene=21#wechat_redirect">解决hive小文件过多问题</a></p><h3 id="11-Hive优化有哪些"><a href="#11-Hive优化有哪些" class="headerlink" title="11. Hive优化有哪些"></a>11. Hive优化有哪些</h3><h5 id="1-数据存储及压缩："><a href="#1-数据存储及压缩：" class="headerlink" title="1. 数据存储及压缩："></a>1. 数据存储及压缩：</h5><p>针对hive中表的存储格式通常有orc和parquet，压缩格式一般使用snappy。相比与textfile格式表，orc占有更少的存储。因为hive底层使用MR计算架构，数据流是hdfs到磁盘再到hdfs，而且会有很多次，所以使用orc数据格式和snappy压缩策略可以降低IO读写，还能降低网络传输量，这样在一定程度上可以节省存储，还能提升hql任务执行效率；</p><h5 id="2-通过调参优化："><a href="#2-通过调参优化：" class="headerlink" title="2. 通过调参优化："></a>2. 通过调参优化：</h5><p>并行执行，调节parallel参数；</p><p>调节jvm参数，重用jvm；</p><p>设置map、reduce的参数；开启strict mode模式；</p><p>关闭推测执行设置。</p><h5 id="3-有效地减小数据集将大表拆分成子表；结合使用外部表和分区表。"><a href="#3-有效地减小数据集将大表拆分成子表；结合使用外部表和分区表。" class="headerlink" title="3. 有效地减小数据集将大表拆分成子表；结合使用外部表和分区表。"></a>3. 有效地减小数据集将大表拆分成子表；结合使用外部表和分区表。</h5><h5 id="4-SQL优化"><a href="#4-SQL优化" class="headerlink" title="4. SQL优化"></a>4. SQL优化</h5><ul><li>大表对大表：尽量减少数据集，可以通过分区表，避免扫描全表或者全字段；</li><li>大表对小表：设置自动识别小表，将小表放入内存中去执行。</li></ul><p>Hive优化详细剖析可查看：<a href="https://mp.weixin.qq.com/s?__biz=Mzg2MzU2MDYzOA==&mid=2247485060&idx=1&sn=d2938aca9680dc45a64c02db29ecd252&scene=21#wechat_redirect">Hive企业级性能优化</a></p><h2 id="附：九个最易出错的SQL讲解"><a href="#附：九个最易出错的SQL讲解" class="headerlink" title="附：九个最易出错的SQL讲解"></a>附：九个最易出错的SQL讲解</h2><p><strong>阅读本节小建议：本文适合细嚼慢咽，不要一目十行，不然会错过很多有价值的细节。</strong></p><p>在进行数仓搭建和数据分析时最常用的就是 sql，其语法简洁明了，易于理解，目前大数据领域的几大主流框架全部都支持sql语法，包括 hive，spark，flink等，所以sql在大数据领域有着不可替代的作用，需要我们重点掌握。</p><p>在使用sql时如果不熟悉或不仔细，那么在进行查询分析时极容易出错，接下来我们就来看下几个容易出错的sql语句及使用注意事项。</p><h4 id="1-decimal"><a href="#1-decimal" class="headerlink" title="1. decimal"></a>1. decimal</h4><p>hive 除了支持 int,double,string等常用类型，也支持 decimal 类型，用于在数据库中存储精确的数值，常用在表示金额的字段上</p><p><strong>注意事项：</strong></p><p>如：decimal(11,2) 代表最多有11位数字，其中后2位是小数，整数部分是9位；<br>如果<strong>整数部分超过9位，则这个字段就会变成null，如果整数部分不超过9位，则原字段显示</strong>；<br>如果<strong>小数部分不足2位，则后面用0补齐两位，如果小数部分超过两位，则超出部分四舍五入</strong>；<br>也可直接写 decimal，后面不指定位数，默认是 decimal(10,0) 整数10位，没有小数</p><h4 id="2-location"><a href="#2-location" class="headerlink" title="2. location"></a>2. location</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">表创建的时候可以用 location 指定一个文件或者文件夹</span><br><span class="line"><span class="keyword">create</span>  <span class="keyword">table</span> stu(id <span class="type">int</span> ,name string)  location <span class="string">&#x27;/user/stu2&#x27;</span>;</span><br></pre></td></tr></table></figure><p><strong>注意事项：</strong></p><p>创建表时使用location， 当<strong>指定文件夹时，hive会加载文件夹下的所有文件，当表中无分区时，这个文件夹下不能再有文件夹，否则报错。</strong><br>当表是分区表时，比如 partitioned by (day string)， 则这个文件夹下的每一个文件夹就是一个分区，且文件夹名为 day&#x3D;20201123 这种格式，然后使用：<strong>msck  repair  table  score</strong>; 修复表结构，成功之后即可看到数据已经全部加载到表当中去了</p><h4 id="3-load-data-和-load-data-local"><a href="#3-load-data-和-load-data-local" class="headerlink" title="3. load data 和 load data local"></a>3. load data 和 load data local</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">从hdfs上加载文件</span><br><span class="line">load data inpath <span class="string">&#x27;/hivedatas/techer.csv&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> techer;</span><br><span class="line"></span><br><span class="line">从本地系统加载文件</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/user/test/techer.csv&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> techer;</span><br></pre></td></tr></table></figure><p><strong>注意事项：</strong></p><ol><li>使用 load data local 表示<strong>从本地文件系统加载，文件会拷贝到hdfs上</strong></li><li>使用 load data 表示<strong>从hdfs文件系统加载，文件会直接移动到hive相关目录下</strong>，注意不是拷贝过去，因为hive认为hdfs文件已经有3副本了，没必要再次拷贝了</li><li>如果表是分区表，load 时不指定分区会报错</li><li>如果加载相同文件名的文件，会被自动重命名</li></ol><h4 id="4-drop-和-truncate"><a href="#4-drop-和-truncate" class="headerlink" title="4. drop 和 truncate"></a>4. drop 和 truncate</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">删除表操作</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> score1;</span><br><span class="line"></span><br><span class="line">清空表操作</span><br><span class="line"><span class="keyword">truncate</span> <span class="keyword">table</span> score2;</span><br></pre></td></tr></table></figure><p><strong>注意事项：</strong></p><p>如果 <strong>hdfs 开启了回收站，drop 删除的表数据是可以从回收站恢复的</strong>，表结构恢复不了，需要自己重新创建；<strong>truncate 清空的表是不进回收站的，所以无法恢复truncate清空的表。</strong><br>所以 truncate 一定慎用，一旦清空除物理恢复外将无力回天</p><h4 id="5-join-连接"><a href="#5-join-连接" class="headerlink" title="5. join 连接"></a>5. join 连接</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">INNER</span> <span class="keyword">JOIN</span> 内连接：只有进行连接的两个表中都存在与连接条件相匹配的数据才会被保留下来</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> techer t [<span class="keyword">inner</span>] <span class="keyword">join</span> course c <span class="keyword">on</span> t.t_id <span class="operator">=</span> c.t_id; <span class="comment">-- inner 可省略</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">LEFT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> 左外连接：左边所有数据会被返回，右边符合条件的被返回</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> techer t <span class="keyword">left</span> <span class="keyword">join</span> course c <span class="keyword">on</span> t.t_id <span class="operator">=</span> c.t_id; <span class="comment">-- outer可省略</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">RIGHT</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> 右外连接：右边所有数据会被返回，左边符合条件的被返回、</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> techer t <span class="keyword">right</span> <span class="keyword">join</span> course c <span class="keyword">on</span> t.t_id <span class="operator">=</span> c.t_id;</span><br><span class="line"></span><br><span class="line"><span class="keyword">FULL</span> <span class="keyword">OUTER</span> <span class="keyword">JOIN</span> 满外(全外)连接: 将会返回所有表中符合条件的所有记录。如果任一表的指定字段没有符合条件的值的话，那么就使用<span class="keyword">NULL</span>值替代。</span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> techer t <span class="keyword">FULL</span> <span class="keyword">JOIN</span> course c <span class="keyword">ON</span> t.t_id <span class="operator">=</span> c.t_id ;</span><br></pre></td></tr></table></figure><p><strong>注意事项：</strong></p><ol><li>hive2版本已经支持不等值连接，就是 <strong>join on条件后面可以使用大于小于符号;并且也支持 join on 条件后跟or</strong> (早前版本 on 后只支持 &#x3D; 和 and，不支持 &gt; &lt; 和 or)</li><li>如hive执行引擎使用MapReduce，一个join就会启动一个job，一条sql语句中如有多个join，则会启动多个job</li></ol><p><strong>注意</strong>：表之间用逗号(,)连接和 inner join 是一样的，例：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> tableA.id, tableB.name <span class="keyword">from</span> tableA , tableB <span class="keyword">where</span> tableA.id<span class="operator">=</span>tableB.id;   </span><br><span class="line">和   </span><br><span class="line"><span class="keyword">select</span> tableA.id, tableB.name <span class="keyword">from</span> tableA <span class="keyword">join</span> tableB <span class="keyword">on</span> tableA.id<span class="operator">=</span>tableB.id;   </span><br></pre></td></tr></table></figure><p><strong>它们的执行效率没有区别，只是书写方式不同</strong>，用逗号是sql 89标准，join 是sql 92标准。用逗号连接后面过滤条件用 where ，用 join 连接后面过滤条件是 on。</p><h4 id="6-left-semi-join"><a href="#6-left-semi-join" class="headerlink" title="6. left semi join"></a>6. left semi join</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">为什么把这个单独拿出来说，因为它和其他的 <span class="keyword">join</span> 语句不太一样，</span><br><span class="line">这个语句的作用和 <span class="keyword">in</span><span class="operator">/</span><span class="keyword">exists</span> 作用是一样的，是 <span class="keyword">in</span><span class="operator">/</span><span class="keyword">exists</span> 更高效的实现</span><br><span class="line"><span class="keyword">SELECT</span> A.<span class="operator">*</span> <span class="keyword">FROM</span> A <span class="keyword">where</span> id <span class="keyword">in</span> (<span class="keyword">select</span> id <span class="keyword">from</span> B)</span><br><span class="line"></span><br><span class="line"><span class="keyword">SELECT</span> A.<span class="operator">*</span> <span class="keyword">FROM</span> A <span class="keyword">left</span> semi <span class="keyword">join</span> B <span class="keyword">ON</span> A.id<span class="operator">=</span>B.id</span><br><span class="line"></span><br><span class="line">上述两个 <span class="keyword">sql</span> 语句执行结果完全一样，只不过第二个执行效率高</span><br></pre></td></tr></table></figure><p><strong>注意事项：</strong></p><ol><li>left semi join 的限制是：join 子句中右边的表<strong>只能在 on 子句中设置过滤条件</strong>，在 where 子句、select 子句或其他地方过滤都不行。</li><li>left semi join 中 on 后面的过滤条件<strong>只能是等于号</strong>，不能是其他的。</li><li>left semi join 是只传递表的 join key 给 map 阶段，因此left semi join 中最后 select 的<strong>结果只许出现左表</strong>。</li><li>因为 left semi join 是 in(keySet) 的关系，遇到<strong>右表重复记录，左表会跳过</strong></li></ol><h4 id="7-聚合函数中-null-值"><a href="#7-聚合函数中-null-值" class="headerlink" title="7. 聚合函数中 null 值"></a>7. 聚合函数中 null 值</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive支持 <span class="built_in">count</span>(),<span class="built_in">max</span>(),<span class="built_in">min</span>(),<span class="built_in">sum</span>(),<span class="built_in">avg</span>() 等常用的聚合函数</span><br></pre></td></tr></table></figure><p><strong>注意事项：</strong></p><p><strong>聚合操作时要注意 null 值</strong>：</p><p>count(*) 包含 null 值，统计所有行数；<br>count(id) 不包含id为 null 的值；<br>min <strong>求最小值是不包含 null</strong>，除非所有值都是 null；<br>avg <strong>求平均值也是不包含 null</strong>。</p><p><strong>以上需要特别注意，null 值最容易导致算出错误的结果</strong></p><h4 id="8-运算符中-null-值"><a href="#8-运算符中-null-值" class="headerlink" title="8. 运算符中 null 值"></a>8. 运算符中 null 值</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hive 中支持常用的算术运算符(<span class="operator">+</span>,<span class="operator">-</span>,<span class="operator">*</span>,<span class="operator">/</span>)  </span><br><span class="line">比较运算符(<span class="operator">&gt;</span>, <span class="operator">&lt;</span>, <span class="operator">=</span>)</span><br><span class="line">逻辑运算符(<span class="keyword">in</span>, <span class="keyword">not</span> <span class="keyword">in</span>)</span><br><span class="line"></span><br><span class="line">以上运算符计算时要特别注意 <span class="keyword">null</span> 值</span><br></pre></td></tr></table></figure><p><strong>注意事项：</strong></p><ol><li><strong>每行中的列字段相加或相减，如果含有 null 值，则结果为 null</strong><br>例：有一张商品表（product）</li></ol><table><thead><tr><th align="left">id</th><th align="left">price</th><th align="left">dis_amount</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">100</td><td align="left">20</td></tr><tr><td align="left">2</td><td align="left">120</td><td align="left">null</td></tr></tbody></table><p>各字段含义：id (商品id)、price (价格)、dis_amount (优惠金额)</p><p>我想算<strong>每个商品优惠后实际的价格</strong>，sql如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> id, price <span class="operator">-</span> dis_amount <span class="keyword">as</span> real_amount <span class="keyword">from</span> product;</span><br></pre></td></tr></table></figure><p>得到结果如下：</p><table><thead><tr><th align="left">id</th><th align="left">real_amount</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">80</td></tr><tr><td align="left">2</td><td align="left">null</td></tr></tbody></table><p>id&#x3D;2的商品价格为 null，结果是错误的。</p><p>我们可以<strong>对 null 值进行处理</strong>，sql如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> id, price <span class="operator">-</span> <span class="built_in">coalesce</span>(dis_amount,<span class="number">0</span>) <span class="keyword">as</span> real_amount <span class="keyword">from</span> product;</span><br><span class="line"></span><br><span class="line">使用 coalesce 函数进行 <span class="keyword">null</span> 值处理下，得到的结果就是准确的</span><br><span class="line"></span><br><span class="line">coalesce 函数是返回第一个不为空的值</span><br><span class="line">如上<span class="keyword">sql</span>：如果dis_amount不为空，则返回dis_amount，如果为空，则返回<span class="number">0</span></span><br></pre></td></tr></table></figure><ol><li><strong>小于是不包含 null 值</strong>，如 id &lt; 10；是不包含 id 为 null 值的。</li><li><strong>not in 是不包含 null 值的</strong>，如 city not in (‘北京’,’上海’)，这个条件得出的结果是 city 中不包含 北京，上海和 null 的城市。</li></ol><h4 id="9-and-和-or"><a href="#9-and-和-or" class="headerlink" title="9. and 和 or"></a>9. and 和 or</h4><p>在sql语句的过滤条件或运算中，如果有多个条件或多个运算，我们都会考虑优先级，如乘除优先级高于加减，乘除或者加减它们之间优先级平等，谁在前就先算谁。那 and 和 or 呢，看似 and 和 or 优先级平等，谁在前先算谁，但是，<strong>and 的优先级高于 or</strong>。</p><p><strong>注意事项：</strong></p><p>例：<br>还是一张商品表（product）</p><table><thead><tr><th align="left">id</th><th align="left">classify</th><th align="left">price</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">电器</td><td align="left">70</td></tr><tr><td align="left">2</td><td align="left">电器</td><td align="left">130</td></tr><tr><td align="left">3</td><td align="left">电器</td><td align="left">80</td></tr><tr><td align="left">4</td><td align="left">家具</td><td align="left">150</td></tr><tr><td align="left">5</td><td align="left">家具</td><td align="left">60</td></tr><tr><td align="left">6</td><td align="left">食品</td><td align="left">120</td></tr></tbody></table><p>我想要统计下电器或者家具这两类中价格大于100的商品，sql如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> product <span class="keyword">where</span> classify <span class="operator">=</span> <span class="string">&#x27;电器&#x27;</span> <span class="keyword">or</span> classify <span class="operator">=</span> <span class="string">&#x27;家具&#x27;</span> <span class="keyword">and</span> price<span class="operator">&gt;</span><span class="number">100</span></span><br></pre></td></tr></table></figure><p>得到结果</p><table><thead><tr><th align="left">id</th><th align="left">classify</th><th align="left">price</th></tr></thead><tbody><tr><td align="left">1</td><td align="left">电器</td><td align="left">70</td></tr><tr><td align="left">2</td><td align="left">电器</td><td align="left">130</td></tr><tr><td align="left">3</td><td align="left">电器</td><td align="left">80</td></tr><tr><td align="left">4</td><td align="left">家具</td><td align="left">150</td></tr></tbody></table><p>结果是错误的，把所有的电器类型都查询出来了，原因就是 and 优先级高于 or，上面的sql语句实际执行的是，先找出 classify &#x3D; ‘家具’ and price&gt;100 的，然后在找出 classify &#x3D; ‘电器’ 的</p><p>正确的 sql 就是加个括号，先计算括号里面的：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> product <span class="keyword">where</span> (classify <span class="operator">=</span> <span class="string">&#x27;电器&#x27;</span> <span class="keyword">or</span> classify <span class="operator">=</span> <span class="string">&#x27;家具&#x27;</span>) <span class="keyword">and</span> price<span class="operator">&gt;</span><span class="number">100</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive入门-四个by详解</title>
      <link href="/2021/10/25/Software/Hive%E5%85%A5%E9%97%A8-%E5%9B%9B%E4%B8%AAby%E8%AF%A6%E8%A7%A3/"/>
      <url>/2021/10/25/Software/Hive%E5%85%A5%E9%97%A8-%E5%9B%9B%E4%B8%AAby%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p>分别是 order by、distribute by、sort by 和 cluster by，它们都是排序相关的函数，输入和输出数据量是一样的，不要和 group by 搞混了。</p><span id="more"></span><h3 id="order-by"><a href="#order-by" class="headerlink" title="order by"></a>order by</h3><p>order by 就是我们熟知的全局排序，在 hive 中使用它的后果就是会导致只有一个 reduce，数据量较大时会造成运算的压力</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> test <span class="keyword">order</span> <span class="keyword">by</span> name <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><h3 id="distribute-by"><a href="#distribute-by" class="headerlink" title="distribute by"></a>distribute by</h3><p>distribute by 是先计算这个字段内数据的 hash 值，hash 值相同的分到一个 reduce 内，若是数据量较小或者指定一个 reduce 的时候就不用考虑这些了，都会分到一个 reduce 中，一般配合 sort by 使用</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">5</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> test distribute <span class="keyword">by</span> name;</span><br></pre></td></tr></table></figure><h3 id="sort-by"><a href="#sort-by" class="headerlink" title="sort by"></a>sort by</h3><p>sort by 是局部排序，一般配合 distribute by 使用，将 hash 值相同的数据分到一个 reduce 内方便排序。它和 order by 不同的地方在于 order by 是全局排序，sort by 是局部排序，若只有一个 reduce 时它俩的结果相同，若有多个 reduce 时并且字段内所分区的数量原大于 reduce 个数就会导致多个分区的数据分到一个 reduce 内最终排序结果会出现问题，所以需要 sort by 分区的字段加排序的字段</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">5</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> test distribute <span class="keyword">by</span> name sort <span class="keyword">by</span> age <span class="keyword">desc</span>;</span><br></pre></td></tr></table></figure><h3 id="cluster-by"><a href="#cluster-by" class="headerlink" title="cluster by"></a>cluster by</h3><p>cluster by 是在 distribute by 和 sort by 的字段相同时使用，缺点是只能升序排序</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">5</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> test cluster <span class="keyword">by</span> name;</span><br></pre></td></tr></table></figure><h3 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h3><p>若是数据量少，只有一个 reduce 的时候，那么 distribute by 后再 sort by 就相当于 group by 后再 order by，所有的数都被分到同一个 reduce 当中</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> grade, points <span class="keyword">from</span> class distribute <span class="keyword">by</span> grade sort <span class="keyword">by</span> points;</span><br></pre></td></tr></table></figure><p>但若是数据量较大或者自定义 reduce 较多时，字段下不同的分区可能被分到同一个 reduce 当中，例如 grade 为高级和低级的被分到一个 reduce 当中，直接 sort by 时可能就会打乱 grade 的顺序，所以需要 sort by grade, points 才能得到想要的结果</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">5</span>;</span><br><span class="line"><span class="keyword">select</span> grade, points <span class="keyword">from</span> class distribute <span class="keyword">by</span> grade sort <span class="keyword">by</span> grade, points;</span><br></pre></td></tr></table></figure><p>随机抽样，使用 distribute by rand() 保证数据分配到 reduce 是随机的，并且使用 sort by rand() 保证 reduce 内的排序也是随机的，那么再加上 limit 就可以从数据源中随机的抽取指定数量的数据</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> test distribute <span class="keyword">by</span> rand() sort <span class="keyword">by</span> rand() limit <span class="number">100</span>;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive应用-面试题</title>
      <link href="/2021/10/23/Software/Hive%E5%BA%94%E7%94%A8-%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
      <url>/2021/10/23/Software/Hive%E5%BA%94%E7%94%A8-%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>常见的 Hive 面试题，记录不只为面试，加深对底层环境的理解</p><span id="more"></span><h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><ol><li><p>Hive SQL执行流程（详细）</p><ul><li><p>Parser：Hive 使用 Antlr 进行语法和词法解析，解析的结果就是 AST 抽象语法树，会进行语法校验，使用<code>explain extented select * from test</code>查看详细的语法树</p></li><li><p>Analyzer：语法分析，这里会去元数据库关联查找相关的表和字段是否存在，然后生成 QB（query block），QB 就是一条 SQL 最基本的组成单元，包括输入源、计算过程、输出三个部分</p></li><li><p>Logical Plan：逻辑执行计划解析，由 QB 生成一堆 Operator Tree，基本的操作符包括TableScanOperator、SelectOperator、FilterOperator、JoinOperator、GroupByOperator、ReduceSinkOperator</p></li><li><p>Logical Optimizer：逻辑执行计划优化，对上一步的执行计划进行优化，还是返回 Operator Tree</p></li><li><p>Physical Paln：物理执行计划，生成 Task Tree</p></li><li><p>Physical Optimizer：物理执行计划优化，同样返回 Task Tree</p></li><li><p>将优化后的 Task Tree 提交给 Map Reduce 或者其他引擎去执行</p></li></ul></li><li><p>简单介绍 SQL 执行映射 MR 流程</p><ul><li><p>首先是过滤类的语句，不带任何聚合和 join 操作，整个过程是没有 reduce 的，类似 ETL 操作，其中 map 的数量是由文件分片数量决定的，分区条件直接在数据读取的时候过滤</p></li><li><p>分组聚合类查询，本质和 Word Count 执行差不多，combiner 是每个 Map 局部的 reduce，好处是减少 shuffle 数据量，但并不是所有的场景都会发生 combiner，例如求均值</p></li></ul></li><li><p>简单介绍一下 Hive 架构和原理以及特点</p><ul><li>Hive 是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据文件映射为一张表，并提供类 SQL 查询功能</li><li>Hive 的数据存储在 HDFS 上，或者说 Hive 的数据就是在 HDFS 上的映射，表就是文件，数据库就是文件夹，只不过底层是分布式存储的；计算引擎默认是 Map Reduce，Hive3.0 开始已经不推荐使用了；资源调度和程序的执行是放在 Yarn 上的</li><li>Hive 是优点是可存储数据量大，可扩展性强，数据安全性较高，结合元数据可对底层数据进行较好的存储和管理。缺点是只可增删查不可改，如需改只能查询修改后全部覆盖或者使用拉链表等其他手段；还有就是计算效率较慢，因为 Map Reduce 计算引擎会产生 Shuffle，多次落盘读盘会导致效率低下；还有一个缺点是不支持索引，如果不建分区表它会暴力扫描整个表；所以 Hive 的使用一定是建立在良好的优化方法之上的</li></ul></li><li><p>查看 hive 自带的函数以及详细使用方法</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查看支持的函数</span></span><br><span class="line"><span class="keyword">show</span> functions;</span><br><span class="line"><span class="keyword">show</span> functions <span class="keyword">like</span> <span class="string">&#x27;unix*&#x27;</span>;</span><br><span class="line"><span class="comment">-- 查看一个函数的详细使用方法</span></span><br><span class="line"><span class="keyword">desc</span> <span class="keyword">function</span> extended unix_timestamp;</span><br></pre></td></tr></table></figure></li><li><p>内部表和外部表的区别</p><ul><li>创建内部表时会把数据移动到指定的目录或者默认的目录下，删除的时候会把元数据以及数据全部删除且无法恢复；创建外部表时不会将数据移动到默认的目录下，只是记录数据所在位置，删除的时候只会删除元数据，数据不动</li><li>创建内部表（默认就是内部表）：<code>create table test...</code>，可以不指定 location 数据存储位置；创建外部表：<code>create external table test...</code> location ‘…’，必须指定数据所在位置</li></ul></li><li><p>介绍一下分区表和分桶表的作用、原理以及构建语句</p><ul><li>分区表是通过将数据按指定字段分区存储的方法减少查询时数据的扫描量，减少不必要的数据扫描，底层是在表所在的 HDFS 目录下再建文件夹存入每个分区的数据</li><li>分桶表则是为了提高大表 join 时的效率，通过对需要 join 的两张表中相同的字段进行分桶，将它们按数量指定分在不同的桶内，按照分桶字段的 hash 值取模除以分桶的个数确定数据存在哪个桶中，最好是排序分桶，这样可以保证两表相同的数据被分在同一个桶内，底层还是存储在不同的文件内。分桶表还有一个作用就是使取样更高效，开发时可以先取某一个桶的数据测试查询</li><li>分区表字段在表中不存在，而分桶表在表中存在；分桶表只能通过查询的方式插入，不可以使用 load 直接导入，建表语句使用<code>...clustered by (state) sorted by (cases desc) into 5 buckets...</code></li></ul></li><li><p>查看表结构、创建语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查看表详细信息</span></span><br><span class="line"><span class="keyword">desc</span> formatted test;</span><br><span class="line"><span class="comment">-- 表的创建语句</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">create</span> <span class="keyword">table</span> test;</span><br></pre></td></tr></table></figure></li><li><p>表有哪些分区、对应的 hdfs 路径、分区数据量大小、分区文件总数</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 查看表的所有分区</span></span><br><span class="line"><span class="keyword">show</span> partitions hero_all;</span><br><span class="line"><span class="comment">-- 查看某一个分区的详细信息（在Detailed Partition Information中）</span></span><br><span class="line"><span class="keyword">desc</span> extended hero_all <span class="keyword">partition</span>(role<span class="operator">=</span><span class="string">&#x27;warrior&#x27;</span>);</span><br><span class="line"><span class="comment">-- 去MySQL元数据库查看分区数量</span></span><br><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(<span class="number">1</span>) `分区数量`</span><br><span class="line"><span class="keyword">from</span> PARTITIONS</span><br><span class="line"><span class="keyword">where</span> TBL_ID <span class="keyword">in</span> (</span><br><span class="line">    <span class="keyword">select</span> TBL_ID <span class="keyword">from</span> TBLS <span class="keyword">where</span> TBL_NAME <span class="operator">=</span> <span class="string">&#x27;students&#x27;</span>);</span><br><span class="line"><span class="comment">-- 查看表或者某一个分区数据量大小</span></span><br><span class="line">hadoop fs <span class="operator">-</span>du <span class="operator">-</span>s <span class="operator">-</span>h <span class="operator">/</span>warehouse<span class="operator">/</span>test<span class="operator">/</span>uscovid<span class="operator">/</span>us_covid</span><br></pre></td></tr></table></figure></li><li><p>hive 支持的基本数据类型</p><table><thead><tr><th>数据类型</th><th>长度</th><th>说明</th><th>示例</th></tr></thead><tbody><tr><td>TINYINT</td><td>1byte</td><td>-128 ~ 127</td><td>100Y</td></tr><tr><td>SMALLINT</td><td>2byte</td><td>-32768 ~ 32767</td><td>100S</td></tr><tr><td>INT</td><td>4byte</td><td>-2^32~ 2^32-1</td><td>100</td></tr><tr><td>BIGINT</td><td>8byte</td><td>-2^64~ 2^64-1</td><td>100L</td></tr><tr><td>FLOAT</td><td>4byte</td><td>单精度浮点数</td><td>5.21</td></tr><tr><td>DOUBLE</td><td>8byte</td><td>双精度浮点数</td><td>5.21</td></tr><tr><td>DECIMAL</td><td>-</td><td>高精度浮点数</td><td>DECIMAL(9,8)</td></tr><tr><td>BOOLEAN</td><td>-</td><td>布尔型</td><td>true&#x2F;false</td></tr><tr><td>BINARY</td><td>-</td><td>字节数组</td><td>-</td></tr><tr><td>STRING</td><td>-</td><td>常用字符串类型</td><td>‘abc’</td></tr><tr><td>VARCHAR</td><td>1-65535</td><td>可变字符串，需要指定长度</td><td>‘abc’</td></tr><tr><td>CHAR</td><td>1-255</td><td>不够长度的使用空格填充</td><td>‘abc’</td></tr><tr><td>DATE</td><td>-</td><td>yyyy-MM-dd</td><td>2020-07-04</td></tr><tr><td>TIMESTAMPS</td><td>-</td><td>yyyy-MM-dd HH:mm:ss.fffffffff</td><td>2020-07-04 12:36:25.111</td></tr><tr><td>STRUCT</td><td>-</td><td>结构体</td><td><code>struct&lt;name:string,weight:double&gt;</code></td></tr><tr><td>ARRAY</td><td>-</td><td>相同数据类型的集合</td><td>array<Int></td></tr><tr><td>MAP</td><td>-</td><td>键值对的组合</td><td>map&lt;string,string&gt;</td></tr></tbody></table></li><li><p>确定一个 hive sql 的 map 数量和 reduce 数量</p><ul><li><p>map 数量通过 split 数量确定，split 数量等于文件大小 &#x2F; split size，split size 默认是128M；reduce 数量不指定的情况下是 -1，是一个动态计算的值，也可以通过<code>set mapred.reduce.tasks=10;</code>手动设定</p></li><li><p>map 数量并不是越大越好，如果有很多小文件，每个小文件都被当作一个块，那么就会造成资源浪费，解决这个问题可以对小文件进行合并</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 每个map最大输入大小</span></span><br><span class="line"><span class="keyword">set</span> mapred.max.split.size<span class="operator">=</span><span class="number">100000000</span>;</span><br><span class="line"><span class="comment">-- 节点中可以处理的最小文件大小,100M</span></span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.node<span class="operator">=</span><span class="number">100000000</span>;</span><br><span class="line"><span class="comment">-- 机架中可以处理的最小的文件大小</span></span><br><span class="line"><span class="keyword">set</span> mapred.min.split.size.per.rack<span class="operator">=</span><span class="number">100000000</span>;</span><br><span class="line"><span class="comment">-- 表示执行map前对小文件进行合并</span></span><br><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure></li><li><p>reduce 数量的确定，同 map 一样也不是越多越好，有多少 reduce 就会产生多少小文件，这些小文件作为下一个任务的输入就会造成资源的浪费，小文件合并后面再说</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 下面两个参数确定reduce的数量</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.reducers.bytes.per.reducer;  <span class="comment">-- 每个reduce任务处理的数据量，默认256M</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.reducers.max;  <span class="comment">-- 每个任务最大的reduce数量，默认1009</span></span><br><span class="line"><span class="comment">-- 根据上面两个参数，reduce数量=min(1009, 总输入数据大小/256M)</span></span><br><span class="line"><span class="comment">-- 如果需要减少reduce个数，可以调大每个reduce任务处理的数据量或者自行设定</span></span><br><span class="line"><span class="keyword">set</span> mapred.reduce.tasks <span class="operator">=</span> <span class="number">5</span>;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>Hive 命令常用参数</p><ul><li><code>hive</code>：进入 hive 交互式 shell</li><li><code>hive --help</code>：查看 hive 命令都支持哪些参数</li><li><code>hive --hiveconf a=b,c=d</code>…：传入变量，新版可以使用 –define </li><li><code>hive -i test.init...</code>：从文件初始化 hive，文件中存放 hive sql 初始化语句</li><li><code>hive -e &quot;use test;select * from test;&quot;</code>：不进入交互式 shell 运行 sql 并返回结果</li><li><code>hive -f ./test.sql</code>：不进入交互式 shell 运行一个 sql 文件</li></ul></li><li><p>解释 sql 运行步骤，以及如何优化</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.id, b.name <span class="keyword">from</span> a <span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.id <span class="operator">=</span> b.id <span class="keyword">where</span> a.dt <span class="operator">=</span> <span class="string">&#x27;2021-10-10&#x27;</span> <span class="keyword">and</span> b.dt <span class="operator">=</span> <span class="string">&#x27;2021-10-10&#x27;</span></span><br></pre></td></tr></table></figure><p>首先是看 on 和 where，它俩的区别是 on 是生成 left join 完成后的临时表用来过滤的条件，就算条件不满足也会返回左表的所有结果，而 where 则是在生成临时表后再对这个结果进行过滤，最终返回过滤后的结果</p><p>根据这个特性可以将 where 条件前置，直接 join 两个筛选后的表，可以减少 join 的数据量</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> aa.id, bb.name </span><br><span class="line"><span class="keyword">from</span> (</span><br><span class="line">    <span class="keyword">select</span> id, name <span class="keyword">from</span> a <span class="keyword">where</span> dt <span class="operator">=</span> <span class="string">&#x27;2021-10-10&#x27;</span>) aa </span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> (</span><br><span class="line">    <span class="keyword">select</span> id, name <span class="keyword">from</span> b <span class="keyword">where</span> dt <span class="operator">=</span> <span class="string">&#x27;2021-10-10&#x27;</span>) bb </span><br><span class="line"><span class="keyword">on</span> aa.id <span class="operator">=</span> bb.id</span><br></pre></td></tr></table></figure></li><li><p>下面的 sql 语句的区别</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> a.<span class="operator">*</span> <span class="keyword">from</span> a <span class="keyword">left</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.key <span class="operator">=</span> b.key <span class="keyword">and</span> a.ds <span class="operator">=</span> xxx <span class="keyword">and</span> b.ds <span class="operator">=</span> xxx</span><br><span class="line"><span class="keyword">select</span> a.<span class="operator">*</span> <span class="keyword">from</span> a <span class="keyword">left</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.key <span class="operator">=</span> b.key <span class="keyword">and</span> b.ds <span class="operator">=</span> xxx</span><br><span class="line"><span class="keyword">select</span> a.<span class="operator">*</span> <span class="keyword">from</span> a <span class="keyword">left</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.key <span class="operator">=</span> b.key <span class="keyword">and</span> a.ds <span class="operator">=</span> xxx <span class="keyword">where</span> b.ds <span class="operator">=</span> xxx</span><br><span class="line"><span class="keyword">select</span> a.<span class="operator">*</span> <span class="keyword">from</span> a <span class="keyword">left</span> <span class="keyword">join</span> b <span class="keyword">on</span> a.key <span class="operator">=</span> b.key <span class="keyword">where</span> a.ds <span class="operator">=</span> xxx <span class="keyword">and</span> b.ds <span class="operator">=</span> xxx</span><br></pre></td></tr></table></figure><p>这个问题其实和上面那个问题差不多，主要是考察 on 和 where 的理解和使用，on 和 where 同时存在时 on 的优先级是高于 where 的，这样就表示如果要对连接后的结果再做聚合可以使用 on 连接两张表再 where 过滤掉不需要的记录，可以减少不必要的计算量。如果要在结果记录中保留某一张表的所有记录，则把连接条件和过滤条件全部放在 on 中，这样就保证了聚合时数据的完整性。</p></li></ol><h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><ol><li><p>a 表 left join b 表，b 表为小表，可以怎样优化</p><ul><li>这里考察的是 map join 的知识，我们一般理解的默认 join 操作是先进行 map 分组，再通过 shuffle 将相同 key 的值划分到同一个 reduce 中去 join。如果其中有一张表比较小的情况下不用采取这种操作，map join 的原理是将两张表中数据量较少的一张表复制到内存中，大表的每一个 map（split）都去和这个小表去 join，最后把结果拼接一下就可以了，这样减少了 shuffle 和 reduce 的操作</li><li>hive 0.7 版本以前还需手动使用<code>/*+ mapjoin(table) */</code>提示才会执行 map join，之后的版本由参数<code>hive.auto.convert.join</code>控制，默认为 true，无需手动开启，自动转为 map join 的条件是其中一张表的数据量大小，由<code>hive.mapjoin.smalltable.filesize</code>来决定，默认是25M，意思若其中有一张表小于25M则自动开启 map join</li><li>其实 hive 现在也支持本地模式，若是两张表数据量都比较小，就会在本地执行任务，区分方式通过任务号中是否含有 local，通过<code>set mapreduce.framework.name=local;set hive.exec.mode.local.auto=true;</code>开启，<code>mapreduce.framework.name=local</code>会让所有任务都走本地</li></ul></li><li><p>两个大表连接，发生数据倾斜，有几个 reduce 无法完成，怎样定位发生数据倾斜的原因，怎样优化</p></li><li><p>两个大表连接，发生数据倾斜，一个reduce无法完成，发现有一张表中 id &#x3D; ‘’ 的记录有很多，其他都是非重复，应该怎样优化</p><ul><li>首先如果其中一张表中 id 包含许多记录为空的情况下，若是后续计算不需要这部分数据，则可以在 join 之前就把这部分数据过滤掉，减少 shuffle 的时候产生的笛卡尔积数量，而且空记录在分配到一个 reduce 的时候也会对效率产生影响</li><li>第二种情况就是在需要保留这些记录的时候，就可以对这些数据打散处理，避免被分到一个 reduce 当中。方法是在 join 之前使用 case 判断若为空的时候分配给一个随机值，这样就会被分到不同的 reduce 当中，提高处理效率</li></ul></li><li><p>sort by、distribute by、cluster by 和 order by 的区别</p><ul><li>首先它们都是排序相关的函数，输入和输出数据量是一样的，不要和 group by 搞混了。order by 就是我们熟知的全局排序，在 hive 中使用它的后果就是会导致只有一个 reduce，数据量较大时会造成运算的压力</li><li>distribute by 是先计算这个字段内数据的 hash 值，hash 值相同的分到一个 reduce 内，若是数据量较小或者指定一个 reduce 的时候就不用考虑这些了，都会分到一个 reduce 中，一般配合 sort by 使用</li><li>sort by 是局部排序，一般配合 distribute by 使用，将 hash 值相同的数据分到一个 reduce 内方便排序。它和 order by 不同的地方在于 order by 是全局排序，sort by 是局部排序，若只有一个 reduce 时它俩的结果相同，若有多个 reduce 时并且字段内所分区的数量原大于 reduce 个数就会导致多个分区的数据分到一个 reduce 内最终排序结果会出现问题，所以需要 sort by 分区的字段加排序的字段</li><li>cluster by 是在 distribute by 和 sort by 的字段相同时使用，缺点是只能升序排序</li></ul></li><li><p>文件存储格式和数据压缩的优化</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 开启hive中间传输数据压缩功能</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.intermediate<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 开启mapreduce中map输出压缩功能</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.map.output.compress<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 设置mapreduce中map输出数据的压缩方式</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.map.output.compress.codec<span class="operator">=</span> org.apache.hadoop.io.compress.SnappyCodec;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 开启hive最终输出数据压缩功能</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.output<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 开启mapreduce最终输出数据压缩</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 设置mapreduce最终数据输出压缩方式</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.codec <span class="operator">=</span> org.apache.hadoop.io.compress.SnappyCodec;</span><br><span class="line"><span class="comment">-- 设置mapreduce最终数据输出压缩为块压缩</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.type<span class="operator">=</span>BLOCK;</span><br></pre></td></tr></table></figure></li><li><p>其他解决数据倾斜的方法 - 包括 join、group by 等一般场景</p></li></ol><h3 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h3><ol><li><p>使用 select 查询时，使用哪个函数给值为 null 的数据设置默认值</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 使用nvl或者coalesce对值为null的直接填充，或者使用if函数判断为null再填充</span></span><br><span class="line"><span class="keyword">select</span> nvl(id, <span class="string">&#x27;123&#x27;</span>) <span class="keyword">from</span> test;</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">coalesce</span>(id, <span class="string">&#x27;123&#x27;</span>) <span class="keyword">from</span> test;</span><br><span class="line"><span class="keyword">select</span> if(isnull(id), <span class="string">&#x27;123&#x27;</span>, id) <span class="keyword">from</span> test;</span><br></pre></td></tr></table></figure></li><li><p>a、b、c 三个表内连接，连接字段都是 key，写出连接语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> a <span class="keyword">inner</span> <span class="keyword">join</span> b <span class="keyword">inner</span> <span class="keyword">join</span> c <span class="keyword">on</span> a.key <span class="operator">=</span> b.key <span class="keyword">and</span> a.key <span class="operator">=</span> c.key;</span><br></pre></td></tr></table></figure></li><li><p>已知 a 是一张内部表，如何将它转为外部表，写出 hive sql 语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 直接修改表属性中的EXTERNAL即可</span></span><br><span class="line"><span class="keyword">show</span> formatted test;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> test <span class="keyword">set</span> tblproperties(<span class="string">&#x27;EXTERNAL&#x27;</span><span class="operator">=</span><span class="string">&#x27;TRUE&#x27;</span>);  <span class="comment">-- 内部转外部</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> test <span class="keyword">set</span> tblproperties(<span class="string">&#x27;EXTERNAL&#x27;</span><span class="operator">=</span><span class="string">&#x27;FALSE&#x27;</span>);  <span class="comment">-- 外部转内部</span></span><br></pre></td></tr></table></figure></li><li><p>行转列以及列转行函数以及使用方法</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 行转列使用侧视图lateral view和explode炸裂（假设需要把列C按逗号拆分再打散）</span></span><br><span class="line"><span class="comment">-- explode函数需要传入array，如果字段本就是array就不用split了</span></span><br><span class="line"><span class="keyword">select</span> explode(<span class="keyword">array</span>(<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>)) <span class="keyword">as</span> col;  <span class="comment">-- col：a,b,c</span></span><br><span class="line"><span class="keyword">select</span> A, B, C_new <span class="keyword">from</span> test a <span class="keyword">lateral</span> <span class="keyword">view</span> explode(split(C, <span class="string">&#x27;,&#x27;</span>)) b <span class="keyword">as</span> C_new;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 列转行使用concat_ws，同样输入array</span></span><br><span class="line"><span class="keyword">select</span> concat_ws(<span class="string">&#x27;|&#x27;</span>,<span class="keyword">array</span>(<span class="string">&#x27;a&#x27;</span>,<span class="string">&#x27;b&#x27;</span>,<span class="string">&#x27;c&#x27;</span>)) <span class="keyword">as</span> col;  <span class="comment">-- col:a|b|c</span></span><br><span class="line"><span class="keyword">select</span> A, concat_ws(<span class="string">&#x27;,&#x27;</span>, collect_set(c_new)) <span class="keyword">as</span> C <span class="keyword">from</span> test <span class="keyword">group</span> <span class="keyword">by</span> A; </span><br></pre></td></tr></table></figure></li><li><p>常用的开窗函数</p><p>参照以前写的 MySQL 中的[开窗函数](<a href="https://wangxukun.top/blogs/language/sql-overchuang-kou-han-shu.html">SQL OVER窗口函数 | 汪寻 (wangxukun.top)</a>)</p></li><li><p>rank、dense_rank 和 row_number 函数的区别</p><p>参照以前写的 MySQL 中的[开窗函数](<a href="https://wangxukun.top/blogs/language/sql-overchuang-kou-han-shu.html">SQL OVER窗口函数 | 汪寻 (wangxukun.top)</a>)</p></li><li><p>将字符串 “k1&#x3D;v1&amp;k2&#x3D;v2&amp;…&amp;kn&#x3D;vn” 进行分割并存入一个字段，可以查出任意 kn 对应的 vn 值，并计算共有多少个 k</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 首先使用str_to_map函数将字符串转为map，随后就可以使用[key]的方式取出对应的value，如果没有会返回NULL</span></span><br><span class="line"><span class="keyword">select</span> str_to_map(<span class="string">&#x27;k1=v1&amp;k2=v2&amp;k3=v3&#x27;</span>, <span class="string">&#x27;&amp;&#x27;</span>, <span class="string">&#x27;=&#x27;</span>)[<span class="string">&#x27;k1&#x27;</span>];</span><br><span class="line"><span class="comment">-- 只返回map的key或者value使用map_keys和map_values</span></span><br><span class="line"><span class="keyword">select</span> map_keys(str_to_map(<span class="string">&#x27;k1=v1&amp;k2=v2&amp;k3=v3&#x27;</span>, <span class="string">&#x27;&amp;&#x27;</span>, <span class="string">&#x27;=&#x27;</span>)) map_key_cnt;</span><br><span class="line"><span class="comment">-- 返回map的长度使用size</span></span><br><span class="line"><span class="keyword">select</span> size(str_to_map(<span class="string">&#x27;k1=v1&amp;k2=v2&amp;k3=v3&#x27;</span>, <span class="string">&#x27;&amp;&#x27;</span>, <span class="string">&#x27;=&#x27;</span>)) map_key_cnt;</span><br></pre></td></tr></table></figure></li><li><p>全量用户登录日志表 login_all，字段信息 login_time、openid；新增用户登录日志表 login_new，字段信息 login_time、openid，计算每天新增用户次日、七日、30日留存率</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tmp <span class="keyword">as</span> (  <span class="comment">-- 字段分别是openid，登录间隔时间，昨日新增用户（用于计算留存率）</span></span><br><span class="line"><span class="keyword">select</span> new.openid, datediff(new.login_time, alll.login_time) diff_time, <span class="built_in">count</span>(<span class="keyword">distinct</span> new.openid) <span class="keyword">over</span>() all_user</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(    <span class="comment">-- 减少数据量，一天只留一条登录记录</span></span><br><span class="line"><span class="keyword">select</span> openid, to_date(login_time) login_time</span><br><span class="line"><span class="keyword">from</span> user_stay_new</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> openid, to_date(login_time)  </span><br><span class="line">) <span class="keyword">new</span></span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span></span><br><span class="line">(    <span class="comment">-- 减少数据量，一天只留一条登录记录</span></span><br><span class="line"><span class="keyword">select</span> openid, to_date(login_time) login_time</span><br><span class="line"><span class="keyword">from</span> user_stay_all</span><br><span class="line"><span class="keyword">where</span> datediff(login_time, <span class="built_in">current_date</span>()) <span class="operator">&lt;=</span> <span class="number">33</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> openid, to_date(login_time) </span><br><span class="line">) alll</span><br><span class="line"><span class="keyword">on</span> alll.openid <span class="operator">=</span> new.openid</span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span> diff_time, round(<span class="built_in">count</span>(<span class="keyword">distinct</span> openid) <span class="operator">/</span> <span class="built_in">max</span>(all_user), <span class="number">2</span>) stay_pct</span><br><span class="line"><span class="keyword">from</span> tmp</span><br><span class="line"><span class="keyword">where</span> diff_time <span class="keyword">in</span> (<span class="number">1</span>, <span class="number">7</span>, <span class="number">30</span>)</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> diff_time;</span><br></pre></td></tr></table></figure><p>基本思路是先将 user_stay_new 表每个用户只留下一次登陆记录记为 new，再从 user_stay_all 表取出每人每日一次的登录记录记为 alll，将 new 和 alll 通过 openid 左连接得到所有用户最新的和之前三十三天的登录数据，随后计算两个登录时间的时间差以及 new 表中的总人数。将结果作为临时表，存的就是几日留存的人数以及昨日登录的总人数</p></li><li><p>发送消息流水表 chat_all：ctime、send_id、receiver_id、content；用户登录流水表 login_tb：ctime、id、location，输出每天有发送消息用户最近的发送消息时间、id、登录位置和登录时间</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tmp <span class="keyword">as</span> (</span><br><span class="line"><span class="keyword">select</span> chat.receiver_id, chat.ctime chat_time, login.ctime login_time, login.location, </span><br><span class="line">    <span class="built_in">row_number</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> chat.receiver_id, to_date(chat.ctime) <span class="keyword">order</span> <span class="keyword">by</span> chat.ctime <span class="keyword">desc</span>) <span class="keyword">rows</span></span><br><span class="line">    <span class="keyword">from</span> chat_all chat</span><br><span class="line">    <span class="keyword">inner</span> <span class="keyword">join</span> login_tb login</span><br><span class="line">    <span class="keyword">on</span> chat.receiver_id <span class="operator">=</span> login.id</span><br><span class="line">    <span class="keyword">and</span> to_date(chat.ctime) <span class="operator">=</span> to_date(login.ctime)</span><br><span class="line">) </span><br><span class="line"><span class="keyword">select</span> receiver_id, chat_time, login_time</span><br><span class="line"><span class="keyword">from</span> tmp</span><br><span class="line"><span class="keyword">where</span> <span class="keyword">rows</span> <span class="operator">=</span> <span class="number">1</span>;</span><br></pre></td></tr></table></figure><p>思路是先将用户每日登录的位置和时间 inner join 到消息流水表，如果有多条登录记录先对 login_tb 表处理，这里假设每天只有一条，然后按用户 id 和日期开窗并按发送消息的时间降序排序，将最后一条发送的时间标记为1，最后再筛选出来标记为1的记录即可</p></li><li><p>一个分区表 T，字段 id、qq、age，按天分区，写出建表语句</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> `T`(</span><br><span class="line">id <span class="type">int</span>,</span><br><span class="line">    qq string,</span><br><span class="line">    age <span class="type">int</span></span><br><span class="line">) <span class="type">row</span> format delimited</span><br><span class="line">fields terminated <span class="keyword">by</span> <span class="string">&#x27;\t&#x27;</span></span><br><span class="line"><span class="keyword">partition</span> <span class="keyword">by</span>(dt string)</span><br><span class="line">stored <span class="keyword">as</span> orc</span><br><span class="line">location <span class="string">&#x27;/warehouse/test/T&#x27;</span></span><br><span class="line">tblproperties(<span class="string">&#x27;orc.compress&#x27;</span><span class="operator">=</span><span class="string">&#x27;SNAPPY&#x27;</span>);</span><br></pre></td></tr></table></figure></li><li><p>上方的分区表，求 20211021 这个分区中，年龄第 10 大的 qq 号列表</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tmp <span class="keyword">as</span>(</span><br><span class="line"><span class="keyword">select</span> id, qq, age,</span><br><span class="line">        <span class="built_in">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> age <span class="keyword">asc</span>) ranks</span><br><span class="line">    <span class="keyword">from</span> T</span><br><span class="line">    <span class="keyword">where</span> dt <span class="operator">=</span> <span class="string">&#x27;20211021&#x27;</span></span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tmp <span class="keyword">where</span> ranks <span class="operator">=</span> <span class="number">10</span>;</span><br></pre></td></tr></table></figure></li><li><p>有一个网站的访问记录表 visit_tb，每条记录有 ctime 和 ip，计算过去五分钟内访问次数最多的 1000 个 ip</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tmp <span class="keyword">as</span> (  <span class="comment">-- 再计算这段时间内每个ip的访问次数</span></span><br><span class="line">    <span class="keyword">select</span> ip, size(collect_list(ip)) cnt <span class="comment">-- count(ip)</span></span><br><span class="line">    <span class="keyword">from</span> (  <span class="comment">-- 先计算当前时间和访问时间的时间差，单位为秒</span></span><br><span class="line">        <span class="keyword">select</span> ip, to_unix_timestamp(<span class="built_in">current_timestamp</span>()) <span class="operator">-</span> to_unix_timestamp(ctime) second_diff</span><br><span class="line">        <span class="keyword">from</span> visit_tb</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">where</span> second_diff <span class="operator">&lt;=</span> <span class="number">300</span></span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> ip</span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span> ip, cnt</span><br><span class="line"><span class="keyword">from</span> tmp</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> cnt <span class="keyword">desc</span></span><br><span class="line">limit <span class="number">1000</span>;</span><br></pre></td></tr></table></figure></li><li><p>解析 extra 中所有的字段：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">id,os,extra</span><br><span class="line"><span class="number">1</span>,iphone,[&#123;&quot;id&quot;:<span class="number">1001</span>,&quot;type&quot;:&quot;show&quot;,&quot;from&quot;:&quot;home&quot;&#125;,&#123;&quot;id&quot;:<span class="number">1002</span>,&quot;type&quot;:&quot;click&quot;,&quot;from&quot;:&quot;swan&quot;&#125;]</span><br><span class="line"><span class="number">2</span>,android,[&#123;&quot;id&quot;:<span class="number">1003</span>,&quot;type&quot;:&quot;slide&quot;,&quot;from&quot;:&quot;tool&quot;&#125;,&#123;&quot;id&quot;:<span class="number">1002</span>,&quot;type&quot;:&quot;del&quot;,&quot;from&quot;:&quot;wode&quot;&#125;,&#123;&quot;id&quot;:<span class="number">1004</span>,&quot;type&quot;:&quot;click&quot;,&quot;from&quot;:&quot;home&quot;&#125;]</span><br></pre></td></tr></table></figure><p>存储数据就不多说了，直接存储为 string 字符串，查询的时候再解析。思路是先去掉左右的数组符号，然后将数组 json 之间的分隔符由逗号替换成分号，接下来使用分号切分再使用侧视图炸裂，得到的就是一个个 json 字符串，最后再使用 json_tuple 从 json 字符串中取出多个值与原字段拼接在一起，或者使用 get_json_object 函数以此取出所需字段</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tmp <span class="keyword">as</span> (</span><br><span class="line">    <span class="keyword">select</span> id, os, b.json</span><br><span class="line">    <span class="keyword">from</span> json_array</span><br><span class="line">        <span class="keyword">lateral</span> <span class="keyword">view</span> explode(split(regexp_replace(regexp_replace(extra, <span class="string">&#x27;\\[|\\]&#x27;</span>, <span class="string">&#x27;&#x27;</span>), <span class="string">&#x27;\\&#125;\\,\\&#123;&#x27;</span>, <span class="string">&#x27;\\&#125;\\;\\&#123;&#x27;</span>), <span class="string">&#x27;\\;&#x27;</span>)) b <span class="keyword">as</span> json</span><br><span class="line">)</span><br><span class="line"><span class="comment">-- select id, os, get_json_object(json, &#x27;$.id&#x27;) iid, get_json_object(json, &#x27;$.type&#x27;) type, get_json_object(json, &#x27;$.from&#x27;) `from`</span></span><br><span class="line"><span class="keyword">select</span> id, os, tmp1.iid, tmp1.type, tmp1.`<span class="keyword">from</span>`</span><br><span class="line"><span class="keyword">from</span> tmp <span class="keyword">lateral</span> <span class="keyword">view</span> json_tuple(json, <span class="string">&#x27;id&#x27;</span>, <span class="string">&#x27;type&#x27;</span>, <span class="string">&#x27;from&#x27;</span>) tmp1 <span class="keyword">as</span> iid, type, `<span class="keyword">from</span>`;</span><br></pre></td></tr></table></figure></li><li><p>有一个表 subscribe_tb，两个字段分别是 gz 和 bgz，对应的 a 关注 b，数据如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">gz,bgz</span><br><span class="line"><span class="number">12</span>,<span class="number">34</span></span><br><span class="line"><span class="number">12</span>,<span class="number">56</span></span><br><span class="line"><span class="number">12</span>,<span class="number">78</span></span><br><span class="line"><span class="number">34</span>,<span class="number">56</span></span><br><span class="line"><span class="number">34</span>,<span class="number">12</span></span><br></pre></td></tr></table></figure><p>找出所有互相关注的记录</p><p>思路是使用关注账号和被关注账号自连接，得到新的字段被关注人所关注的账号，然后再筛选关注账号和被关注人所关注账号相同的行就是两个账号互相关注的记录</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tmp <span class="keyword">as</span> (</span><br><span class="line"><span class="keyword">select</span> a.gz, a.bgz, b.gz bgzgz </span><br><span class="line">    <span class="keyword">from</span> subscribe_tb a</span><br><span class="line">    <span class="keyword">inner</span> <span class="keyword">join</span> subscribe_tb b</span><br><span class="line">    <span class="keyword">on</span> a.gz <span class="operator">=</span> b.bgz</span><br><span class="line">)</span><br><span class="line"><span class="keyword">select</span> gz, bgz</span><br><span class="line"><span class="keyword">from</span> tmp <span class="keyword">where</span> gz <span class="operator">=</span> bgzgz</span><br></pre></td></tr></table></figure></li><li><p>UDF、UDAF 和 UDTF 函数的区别以及怎么开发</p><p>UDF 是一进一出，类似于 round、year、hour、floor、abs 等函数；UDAF 则是多进一出，类似 sum、count、max、avg 等函数；UDTF 则是一进多处，在 Hive 中多用于解析 Map 或者 Array 类型并返回多行记录，类似于 explode、json_tuple 等函数</p><p>至于 UDF 开发写了一个 Scala 版本的 UDF，<a href="https://wangxukun.top/blogs/software/hiveru-men-zi-ding-yi-han-shu-scala-.html">详见这里</a>，UDAF 看不懂，有时间了可以看看<a href="https://cwiki.apache.org/confluence/display/Hive/GenericUDAFCaseStudy">官网</a></p></li></ol><h3 id="建模"><a href="#建模" class="headerlink" title="建模"></a>建模</h3><ol><li>数仓的流程，你做的是什么</li><li>常见的建模方法有什么，你们用的是什么</li><li>维度建模有什么好处？比如业务需要增加一个维度，需要怎么做</li><li>怎样判断一个需求能不能实现，你的判断标准是什么，需求变更要做什么</li><li>ads 每天的数据量有多大，ads 层在 MySQL 中的表是怎样创建的，有什么注意事项，索引怎样创建</li><li>拉链表的原理</li><li>拉链表的整合方式</li><li>时点数和时期数</li><li>简述几种缓慢变化维度（SCD）的处理方法</li></ol>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
            <tag> 面试题 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark应用-面试题</title>
      <link href="/2021/10/23/Software/Spark%E5%BA%94%E7%94%A8-%E9%9D%A2%E8%AF%95%E9%A2%98/"/>
      <url>/2021/10/23/Software/Spark%E5%BA%94%E7%94%A8-%E9%9D%A2%E8%AF%95%E9%A2%98/</url>
      
        <content type="html"><![CDATA[<p>常见的 Spark 面试题，记录不只为面试，加深对底层环境的理解</p><span id="more"></span><h3 id="基础"><a href="#基础" class="headerlink" title="基础"></a>基础</h3><ol><li>Spark 任务提交流程</li><li>Spark 的 Shuffle 及优化</li><li>Spark 的宽窄依赖以及 Stage 的划分</li><li>如何将宽依赖转化为窄依赖</li><li>reduceByKey、groupByKey 有什么区别</li><li>DataFrame、RDD、DataSet 有什么区别</li><li>Spark 调优都会做什么</li><li>Spark 内存管理</li><li>spark-submit 常用提交参数</li><li>Spark 缓存时数据存在哪里</li><li>Spark 的几种部署模式和区别</li><li>Spark 运行的是 cluster 还是 client，有什么区别</li></ol><h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><ol><li>数据倾斜怎么处理</li><li>mr 和 spark 的 shuffle 区别</li></ol><h3 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h3><ol><li>使用累加器对 LIst(1, 2, 3, 4) 每个元素实现类加操作并输出结果</li><li>写一个自定义函数，实现给所有字符串加一个前缀，分别实现增加相同前缀和自增前缀</li><li>写一个 UDAF 函数，实现拼接 group by 后字段内的所有字符串（[“a”,”b”,”c”] -&gt; “a-b-c”）</li><li>自定义 UDTF 函数</li></ol>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 面试题 </tag>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>人生的意义</title>
      <link href="/2021/10/12/Mind/%E4%BA%BA%E7%94%9F%E7%9A%84%E6%84%8F%E4%B9%89/"/>
      <url>/2021/10/12/Mind/%E4%BA%BA%E7%94%9F%E7%9A%84%E6%84%8F%E4%B9%89/</url>
      
        <content type="html"><![CDATA[<p>最近一直想写这个问题，人生的意义是什么，生活的意义是什么，面对苦难需要以什么姿态去应对，面对理想与现实的落差如何平衡自己的心态。但是关注不代表我为之所困，我并不担心这些问题，当然不是我处理的有多好，而是在源源不断的问题和周遭的环境影响下有了自己的答案，就趁这次机会总结一下。</p><span id="more"></span><p>一直以来我都是一个容易多想的人，年轻的时候是多愁善感，遇事下不定决心，但大多数时间我把它用在了认识自己和完善自己上。高中时期不好好学习，唯一不后悔的是看了不少书，加深了对自己以及外界的认识；大学时期也是没好好学习，唯一不后悔的是四年中有两年多都在外面兼职打工，工作特别辛苦，也谈不上技术含量，但我从中收获了汗水和勤劳，也牢记了一句话：吃得苦中苦，方为人上人。</p><p>终于大学毕业了，以前想的所有不切实际的愿想都没有实现，周围同学都找着了工作，我却连自己会什么能做什么都答不上来，从那时起我开始有了焦虑的情绪，这种情绪一直伴我至今。经过两年的过渡，手头攒了点钱，去年发生了不少事情，在易县买房，去旭旭家待了几个月，在北京暂时落脚再到找着了勉强看的过去的工作，仿佛一下子就把自己翻新了，房子也买了，坐标北京，互联网行业，明年也要装修结婚，接下来的日子一片大好，我常常对自己这样说。</p><p>有时候我在想，前二十二年的成长居然抵不过毕业后三年的变化，19年20年的时候我也会因为结婚以及工作的事情焦虑不安，愁破了头，但现在因为这种事情的焦虑已经弱了许多，我的焦虑又变成了对好工作的渴望以及幸福生活的向往，看似目标更远大了，实则都差不多，不同的时期有不同的烦恼，不同的焦虑，没有最烦恼，只有更烦恼。自从毕业以来就在解决一个一个的问题，人生不就是这样的吗。</p><p>虽然如此，但我不慌，哪怕好工作依然没有着落，富足的生活离我还很遥远，但我的精神世界很丰满：遇到烦恼我不烦恼，想办法去解决，以后的麻烦事就交给以后的自己去烦恼吧；我有自己兴趣爱好，跑步和读书，闲暇之余放松放松对身心都有好处；我有明确的目标，肯吃苦肯钻研，虽说有一部分原因是因为压力被迫努力但我并不痛苦，身体和心灵的痛苦让我能感受到自己的存在；我有爱的人和爱我的人，三观也都合得来，虽说现在生活不那么富足，但我俩都对未来充满希望；最后是自己的胡思乱想，没有这个也不会有这些感悟，成长之路又是另一番模样了。</p><p>生活的意义就是不断解决一个个问题，不断的烦恼再烦恼，不断征服一座座大山，最后不断发现生活的意义。日子有奔头，享受痛苦并乐在当下，接受种种的不如意并靠双手去改变它，这就是生活的意义。</p>]]></content>
      
      
      <categories>
          
          <category> Mind </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mind </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Algorithm-初级</title>
      <link href="/2021/09/30/Algorithm/Algorithm-%E5%88%9D%E7%BA%A7/"/>
      <url>/2021/09/30/Algorithm/Algorithm-%E5%88%9D%E7%BA%A7/</url>
      
        <content type="html"><![CDATA[<p>LeetCode 算法-初级</p><span id="more"></span><h2 id="1-删除排序数组中的重复项"><a href="#1-删除排序数组中的重复项" class="headerlink" title="1. 删除排序数组中的重复项"></a>1. 删除排序数组中的重复项</h2><p>给你一个有序数组 nums ，请你 原地 删除重复出现的元素，使每个元素 只出现一次 ，返回删除后数组的新长度。不要使用额外的数组空间，你必须在 原地 修改输入数组 并在使用 O(1) 额外空间的条件下完成。</p><p> <strong>说明:</strong></p><p>为什么返回数值是整数，但输出的答案是数组呢?请注意，输入数组是以「引用」方式传递的，这意味着在函数里修改输入数组对于调用者是可见的。你可以想象内部操作如下:</p><blockquote><p>&#x2F;&#x2F; nums 是以“引用”方式传递的。也就是说，不对实参做任何拷贝<br>int len &#x3D; removeDuplicates(nums);</p><p>&#x2F;&#x2F; 在函数里修改输入数组对于调用者是可见的。<br>&#x2F;&#x2F; 根据你的函数返回的长度, 它会打印出数组中 该长度范围内 的所有元素。<br>for (int i &#x3D; 0; i &lt; len; i++) {<br>    print(nums[i]);<br>}</p></blockquote><p><strong>示例 1：</strong></p><blockquote><p>输入：nums &#x3D; [1,1,2]<br>输出：2, nums &#x3D; [1,2]<br>解释：函数应该返回新的长度 2 ，并且原数组 nums 的前两个元素被修改为 1, 2 。不需要考虑数组中超出新长度后面的元素。</p></blockquote><p><strong>示例 2：</strong></p><blockquote><p>输入：nums &#x3D; [0,0,1,1,1,2,2,3,3,4]<br>输出：5, nums &#x3D; [0,1,2,3,4]<br>解释：函数应该返回新的长度 5 ， 并且原数组 nums 的前五个元素被修改为 0, 1, 2, 3, 4 。不需要考虑数组中超出新长度后面的元素。</p></blockquote><p>提示：</p><blockquote><p>0 &lt;&#x3D; nums.length &lt;&#x3D; 3 * 104<br>-104 &lt;&#x3D; nums[i] &lt;&#x3D; 104<br>nums 已按升序排列</p></blockquote><h3 id="题解（2021-09-25）"><a href="#题解（2021-09-25）" class="headerlink" title="题解（2021-09-25）"></a>题解（2021-09-25）</h3><blockquote><p>变量：<code>low: Int</code>、<code>fast: Int</code><br>时间复杂度：<code>O(1)</code><br>空间复杂度：<code>log(1)</code><br>思路：使用快慢指针，快指针用来遍历数组，慢指针用来定位与快指针所指向的元素比较，若不同则将快指针指向的元素与慢指针之后的元素调换位置，若相等则快指针继续遍历。</p></blockquote><p><strong>Java</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">removeDuplicates</span><span class="params">(<span class="type">int</span>[] nums)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">low</span> <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">        <span class="type">int</span> <span class="variable">fast</span> <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span>(nums[fast] != nums[low-<span class="number">1</span>]) &#123;</span><br><span class="line">                nums[low] = nums[fast];</span><br><span class="line">                low++;</span><br><span class="line">            &#125;</span><br><span class="line">            fast++;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> low;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Python</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">removeDuplicates</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        low: <span class="built_in">int</span> = <span class="number">1</span></span><br><span class="line">        fast: <span class="built_in">int</span> = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums) - <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> nums[fast] != nums[low - <span class="number">1</span>]:</span><br><span class="line">                nums[low] = nums[fast]</span><br><span class="line">                low += <span class="number">1</span></span><br><span class="line">            fast += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> low</span><br></pre></td></tr></table></figure><p><strong>Scala</strong></p><p>Scala需要注意的是for循环的遍历次数，使用until后还需要再减1，否则就会报数组越界异常；还有一点是空数组的情况，它也会报数组越界异常，所以要在程序开始前加判断语句，数组若为空则直接return0。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">removeDuplicates</span></span>(nums: <span class="type">Array</span>[<span class="type">Int</span>]): <span class="type">Int</span> = &#123;</span><br><span class="line">        <span class="keyword">if</span> (nums.length == <span class="number">0</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">var</span> low: <span class="type">Int</span> = <span class="number">1</span></span><br><span class="line">        <span class="keyword">var</span> fast: <span class="type">Int</span> = <span class="number">1</span></span><br><span class="line">        <span class="keyword">var</span> i: <span class="type">Int</span> = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">0</span> until nums.length<span class="number">-1</span>) &#123;</span><br><span class="line">            <span class="keyword">if</span> (nums(fast) != nums(low<span class="number">-1</span>)) &#123;</span><br><span class="line">                nums(low) = nums(fast)</span><br><span class="line">                low += <span class="number">1</span></span><br><span class="line">            &#125;</span><br><span class="line">            fast += <span class="number">1</span></span><br><span class="line">        &#125;</span><br><span class="line">        low</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="2-买卖股票的最佳时机-II"><a href="#2-买卖股票的最佳时机-II" class="headerlink" title="2. 买卖股票的最佳时机 II"></a>2. 买卖股票的最佳时机 II</h2><p>给定一个数组 prices ，其中 prices[i] 是一支给定股票第 i 天的价格。设计一个算法来计算你所能获取的最大利润。你可以尽可能地完成更多的交易（多次买卖一支股票）。注意：你不能同时参与多笔交易（你必须在再次购买前出售掉之前的股票）。</p><p><strong>示例 1:</strong></p><blockquote><p>输入: prices &#x3D; [7,1,5,3,6,4]<br>输出: 7<br>解释: 在第 2 天（股票价格 &#x3D; 1）的时候买入，在第 3 天（股票价格 &#x3D; 5）的时候卖出, 这笔交易所能获得利润 &#x3D; 5-1 &#x3D; 4 。随后，在第 4 天（股票价格 &#x3D; 3）的时候买入，在第 5 天（股票价格 &#x3D; 6）的时候卖出, 这笔交易所能获得利润 &#x3D; 6-3 &#x3D; 3 。</p></blockquote><p><strong>示例 2:</strong></p><blockquote><p>输入: prices &#x3D; [1,2,3,4,5]<br>输出: 4<br>解释: 在第 1 天（股票价格 &#x3D; 1）的时候买入，在第 5 天 （股票价格 &#x3D; 5）的时候卖出, 这笔交易所能获得利润 &#x3D; 5-1 &#x3D; 4 。注意你不能在第 1 天和第 2 天接连购买股票，之后再将它们卖出。因为这样属于同时参与了多笔交易，你必须在再次购买前出售掉之前的股票。</p></blockquote><p><strong>示例 3:</strong></p><blockquote><p>输入: prices &#x3D; [7,6,4,3,1]<br>输出: 0<br>解释: 在这种情况下, 没有交易完成, 所以最大利润为 0。</p></blockquote><h3 id="题解（2021-09-30）"><a href="#题解（2021-09-30）" class="headerlink" title="题解（2021-09-30）"></a>题解（2021-09-30）</h3><blockquote><p>变量：<code>Int: total = 0</code><br>时间复杂度：<code>O(n)</code><br>空间复杂度：<code>log(n)</code><br>思路：不用考虑太多，记着一个原则，如果当天大于前一天的则买入，同时卖掉手里前一天的，否则不买不卖；当天买入同时卖掉前一天实则赚的是当天和前一天的差价，所以可以将循环内的代码简化为求当天减去前一天价格的值和0的最大值，若为负数则是当天小于前一天价格，total加0，否则total加差值</p></blockquote><p><strong>Java</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">maxProfit</span><span class="params">(<span class="type">int</span>[] prices)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">total</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt; prices.length; i++) &#123;</span><br><span class="line">            <span class="keyword">if</span> (prices[i] &gt; prices[i-<span class="number">1</span>]) &#123;</span><br><span class="line">                total -= prices[i-<span class="number">1</span>];</span><br><span class="line">                total += prices[i];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> total;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Python</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">maxProfit</span>(<span class="params">self, prices: <span class="type">List</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        total = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(prices)):</span><br><span class="line">            <span class="keyword">if</span> prices[i] &gt; prices[i-<span class="number">1</span>]:</span><br><span class="line">                total -= prices[i-<span class="number">1</span>]</span><br><span class="line">                total += prices[i]</span><br><span class="line">        <span class="keyword">return</span> total</span><br></pre></td></tr></table></figure><p><strong>Scala</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">maxProfit</span></span>(prices: <span class="type">Array</span>[<span class="type">Int</span>]): <span class="type">Int</span> = &#123;</span><br><span class="line">        <span class="keyword">var</span> total: <span class="type">Int</span> = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> (i &lt;- <span class="number">1</span> until prices.length) &#123;</span><br><span class="line">            <span class="keyword">if</span> (prices(i) &gt; prices(i<span class="number">-1</span>)) &#123;</span><br><span class="line">                total -= prices(i<span class="number">-1</span>)</span><br><span class="line">                total += prices(i)</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> total</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><strong>Java改良版</strong></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="type">int</span> <span class="title function_">maxProfit</span><span class="params">(<span class="type">int</span>[] prices)</span> &#123;</span><br><span class="line">        <span class="type">int</span> <span class="variable">total</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt; prices.length; i++) &#123;</span><br><span class="line">            total += Math.max(prices[i] - prices[i-<span class="number">1</span>], <span class="number">0</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> total;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>完善数据思维</title>
      <link href="/2021/09/29/Data/%E5%AE%8C%E5%96%84%E6%95%B0%E6%8D%AE%E6%80%9D%E7%BB%B4/"/>
      <url>/2021/09/29/Data/%E5%AE%8C%E5%96%84%E6%95%B0%E6%8D%AE%E6%80%9D%E7%BB%B4/</url>
      
        <content type="html"><![CDATA[<p>我所要说的数据思维不是常规下数据分析和业务分析下的数据思维，而是作为一个数据工作者，处理者的数据全局思维。</p><span id="more"></span><p>起因是偶然发现的一个问题，业务方想回溯某一短时间内的数，他们感觉这个数减少的太厉害，超过心理预期，所以想从其他的角度重新计算这个数，对比是否是计算上出了问题，这个要求本身没问题，问题在于他的回溯方向和我的反应。<br>原本的需求是用户经过A事件再经过B事件后统计最终的人数，他回溯的方法是经过B事件的人之中有多少经过了A事件。从计算的角度来看，在两个事件时间先后顺序不发生改变的前提下，这两种计算方式的结果就是一样的，都是对两部分人求交集，这是业务方选择的回溯方式有问题。</p><p>我的问题在于第一时间没有反应过来回溯方式的问题，二是对于以前的需求跑完就忘了，得看着代码想半天或者别人提醒才想的起来，这一方面是公司业务规模小，没有元数据管理以及数据治理，需求完成了就是完成了，除了代码和邮件没有其他的记录，使得回溯起来稍有困难，并且不利于综合管理。</p><p>作为一个数据管理者不只是完成业务方的需求和技术上的不断精进，更要深入理解业务以及管理好数据资产，对已完成的数据需求进行梳理归纳，对数据质量进行严格把关，对数据血缘以及流程进行记录。在数据规模达到一定程度的时候这些工作都由专人负责，但自己也必须有这方面的意识，这也是一个数据人必须要依托一个大平台的原因，进一步完善数据世界观。</p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mind </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark应用-常见应用案例</title>
      <link href="/2021/09/26/Software/Spark%E5%BA%94%E7%94%A8-%E5%B8%B8%E8%A7%81%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B/"/>
      <url>/2021/09/26/Software/Spark%E5%BA%94%E7%94%A8-%E5%B8%B8%E8%A7%81%E5%BA%94%E7%94%A8%E6%A1%88%E4%BE%8B/</url>
      
        <content type="html"><![CDATA[<p>Spark常见应用案例，连接数据库，压缩，输入输出等操作方法。</p><span id="more"></span><h2 id="工具函数"><a href="#工具函数" class="headerlink" title="工具函数"></a>工具函数</h2><p><a href="https://github.com/w749/bigdata-example/blob/master/spark-test/src/main/scala/org/example/spark/Utils.scala">Utils</a></p><p>常用工具函数</p><h2 id="读写-Excel"><a href="#读写-Excel" class="headerlink" title="读写 Excel"></a>读写 Excel</h2><p><a href="https://github.com/w749/bigdata-example/blob/master/spark-test/src/main/scala/org/example/spark/io/ExcelTest.scala">ExcelTest</a></p><p>导入Maven依赖：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>com.crealytics<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>spark-excel_2.12<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">version</span>&gt;</span>0.13.7<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><p>使用的是 crealytics 的 <a href="(https://codechina.csdn.net/mirrors/crealytics/spark-excel)">spark-excel</a> （CSDN 的国内镜像仓库）解决 Excel 的读写问题，可以直接读取为 DataFrame ，<br>常用的表头、读取 sheet 以及读取位置等参数都可以配置，写入的话支持写入到单个 Excel 文件而不是目录，常用的参数有写入表头、写入位置以及写入模式都参数配置，还支持同一个Excel文件多次写入。</p><h2 id="读写-MySQL"><a href="#读写-MySQL" class="headerlink" title="读写 MySQL"></a>读写 MySQL</h2><p><a href="https://github.com/w749/bigdata-example/blob/master/spark-test/src/main/scala/org/example/spark/io/MysqlTest.scala">MysqlTest</a></p><p>Spark分别使用RDD和DataFrame读写Mysql</p><h2 id="Spark正则多模匹配"><a href="#Spark正则多模匹配" class="headerlink" title="Spark正则多模匹配"></a>Spark正则多模匹配</h2><p><a href="https://github.com/w749/bigdata-example/blob/master/spark-test/src/main/scala/org/example/spark/cases/HyperScanJava.scala">HyperScanJava</a></p><p>使用 <a href="https://github.com/gliwka/hyperscan-java">hyperscan-java</a> 完成多个正则表达式快速匹配一个字符串的需求，<br>它其实是对C语言编写的HyperScan做了封装，使得效率大大提升，具体查看 <a href="https://github.com/intel/hyperscan">hyperscan</a> 。如果要在项目中使用在运行时还需添加几个依赖，hyperscan的动态库目前支持windows, linux (glibc &gt;&#x3D;2.12) and osx for x86_64：</p><ul><li><a href="https://repo1.maven.org/maven2/org/bytedeco/javacpp/1.5.4/javacpp-1.5.4.jar">javacpp-1.5.4.jar</a></li><li><a href="https://repo1.maven.org/maven2/org/bytedeco/javacpp-platform/1.5.4/javacpp-platform-1.5.4.jar">javacpp-platform-1.5.4.jar</a></li><li><a href="https://repo1.maven.org/maven2/org/bytedeco/javacpp/1.5.5/javacpp-1.5.5-linux-x86.jar">javacpp-1.5.5-linux-x86_64.jar</a></li><li><a href="https://repo1.maven.org/maven2/com/gliwka/hyperscan/hyperscan/5.4.0-2.0.0/hyperscan-5.4.0-2.0.0.jar">hyperscan-5.4.0-2.0.0.jar</a></li><li><a href="https://repo1.maven.org/maven2/com/gliwka/hyperscan/native/5.4.0-1.0.0/native-5.4.0-1.0.0.jar">native-5.4.0-1.0.0.jar</a></li><li><a href="https://repo1.maven.org/maven2/com/gliwka/hyperscan/native/5.4.0-1.0.0/native-5.4.0-1.0.0-linux-x86_64.jar">native-5.4.0-1.0.0-linux-x86_64.jar</a></li><li><a href="https://repo1.maven.org/maven2/com/gliwka/hyperscan/native/5.4.0-1.0.0/native-5.4.0-1.0.0-windows-x86_64.jar">native-5.4.0-1.0.0-windows-x86_64.jar</a></li><li><a href="https://repo1.maven.org/maven2/com/gliwka/hyperscan/native/5.4.0-1.0.0/native-5.4.0-1.0.0-macosx-x86_64.jar">native-5.4.0-1.0.0-macosx-x86_64.jar</a></li></ul><div align=center><img src="16657181106060.png"></div><h2 id="Spark解压缩tar-gz"><a href="#Spark解压缩tar-gz" class="headerlink" title="Spark解压缩tar.gz"></a>Spark解压缩tar.gz</h2><p><a href="https://github.com/w749/bigdata-example/blob/master/spark-test/src/main/scala/org/example/spark/cases/TarGzipCodec.scala">TarGzipCodec</a></p><p>自定义CompressionCodec实现输入输出tar.gz压缩格式。其中读取数据的实现没什么问题，压缩为tar.gz的时候会有一些问题，<br>创建tar输出流时必须指定一个TarArchiveEntry的size，代表需要归档的数据大小，但是spark运行时无法获取最终写入的数据大小，所以就无法通过这种方式写入，<br>最终想到一个办法是先将需要写入的数据放在一个ArrayBuffer中，close流的时候再遍历写入，这种方法在输出数据量特别大时可能会造成内存溢出</p><p><a href="https://github.com/w749/bigdata-example/blob/master/spark-test/src/main/scala/org/example/spark/cases/TarGzipCodec.scala#L48">compressTextFile</a></p><p>还可以通过另外一种方式就是先输出为text file，然后再将它压缩为tar.gz文件，缺点是先要写出到一个临时目录中，而且text file会占用大量空间，不过好在这种方式速度并不会很慢</p><h2 id="Spark多目录输出"><a href="#Spark多目录输出" class="headerlink" title="Spark多目录输出"></a>Spark多目录输出</h2><p><a href="https://github.com/w749/bigdata-example/blob/master/spark-test/src/main/scala/org/example/spark/cases/MultipleOutputFormat.scala">MultipleOutputFormat</a></p><p>自定义MultipleTextOutputFormat，满足根据key自定义输出目录以及输出文件名称的需求，并且不输出key</p><h2 id="Spark获取输入的数据所属文件名称"><a href="#Spark获取输入的数据所属文件名称" class="headerlink" title="Spark获取输入的数据所属文件名称"></a>Spark获取输入的数据所属文件名称</h2><p><a href="https://github.com/w749/bigdata-example/blob/master/spark-test/src/main/scala/org/example/spark/cases/GetInputFileName.scala">GetInputFileName</a></p><p>Spark获取输入的数据所属文件名称，如需获取全路径可以在getPath后调用toUri方法再调用getName</p><h2 id="Spark指定每个分区的输出大小"><a href="#Spark指定每个分区的输出大小" class="headerlink" title="Spark指定每个分区的输出大小"></a>Spark指定每个分区的输出大小</h2><p><a href="https://github.com/w749/bigdata-example/blob/master/spark-test/src/main/scala/org/example/spark/cases/ControlPartitionSize.scala">ControlPartitionSize</a></p><p>Spark指定每个分区的输出大小，提供了三种方法，分别是自定义分区器（repartitionData）、重分区（restoreData），这两种方法主要利用了FileSystem的getContentSummary方法获取到输入数据的大小，计算出输出指定大小的分区所需的分区数量，<br>第三种方法通过控制split size的目的达到控制每个分区的数据大小，但是这种方法会将超出指定大小的数据单存到一个文件中（128M会分为100M和28M），达不到所有文件的大小相等，经过测试restoreData方法最靠谱。</p><p>这几种方式必须是输入和输出数据相同未经过过滤或者flat，如果输入输出数据大小不相同可以借助临时目录</p><h2 id="SparkListener事件监听"><a href="#SparkListener事件监听" class="headerlink" title="SparkListener事件监听"></a>SparkListener事件监听</h2><p><a href="https://github.com/w749/bigdata-example/blob/master/spark-test/src/main/scala/org/example/spark/cases/SparkListenerCase.scala">SparkListenerCase</a></p><p>SparkListener监听器，负责监视Spark作业运行时的状态，可以自定义实现SparkListener收集运行时的状态</p>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark入门-累加器和广播变量</title>
      <link href="/2021/09/15/Software/Spark%E5%85%A5%E9%97%A8-%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%92%8C%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/"/>
      <url>/2021/09/15/Software/Spark%E5%85%A5%E9%97%A8-%E7%B4%AF%E5%8A%A0%E5%99%A8%E5%92%8C%E5%B9%BF%E6%92%AD%E5%8F%98%E9%87%8F/</url>
      
        <content type="html"><![CDATA[<h3 id="累加器"><a href="#累加器" class="headerlink" title="累加器"></a>累加器</h3><p>Spark 中累加器用来解决一些特殊的需求。假设有一个需求，对 RDD 中的数据进行累加求和并返回，当然使用 reduce 可以很好地实现，但我们想尝试一下定义一个新的变量，依次遍历 RDD 对每个元素进行累加最后返回这个变量的值。</p><span id="more"></span><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().master(<span class="string">&quot;local[*]&quot;</span>).getOrCreate()</span><br><span class="line"><span class="keyword">val</span> sc: <span class="type">SparkContext</span> = spark.sparkContext</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">var</span> total: <span class="type">Int</span> = <span class="number">0</span></span><br><span class="line">rdd.foreach(total += _)</span><br><span class="line">println(total)  <span class="comment">// 0</span></span><br></pre></td></tr></table></figure><p>没错结果为 0 ，其实原因很简单，不管是 foreach 还是 map ，它的计算都是在 executor 端执行的，而变量 total 在 driver 端，在 foreach 里面累加的只是 executor 端 total 变量的副本，而这个副本并没有再传回给 driver 端，所以 driver 端的 total 变量还是 0 ，上方代码的实现如下图所示。</p><div align=center><img src="累加器-错误实现.png"></div><p>累加器用来把 executor 端变量信息聚合到 driver 端。在 driver 程序中定义的变量，在 executor 端的每个 Task 都会得到这个变量的一份新的副本，每个 task 更新这些副本的值后，传回 driver 端进行 merge。他是一个分布式只写共享变量，每个 executor 之间是不可见的，只有 driver 端可以对其读操作。例如同样是上方的例子，将它注册为累加器即可。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>))</span><br><span class="line"><span class="keyword">val</span> total: <span class="type">LongAccumulator</span> = sc.longAccumulator(<span class="string">&quot;total&quot;</span>)  <span class="comment">// 注册为Long类型的累加器</span></span><br><span class="line">rdd.foreach(&#123;</span><br><span class="line">  num =&gt; &#123;</span><br><span class="line">    total.add(num)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;)</span><br><span class="line">println(total.value)  <span class="comment">// 10</span></span><br></pre></td></tr></table></figure><p>Spark 中内置了 longAccumulator、doubleAccumulator 和 collectionAccumulator 几种累加器，但是生产中可能有其他的需求需要自定义累加器，查看源码得知需要混入 AccumulatorV2 特质后注册即可使用。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// longAccumulator 累加器注册</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">longAccumulator</span></span>(name: <span class="type">String</span>): <span class="type">LongAccumulator</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> acc = <span class="keyword">new</span> <span class="type">LongAccumulator</span></span><br><span class="line">    register(acc, name)</span><br><span class="line">    acc</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 继续看 LongAccumulator 类发现混入了特质并定义了输入输出泛型</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LongAccumulator</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[jl.<span class="type">Long</span>, jl.<span class="type">Long</span>]</span></span><br></pre></td></tr></table></figure><p>下面实现 WordCount 累加器，从过程上来讲直接省略了 Shuffle 阶段， executor 计算完之后再传给 driver 端 merge 。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.makeRDD(<span class="type">List</span>(<span class="string">&quot;Hello World&quot;</span>, <span class="string">&quot;Hello Scala&quot;</span>, <span class="string">&quot;Scala Java&quot;</span>))</span><br><span class="line"><span class="keyword">val</span> wcAcc = <span class="keyword">new</span> <span class="type">WordCountAccumulator</span></span><br><span class="line">sc.register(wcAcc, <span class="string">&quot;wcAcc&quot;</span>)  <span class="comment">// 注册累加器</span></span><br><span class="line">rdd.flatMap(_.split(<span class="string">&quot; &quot;</span>)).foreach(</span><br><span class="line">  word =&gt; &#123;</span><br><span class="line">    wcAcc.add(word)  <span class="comment">// 传入泛型中定义的输入</span></span><br><span class="line">  &#125;</span><br><span class="line">)</span><br><span class="line">println(wcAcc.value)  <span class="comment">// 使用value返回输出</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 自定义累加器，main方法外</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordCountAccumulator</span> <span class="keyword">extends</span> <span class="title">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]] </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 定义累加器的数据类型，也是返回的类型</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">val</span> map: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = mutable.<span class="type">Map</span>()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 累加器是否为初始状态</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">isZero</span></span>: <span class="type">Boolean</span> = map.isEmpty</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 复制累加器</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">copy</span></span>(): <span class="type">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]] = <span class="keyword">new</span> <span class="type">WordCountAccumulator</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 重置累加器</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reset</span></span>(): <span class="type">Unit</span> = map.clear()</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 向累加器增加数据（在每个executor中副本的运算规则）</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">add</span></span>(word: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">      map(word) = map.getOrElse(word, <span class="number">0</span>L) + <span class="number">1</span>L</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 合并累加器（每个executor运算完成后返回给driver端所有副本的合并规则）</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(other: <span class="type">AccumulatorV2</span>[<span class="type">String</span>, mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>]]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">      <span class="keyword">val</span> map1: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = map</span><br><span class="line">      <span class="keyword">val</span> map2: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = other.value</span><br><span class="line"></span><br><span class="line">      map2.foreach&#123;</span><br><span class="line">        kv =&gt; &#123;</span><br><span class="line">          map1(kv._1) = map1.getOrElse(kv._1, <span class="number">0</span>L) + kv._2</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 返回累加器的结果</span></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">value</span></span>: mutable.<span class="type">Map</span>[<span class="type">String</span>, <span class="type">Long</span>] = map</span><br></pre></td></tr></table></figure><p><strong>累加器在使用过程中有两个需要注意的点，分别是少加和多加。</strong>少加比较容易理解，在没有行动算子调用之前它是不会执行的，直接使用 foreach 或者使用 map 后再使用 collect 就可以避免这种情况；再就是多加，每调用一次行动算子累加器就会执行一次，但是在程序结束前它不会自动清空，所以需要手动使用 wcAcc.reset() 方法重置累加器，或者重新注册新的累加器。</p><h3 id="广播变量"><a href="#广播变量" class="headerlink" title="广播变量"></a>广播变量</h3><p>广播变量是一个分布式共享只读变量，用来高效分发较大的对象。向所有 executor 发送一个较大的只读值，以供一个或多个 Spark 操作使用。比如，如果你的应用需要向所有 executor 发送一个较大的只读查询表， 广播变量用起来就很顺手。</p><p>在多个并行操作中使用同一个变量，但是 Spark 会为每个 task 分别发送，这样就会造成资源和空间的浪费，将变量广播到所有的 executor ，那么每个 executor 下的所有 task 都可以访问到这个变量而不用在 task 内部存储变量。使用 broadcast 声明并使用 value 调用即可。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(<span class="type">List</span>( (<span class="string">&quot;a&quot;</span>,<span class="number">1</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">4</span>) ))</span><br><span class="line"><span class="keyword">val</span> list = <span class="type">List</span>( (<span class="string">&quot;a&quot;</span>,<span class="number">4</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;c&quot;</span>, <span class="number">6</span>), (<span class="string">&quot;d&quot;</span>, <span class="number">7</span>) )</span><br><span class="line"><span class="keyword">val</span> broadcast: <span class="type">Broadcast</span>[<span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)]] = sc.broadcast(list)  <span class="comment">// 声明广播变量</span></span><br><span class="line">rdd.map &#123;</span><br><span class="line">  <span class="keyword">case</span> (key, num) =&gt; &#123;</span><br><span class="line">    <span class="keyword">var</span> num2 = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> ((k, v) &lt;- broadcast.value) &#123;  <span class="comment">// 使用广播变量</span></span><br><span class="line">      <span class="keyword">if</span> (k == key) num2 = v</span><br><span class="line">    &#125;</span><br><span class="line">    (key, (num, num2))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;.foreach(println)</span><br></pre></td></tr></table></figure><p>需要注意的是广播变量不可以广播 RDD ，因为 RDD 不存储数据，可以将 RDD 的结果广播出去。</p>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark入门-Cache、Persist and Checkpoint</title>
      <link href="/2021/09/11/Software/Spark%E5%85%A5%E9%97%A8-Cache%E3%80%81Persist%20and%20Checkpoint/"/>
      <url>/2021/09/11/Software/Spark%E5%85%A5%E9%97%A8-Cache%E3%80%81Persist%20and%20Checkpoint/</url>
      
        <content type="html"><![CDATA[<p>Spark 在运算过程中提供了几个不同程度的持久化操作，通过将数据保存在内存中、磁盘中或者 HDFS 中来满足我们计算中的持久化需求。</p><span id="more"></span><h3 id="持久化（Persist、Cache）"><a href="#持久化（Persist、Cache）" class="headerlink" title="持久化（Persist、Cache）"></a>持久化（Persist、Cache）</h3><p>在实际开发中某些 RDD 的计算或转换可能会比较耗费时间，如果这些 RDD 后续还会频繁的被使用到，那么可以将这些 RDD 进行持久化&#x2F;缓存，这样下次再使用到的时候就不用再重新计算了，提高了程序运行的效率。Spark 提供了 Persist 和 Cache 两个操作对 RDD 的运算结果进行持久化。</p><p>先看一个例子，我们要对一个文件内的单词进行 WordCount 操作，就按 textFile、flatMap、map、reduceByKey 这个顺序执行，然后有一个新需求，需要按单词将所有出现的单词放到一起，这样我们想到了直接使用上一步操作 WordCount 时 map 操作的结果，想象它可以直接使用这一步的数据进行两步操作，但事实上 RDD 是不存储数据的，如果再用到这个数据的话只能是依据它的血缘关系再计算一遍它所依赖的 RDD ，就像下图这样执行。</p><div align=center><img src="RDD未缓存.png"></div><p>那么 Spark 就给我们提供了 Cache 持久化操作，它允许我们将感兴趣的 RDD 中的数据暂时存储到内存中，以供其他计算再次使用而不需要重新计算之前的 RDD ，这样就达到了我们想要的效果。</p><div align=center><img src="RDD缓存.png"></div><p>Cache 是 Persist 操作的一个特例，源码<code>def cache(): this.type = persist()</code>直接调用的就是无参默认的 Persist ，Persist 需要提供一个参数，默认是MEMORY_ONLY，有 <code>MEMORY_ONLY（只存储在内存中）、MEMORY_ONLY_SER（在内存中存储序列化数据）、DISK_ONLY（只存储在磁盘中）、DISK_ONLY_2（在内存中存取副本）、MEMORY_AND_DISK（存储在内存中和磁盘中）</code>等等。只有触发行动算子的时候才会对数据进行缓存操作，需要注意的是 Cache 和 Persist 都只是在 Application 运行过程中存在，一旦 Job 运行完成这些数据就会销毁。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// persist持久化操作会将RDD数据存储在内存或者磁盘用来复用或是保存重要数据，当前作业完成就会清理掉</span></span><br><span class="line">rdd.persist(<span class="type">StorageLevel</span>.<span class="type">MEMORY_ONLY</span>)</span><br><span class="line">rdd.cache()  <span class="comment">// cache其实就是persist传入MEMORY_ONLY参数值</span></span><br></pre></td></tr></table></figure><h3 id="检查点（Checkpoint）"><a href="#检查点（Checkpoint）" class="headerlink" title="检查点（Checkpoint）"></a>检查点（Checkpoint）</h3><p>持久化 &#x2F; 缓存可以把数据存储在内存或是磁盘中，但也不是最靠谱的，因为可能会发生宕机或者磁盘损坏，所以就有了检查点 Checkpoint 。它可以将数据存储在 HDFS 上，提供了更高的容错机制，同样也可以选择存储在磁盘上，而且数据也不会随着程序的结束而销毁，对于一些重要数据可以选择 Checkpoint 进行存储。使用它需要指定检查点的存放目录，并在需要的 RDD 下开启它。</p><p>关于检查点还有一点需要注意，缓存当前 RDD 时会在它计算完成时将数据缓存到内存或者磁盘中，而检查点并不是，它会重新启动一个 Task 专门为检查点计算数据，所以当前 RDD 会被执行两次，一般为了减少不必要的计算所造成的开销会先将数据缓存到内存中，这样检查点就可以直接从内存中取数并落盘。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sc.setCheckpointDir(<span class="string">&quot;hdfs://node01:9000//spark/checkpoint&quot;</span>)  <span class="comment">// 设置检查点目录，一般放在HDFS</span></span><br><span class="line">rdd.cache()</span><br><span class="line">rdd.checkpoint()  <span class="comment">// 开启检查点</span></span><br></pre></td></tr></table></figure><p>最后是检查点和缓存的血缘关系问题，缓存时会将当前 RDD 的数据以及血缘关系全部保存下来，哪怕分区丢失也可以重新计算，而检查点会将之前的血缘关系全部强制转为 PersistCheckpoint ，无法根据血缘关系再重新计算，这也算是它的一个弊端。</p>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark入门-Dependencies、Lineage and Stage</title>
      <link href="/2021/09/10/Software/Spark%E5%85%A5%E9%97%A8-Dependencies%E3%80%81Lineage%20and%20Stage/"/>
      <url>/2021/09/10/Software/Spark%E5%85%A5%E9%97%A8-Dependencies%E3%80%81Lineage%20and%20Stage/</url>
      
        <content type="html"><![CDATA[<p>Spark 中比较重要的一块就是血缘关系和阶段划分，虽说并不能像累加器或者广播变量解决特定的需求，但对于理解 Spark 计算的任务执行调度有很大的帮助。</p><span id="more"></span><h3 id="Lineage（血缘关系）"><a href="#Lineage（血缘关系）" class="headerlink" title="Lineage（血缘关系）"></a>Lineage（血缘关系）</h3><p>RDD 只支持粗粒度转换，即在大量记录上执行的单个操作。将创建 RDD 的一系列 Lineage (血统)记录下来，以便恢复丢失的分区。RDD 的 Lineage 会记录 RDD 的元数据信息和转换行为，当该 RDD 的部分分区数据丢失时，它可以根据这些信息来重新运算和恢复丢失的数据分区。</p><p>RDD 不保存数据，在没有缓存和检查点的情况下如果需要重复使用 RDD 或者分区丢失只能通过依赖上游的血缘关系恢复当前 RDD 的操作。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;input/1.txt&quot;</span>)</span><br><span class="line">println(<span class="string">&quot;-----------textFile-----------&quot;</span>)</span><br><span class="line">println(fileRDD.toDebugString)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> wordRDD: <span class="type">RDD</span>[<span class="type">String</span>] = fileRDD.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">println(<span class="string">&quot;-----------flatMap-----------&quot;</span>)</span><br><span class="line">println(wordRDD.toDebugString)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordRDD.map((_,<span class="number">1</span>))</span><br><span class="line">println(<span class="string">&quot;-----------map-----------&quot;</span>)</span><br><span class="line">println(mapRDD.toDebugString)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = mapRDD.reduceByKey(_+_)</span><br><span class="line">println(<span class="string">&quot;-----------reduceByKey-----------&quot;</span>)</span><br><span class="line">println(resultRDD.toDebugString)</span><br><span class="line"></span><br><span class="line">resultRDD.collect()</span><br></pre></td></tr></table></figure><p>以上是 WordCount 的步骤，将每一个步骤中的 RDD 的血缘关系打印出来就会发现它们彼此之间存在联系相互依赖。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">-----------textFile-----------</span><br><span class="line">(2) data/WordCount01 MapPartitionsRDD[1] at textFile at RRR.scala:12 []</span><br><span class="line"> |  data/WordCount01 HadoopRDD[0] at textFile at RRR.scala:12 []</span><br><span class="line">-----------flatMap-----------</span><br><span class="line">(2) MapPartitionsRDD[2] at flatMap at RRR.scala:15 []</span><br><span class="line"> |  data/WordCount01 MapPartitionsRDD[1] at textFile at RRR.scala:12 []</span><br><span class="line"> |  data/WordCount01 HadoopRDD[0] at textFile at RRR.scala:12 []</span><br><span class="line">-----------map-----------</span><br><span class="line">(2) MapPartitionsRDD[3] at map at RRR.scala:18 []</span><br><span class="line"> |  MapPartitionsRDD[2] at flatMap at RRR.scala:15 []</span><br><span class="line"> |  data/WordCount01 MapPartitionsRDD[1] at textFile at RRR.scala:12 []</span><br><span class="line"> |  data/WordCount01 HadoopRDD[0] at textFile at RRR.scala:12 []</span><br><span class="line">-----------reduceByKey-----------</span><br><span class="line">(2) ShuffledRDD[4] at reduceByKey at RRR.scala:21 []</span><br><span class="line"> +-(2) MapPartitionsRDD[3] at map at RRR.scala:18 []</span><br><span class="line">    |  MapPartitionsRDD[2] at flatMap at RRR.scala:15 []</span><br><span class="line">    |  data/WordCount01 MapPartitionsRDD[1] at textFile at RRR.scala:12 []</span><br><span class="line">    |  data/WordCount01 HadoopRDD[0] at textFile at RRR.scala:12 []</span><br></pre></td></tr></table></figure><p>上方输出结果一目了然，下游依次依赖上游直至创建 RDD 的最初状态，看下图可以更直观的感受这个血缘关系。</p><div align=center><img src="血缘关系.png"></div><h3 id="Dependencies（依赖关系）"><a href="#Dependencies（依赖关系）" class="headerlink" title="Dependencies（依赖关系）"></a>Dependencies（依赖关系）</h3><p>这里所谓的依赖关系，其实就是两个相邻 RDD 之间的关系。查看依赖使用 dependencies 属性，不过并没有血缘关系展示的直观。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> fileRDD: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;data/WordCount01&quot;</span>)</span><br><span class="line">println(<span class="string">&quot;-----------textFile-----------&quot;</span>)</span><br><span class="line">println(fileRDD.dependencies)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> wordRDD: <span class="type">RDD</span>[<span class="type">String</span>] = fileRDD.flatMap(_.split(<span class="string">&quot; &quot;</span>))</span><br><span class="line">println(<span class="string">&quot;-----------flatMap-----------&quot;</span>)</span><br><span class="line">println(wordRDD.dependencies)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = wordRDD.map((_,<span class="number">1</span>))</span><br><span class="line">println(<span class="string">&quot;-----------map-----------&quot;</span>)</span><br><span class="line">println(mapRDD.dependencies)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> resultRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = mapRDD.reduceByKey(_+_)</span><br><span class="line">println(<span class="string">&quot;-----------reduceByKey-----------&quot;</span>)</span><br><span class="line">println(resultRDD.dependencies)</span><br><span class="line"></span><br><span class="line">resultRDD.collect()</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">-----------textFile-----------</span><br><span class="line">List(org.apache.spark.OneToOneDependency@787e4357)</span><br><span class="line">-----------flatMap-----------</span><br><span class="line">List(org.apache.spark.OneToOneDependency@21ea996f)</span><br><span class="line">-----------map-----------</span><br><span class="line">List(org.apache.spark.OneToOneDependency@6af5b246)</span><br><span class="line">-----------reduceByKey-----------</span><br><span class="line">List(org.apache.spark.ShuffleDependency@3079c26a)</span><br></pre></td></tr></table></figure><p>RDD 之间的依赖又分为窄依赖和宽依赖，它其实是根据前后两个 RDD 之间的转变是否打乱分区决定的，看图</p><div align=center><img src="窄依赖.png"></div><p>上图为窄依赖，父 RDD 的一个分区只会被子 RDD 的一个分区依赖。RDD 操作前后的分区数和分区内的数据是不变的，不用打乱 Shuffle，一般 map、foreach 这种操作都会形成窄依赖。</p><div align=center><img src="宽依赖.png"></div><p>上图为宽依赖，父 RDD 的一个分区会被子 RDD 的多个分区依赖(涉及到 shuffle )。如果有 group、reduce 这些会产生 Shuffle 打乱原 RDD 分区的操作，那么两个 RDD 之间就是宽依赖。</p><p>那么为什么要设计宽窄依赖呢，对于窄依赖：它的多个分区可以并行计算，而且每一个分区的数据如果丢失只需要重新计算对应的分区的数据就可以了。而对于宽依赖：它是划分 Stage 的依据，宽依赖必须等到上一阶段计算完成才能计算下一阶段。</p><h3 id="Stage（阶段划分）"><a href="#Stage（阶段划分）" class="headerlink" title="Stage（阶段划分）"></a>Stage（阶段划分）</h3><h4 id="DAG"><a href="#DAG" class="headerlink" title="DAG"></a>DAG</h4><p>DAG(Directed Acyclic Graph 有向无环图)指的是数据转换执行的过程，有方向，无闭环(其实就是 RDD 执行的流程)；原始的 RDD 通过一系列的转换操作就形成了 DAG 有向无环图，任务执行时，可以按照 DAG 的描述，执行真正的计算(数据被操作的一个过程)。</p><p>DAG 的边界是通过 Action 行动算子来划分的，开始：通过 SparkContext 创建的 RDD；结束：触发 Action，一旦触发 Action 就形成了一个完整的 DAG。</p><h4 id="DAG-划分-Stage"><a href="#DAG-划分-Stage" class="headerlink" title="DAG 划分 Stage"></a>DAG 划分 Stage</h4><div align=center><img src="Stage.png"></div><ul><li>一个 Spark 程序中可以有多个 DAG（有几个 Action 算子就有几个 DAG，上图有一个 Action 算子就只有一个 DAG ），一个 DAG 可以有多个 Stage（根据宽依赖 &#x2F; Shuffle 进行划分）。</li><li>同一个 Stage 可以有多个 Task 并行执行（ Task 数&#x3D;分区数，上图有三个 Task 就有三个分区，需要注意这和有几个 executor 没关系，每台机器分配几个核就有几个 executor，然后根据这台机器上运行几个 Task 来决定每个 executor 运行几个 Task ）。可以看到上图 DAG 中只有 reduceByKey 操作是一个宽依赖，Spark 内核会以此为边界将其前后划分成不同的 Stage。</li><li>同时我们可以注意到，在图中 Stage1 中，从 textFile 到 flatMap 到 map 都是窄依赖，这几步操作可以形成一个流水线操作，通过 flatMap 操作生成的 partition 可以不用等待整个 RDD 计算结束，而是继续进行 map 操作，这样大大提高了计算的效率。</li></ul><h4 id="为何要划分-Stage"><a href="#为何要划分-Stage" class="headerlink" title="为何要划分 Stage"></a>为何要划分 Stage</h4><p>一个复杂的业务逻辑如果有 shuffle，那么就意味着前面阶段产生结果后，才能执行下一个阶段，即下一个阶段的计算要依赖上一个阶段的数据。那么我们按照 shuffle &#x2F; 宽依赖进行划分，就可以将一个 DAG 划分成多个 Stage &#x2F; 阶段，在同一个 Stage 中，会有多个算子操作，可以形成一个 pipeline 流水线，流水线内的多个平行的分区可以并行执行。</p><p>对于窄依赖划分 Stage 时，partition 的转换处理在 Stage 中完成计算，不划分(将窄依赖尽量放在在同一个 Stage 中，可以实现流水线计算)。对于宽依赖，由于有 Shuffle 的存在，只能在父 RDD 处理完成后，才能开始接下来的计算，也就是说需要划分 Stage 。</p><p>Spark 会根据 Shuffle &#x2F; 宽依赖使用回溯算法来对 DAG 进行 Stage 划分，从后往前，遇到宽依赖就断开，遇到窄依赖就把当前的 RDD 加入到当前的 Stage &#x2F; 阶段中，这一点可以使用 RDD 的 toDebugString 方法查看，看到<code>+-</code>符号就是断开划分阶段。</p><p>DAGScheduler 按照 ShuffleDependency 作为 Stage 的划分节点对 RDD 的 DAG 进行 Stage 划分（上游的 Stage 将为 ShuffleMapStage）。因此一个 Job 可能被划分为一到多个 Stage 。Stage 分为 ShuffleMapStage 和 ResultStage 两种。</p><h3 id="Job-和-Task"><a href="#Job-和-Task" class="headerlink" title="Job 和 Task"></a>Job 和 Task</h3><ul><li>Job：用户提交的作业。当 RDD 及其 DAG 被提交给 DAGScheduler 调度后，DAGScheduler 会将所有 RDD 中的转换及动作视为一个 Job。一个 Job 由一到多个 Task 组成。</li><li>Task：具体执行任务。一个 Job 在每个 Stage 内都会按照 RDD 的 Partition 数量，创建多个 Task。Task 分为 ShuffleMapTask 和 ResultTask 两种。ShuffleMapStage 中的 Task 为 ShuffleMapTask，而 ResultStage 中 的 Task 为 ResultTask。ShuffleMapTask 和 ResultTask 类似于 Hadoop 中的 Map 任务和 Reduce 任务。<br>oozie job -oozie <a href="http://localhost:11000/oozie">http://localhost:11000/oozie</a> -rerun 0000411-180116183039102-oozie-hado-C</li></ul>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark入门-UDF和UDAF自定义函数</title>
      <link href="/2021/09/09/Software/Spark%E5%85%A5%E9%97%A8-UDF%E5%92%8CUDAF%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0/"/>
      <url>/2021/09/09/Software/Spark%E5%85%A5%E9%97%A8-UDF%E5%92%8CUDAF%E8%87%AA%E5%AE%9A%E4%B9%89%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p>在 Spark 处理数据的过程中，虽然 DataSet 下的算子不多，但已经可以处理大多数的数据需求，但仍有少数需求需要自定义函数。UDF(User Defined Functions) 是普通的不会产生 Shuffle 不会划分新的阶段的用户自定义函数，UDAF(User Defined Aggregator Functions) 则会打乱分区，用户自定义聚合函数。</p><span id="more"></span><h3 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h3><p>因为 UDF 不需要打乱分区，直接对 RDD 每个分区中的数据进行处理并返回当前分区，所以可以直接注册 UDF 函数，甚至可以传入匿名函数。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> org.apache.spark.sql.functions  <span class="comment">// DSL中定义UDF需要</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">User</span>] = spark.sparkContext.makeRDD(</span><br><span class="line">  <span class="type">List</span>(<span class="type">User</span>(<span class="string">&quot;Bob&quot;</span>, <span class="number">23</span>), <span class="type">User</span>(<span class="string">&quot;Alice&quot;</span>, <span class="number">22</span>), <span class="type">User</span>(<span class="string">&quot;John&quot;</span>, <span class="number">24</span>)))</span><br><span class="line"><span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">User</span>] = rdd.toDS</span><br><span class="line">ds.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL中使用就需要注册UDF</span></span><br><span class="line">spark.udf.register(<span class="string">&quot;add_name&quot;</span>, (str: <span class="type">String</span>) =&gt; &#123; <span class="string">&quot;Name: &quot;</span> + str &#125;)</span><br><span class="line">spark.sql(<span class="string">&quot;select name, add_name(name) as new_name from user&quot;</span>).show()</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用DSL则不用注册，定义好直接使用即可</span></span><br><span class="line"><span class="keyword">val</span> add_name2: <span class="type">UserDefinedFunction</span> = functions.udf((str: <span class="type">String</span>) =&gt; &#123;</span><br><span class="line">  <span class="string">&quot;Name: &quot;</span> + str</span><br><span class="line">&#125;)</span><br><span class="line">ds.withColumn(<span class="string">&quot;name&quot;</span>, add_name2($<span class="string">&quot;name&quot;</span>)).show()  </span><br></pre></td></tr></table></figure><h3 id="UDAF"><a href="#UDAF" class="headerlink" title="UDAF"></a>UDAF</h3><p>相比较 UDF 而言因为 UDAF 是聚合函数所以要打乱分区，所以也就比较复杂，并且需要重写指定的方法来定义。需要注意的是针对弱类型的 UserDefinedAggregateFunction 已经弃用，普遍使用强类型的 Aggregator ，同时若想在 Spark3.0 版本之前使用强类型 UDAF 和 Spark3.0 版本之后的定义方式略有不同。数据如下，计算每家门店的用户数量以及总付款额</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">store,user,payment</span><br><span class="line">1,Bob,12.00</span><br><span class="line">1,Alice,44.12</span><br><span class="line">1,John,23.20</span><br><span class="line">2,Davin,79.00</span><br><span class="line">2,Lim,33.30</span><br><span class="line">...</span><br></pre></td></tr></table></figure><h4 id="UserDefinedAggregateFunction"><a href="#UserDefinedAggregateFunction" class="headerlink" title="UserDefinedAggregateFunction"></a>UserDefinedAggregateFunction</h4><p>首先是已经弃用的 UserDefinedAggregateFunction，以防生产环境中仍有使用老版本的 Spark。它使用的是弱类型，所以在编写过程中你会看到使用 0 或者 1 来指定位置，这十分不方便。先是数据构建和调用部分</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 注册并调用UDAF，写在main方法中</span></span><br><span class="line"><span class="keyword">val</span> ds: <span class="type">Dataset</span>[<span class="type">Record</span>] = spark</span><br><span class="line">  .sparkContext</span><br><span class="line">  .makeRDD(</span><br><span class="line">    <span class="type">List</span>(<span class="type">Record</span>(<span class="number">1</span>, <span class="string">&quot;Bob&quot;</span>, <span class="number">12.00</span>), <span class="type">Record</span>(<span class="number">1</span>, <span class="string">&quot;Alice&quot;</span>, <span class="number">44.12</span>), <span class="type">Record</span>(<span class="number">1</span>, <span class="string">&quot;John&quot;</span>, <span class="number">23.20</span>),</span><br><span class="line">      <span class="type">Record</span>(<span class="number">2</span>, <span class="string">&quot;Davin&quot;</span>, <span class="number">79.00</span>), <span class="type">Record</span>(<span class="number">2</span>, <span class="string">&quot;Lim&quot;</span>, <span class="number">33.30</span>))).toDS</span><br><span class="line"></span><br><span class="line">ds.createOrReplaceTempView(<span class="string">&quot;record&quot;</span>)</span><br><span class="line">spark.udf.register(<span class="string">&quot;myudaf01&quot;</span>, <span class="keyword">new</span> <span class="type">MyUDAF01</span>)  <span class="comment">// 注册UDAF函数</span></span><br><span class="line">spark.sql(</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    |select store, myudaf01(payment) as summary from record group by store</span></span><br><span class="line"><span class="string">    |&quot;&quot;&quot;</span>.stripMargin).show(truncate = <span class="literal">false</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 样例类，写在main方法外</span></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">Record</span>(<span class="params">store: <span class="type">Int</span>, name: <span class="type">String</span>, payment: <span class="type">Double</span></span>)</span></span><br></pre></td></tr></table></figure><p>下面是 UDAF 类</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// UDAF部分，写在main方法外</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyUDAF01</span> <span class="keyword">extends</span> <span class="title">UserDefinedAggregateFunction</span> </span>&#123;</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 聚合函数输入参数的数据类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">inputSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="type">StructType</span>(<span class="type">Array</span>(<span class="type">StructField</span>(<span class="string">&quot;payment&quot;</span>, <span class="type">DoubleType</span>)))</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 聚合函数缓冲区中值的类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferSchema</span></span>: <span class="type">StructType</span> = &#123;</span><br><span class="line">    <span class="type">StructType</span>(<span class="type">Array</span>(</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">&quot;total_user&quot;</span>, <span class="type">IntegerType</span>),</span><br><span class="line">      <span class="type">StructField</span>(<span class="string">&quot;total_payment&quot;</span>, <span class="type">DoubleType</span>)</span><br><span class="line">    ))</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 函数返回的数据类型</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">dataType</span></span>: <span class="type">DataType</span> = <span class="type">StringType</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">// 对于相同的输入是否一直返回相同的输出</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">deterministic</span></span>: <span class="type">Boolean</span> = <span class="literal">true</span></span><br><span class="line">  </span><br><span class="line"><span class="comment">// 函数buffer缓冲区初始化，初始值</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">initialize</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer(<span class="number">0</span>) = <span class="number">0</span>  <span class="comment">// 人数初始值</span></span><br><span class="line">    buffer(<span class="number">1</span>) = <span class="number">0.00</span>  <span class="comment">// 总额初始值</span></span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 更新缓冲区中的数据</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">update</span></span>(buffer: <span class="type">MutableAggregationBuffer</span>, input: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer(<span class="number">0</span>) = buffer.getInt(<span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">    buffer(<span class="number">1</span>) = buffer.getDouble(<span class="number">1</span>) + input.getDouble(<span class="number">0</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 合并缓冲区（类似于reduce，属于两个元素的合并规则）</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(buffer1: <span class="type">MutableAggregationBuffer</span>, buffer2: <span class="type">Row</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    buffer1(<span class="number">0</span>) = buffer1.getInt(<span class="number">0</span>) + buffer2.getInt(<span class="number">0</span>)</span><br><span class="line">    buffer1(<span class="number">1</span>) = buffer1.getDouble(<span class="number">1</span>) + buffer2.getDouble(<span class="number">1</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line"><span class="comment">// 计算最终结果</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">evaluate</span></span>(buffer: <span class="type">Row</span>): <span class="type">Any</span> = </span><br><span class="line">  <span class="string">&quot;user: &quot;</span> + buffer.getInt(<span class="number">0</span>) + <span class="string">&quot;,payment: &quot;</span> + buffer.getDouble(<span class="number">1</span>)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Aggregator（Spark3-0版本以后）"><a href="#Aggregator（Spark3-0版本以后）" class="headerlink" title="Aggregator（Spark3.0版本以后）"></a>Aggregator（Spark3.0版本以后）</h3><p>接下来是 Aggregator，使用的是强类型编写，需要实现不同的特质，将输入、Buffer 以及输出全部定义在了泛型中，这样编写过程中就不需要使用位置来定位了，而且重写方法也简单易懂。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">spark.udf.register(<span class="string">&quot;myudaf02&quot;</span>, functions.udaf(<span class="keyword">new</span> <span class="type">MyUDAF02</span>))  <span class="comment">// 注册UDAF函数</span></span><br><span class="line">spark.sql(</span><br><span class="line">  <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    |select store, myudaf02(payment) from record group by store</span></span><br><span class="line"><span class="string">    |&quot;&quot;&quot;</span>.stripMargin).show(truncate = <span class="literal">false</span>)</span><br></pre></td></tr></table></figure><p>下面是 UDAF 类</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">StoreSummary</span>(<span class="params">var user: <span class="type">Int</span>, var payment: <span class="type">Double</span></span>)  <span class="comment">// 强类型UDAF函数Buffer类型</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyUDAF02</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Double</span>, <span class="type">StoreSummary</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 初始化Buffer中的字段</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">StoreSummary</span> = &#123;</span><br><span class="line">    <span class="type">StoreSummary</span>(<span class="number">0</span>, <span class="number">0.00</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 输入到Buffer的聚合</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(b: <span class="type">StoreSummary</span>, a: <span class="type">Double</span>): <span class="type">StoreSummary</span> = &#123;</span><br><span class="line">    b.user += <span class="number">1</span></span><br><span class="line">    b.payment += a</span><br><span class="line">    b</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 合并Buffer</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">StoreSummary</span>, b2: <span class="type">StoreSummary</span>): <span class="type">StoreSummary</span> = &#123;</span><br><span class="line">    b1.user += b2.user</span><br><span class="line">    b1.payment += b2.payment</span><br><span class="line">    b1</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 最终的计算结果</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">StoreSummary</span>): <span class="type">String</span> = &#123;</span><br><span class="line">    <span class="string">&quot;user: &quot;</span> + reduction.user + <span class="string">&quot;,payment: &quot;</span> + reduction.payment</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Dataset默认编码器，用于序列化，固定写法</span></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">StoreSummary</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">String</span>] = <span class="type">Encoders</span>.<span class="type">STRING</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到不管是从代码量还是调用参数相比于弱类型便捷了很多，需要注意的是注册函数时需要调用 functions 下的 udaf 方法，还有一点就是这是 Spark3.0 以后的写法，Spark3.0 以前如果想用强类型有其他的写法。</p><p>输出使用强弱类型 UDAF 查询的结果</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">+-----+----------------------+----------------------+</span><br><span class="line">|store|myudaf01(payment)     |myudaf02(payment)     |</span><br><span class="line">+-----+----------------------+----------------------+</span><br><span class="line">|1    |user: 3,payment: 79.32|user: 3,payment: 79.32|</span><br><span class="line">|2    |user: 2,payment: 112.3|user: 2,payment: 112.3|</span><br><span class="line">+-----+----------------------+----------------------+</span><br></pre></td></tr></table></figure><h3 id="Aggregator（Spark3-0版本以前）"><a href="#Aggregator（Spark3-0版本以前）" class="headerlink" title="Aggregator（Spark3.0版本以前）"></a>Aggregator（Spark3.0版本以前）</h3><p>早期版本中不能在 SQL 中使用强类型 UDAF ，但是可以在 DSL 中使用，代码编写和调用方式都有所不同，DSL 注重的是类型，所以在 UDAF 输入类型这里传入的应该是 DataSet 每一行的类型，而不是固定字段的某个类型。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> myudaf03: <span class="type">TypedColumn</span>[<span class="type">Double</span>, <span class="type">String</span>] = (<span class="keyword">new</span> <span class="type">MyUDAF03</span>).toColumn</span><br><span class="line">ds.select(myudaf03).show  <span class="comment">// 输出的并没有按门店分组，与预想结果不同，没深究</span></span><br></pre></td></tr></table></figure><p>下面是 UDAF 类</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyUDAF03</span> <span class="keyword">extends</span> <span class="title">Aggregator</span>[<span class="type">Record</span>, <span class="type">StoreSummary</span>, <span class="type">String</span>] </span>&#123;</span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">zero</span></span>: <span class="type">StoreSummary</span> = &#123;</span><br><span class="line">      <span class="type">StoreSummary</span>(<span class="number">0</span>, <span class="number">0.00</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">reduce</span></span>(b: <span class="type">StoreSummary</span>, a: <span class="type">Record</span>): <span class="type">StoreSummary</span> = &#123;</span><br><span class="line">      b.user += <span class="number">1</span></span><br><span class="line">      b.payment += a.payment</span><br><span class="line">      b</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">merge</span></span>(b1: <span class="type">StoreSummary</span>, b2: <span class="type">StoreSummary</span>): <span class="type">StoreSummary</span> = &#123;</span><br><span class="line">      b1.user += b2.user</span><br><span class="line">      b1.payment += b2.payment</span><br><span class="line">      b1</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">finish</span></span>(reduction: <span class="type">StoreSummary</span>): <span class="type">String</span> = &#123;</span><br><span class="line">      <span class="string">&quot;user: &quot;</span> + reduction.user + <span class="string">&quot;,payment: &quot;</span> + reduction.payment</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">bufferEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">StoreSummary</span>] = <span class="type">Encoders</span>.product</span><br><span class="line"></span><br><span class="line">    <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">outputEncoder</span></span>: <span class="type">Encoder</span>[<span class="type">String</span>] = <span class="type">Encoders</span>.<span class="type">STRING</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>其实就是将输入类型改成了 DataSet 的类型，代码中再调用指定的字段即可。</p>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark入门-RDD、DataFrame and DataSet</title>
      <link href="/2021/09/08/Software/Spark%E5%85%A5%E9%97%A8-RDD%E3%80%81DataFrame%20and%20DataSet/"/>
      <url>/2021/09/08/Software/Spark%E5%85%A5%E9%97%A8-RDD%E3%80%81DataFrame%20and%20DataSet/</url>
      
        <content type="html"><![CDATA[<p><code>RDD(Resilient Distributed Dataset)</code>叫做弹性分布式数据集，是 Spark 中最基本的数据处理模型。代码中是一个抽象类，它代表一个弹性的、不可变、可分区、里面的元素可并行 计算的集合。而 DataFrame 和 DataSet 分别是 Spark1.3 版本和 1.6 版本开始支持的数据集类型。它们之间彼此依赖也可以互相转换，分别应用在不同的场景下。</p><span id="more"></span><h3 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h3><p>RDD 是 Spark 计算的基础数据集合，之后的 DataFrame 和 DataSet 底层也是封装了 RDD ，所以掌握 RDD 对是学习Spark的第一步，源码中列出了 RDD 的五个特征，分别是：</p><ul><li>A list of partitions</li><li>A function for computing each split</li><li>A list of dependencies on other RDDs</li><li>Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)</li><li>Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)</li></ul><p>总结下来就是 RDD 数据结构中存在分区列表，用于执行任务时并行计算；Spark 在计算时使用的是分区函数对每个分区进行计算；RDD 是计算模型的封装，当需要将多个计算模型进行组合时，需要在 RDD 之间建立彼此之间的依赖关系；当数据为 KV 键值对时，可设定分区器自定义数据的分区，可选；计算时可根据节点的状态选择不同的节点位置进行计算，可选。</p><p>从代码或者数据方面考虑，RDD 只存储数据，不存在 Schema，例如给它由多个元组组成的 List，它并不知道每个元组的第一个元素代表姓名，第二个元素代表年龄，但我们可以使用样例类封装每个元组，让它知道每个元组代表一个 User，如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 元组对组成的RDD</span></span><br><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;Bob&quot;</span>, <span class="number">23</span>), (<span class="string">&quot;Alice&quot;</span>, <span class="number">22</span>), (<span class="string">&quot;John&quot;</span>, <span class="number">24</span>)))</span><br><span class="line"></span><br><span class="line"><span class="comment">// 给每个元组对加上属性</span></span><br><span class="line"><span class="keyword">val</span> rdd01 = sc.makeRDD(<span class="type">List</span>(<span class="type">User</span>(<span class="string">&quot;Bob&quot;</span>, <span class="number">23</span>), <span class="type">User</span>(<span class="string">&quot;Alice&quot;</span>, <span class="number">22</span>), <span class="type">User</span>(<span class="string">&quot;John&quot;</span>, <span class="number">24</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)  <span class="comment">// 样例类要放在main方法外</span></span></span><br></pre></td></tr></table></figure><h3 id="DataFrame"><a href="#DataFrame" class="headerlink" title="DataFrame"></a>DataFrame</h3><p>DataFrame 则是为了使 Spark 可以处理结构化数据或者半结构化数据而产生的数据集合，它可以从多个数据源读取数据加以处理，处理方式也可以使用我们熟知的 SQL 或者 Spark 独有的 DSL 语言，你可以将它想象成 RDD 中的每个元素都拥有了 Schema，有了字段名描述数据，但是没有了属性，最新版的 Spark 已经把 DataFrame 作为 DataSet 的一种特殊形式来构建，源码：<code>type DataFrame = Dataset[Row]</code>。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 从RDD转换</span></span><br><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;Bob&quot;</span>, <span class="number">23</span>), (<span class="string">&quot;Alice&quot;</span>, <span class="number">22</span>), (<span class="string">&quot;John&quot;</span>, <span class="number">24</span>)))</span><br><span class="line"><span class="keyword">val</span> df = rdd.toDF(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 从文件中获取</span></span><br><span class="line">spark.read.json(<span class="string">&quot;data/user.json&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// DSL</span></span><br><span class="line">df.select(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>, $<span class="string">&quot;age&quot;</span> + <span class="number">1</span> as <span class="string">&quot;new_age&quot;</span>)</span><br><span class="line">df.filter($<span class="string">&quot;age&quot;</span> &gt; <span class="number">21</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">// SQL</span></span><br><span class="line">df.createOrReplaceTempView(<span class="string">&quot;user&quot;</span>)</span><br><span class="line">spark.sql(<span class="string">&quot;select *, age + 1 as new_age from user&quot;</span>).show()</span><br></pre></td></tr></table></figure><h3 id="DataSet"><a href="#DataSet" class="headerlink" title="DataSet"></a>DataSet</h3><p>DataSet 是具有强类型的数据集合，需要提供对应的类型信息。在后期的 Spark 版本中，DataSet 有可能会逐步取代 RDD 和 DataFrame 成为唯一的 API 接口。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._  <span class="comment">// spark是SparkSession对象名，如果涉及到转换，需要引入转换规则</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 从RDD创建</span></span><br><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>(<span class="type">User</span>(<span class="string">&quot;Bob&quot;</span>, <span class="number">23</span>), <span class="type">User</span>(<span class="string">&quot;Alice&quot;</span>, <span class="number">22</span>), <span class="type">User</span>(<span class="string">&quot;John&quot;</span>, <span class="number">24</span>)))</span><br><span class="line">rdd.toDS  <span class="comment">// RDD具有属性时可以直接转，Schema和属性都有了</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 从DF创建</span></span><br><span class="line"><span class="keyword">val</span> df = rdd.toDF(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line">df.as[<span class="type">User</span>]  <span class="comment">// 需要给DF明确属性</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 需要注意的是从文件中导入的数据在没有指定属性的情况下默认都是DataFrame，加上属性转换就可以转换为DataSet</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)  <span class="comment">// 样例类要放在main方法外</span></span></span><br></pre></td></tr></table></figure><p>DataSet 和 DataFrame 的最大区别是每一行的类型，也就是强类型和弱类型，我们在处理数据的时候是按行处理的，对每一行的不同字段进行处理就需要定位指定的字段，而 DataFrame 每一行是没有属性的，或者说它是 DataSet 每一行属性固定为 Row 的特例，取指定的值只能通过位置，这有很大的不确定性，并且也不好维护；而 DataSet 在一行内可以使用属性.字段的方式定位属性并加以操作。在自定义 UDF 函数的时候就能感受到强类型和弱类型的区别了。</p><h3 id="三者的联系与区别"><a href="#三者的联系与区别" class="headerlink" title="三者的联系与区别"></a>三者的联系与区别</h3><p>三者的共性、区别和相互之间的转换。</p><h4 id="三者的共性"><a href="#三者的共性" class="headerlink" title="三者的共性"></a>三者的共性</h4><ul><li>RDD 、DataFrame 、DataSet 全都是 Spark 平台下的分布式弹性数据集，为处理超大型数据提供便利;</li><li>三者都有惰性机制，在进行创建、转换，如 map 方法时，不会立即执行，只有在遇到 Action 如 foreach 时，三者才会开始遍历运算;</li><li>三者有许多共同的函数，如 filter，排序等;</li><li>在对 DataFrame 和 Dataset 进行操作许多操作都需要这个包：<code>import spark.implicits._</code>(在创建好 SparkSession  对象后尽量直接导入)；</li><li>三者都会根据 Spark 的内存情况自动缓存运算，这样即使数据量很大，也不用担心会内存溢出；</li><li>三者都有 partition 的概念；</li><li>DataFrame 和 DataSet 均可使用模式匹配获取各个字段的值和类型。</li></ul><h4 id="三者的区别"><a href="#三者的区别" class="headerlink" title="三者的区别"></a>三者的区别</h4><ol><li><strong>RDD</strong><ul><li>RDD 一般和 Spark Mllib 同时使用；</li><li>RDD 不支持 Spark SQL 操作；</li></ul></li><li><strong>DataFrame</strong><ul><li>与 RDD 和 Dataset 不同，DataFrame 每一行的类型固定为 Row，每一列的值没法直接访问，只有通过解析才能获取各个字段的值；</li><li>DataFrame 与 DataSet 一般不与 Spark Mllib 同时使用；</li><li>DataFrame 与 DataSet 均支持 Spark SQL 的操作，比如 select，groupby 之类，还能注册临时表&#x2F;视窗，进行 SQL 语句操作；</li><li>DataFrame 与 DataSet 支持一些特别方便的保存方式，比如保存成 csv，可以带上表头；</li></ul></li><li><strong>DataSet</strong><ul><li>Dataset 和 DataFrame 拥有完全相同的成员函数，区别只是每一行的数据类型不同；</li><li>DataFrame 其实就是 DataSet 的一个特例 <code>type DataFrame = Dataset[Row]</code>；</li><li>DataFrame 也可以叫 Dataset[Row] ，每一行的类型是 Row ，不解析，每一行究竟有哪些字段，各个字段又是什么类型都无从得知，只能用上面提到的 getAS 方法或者共性中的第七条提到的模式匹配拿出特定字段。而 Dataset 中，每一行是什么类型是不一定的，在自定义了 case class 之后可以很自由的获得每一行的信息。</li></ul></li></ol><h4 id="相互转换"><a href="#相互转换" class="headerlink" title="相互转换"></a>相互转换</h4><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spark.implicits._  <span class="comment">// spark是SparkSession对象名，如果涉及到转换，需要引入转换规则</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// RDD</span></span><br><span class="line"><span class="keyword">val</span> rdd = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;Bob&quot;</span>, <span class="number">23</span>), (<span class="string">&quot;Alice&quot;</span>, <span class="number">22</span>), (<span class="string">&quot;John&quot;</span>, <span class="number">24</span>)))</span><br><span class="line">rdd.toDF(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>)  <span class="comment">// RDD转DataFrame</span></span><br><span class="line"><span class="keyword">val</span> rdd01 = rdd.map(<span class="type">User</span>(_._1, _._2))  <span class="comment">// 需要给RDD带上属性才可以直接转DataSet</span></span><br><span class="line">rdd01.toDS  <span class="comment">// RDD转DataSet</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// DataFrame</span></span><br><span class="line"><span class="keyword">val</span> df = rdd.toDF(<span class="string">&quot;name&quot;</span>, <span class="string">&quot;age&quot;</span>)</span><br><span class="line">df.rdd  <span class="comment">// DataFrame转RDD</span></span><br><span class="line">df.as[<span class="type">User</span>].toDS  <span class="comment">// Dataframe转DataSet，加上属性直接转</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// DataSet</span></span><br><span class="line"><span class="keyword">val</span> ds = rdd.toDs</span><br><span class="line">ds.rdd  <span class="comment">// DataSet转RDD</span></span><br><span class="line">ds.toDF  <span class="comment">// DataSet转DataFrame</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">User</span>(<span class="params">name: <span class="type">String</span>, age: <span class="type">Int</span></span>)</span></span><br></pre></td></tr></table></figure><p>只需要记住每个数据集合的特征就可以灵活的相互转换。RDD 可以是无属性的数据元素，也可以是有属性的数据元素，但是没有 Schema；DataFrame 是有 Schema 的数据元素，但是没有属性；DataSet是有Schema有属性的数据集合，所以从 RDD 到 DataFrame 再到 DataSet 是依次递进的过程。</p>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark入门-聚合算子详解</title>
      <link href="/2021/08/30/Software/Spark%E5%85%A5%E9%97%A8-%E8%81%9A%E5%90%88%E7%AE%97%E5%AD%90%E8%AF%A6%E8%A7%A3/"/>
      <url>/2021/08/30/Software/Spark%E5%85%A5%E9%97%A8-%E8%81%9A%E5%90%88%E7%AE%97%E5%AD%90%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p>Spark 中常用的算子加起来有三四十个，其中针对 key 的聚合算子有五个，分别是<code>groupBy、groupByKey、reduceByKey、aggregateByKey 和 flodByKey</code>，有几个底层其实调用的都是一个方法，只不过传入的参数不一样产生了这几个算子，但我仍打算分开来详解每个算子的计算过程，加深理解。</p><span id="more"></span><h2 id="Overview"><a href="#Overview" class="headerlink" title="Overview"></a>Overview</h2><p>这几个聚合算子要解决的问题都是将所需操作的 RDD 中的 key 值相同的 value 值聚合在一起然后两两做计算也就是聚合最终得出一个结果，这里有三个点需要注意，一是两两聚合的初始值，是从外部传入还是使用默认值；二是分区内聚合方式，因为 RDD 默认是并行计算，会分成多个分区，每个分区内部可以指定聚合方式；三是分区间聚合方式，拿到分区内的聚合结果就要考虑分区间的聚合方式了，这个参数也可以指定。所以这几种算子的区别就是因为传入了不同的参数。</p><h2 id="groupBy"><a href="#groupBy" class="headerlink" title="groupBy"></a>groupBy</h2><p>先来说说 groupBy，它是最容易理解的，就是把 key 值相同的 value 值放在一起形成<code>(key, iter)</code>的键值对，聚合的话需要使用 map 再对每个 key 对应的 iter 内容做聚合。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">groupBy[<span class="type">K</span>](f: <span class="type">T</span> =&gt; <span class="type">K</span>)(<span class="keyword">implicit</span> kt: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">T</span>])]</span><br></pre></td></tr></table></figure><p>先来看看源码，需要传入一个 group 方法 f ，这个f方法传入待分组的元素，返回另外一个值 K，而这个 K 就是分组的依据，注意看最后 groupBy 返回的结果类型也是以 K 和相同 K 的初始元素生成的迭代器所组成的元组，需要对相同K下的 iter 进行聚合就需要再进行 map 操作。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 计算初始RDD不同首字母开头的元素数量</span></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">String</span>] = sc.makeRDD(<span class="type">List</span>(<span class="string">&quot;Hello&quot;</span>, <span class="string">&quot;Java&quot;</span>, <span class="string">&quot;Python&quot;</span>, <span class="string">&quot;PHP&quot;</span>, <span class="string">&quot;Help&quot;</span>))</span><br><span class="line"><span class="comment">// (&#x27;H&#x27;, (&quot;Hello&quot;, &quot;Help&quot;)), (&#x27;J&#x27;, (&quot;Java&quot;)), (&#x27;P&#x27;, (&quot;Python&quot;, &quot;PHP&quot;))</span></span><br><span class="line"><span class="keyword">val</span> groupRDD: <span class="type">RDD</span>[(<span class="type">Char</span>, <span class="type">Iterable</span>[<span class="type">String</span>])] = rdd.groupBy(_.charAt(<span class="number">0</span>))</span><br><span class="line"><span class="comment">// (&#x27;H&#x27;, 2), (&#x27;J&#x27;, 1), (&#x27;P&#x27;, 2)</span></span><br><span class="line"><span class="keyword">val</span> sizeRDD: <span class="type">RDD</span>[(<span class="type">Char</span>, <span class="type">Int</span>)] = groupRDD.map(_._2.size)</span><br><span class="line">sizeRDD.collect().foreach(println)</span><br></pre></td></tr></table></figure><p>具体过程可以参考下图，第二步添加了 File 落盘动作，因为 group 操作会计算每个分区所有单词的首字母并缓存下来，如果放在内存中若数据过多则会产生内存溢出；再就是第三步从文件读取回来，并不一定是三个分区，这里只是为了便于理解。</p><div align=center><img src="groupBy.png"></div><h2 id="groupByKey"><a href="#groupByKey" class="headerlink" title="groupByKey"></a>groupByKey</h2><p>groupByKey 相比于 groupBy 不同的是，groupBy 需要指定分组的 key ，而 groupByKey 是将元组这种类型的第一个值作为 key ，对第二个值进行分组的操作。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">groupByKey(): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">Iterable</span>[<span class="type">V</span>])]</span><br></pre></td></tr></table></figure><p>可以看到这个算子不需要传入参数，就是针对元组这种 KV 类型定义的，至于返回值的类型，K 就是元组的第一个值，<code>Iterable(V)</code>则是相同 K 值的所有 V 组成的迭代器，那么同时处理元组类型时 groupByKey 和 groupBy 的不同之处就是这里的 V 是元组内的第二个值，而 groupBy 是初始的元素值，具体看下面的例子：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 根据RDD内元组的第一个元素将数据分类并对第二个元素求和</span></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="comment">// (&quot;a&quot;, (1, 3, 4)), (&quot;b&quot;, (2))</span></span><br><span class="line"><span class="keyword">val</span> groupByKeyRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[<span class="type">Int</span>])] = sc.groupByKey()</span><br><span class="line"><span class="comment">// (&quot;a&quot;, ((&quot;a&quot;, 1), (&quot;a&quot;, 3), (&quot;a&quot;, 4))), (&quot;b&quot;, (&quot;b&quot;, 2))</span></span><br><span class="line"><span class="keyword">val</span> groupByRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Int</span>)])] = sc.groupBy(_._1)</span><br><span class="line"></span><br><span class="line"><span class="comment">// 当然聚合方式也不相同</span></span><br><span class="line">groupByKeyRDD.map(_._2.sum)</span><br><span class="line">groupByRDD.map(_._2.map(_._2).sum)</span><br></pre></td></tr></table></figure><p>groupByKey 也需要落盘操作，会导致数据打乱重组，存在 shuffle 操作，效率相对来说比较低下，这也就引出了 reduceByKey，下面再详细比较两者的不同之处。</p><div align=center><img src="groupByKey.png"></div><h2 id="reduceByKey"><a href="#reduceByKey" class="headerlink" title="reduceByKey"></a>reduceByKey</h2><p>reduceByKey 相比于 groupByKey 就是把 map 操作集成在算子当中了，不需要再额外进行 map 操作，它和aggregateByKey以及 flodByKey 的操作类似，只不过细节之处需要传入不同的参数区分彼此不同的功能。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reduceByKey(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure><p>可以看到 reduceByKey 接收一个 func 参数，而这个 func 参数接收两个 V 类型的参数并返回一个 V 类型的结果，这里的 V 其实就是初始 RDD 中的元素，这里需要传入的 func 就是元素两两计算的逻辑。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 根据RDD内元组的第一个元素将数据分类并对第二个元素求和</span></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">2</span>)))</span><br><span class="line"><span class="keyword">val</span> <span class="type">ReduceByKeyRDD</span>: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = rdd.reduceByKey(_ + _)  <span class="comment">// (&quot;a&quot;, 8), (&quot;b&quot;, 2)</span></span><br><span class="line"><span class="type">ReduceByKeyRDD</span>.collect().foreach(println)</span><br></pre></td></tr></table></figure><p>从下图中的第一张图看相对于 groupByKey 只是少了 map 的步骤将它整合在 reduceByKey 中，但是实际上 reduceByKey 的作用不止于此，第二张图才是实际的运行模式，它提供了 Combine 预聚合的功能，支持在分区中先进行聚合，称作分区内聚合，然后再落盘等待分区间聚合。这样下来它不只是减少了 map 的操作，同时提供了分区内聚合使得 shuffle 落盘时的数据量尽量小，IO 效率也会提高不少。最后它引出了分区内聚合和分区间聚合，reduceByKey 的分区内聚合和分区间聚合是一样的。</p><div align=center><img src="reduceByKey.png"></div><h2 id="aggregateByKey"><a href="#aggregateByKey" class="headerlink" title="aggregateByKey"></a>aggregateByKey</h2><p>aggregateByKey 是对 reduceByKey 的高级应用，它可以分开来指定分区内聚合和分区间聚合，并提供了一个计算初始值。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">aggregateByKey[<span class="type">U</span>: <span class="type">ClassTag</span>](zeroValue: <span class="type">U</span>)(seqOp: (<span class="type">U</span>, <span class="type">V</span>) =&gt; <span class="type">U</span>, combOp: (<span class="type">U</span>, <span class="type">U</span>) =&gt; <span class="type">U</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">U</span>)]</span><br></pre></td></tr></table></figure><p>来看上方源码，它采用柯里化操作，第一个参数列表接收一个参数 zeroValue，它提供一个初始值，不同于 reduceByKey 直接开始计算第一个元素和第二个元素，aggregateByKey 允许先用初始值和第一个元素进行两两计算；第二个参数列表接收两个参数，第一个是 seqOp 表示分区内聚合方式，它接收两个参数返回一个参数，注意接收的参数一个 U 的类型和 zeroValue 类型相同，另外一个是初始元素的类型，返回类型是 U 类型，说明返回类型是由 zeroValue 决定的，这很重要；第二个参数 combOp 表示分区间聚合方式，接收两个 U 类型的参数并返回一个 U 类型的参数。最终返回初始元素和聚合后的元素。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 给定初始RDD并指定两个分区，分区内计算最大值分区间求和</span></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(</span><br><span class="line">    <span class="type">List</span>((<span class="string">&quot;a&quot;</span>,<span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">6</span>)), </span><br><span class="line">    numSlices = <span class="number">2</span></span><br><span class="line">)</span><br><span class="line"><span class="comment">// ((&quot;a&quot;, 8), (&quot;b&quot;, 8))</span></span><br><span class="line"><span class="keyword">val</span> aggregateByKeyRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = rdd.aggregateByKey(<span class="number">0</span>)(</span><br><span class="line">    (x, y) =&gt; math.max(x, y),  <span class="comment">// 分区内求最大值</span></span><br><span class="line">    (x, y) =&gt; x + y  <span class="comment">// 分区间求和</span></span><br><span class="line">)</span><br><span class="line">aggregateByKeyRDD.collect().foreach(println)</span><br></pre></td></tr></table></figure><p>看运行过程就更清晰了，相比于 reduceByKey 只是将分区内聚合和分区间聚合分开来了，并且提供了一个初始值，这个初始值作为第一个元素与初始 RDD 的第一个元素计算，这也就使得初始值不一样哪怕聚合方式相同结果也可能不一样，详情看下图。其次就是分区数量对结果的影响，上方例子如果按三个分区计算结果又不一样了，它作为 aggregateByKey 的第四个决定结果的隐形参数在聚合时也需要考虑在内。</p><div align=center><img src="aggregateByKey.png"></div><h2 id="flodByKey"><a href="#flodByKey" class="headerlink" title="flodByKey"></a>flodByKey</h2><p>flodByKey 是 aggregateByKey 的特例情况，在分区内聚合方式和分区间聚合方式相同的时候使用。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">foldByKey(zeroValue: <span class="type">V</span>)(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)]</span><br></pre></td></tr></table></figure><p>仍然是柯里化传参，第一个参数列表给定一个初始值，第二个参数列表传入一个聚合函数 func，在一定条件下和 reduceByKey 的结果和聚合方式是相同的。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 计算初始值与所有元素的和</span></span><br><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[<span class="type">Int</span>] = sc.makeRDD(<span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>)), <span class="number">2</span>)</span><br><span class="line"><span class="keyword">val</span> foldByKeyRDD: <span class="type">RDD</span>[<span class="type">Int</span>] = rdd.foldByKey(<span class="number">10</span>)(_ + _)  <span class="comment">// 10 + 1 + 3 + 10 + 4 + 2 = 30</span></span><br></pre></td></tr></table></figure><p>看运行过程还是比较容易理解的，尤其需要注意初始值的设定，不然会产生意想不到的结果。</p><div align=center><img src="foldByKey.png"></div><h2 id="Compare"><a href="#Compare" class="headerlink" title="Compare"></a>Compare</h2><p>前面把每个算子的详细计算过程都画了一遍，接下来从源码中函数的接收参数中继续看<code>reduceByKey、aggregateByKey和flodByKey</code>这三个算子的联系和不同之处，它们的源码中都调用了一个函数<code>combineByKeyWithClassTag</code>，接下来来看一看传入的参数。从它们源码调用的函数就可以很清楚的区分这几个算子了，结合不同环境使用不同的算子。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// reduceByKey</span></span><br><span class="line">combineByKeyWithClassTag[<span class="type">V</span>]((v: <span class="type">V</span>) =&gt; </span><br><span class="line">                            v,  <span class="comment">// 就是初始RDD的值，当作每个分区开始计算的初始值，不需要指定</span></span><br><span class="line">                            func,  <span class="comment">// 分区内聚合</span></span><br><span class="line">                            func,  <span class="comment">// 分区间聚合</span></span><br><span class="line">                            partitioner)</span><br><span class="line"><span class="comment">// aggregateByKey</span></span><br><span class="line">combineByKeyWithClassTag[<span class="type">U</span>]((v: <span class="type">V</span>) =&gt; </span><br><span class="line">                            cleanedSeqOp(createZero(), v),  <span class="comment">// 初始值，柯里化的第一个参数列表</span></span><br><span class="line">                            cleanedSeqOp,  <span class="comment">// 分区内聚合，柯里化的第二个参数列表</span></span><br><span class="line">                            combOp,  <span class="comment">// 分区间聚合，与分区内聚合不相同</span></span><br><span class="line">                            partitioner)</span><br><span class="line"><span class="comment">// flodByKey</span></span><br><span class="line">combineByKeyWithClassTag[<span class="type">V</span>]((v: <span class="type">V</span>) =&gt; </span><br><span class="line">                            cleanedFunc(createZero(), v),  <span class="comment">// 初始值，柯里化的第一个参数列表</span></span><br><span class="line">                            cleanedFunc,  <span class="comment">// 分区内聚合，柯里化的第二个参数列表</span></span><br><span class="line">                            cleanedFunc,  <span class="comment">// 分区间聚合，与分区内聚合相同</span></span><br><span class="line">                            partitioner)</span><br></pre></td></tr></table></figure><h2 id="Application"><a href="#Application" class="headerlink" title="Application"></a>Application</h2><ol><li>获取首字母相同 key 数据的和</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(</span><br><span class="line">  <span class="type">List</span>((<span class="string">&quot;Hello&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;Java&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;Python&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;PHP&quot;</span>, <span class="number">7</span>), (<span class="string">&quot;Help&quot;</span>, <span class="number">9</span>)))</span><br><span class="line"></span><br><span class="line">rdd.map(kv =&gt; (kv._1.charAt(<span class="number">0</span>), kv._2))  <span class="comment">// 先将原始RDD的首字母提出来</span></span><br><span class="line">  .reduceByKey(_ + _)  <span class="comment">// 再按照key进行求和</span></span><br><span class="line">  .collect()</span><br><span class="line">  .foreach(println)  <span class="comment">// (&quot;P&quot;,12), (&quot;H&quot;, 10), (&quot;C&quot;, 3)</span></span><br></pre></td></tr></table></figure><ol start="2"><li>获取相同 key 数据的平均值</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sc.makeRDD(</span><br><span class="line">  <span class="type">List</span>((<span class="string">&quot;a&quot;</span>, <span class="number">1</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">2</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">3</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">4</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">5</span>), (<span class="string">&quot;a&quot;</span>, <span class="number">6</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">7</span>), (<span class="string">&quot;b&quot;</span>, <span class="number">8</span>)), <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">rdd.aggregateByKey((<span class="number">0.0</span>, <span class="number">0</span>))(  <span class="comment">// 元组第一个元素接收求和数据，0.0避免求均值强转为Int，第二个接收数据计数</span></span><br><span class="line">  (k, v) =&gt; (k._1 + v, k._2 + <span class="number">1</span>),  <span class="comment">// 分区内按key累加、计数</span></span><br><span class="line">  (k, v) =&gt; (k._1 + v._1, k._2 + v._2)  <span class="comment">// 分区间将分区内的统计结果累加</span></span><br><span class="line">)</span><br><span class="line">  .map(kv =&gt; (kv._1, kv._2._1 / kv._2._2))  <span class="comment">// 求均值</span></span><br><span class="line">  .collect()</span><br><span class="line">  .foreach(println)  <span class="comment">// (&quot;a&quot;,3.5), (&quot;b&quot;, 5.5)</span></span><br></pre></td></tr></table></figure><ol start="3"><li>获取相同 key 的数据分区内求均值分区间求和的结果</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">rdd.aggregateByKey((<span class="number">0.0</span>, <span class="number">0</span>))(</span><br><span class="line">  (k, v) =&gt; ((k._1 + v), k._2 + <span class="number">1</span>),</span><br><span class="line">  (k, v) =&gt; (k._1 / k._2 + v._1 / v._2, k._2 + v._2)  <span class="comment">// 直接在这一步先计算分区内均值再求和</span></span><br><span class="line">)</span><br><span class="line">  .collect()</span><br><span class="line">  .foreach(println) <span class="comment">// (&quot;a&quot;,(7.0, 4)), (&quot;b&quot;, (11.0, 4))</span></span><br></pre></td></tr></table></figure><ol start="4"><li>数据如下所示每一行数据是一条点击记录，字段分别为（时间戳 省份 市 用户 广告），计算每个省份点击次数前三名的广告。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">1516609143867 6 7 64 16</span><br><span class="line">1516609143869 9 4 75 18</span><br><span class="line">1516609143869 1 7 87 12</span><br><span class="line">1516609143869 2 8 92 9</span><br><span class="line">1516609143869 6 7 84 24</span><br><span class="line">1516609143869 1 8 95 5</span><br><span class="line">1516609143869 8 1 90 29</span><br><span class="line">1516609143869 3 3 36 16</span><br><span class="line">1516609143869 3 3 54 22</span><br><span class="line">1516609143869 7 6 33 5</span><br></pre></td></tr></table></figure><p>思考的重点是中间数据结构的转换，刚开始计算的 key 是省份 + 广告，后面的 key 就只有省份了，需要在省份内部做计算。计算过程如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> original: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">&quot;data/agent.log&quot;</span>)  <span class="comment">// 时间戳 省份 市 用户 广告</span></span><br><span class="line"><span class="keyword">val</span> mapRDD: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = original.map(str =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> strings: <span class="type">Array</span>[<span class="type">String</span>] = str.split(<span class="string">&quot; &quot;</span>)  <span class="comment">// 拆分数据并取到省份和广告字段</span></span><br><span class="line">  ((strings(<span class="number">1</span>), strings(<span class="number">4</span>)), <span class="number">1</span>)</span><br><span class="line">&#125;)</span><br><span class="line"><span class="comment">// 计算每个省份每条广告的点击人数</span></span><br><span class="line"><span class="keyword">val</span> reduceRDD: <span class="type">RDD</span>[((<span class="type">String</span>, <span class="type">String</span>), <span class="type">Int</span>)] = mapRDD.reduceByKey(_ + _)  </span><br><span class="line"><span class="comment">// 转换数据结构，因为最终是计算每个省份内的，所以省份是key，将广告跟点击数放一起</span></span><br><span class="line"><span class="keyword">val</span> newMapRDD: <span class="type">RDD</span>[(<span class="type">String</span>, (<span class="type">String</span>, <span class="type">Int</span>))] = reduceRDD.map&#123; </span><br><span class="line">  <span class="keyword">case</span> ((pro, ad), sum) =&gt; (pro, (ad, sum)) </span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// groupBy省份，将所有省份下的所有广告点击数放一起</span></span><br><span class="line"><span class="keyword">val</span> groupRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Iterable</span>[(<span class="type">String</span>, <span class="type">Int</span>)])] = newMapRDD.groupByKey()</span><br><span class="line"><span class="comment">// 在每个省份内排序，取前三条数据</span></span><br><span class="line"><span class="keyword">val</span> mapValuesRDD: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">List</span>[(<span class="type">String</span>, <span class="type">Int</span>)])] = groupRDD.mapValues(iter =&gt; &#123;</span><br><span class="line">  iter.toList.sortWith(_._2 &gt; _._2).take(<span class="number">3</span>) <span class="comment">// sortBy(_._2)(Ordering.Int.reverse)</span></span><br><span class="line">&#125;)</span><br><span class="line">mapValuesRDD.collect().foreach(println)</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hive常见性能优化方式</title>
      <link href="/2021/08/06/Software/Hive%E5%B8%B8%E8%A7%81%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%96%B9%E5%BC%8F/"/>
      <url>/2021/08/06/Software/Hive%E5%B8%B8%E8%A7%81%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%E6%96%B9%E5%BC%8F/</url>
      
        <content type="html"><![CDATA[<p>从设计优化、数据存储优化和 Job 作业优化三方面提升 Hive 处理性能。</p><span id="more"></span><p>Hive 底层存储使用的是 HDFS ，计算使用的是资源调度 Yarn ，Hive 仅仅是将 HDFS 上存储的文件数据映射一张表，将这些存储和映射信息放入元数据管理系统，用户操作数据仓库的时候是先查询元数据管理系统，拿到如数据存储位置和存储方式数据压缩方式等元数据后，再将 SQL 转化为 MapReduce 程序运行，完成后返回给用户。既然如此可供优化的场景就有 HDFS 数据存储层，Yarn 数据计算层以及数据仓库元数据管理以及执行查询操作作业转化流程</p><h2 id="Hive-表设计优化"><a href="#Hive-表设计优化" class="headerlink" title="Hive 表设计优化"></a>Hive 表设计优化</h2><p>谈表设计优化主要是谈分区表、分桶表以及索引（ Hive3 已经弃用）的优化，通过建立分区表和分桶表提升数据查询效率。</p><h3 id="分区表的设计使用"><a href="#分区表的设计使用" class="headerlink" title="分区表的设计使用"></a>分区表的设计使用</h3><p>分区表比较容易理解，假设你的数据有一个字段是省份，用来区分数据所属省份，那么就可以将它当做分区列，Hive 就会按省份这一列将数据分为每个省份一个目录，将数据存入其中，这样做的好处是我们在后续查询数据使用省份字段筛选的时候就不用扫描包含所有省份的全部数据，只需要扫描 where 后指定省份对应的文件。减少全表扫描，对数据进行分区裁剪。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 设置支持动态分区以及非严格模式-严格模式分区表中必须至少有一个静态分区</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.exec.dynamic.partition.mode<span class="operator">=</span>nonstrict;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> usa_covid_orignal(</span><br><span class="line">    count_date string,</span><br><span class="line">    country string,</span><br><span class="line">    state string,</span><br><span class="line">    fips <span class="type">int</span>,</span><br><span class="line">    cases <span class="type">int</span>,</span><br><span class="line">    deaths <span class="type">int</span></span><br><span class="line">) comment <span class="string">&#x27;美国新冠疫苗初始表&#x27;</span></span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">location <span class="string">&#x27;/warehouse/test/dql/usa_covid_orignal&#x27;</span>;</span><br><span class="line"><span class="comment">-- 普通表导入数据后查看执行计划</span></span><br><span class="line">explain extended <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> usa_covid_orignal <span class="keyword">where</span> state<span class="operator">=</span><span class="string">&#x27;Guam&#x27;</span> <span class="keyword">or</span> state<span class="operator">=</span><span class="string">&#x27;New Mexico&#x27;</span>;</span><br></pre></td></tr></table></figure><div align=center><img src="image-20210727145515100.png"></div><p>注意看扫描的是整张表，将条件放到了筛选操作中。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建分区表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> usa_covid(</span><br><span class="line">    country string,</span><br><span class="line">    fips <span class="type">int</span>,</span><br><span class="line">    cases <span class="type">int</span>,</span><br><span class="line">    deaths <span class="type">int</span></span><br><span class="line">) comment <span class="string">&#x27;美国新冠疫苗动态分区表&#x27;</span></span><br><span class="line">partitioned <span class="keyword">by</span>(count_date string, state string)</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span></span><br><span class="line">location <span class="string">&#x27;/warehouse/test/dql/usa_covid&#x27;</span>;</span><br><span class="line"><span class="comment">-- 使用表中已有字段作为分区字段就是动态分区，这里会将select后最后两个字段作为分区字段</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> usa_covid <span class="keyword">partition</span>(count_date, state)</span><br><span class="line"><span class="keyword">select</span> country, fips, cases, deaths, count_date, state <span class="keyword">from</span> usa_covid_orignal;</span><br><span class="line"><span class="keyword">show</span> partitions usa_covid;  <span class="comment">-- 查看表中所有的分区，若分区太多谨慎使用</span></span><br><span class="line"></span><br><span class="line">explain extended <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> usa_covid <span class="keyword">where</span> state<span class="operator">=</span><span class="string">&#x27;Guam&#x27;</span> <span class="keyword">or</span> state<span class="operator">=</span><span class="string">&#x27;New Mexico&#x27;</span>;</span><br></pre></td></tr></table></figure><p>我分开放了两张图，因为我选了两个条件，所以它扫描了两个分区，使用上述语句查看详细语法树中有扫描文件地址。</p><div align=center><img src="image-20210727150305971.png"></div><div align=center><img src="image-20210727150356657.png"></div><p>常见的分区表是以时间分区，每年每个月的数据放到一个分区，在对时间做筛选的时候就可以减少扫描数据，所以以哪个字段分区取决于查询的时候以哪个字段筛选。分区字段分为静态分区和动态分区，静态分区是在插入数据的时候我可以指定我所要插入的数据是属于哪个分区的，例如<code>province=&#39;beijing&#39;</code>，动态分区则是根据已有的某个字段对数据进行分区。需要注意的是分区字段是不在底层文件中存储的，虽然<code>select *</code>它会显示出来。关于分区表的存储可以查看元数据中存储的 PARTITIONS 相关表以及 SDS 表。</p><h3 id="分桶表的设计使用"><a href="#分桶表的设计使用" class="headerlink" title="分桶表的设计使用"></a>分桶表的设计使用</h3><p>关于分桶表刚开始我认为它和分区表没什么区别，但实际上不管是建表方式还是计算方式以及优化目的都不相同。分区表是指定分区或者动态分区来区分属于哪个区，存入对应的目录，而分桶表是将数据按某个字段的 hash 值分到对应的桶并存入不同的文件；分区表优化的方向是按字段筛选查询，分桶表优化的是多表之间的join操作。</p><p>假设两张需要 join 的表，它们用来 join 的字段都是分桶字段（分桶的数量最好相同，确保同一个数据落在相同的桶中），在 join 操作时 Hive 就会对两个字段中同一个桶中的数据进行 join 操作，目的是减少两表join操作不必要的笛卡尔积数量。当然分桶表在数据抽样中也有应用，但实际用的较少。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 分别有四张表bucket_emp01和bucket_emp02数据相同，bucket_emp02按deptno分桶；</span></span><br><span class="line"><span class="comment">-- bucket_dept01和bucket_dept02数据相同，bucket_dept02按deptno分桶，语句太多就不贴了</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 开启分桶SMB(Sort-Merge-Bucket) join</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 分别查看两个未分桶表和分桶表的join执行计划</span></span><br><span class="line">explain extended <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> bucket_emp01 e <span class="keyword">join</span> bucket_dept01 d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br><span class="line">explain extended <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> bucket_emp02 e <span class="keyword">join</span> bucket_dept02 d <span class="keyword">on</span> e.deptno <span class="operator">=</span> d.deptno;</span><br></pre></td></tr></table></figure><div align=center><img src="image-20210727173900380.png"></div><p>可以看到需要在 reduce 计算进行 join 操作，而且就算普通的 inner join ，届时会对两张表进行常规 join 操作。</p><div align=center><img src="image-20210727175040539.png"></div><p>而对两个分桶的表进行 join 则可以看到利用分桶对 map 操作进行优化，使用的是 SMB join 操作，而且没有 reduce 操作。</p><h2 id="Hive-表数据存储优化"><a href="#Hive-表数据存储优化" class="headerlink" title="Hive 表数据存储优化"></a>Hive 表数据存储优化</h2><p>Hive 的数据存储在 Hadoop 的 HDFS 上，所以数据的读写 操作都是针对 HDFS 的。对 HDFS 存储优化也是对 Hive 查询效率的提升。这里涉及的优化主要是数据的存储、传输、压缩的优化，优缺点也有优点，关键在于使用合适的优化方法。</p><h3 id="文件存储格式（TEXT、ORC、Parquet）"><a href="#文件存储格式（TEXT、ORC、Parquet）" class="headerlink" title="文件存储格式（TEXT、ORC、Parquet）"></a>文件存储格式（TEXT、ORC、Parquet）</h3><h4 id="TextFile"><a href="#TextFile" class="headerlink" title="TextFile"></a>TextFile</h4><p>Hive数据默认的存储格式是 TextFile ，就是平时看到的以分隔符分隔的文本文件，建表时不指定<code>stored as</code>操作符数据则默认以 TextFile 存储在 HDFS 上。同时我们使用<code>load data</code>的时候是将数据直接搬到Hive表指定的路径下，不会对数据做任何处理。</p><ul><li>TextFile 的优点是按行存储，操作理解导入方便，可使用任意分隔符分割数据，直接在 HDFS 使用命令查看数据，并且可以搭配压缩一起使用；</li><li>缺点是耗费存储空间，I&#x2F;O 性能较低，结合压缩时不能进行文件切分合并，不能进行并行操作，按行存储导致读取列时效率低下。适用于小量数据的查询，在 ODS 层一般使用这种存储方式。</li></ul><h4 id="SequenceFile"><a href="#SequenceFile" class="headerlink" title="SequenceFile"></a>SequenceFile</h4><p>SequenceFile 是 Hadoop 中用来存储序列化即二进制的一种文件格式，可以作为 Map 和 Reduce 端的输入和输出，Hive 也支持这种数据格式。</p><ul><li>它的优点是以二进制 kv 键值对形式存储数据，与底层交互更加友好，性能更快，可压缩和分隔，优化磁盘利用率和 I&#x2F;O，可并行操作数据，查询效率更快；</li><li>缺点是存储时消耗的空间较大，与非 Hadoop 生态系统之外的工具兼容性较差，构建 SequenceFile 需要通过 TextFile 文件转化而来。适用于小量数据查询并且查询列比较多的情况。</li></ul><h4 id="Parquet"><a href="#Parquet" class="headerlink" title="Parquet"></a>Parquet</h4><p>Parquet 是一种支持嵌套结构的列式存储文件格式，作为 OLAP 查询的优化方案，已被多种查询引擎支持，通过数据编码和压缩，以及映射下推和谓词下推，它的性能比大多数文件格式都要更好。</p><ul><li>它的优点是采用列式存储，具有更高效的压缩和编码，可编码可分割，优化磁盘利用率和 I&#x2F;O，可用于多种数据处理框架；</li><li>缺点是不支持 update、delete、insert 等 ACID 操作。适用于字段数量非常多，只查询无更新的数据操作。</li></ul><h4 id="ORC"><a href="#ORC" class="headerlink" title="ORC"></a>ORC</h4><p>ORC 文件格式也是 Hadoop 生态圈的列式存储格式，最初产生自 Hive，用来降低存储空间提高查询效率，后被分离出来成为独立的 Apache 项目。目前被 Hive、Spark SQL、Presto 等查询引擎支持。</p><ul><li>它的优点是列式存储，存储效率非常高，可压缩，查询效率也较高，支持索引和矢量化查询；</li><li>缺点是加载时消耗资源较大，读取全量数据时性能较差，并且需要通过 TextFile 转化而来。适用于 Hive 中大型数据的存储和查询，只要系统资源配置高，对数据的存储和查询效率将会是一个很大的提升。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> tmp_orc</span><br><span class="line">    <span class="keyword">like</span> bucket_dept01</span><br><span class="line">    stored <span class="keyword">as</span> orc</span><br><span class="line">    location <span class="string">&#x27;/warehouse/test/tmp_orc&#x27;</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> tmp_orc <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> bucket_dept01;</span><br></pre></td></tr></table></figure><h3 id="数据压缩设置（ORC、Snappy）"><a href="#数据压缩设置（ORC、Snappy）" class="headerlink" title="数据压缩设置（ORC、Snappy）"></a>数据压缩设置（ORC、Snappy）</h3><p>Hive 底层运行 MapReduce 程序时，磁盘 I&#x2F;O 操作、网络数据传输、shuffle 和 merge 都会花费大量时间，为了使资源最大化合理利用，数据压缩就显得十分有必要。Hive 压缩实际上就是 MapReduce 压缩。</p><div align=center><img src="u=1113059533,2007703106.png"></div><p>压缩是优点是减少文件存储所占空间，加快文件传输效率，降低磁盘 I&#x2F;O 次数；压缩的缺点是使用数据时要先对数据进行解压，加重 CPU 负载，压缩算法越复杂，解压时间就越长。下面是几种常用的压缩算法。</p><div align=center><img src="image-20210728110759174.png"></div><p>使用压缩需要开启以及指定几项参数，开启后新建表指定使用 orc 存储文件以及 snappy 压缩算法。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 开启hive中间传输数据压缩功能</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.intermediate<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 开启mapreduce中map输出压缩功能</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.map.output.compress<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 设置mapreduce中map输出数据的压缩方式</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.map.output.compress.codec<span class="operator">=</span>org.apache.hadoop.io.compress.SnappyCodec;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 开启Reduce输出阶段压缩</span></span><br><span class="line"><span class="comment">-- 开启hive最终输出数据压缩功能</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.compress.output<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 开启mapreduce最终输出数据压缩</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 设置mapreduce最终数据输出压缩方式</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.codec <span class="operator">=</span> org.apache.hadoop.io.compress.SnappyCodec;</span><br><span class="line"><span class="comment">-- 设置mapreduce最终数据输出压缩为块压缩</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.output.fileoutputformat.compress.type<span class="operator">=</span>BLOCK;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> tmp_orc_snappy</span><br><span class="line">    <span class="keyword">like</span> tmp_orc</span><br><span class="line">    stored <span class="keyword">as</span> orc</span><br><span class="line">    location <span class="string">&#x27;/warehouse/test/tmp_orc_snappy&#x27;</span></span><br><span class="line">    tblproperties (&quot;orc.compress&quot;<span class="operator">=</span>&quot;SNAPPY&quot;);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> tmp_orc_snappy <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tmp_orc;</span><br></pre></td></tr></table></figure><h3 id="小文件场景"><a href="#小文件场景" class="headerlink" title="小文件场景"></a>小文件场景</h3><p>HDFS 并不利于小文件存储，因为一个小文件就会产生一条元数据信息，这对 NameNode 以及 Hive 元数据管理都会造成很大的负荷，而且一个小文件也会启动一个 MapTask 计算处理，导致资源的浪费，所以在使用 Hive 处理分析时，应尽量避免小文件的生成。Hive 提供了一个特殊的机制，可以自动判断是否为小文件，如果是则自动将小文件合并。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 如果hive的程序，只有maptask，将MapTask产生的所有小文件进行合并</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.mapfiles<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 如果hive的程序，有Map和ReduceTask,将ReduceTask产生的所有小文件进行合并</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.mapredfiles<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 每一个合并的文件的大小（244M）</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.size.per.task<span class="operator">=</span><span class="number">256000000</span>;</span><br><span class="line"><span class="comment">-- 平均每个文件的大小，如果小于这个值就会进行合并(15M)</span></span><br><span class="line"><span class="keyword">set</span> hive.merge.smallfiles.avgsize<span class="operator">=</span><span class="number">16000000</span>;</span><br></pre></td></tr></table></figure><p>若是遇到需要处理的数据小文件过多，Hive 也提供了一种输入类，它支持合并小文件之后再做数据处理，也就是跨文件读取。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 设置Hive中底层MapReduce读取数据的输入类：将所有文件合并为一个大文件作为输入</span></span><br><span class="line"><span class="keyword">set</span> hive.input.format<span class="operator">=</span>org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure><div align=center><img src="image-20210728114831871.png"></div><h3 id="ORC-索引、ORC-矢量化"><a href="#ORC-索引、ORC-矢量化" class="headerlink" title="ORC 索引、ORC 矢量化"></a>ORC 索引、ORC 矢量化</h3><h4 id="ORC-索引"><a href="#ORC-索引" class="headerlink" title="ORC 索引"></a>ORC 索引</h4><p>在使用 ORC 格式的文件时，为了加快读取文件内容，ORC 支持了两种索引机制：<code>Row Group Index</code>和<code>Bloom Filter Index</code>可以帮助用户提高查询 ORC 文件的性能。当用户写入数据时可以指定构建索引，当用户查询数据时，可以根据索引提前对数据进行过滤，避免不必要的扫描。</p><p><strong>Row Group Index</strong></p><ul><li>一个 ORC 文件包含一个或多个 **stripes(groups of row data)**，每个 stripe 中包含了每个 column 的 <strong>min&#x2F;max</strong> 值的索引数据；</li><li>当查询中有大于等于小于的操作时，会<strong>根据 min&#x2F;max 值，跳过扫描不包含的 stripes</strong>。</li><li>而其中为每个 stripe 建立的包含 min&#x2F;max 值的索引，就称为 Row Group Index 行组索引，也叫 min-max Index 大小对比索引，或者 Storage Index 。</li></ul><div align=center><img src="image-20210728135739336.png"></div><ul><li>建立 ORC 格式表时，指定表参数<code>&quot;orc.create.index&quot;=&quot;true&quot;</code>之后，便会建立 Row Group Index ；</li><li>为了使 Row Group Index 有效利用，向表中加载数据时，<strong>必须对需要使用索引的字段进行排序</strong>。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> tmp_orc_index</span><br><span class="line">    stored <span class="keyword">as</span> orc</span><br><span class="line">    location <span class="string">&#x27;/warehouse/test/tmp_orc_index&#x27;</span></span><br><span class="line">    tblproperties (&quot;orc.create.index&quot;<span class="operator">=</span>&quot;true&quot;)</span><br><span class="line">    <span class="keyword">as</span></span><br><span class="line">    <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tmp_orc</span><br><span class="line">    distribute <span class="keyword">by</span> deptno</span><br><span class="line">    sort <span class="keyword">by</span> deptno;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tmp_orc_index <span class="keyword">where</span> deptno <span class="keyword">between</span> <span class="number">10</span> <span class="keyword">and</span> <span class="number">20</span>;</span><br></pre></td></tr></table></figure><p><strong>Bloom Filter</strong> <strong>Index</strong></p><ul><li>建表时候通过表参数<code>&quot;orc.bloom.filter.columns&quot;=&quot;columnName&quot;</code>来指定为哪些字段建立 BloomFilter 索引，在生成数据的时候，会在每个 stripe 中，为该字段（或多字段）建立 BloomFilter 的数据结构；</li><li>当查询条件中包含对该字段的等值过滤时候，先从 BloomFilter 中获取以下是否包含该值，如果不包含，则跳过该 stripe ，如果包含则可能存在，进一步扫描该 stripe 。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> if <span class="keyword">not</span> <span class="keyword">exists</span> tmp_orc_bloom_index</span><br><span class="line">    stored <span class="keyword">as</span> orc</span><br><span class="line">    location <span class="string">&#x27;/warehouse/test/tmp_orc_bloom_index&#x27;</span></span><br><span class="line">    tblproperties (&quot;orc.create.index&quot;<span class="operator">=</span>&quot;true&quot;, &quot;orc.bloom.filter.columns&quot;<span class="operator">=</span>&quot;deptno,deptno&quot;)</span><br><span class="line">    <span class="keyword">as</span></span><br><span class="line">    <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tmp_orc</span><br><span class="line">    distribute <span class="keyword">by</span> deptno</span><br><span class="line">    sort <span class="keyword">by</span> deptno;</span><br><span class="line"><span class="keyword">select</span> <span class="built_in">count</span>(deptno) <span class="keyword">from</span> tmp_orc_bloom_index <span class="keyword">where</span> deptno <span class="keyword">between</span> <span class="number">10</span> <span class="keyword">and</span> <span class="number">20</span>;</span><br></pre></td></tr></table></figure><h4 id="ORC-矢量化查询"><a href="#ORC-矢量化查询" class="headerlink" title="ORC 矢量化查询"></a>ORC 矢量化查询</h4><p>Hive 的默认查询执行引擎一次处理一行，而矢量化查询执行是一种 Hive 针对 ORC 文件操作的特性，目的是按照每批1024行读取数据，并且一次性对整个记录整合（而不是对单条记录）应用操作，提升了像过滤, 联合, 聚合等等操作的性能。要使用矢量化查询执行，就必须以 ORC 格式存储数据。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 开启矢量化查询</span></span><br><span class="line"><span class="keyword">set</span> hive.vectorized.execution.enabled <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.vectorized.execution.reduce.enabled <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure><h2 id="Job-执行作业优化"><a href="#Job-执行作业优化" class="headerlink" title="Job 执行作业优化"></a>Job 执行作业优化</h2><h3 id="Explain-执行计划"><a href="#Explain-执行计划" class="headerlink" title="Explain 执行计划"></a>Explain 执行计划</h3><ul><li>HQL是一种类 SQL 的语言，从编程语言规范来说是一种声明式语言，用户会根据查询需求提交声明式的 HQL 查询，而 Hive 会根据底层计算引擎将其转化成<code>Mapreduce/Tez/Spark</code>的job；</li><li>explain 命令可以帮助用户了解一条 HQL 语句在底层的实现过程。通俗来说就是 Hive 打算如何去做这件事。</li><li>explain 会解析 HQL 语句，将整个 HQL 语句的实现步骤、依赖关系、实现过程都会进行解析返回，可以了解一条 HQL 语句在底层是如何实现数据的查询及处理的过程，辅助用户对 Hive 进行优化。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">EXPLAIN [FORMATTED<span class="operator">|</span>EXTENDED<span class="operator">|</span>DEPENDENCY<span class="operator">|</span><span class="keyword">AUTHORIZATION</span><span class="operator">|</span>] query</span><br><span class="line"><span class="comment">-- FORMATTED：对执行计划进行格式化，返回JSON格式的执行计划</span></span><br><span class="line"><span class="comment">-- EXTENDED：提供一些额外的信息，比如文件的路径信息</span></span><br><span class="line"><span class="comment">-- DEPENDENCY：以JSON格式返回查询所依赖的表和分区的列表 </span></span><br><span class="line"><span class="comment">-- AUTHORIZATION：列出需要被授权的条目，包括输入与输出</span></span><br></pre></td></tr></table></figure><h3 id="MapReduce-属性优化"><a href="#MapReduce-属性优化" class="headerlink" title="MapReduce 属性优化"></a>MapReduce 属性优化</h3><h4 id="本地模式"><a href="#本地模式" class="headerlink" title="本地模式"></a>本地模式</h4><p>使用Hive的过程中，对于数据量不大的表计算它也会提交给 YARN ，申请分配资源，这对于集群来说增加了不必要的开销，而且执行效率也会更慢。Hive 沿用了 MapReduce 的设计，提供本地计算模式，允许程序不提交给 YARN ，直接在本地计算并返回结果，以便提高小数据量程序的性能。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 开启本地模式，自动判断是否在本地运行</span></span><br><span class="line"><span class="keyword">set</span> hive.exec.mode.local.auto <span class="operator">=</span> <span class="literal">true</span>;</span><br></pre></td></tr></table></figure><div align=center><img src="image-20210729101144410.png"></div><p>这项配置默认是关闭的，开启后 Hive 会根据几个阈值判断是否需要使用本地模式，分别是整体输入的数据量小于 128M，所有 Map-Task 的个数少于 4 个，Reduce-Task 的个数等于 1 或者 0，只有满足这几项条件 Hive 才会在本地执行，否则就提交给 YARN 集群执行。</p><h4 id="JVM-重用（Hadoop3-已弃用）"><a href="#JVM-重用（Hadoop3-已弃用）" class="headerlink" title="JVM 重用（Hadoop3 已弃用）"></a>JVM 重用（Hadoop3 已弃用）</h4><ul><li>Hadoop 默认会为每个 Task 启动一个 JVM 来运行，而在 JVM 启动时内存开销大；</li><li>Job 数据量大的情况，如果单个 Task 数据量比较小，也会申请 JVM ，这就导致了资源紧张及浪费的情况；</li><li>JVM 重用可以使得 JVM 实例在同一个 job 中重新使用 N 次，当一个 Task 运行结束以后，JVM 不会进行释放，而是继续供下一个 Task 运行，直到运行了 N 个 Task 以后，就会释放；</li><li>N 的值可以在 Hadoop 的 mapred-site.xml 文件中进行配置，通常在 10-20 之间。</li></ul><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- Hadoop3之前的配置，在mapred-site.xml中添加以下参数</span></span><br><span class="line"><span class="comment">-- Hadoop3中已不再支持该选项</span></span><br><span class="line">mapreduce.job.jvm.numtasks<span class="operator">=</span><span class="number">10</span> </span><br></pre></td></tr></table></figure><h4 id="并行执行"><a href="#并行执行" class="headerlink" title="并行执行"></a>并行执行</h4><p>Hive 在执行程序时，会生成多个 Stage（explain 查看），有些 Stage 之间存在依赖关系，只能串行执行，但有些 Stage 之间没有关联关系，彼此之间的执行互不影响，例如 union、join 语句等。这时就可以开启并行执行，当多个 Stage 之间没有关联关系时，允许多个 Stage 并行执行，提高性能。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 开启Stage并行化，默认为false</span></span><br><span class="line"><span class="keyword">SET</span> hive.exec.parallel<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 指定并行化线程数，默认为8</span></span><br><span class="line"><span class="keyword">SET</span> hive.exec.parallel.thread.number<span class="operator">=</span><span class="number">16</span>;</span><br></pre></td></tr></table></figure><h3 id="Join优化"><a href="#Join优化" class="headerlink" title="Join优化"></a>Join优化</h3><p>Hive 针对 Join 操作的优化分为 Map 端 Join 、Reduce 端 Join 以及 Bucket Join ，针对不同的数据量以及环境选用不同的优化方法。</p><h4 id="Map-端-Join"><a href="#Map-端-Join" class="headerlink" title="Map 端 Join"></a>Map 端 Join</h4><p>适用于小表 Join 大表或者小表 Join 小表。原理是将小表的那份数据给每个 MapTask 的内存都放一份（ Distribute Cache ），这样大表的每个部分都可以与小表进行 Join 操作，底层不需要 Shuffle 阶段，但需要耗费内存空间存储小表数据。Hive中尽量适用 Map 端 Join 来实现 Join 过程，默认开启了 Map 端 Join ，而对于小表的大小定义则有指定的参数控制。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 开启Map端Join</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- Hive2.0版本之前的控制属性</span></span><br><span class="line"><span class="keyword">set</span> hive.mapjoin.smalltable.filesize<span class="operator">=</span><span class="number">25</span>M;</span><br><span class="line"><span class="comment">-- Hiv2.0版本之后的参数控制</span></span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.join.noconditionaltask.size<span class="operator">=</span><span class="number">512000000</span>;</span><br></pre></td></tr></table></figure><h4 id="Reduce-端-Join"><a href="#Reduce-端-Join" class="headerlink" title="Reduce 端 Join"></a>Reduce 端 Join</h4><p>那么对于大表 Join 大表的情况下 Map 端 Join 就不在适用了，将大表放在每个 MapTask 的内存中无论是存储还是传输都会使效率降低。这是就会启用 Reduce 端 Join，原理是将两张表的数据在 Shuffle 阶段的分组来将数据按照关联字段进行 Join，必须经过 Shuffle。Hive 会自动判断是否满足 Map 端 Join，如不满足则自动使用 Reduce 端 Join。</p><h4 id="Bucket-Join"><a href="#Bucket-Join" class="headerlink" title="Bucket Join"></a>Bucket Join</h4><p>关于 Bucket Join 在最前面分桶表的设计使用那一节就讲的很清楚了，它的目的就是加快表与表之间 Join 的效率。那么关于分桶表的新建和原理就不多说了，这里再细说一下普通的 Bucket Join 和 SMB。</p><p>一般情况下按照 Join 字段为分桶字段新建两个分桶表，开启 bucketmapjoin 优化参数即可，但是还有效率更高的 SMB，它是先将字段排序再分桶，这样确保每个桶中的数据都是有序的，在 Join 时效率会更高，同时它也需要开启多个属性参数。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 普通Bucket Join</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- SMB（Sort Merge Bucket Join）</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge <span class="operator">=</span> <span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join.noconditionaltask<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure><p>需要注意的是 Join 时最好使用分桶字段，否则将不会有任何效率提升，在建表和导入数据的时候还会产生额外的资源开销。</p><h3 id="关联优化"><a href="#关联优化" class="headerlink" title="关联优化"></a>关联优化</h3><p>当一个程序中如果有一些操作彼此之间有关联性，是可以在一个 MapReduce 中实现的，但是 Hive 会不智能的选择，Hive 会使用两个 MapReduce 来完成这两个操作。</p><p>例如：当我们执行<code>select …… from table group by id order by id desc</code>。该 SQL 语句转换为 MapReduce 时，我们可以有两种方案来实现：</p><p><strong>方案一</strong></p><ul><li>第一个 MapReduce 做 group by，经过 shuffle 阶段对 id 做分组；</li><li>第二个 MapReduce 对第一个 MapReduce 的结果做 order by，经过 shuffle 阶段对 id 进行排序。</li></ul><p><strong>方案二</strong></p><ul><li>因为都是对 id 处理，可以使用一个 MapReduce 的 shuffle 既可以做分组也可以排序。</li></ul><p>在这种场景下，Hive 会默认选择用第一种方案来实现，这样会导致性能相对较差；可以在 Hive 中开启关联优化，对有关联关系的操作进行解析时，可以尽量放在同一个 MapReduce 中实现，这项配置在 Hive3 默认开启。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.optimize.correlation<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure><h3 id="优化器引擎"><a href="#优化器引擎" class="headerlink" title="优化器引擎"></a>优化器引擎</h3><p>Hive 默认的优化器在解析一些聚合统计类的处理时，底层解析的方案有时候不是最佳的方案。例如当前有一张表【共 1000 条数据】，id构建了索引，id &#x3D;100 值有 900 条。需求：查询所有 id &#x3D; 100 的数据，SQL 语句为：<code>select * from table where id = 100</code>;</p><p><strong>方案一</strong>：由于 id 这一列构建了索引，索引默认的优化器引擎 RBO，会选择先从索引中查询 id &#x3D; 100 的值所在的位置，再根据索引记录位置去读取对应的数据，但是这并不是最佳的执行方案。</p><p><strong>方案二</strong>：有 id&#x3D;100 的值有 900 条，占了总数据的 90%，这时候是没有必要检索索引以后再检索数据的，可以直接检索数据返回，这样的效率会更高，更节省资源，这种方式就是 CBO 优化器引擎会选择的方案。</p><h4 id="RBO"><a href="#RBO" class="headerlink" title="RBO"></a><strong>RBO</strong></h4><p><code>rule basic optimise</code>：基于规则的优化器，根据设定好的规则来对程序进行优化。</p><h4 id="CBO"><a href="#CBO" class="headerlink" title="CBO"></a><strong>CBO</strong></h4><p><code>cost basic optimise</code>：基于代价的优化器，根据不同场景所需要付出的代价来合适选择优化的方案。对数据的分布的信息（数值出现的次数，条数，分布）来综合判断用哪种处理的方案是最佳方案。</p><p>Hive 中支持 RBO 与 CBO 这两种引擎，默认使用的是 RBO 优化器引擎。很明显 CBO 引擎更加智能，所以在使用 Hive 时，我们可以配置底层的优化器引擎为 CBO 引擎。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.cbo.enable<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.compute.query.using.stats<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="keyword">set</span> hive.stats.fetch.column.stats<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure><h4 id="Analyze-分析器"><a href="#Analyze-分析器" class="headerlink" title="Analyze 分析器"></a>Analyze 分析器</h4><p>CBO 引擎是基于代价的优化引擎，那么 CBO 是如何确定每种方案的运行代价呢，这就用到了 Analyze 分析器。它用于提前运行一个 MapReduce 程序将表或者分区的信息构建一些元数据（表的信息、分区信息、列的信息），搭配 CBO 引擎一起使用。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 为表中的分区构建元数据信息</span></span><br><span class="line">analyze <span class="keyword">table</span> usa_covid <span class="keyword">partition</span> (count_date,state) compute statistics;</span><br><span class="line"><span class="comment">-- 为字段构建元数据信息</span></span><br><span class="line">analyze <span class="keyword">table</span> students compute statistics <span class="keyword">for</span> columns age;</span><br><span class="line"><span class="comment">-- 查看字段的详细元数据信息，包含最大最小值，非重复计数等数据</span></span><br><span class="line"><span class="keyword">desc</span> formatted students age;</span><br></pre></td></tr></table></figure><h3 id="谓词下推（PPD）"><a href="#谓词下推（PPD）" class="headerlink" title="谓词下推（PPD）"></a>谓词下推（PPD）</h3><p>谓词：用来描述或判定客体性质、特征或者客体之间关系的词项。比如”3 大于 2”中”大于”是一个谓词。谓词下推 Predicate Pushdown（PPD）基本思想：将过滤表达式尽可能移动至靠近数据源的位置，以使真正执行时能直接跳过无关的数据。简单点说就是在不影响最终结果的情况下，尽量将过滤条件提前执行，而不是 join 或者其他计算完成后才进行条件筛选。</p><p>Hive 中谓词下推后，过滤条件会下推到 map 端，提前执行过滤，减少 map 到 reduce 的传输数据，提升整体性能。参数默认开启。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 开启谓词下推</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.ppd<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 举例说明-方式一就类似谓词下推的优化，先过滤再join</span></span><br><span class="line"><span class="keyword">select</span> a.id, a.value1, b.value2 <span class="keyword">from</span> table1 a</span><br><span class="line"><span class="keyword">join</span> (<span class="keyword">select</span> b.<span class="operator">*</span> <span class="keyword">from</span> table2 b <span class="keyword">where</span> b.ds<span class="operator">&gt;=</span><span class="string">&#x27;20181201&#x27;</span> <span class="keyword">and</span> b.ds<span class="operator">&lt;</span><span class="string">&#x27;20190101&#x27;</span>) c</span><br><span class="line"><span class="keyword">on</span> (a.id<span class="operator">=</span>c.id)；</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> a.id, a.value1, b.value2 <span class="keyword">from</span> table1 a</span><br><span class="line"><span class="keyword">join</span> table2 b <span class="keyword">on</span> a.id<span class="operator">=</span>b.id</span><br><span class="line"><span class="keyword">where</span> b.ds<span class="operator">&gt;=</span><span class="string">&#x27;20181201&#x27;</span> <span class="keyword">and</span> b.ds<span class="operator">&lt;</span><span class="string">&#x27;20190101&#x27;</span>；</span><br></pre></td></tr></table></figure><p>下面给出了一张图，是常用的一些 join 操作，第一列显示语句是否采用了谓词下推优化。</p><div align=center><img src="image-20210729115237472.png"></div><p>举例说明一下，分别有两张表，现需要 join 操作，两表分别有 where 条件约束，那么就有以下结论：</p><ul><li>对于 Join(Inner Join)、Full outer Join ，条件写在 on 后面，还是 where 后面，性能上面没有区别；</li><li>对于 Left outer Join ，右侧的表条件写在 on 后面、左侧的表条件写在 where 后面，性能上有提高；</li><li>对于 Right outer Join ，左侧的表条件写在 on 后面、右侧的表条件写在 where 后面，性能上有提高；</li><li>当条件分散在两个表时，谓词下推可按上述结论 2 和 3 自由组合。</li></ul><div align=center><img src="1627531125343.png"></div><h3 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h3><p>分布式计算中最常见的，最容易遇到的问题就是数据倾斜；数据倾斜的现象是，当提交运行一个程序时，这个程序的大多数的 Task 都已经运行结束了，只有某一个 Task 一直在运行，迟迟不能结束，导致整体的进度卡在 99% 或者 100%，这时候就可以判定程序出现了数据倾斜的问题。</p><h4 id="数据分配"><a href="#数据分配" class="headerlink" title="数据分配"></a>数据分配</h4><p>当程序中出现 group by 或者 count（distinct）等分组聚合的场景时，如果数据本身是倾斜的，根据 MapReduce 的 Hash 分区规则，肯定会出现数据倾斜的现象。根本原因是因为分区规则导致的，所以可以通过以下几种方案来解决 group by 导致的数据倾斜的问题。</p><p><strong>方案一：开启 Map 端聚合</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.map.aggr<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure><p>通过减少 shuffle 数据量和 Reducer 阶段的执行时间，避免每个 Task 数据差异过大导致数据倾斜。</p><p><strong>方案二：实现随机分区</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> <span class="keyword">table</span> distribute <span class="keyword">by</span> rand();</span><br></pre></td></tr></table></figure><p>distribute by 用于指定底层按照哪个字段作为 Key 实现分区、分组等。通过 rank 函数随机值实现随机分区，避免数据倾斜。</p><p><strong>方案三：数据倾斜时自动负载均衡</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive.groupby.skewindata<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure><p>开启该参数以后，当前程序会自动通过两个 MapReduce 来运行。</p><ul><li>第一个 MapReduce 自动进行随机分布到 Reducer 中，每个 Reducer 做部分聚合操作，输出结果；</li><li>第二个 MapReduce 将上一步聚合的结果再按照业务（group by key）进行处理，保证相同的分布到一起，最终聚合得到结果。</li></ul><h4 id="Join-数据倾斜"><a href="#Join-数据倾斜" class="headerlink" title="Join 数据倾斜"></a>Join 数据倾斜</h4><p>Join 操作时，如果两张表比较大，无法实现 Map Join ，只能走 Reduce Join ，那么当关联字段中某一种值过多的时候依旧会导致数据倾斜的问题；面对 Join 产生的数据倾斜，核心的思想是尽量避免 Reduce Join 的产生，优先使用 Map Join 来实现；但往往很多的 Join 场景不满足 Map Join 的需求，那么可以以下几种方案来解决 Join 产生的数据倾斜问题：</p><p><strong>方案一：提前过滤，将大数据变成小数据，实现 Map Join</strong></p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 先过滤再join，避免两张大表先join再过滤</span></span><br><span class="line"><span class="keyword">select</span> a.id,a.value1,b.value2 <span class="keyword">from</span> table1 a</span><br><span class="line"><span class="keyword">join</span> (<span class="keyword">select</span> b.<span class="operator">*</span> <span class="keyword">from</span> table2 b <span class="keyword">where</span> b.ds<span class="operator">&gt;=</span><span class="string">&#x27;20181201&#x27;</span> <span class="keyword">and</span> b.ds<span class="operator">&lt;</span><span class="string">&#x27;20190101&#x27;</span>) c</span><br><span class="line"><span class="keyword">on</span> (a.id<span class="operator">=</span>c.id)</span><br></pre></td></tr></table></figure><p><strong>方案二：使用 Bucket Join</strong></p><p>如果使用方案一，过滤后的数据依旧是一张大表，那么最后的 Join 依旧是一个 Reduce Join 。这种场景下，可以将两张表的数据构建为分桶表，实现 Bucket Map Join  ，避免数据倾斜。</p><p><strong>方案三：使用 Skew Join</strong></p><p>Skew Join 是Hive 中一种专门为了避免数据倾斜而设计的特殊的 Join 过程。这种 Join 的原理是将 Map Join 和 Reduce Join 进行合并，如果某个值出现了数据倾斜，就会将产生数据倾斜的数据单独使用 Map Join 来实现；其他没有产生数据倾斜的数据由 Reduce Join 来实现，这样就避免了 Reduce Join 中产生数据倾斜的问题。最终将 Map Join 的结果和Reduce Join的结果进行 Union 合并。</p><div align=center><img src="image-20210729142028259.png"></div><p>使用 Skew Join 需要开启一些配置：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 开启运行过程中skewjoin</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.skewjoin<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 如果这个key的出现的次数超过这个范围</span></span><br><span class="line"><span class="keyword">set</span> hive.skewjoin.key<span class="operator">=</span><span class="number">100000</span>;</span><br><span class="line"><span class="comment">-- 在编译时判断是否会产生数据倾斜</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.skewjoin.compiletime<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 不合并，提升性能</span></span><br><span class="line"><span class="keyword">set</span> hive.optimize.union.remove<span class="operator">=</span><span class="literal">true</span>;</span><br><span class="line"><span class="comment">-- 如果Hive的底层走的是MapReduce，必须开启这个属性，才能实现不合并</span></span><br><span class="line"><span class="keyword">set</span> mapreduce.input.fileinputformat.input.dir.recursive<span class="operator">=</span><span class="literal">true</span>;</span><br></pre></td></tr></table></figure><p>常用的需要掌握或者了解的优化操作就这么多，生产过程中完全可以按照表设计优化、存储优化、执行作业优化这三个方面根据实际环境进行操作，其中的底层原理还需要通过源码以及实践深入理解。当然这里面少了执行引擎的优化，这就不用多说了，一般 MapReduce 和 Spark 引擎用的比较多，若对实效性要求较高就选择 Spark ，若对实效性要求不高且数据量巨大再加上硬件条件较差就是用 MapReduce 空闲时间慢慢跑，在执行脚本中通过<code>set hive.execution.engine=mr/spark/tez</code>切换执行引擎，Tez 则是夹在两者之间不被常用。</p>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hive </tag>
            
            <tag> 优化 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Algorithm-算法入门</title>
      <link href="/2021/07/30/Algorithm/Algorithm-%E7%AE%97%E6%B3%95%E5%85%A5%E9%97%A8/"/>
      <url>/2021/07/30/Algorithm/Algorithm-%E7%AE%97%E6%B3%95%E5%85%A5%E9%97%A8/</url>
      
        <content type="html"><![CDATA[<p>基础数据结构以及常用算法介绍。</p><span id="more"></span><h2 id="数据结构"><a href="#数据结构" class="headerlink" title="数据结构"></a>数据结构</h2><p>数据结构定义了数据在计算机内部的顺序和位置关系，每种数据结构的存储和取出方式各不相同。</p><h3 id="链表"><a href="#链表" class="headerlink" title="链表"></a>链表</h3><p>首先链表在内存中的地址并不是连续的，它是通过数据与数据之间的指针单向指向来确定彼此的位置关系，由此也就确定了它们的相对位置和顺序，其中的数据呈线性排列。</p><p>对数据的访问是对链表进行线性查找，从左到右依次遍历，直到找到目标数据为止；数据的删除则比较简单，直接将指向要删除数据的指针更改到要删除数据指向的数据，这要就没法通过线性查找在链表中找到这条数据，也就达到了删除的目的；数据的新增则是将目标位置之前的数据指针指向新增数据，新增数据指向目标位置之后的数据。</p><p>对链表的查询要从头开始查找，所以查询时间复杂度为O(n)；删除和增加则只需要更改两个指针的指向，和链表长度n无关，所以时间复杂度为O(1)。总结下来链表呈线性排列，新增和删除数据较为方便，查找需要进行线性查找，比较耗时。</p><p>上面说的是单向链表，还有环形链表和双向链表，环形链表是将单向链表的头和尾连接起来，没有首尾的概念；双向链表则是数据从后往前也有指针指向前一个数据，查找可以从前往后也可以从后往前，但删除和增加就会比较耗时。</p><div align=center><img src="数据结构-链表.png"></div><h3 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h3><p>数组是在内存中占用连续内存地址按顺序排列的数据结构，每个数据都有一个下标，用来定位位置返回数据，数据也是呈线性排列。</p><p>数据的访问比较简单，直接通过数组的下标定位到指定的位置返回数据；新增和删除则比较麻烦，若是最后一个元素则直接删除或新增，若是新增到数组中间，则需要先在数组最后新增一个空的元素，然后依次将需要插入的位置及以后的元素从后往前依次往后移动一位，直到要插入的位置空出来再把数据放进去；反之删除一个指定数据，则先将目标位置置空，再从这个位置往后将数据依次往前移动一位，最后将最后一个置空位置删除。</p><p>数组的查询直接根据角标，和数组长度n无关，所以查询时间复杂度为O(1)，删除和增加数据需要移动数据，所以时间复杂度为O(n)。总结下来数组和链表一样都是线性排列，链表是分散的，而数组是存储在连续内存空间内，每个空间都有角标用来定位数据，查询数据较为方便，增加和删除数据比较耗时。</p><div align=center><img src="数据结构-数组.png"></div><h3 id="栈"><a href="#栈" class="headerlink" title="栈"></a>栈</h3><p>栈也是一种呈线性排列的数据结构，可以把它想象成一个箱子，然后往里面放数据，先放进去的得最后才能取出来，后放进去的最先取出来，要想取最先放进去的就得把之后放进去的全部取出来，这种存取数据的方式成为后进先出（LIFO），存数据称作入栈，取数据称作出栈。经典的应用是深度优先搜索算法。</p><div align=center><img src="数据结构-栈.png"></div><h3 id="队列"><a href="#队列" class="headerlink" title="队列"></a>队列</h3><p>队列也是一种呈线性排列的数据结构，与栈不同的地方是先存进去的数据先取出来，就像排队一样，先来的先出，这种存取方式称为先进先出（FIFO），存数据称为入队，取数据称为出队。经典的应用是广度优先搜索算法。</p><div align=center><img src="数据结构-队列.png"></div><h3 id="哈希表"><a href="#哈希表" class="headerlink" title="哈希表"></a>哈希表</h3><p>哈希表存储的是由键值对（key value）组成的数据，通过查询key值访问value，那么最重要的就是key值的确定，因为数据不只是数字，还有字符和字符串、汉字等，要通过哈希函数算出数据对应的哈希值，数字的话一般采用mod取模的方法，除数选择key数量，假设哈希表长度为3，那么余数就是0、1、2，那么每个数字对应的模就放到对应0、1、2的后面。</p><p>一般称存放哈希数据的数组称为哈希表，首先新建一个固定长度的数组，经过哈希函数计算的哈希值确定为数据存放的位置，对应数组的角标，如果哈希值相同，则使用链表的方式将数据插入到已有数据之后。需要查找数据则先计算数据对应的哈希值，若哈希值对应的数据为空则数据不在哈希表中，若不为空则从哈希值对应的链表中线性查找数据。</p><p>哈希表的查询因为要对哈希值的链表进行线性查询，所以时间复杂度为O(n)，新增的时间复杂度为O(1)，删除也是查询到位置再进行链表的删除，所以时间复杂度为O(1)。</p><p>在存储数据的过程中，哈希值相同就会发生冲突，上述利用链表在已有数据后再增加数据解决冲突的方法称为“链地址法”，还有一个比较常用的方法称作“开放地址法”，如果发生冲突则立即计算一个候补地址（从数据地址中），如果还有冲突则继续计算候补地址，直到不冲突为止。</p><div align=center><img src="数据结构-哈希表.png"></div><h3 id="堆"><a href="#堆" class="headerlink" title="堆"></a>堆</h3><p>堆是一种图的树形结构，被用于实现优先队列，优先队列是一种数据结构，可以自由添加数据，但取出数据要从最小值开始取出，在堆的数据结构中，各个顶点被称为节点，数据就存在各个节点中。其中每个节点最多拥有两个子节点，节点的排列顺序从上至下，从左往右排列。</p><p>在堆中添加数据需遵循一条规则：子节点必定大于父节点，因此最小值被存在最上方的节点中。往堆中添加数据时优先放在左边，若这一行没有位置则另起一行，然后判断子节点是否大于父节点，若不是则将父子节点数据调换，再次判断若不符合继续调换，直至符合规则为止。</p><p>从堆中取出数据时，取出的是最上面最小的数据，然后重新调整堆的顺序，随后将最后一个位置的数据移到最顶端（最下最右边的数据）。根据遵循的规则，若不符合子节点大于父节点则将父节点与两个子节点中较小的一个调换位置，再次判断若不符合继续调换，直至符合规则为止。</p><p>从堆中取出数据因为取出的是最上方的最小值，所以时间复杂度为O(1)，但是因为取出数据后要将数据移到最上方，需要调换数据顺序，所以调换的次数和数的高度成正比，假设数据量为n，那么树的高度为log2n，那么取出数据的总时间复杂度为O(logn)。添加数据也是一样，根据子节点必定大于父节点的规则若是最小值就要从最下方移到最上方，时间复杂度为O(logn)。若要频繁的从数据中取出最小值堆数据结构就是最适合的。</p><div align=center><img src="数据结构-堆.png"></div><h3 id="二叉查找树"><a href="#二叉查找树" class="headerlink" title="二叉查找树"></a>二叉查找树</h3><p>也称为二叉搜索树或者二叉排序树，和堆一样也是一种图的树形结构，每个父节点只有两个子节点，数据存储于各个节点之中。二叉查找树遵循两个规则，一是父节点必定大于其左节点，二是父节点必定大于右子节点，例如父节点A拥有左右子节点B和C，那么就有B&lt;A&lt;C。若不符合则通过调换位置直至满足条件。每个父节点大于其左子节点以及左子节点所有的子节点，小于右子节点以及其所有的子节点。</p><p>在二叉查找树中查找数据时则从顶层节点开始比较和目标数据的大小关系，若大于目标数据则从右子节点查找，若小于目标数据则从左子节点查找，如此循环下去即可；往二叉查找树中插入数据时，从顶层节点开始判断两者大小关系，若大于顶层节点则往右节点移动，反之往左节点移动，若不符合规则则继续上述操作（注意所有添加的数据都会根据这两个规则移至最后一层，不会对原有数据位置产生影响）；删除数据时若没有子节点则直接删除，若含有子节点则挑选出删除节点左子节点下的最大值移动到删除的位置，这样就可以同时符合二叉查找树的两个规则（挑选出右子节点的最小值也符合规则）。</p><p>比较数据的次数取决于树的高度，假设有n条数据，那么树的高度就是log2n，时间复杂度就是O(logn)，但是若树的形状靠单侧纵向延伸，那么时间复杂度就成了O(n)。</p><div align=center><img src="数据结构-二叉查找树.png"></div><h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><h3 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h3><p>冒泡排序是最容易理解也是最耗时的一种排序方法。假设有n个数字，它首先依次对相邻的两个数字进行比较大小，将较小的数字往后移动，遍历完之后最小的数字已经移动到最后一位了，随后再遍历n-1次，将次小的数字移动到倒数第二位，直到遍历完最后两个数字，这组数字已经按降序排序好了。</p><p>冒泡排序容易理解，但是要遍历多次来移动较小或较大的数字，需要比较的次数为<code>(n-1)+(n-2)+(n-3)+...+1=(n²-n)/2</code>约为n²&#x2F;2，则冒泡排序的时间复杂度为O(n²)。</p><h3 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h3><p>选择排序和冒泡排序略有不同，但同样比较耗时。假设有n个数字，首先从n个数字中使用线性查找寻找最小值，将它调换到第一位（直接和原第一位数字调换），随后从第二位开始在n-1个数字中寻找次小的数字，将它调换到第二位，依次下去直至全部调换完，结果已经按升序排列好了。</p><p>需要比较的最大次数为<code>(n-1)+(n-2)+(n-3)+...+1=(n²-n)/2</code>约为n²&#x2F;2，所以选择的时间复杂度也是O(n²)。</p><h3 id="插入排序"><a href="#插入排序" class="headerlink" title="插入排序"></a>插入排序</h3><p>插入排序理解起来也比较容易，它是通过从一列数字中未排序的数字中取出一个数字与已排好序的数字比较并插入到合适的位置，已排好序的数字不再变动，仅将未排序的数字插入合适的位置即可。假设有n个数字，那么第一轮是第一个数字和自身比较直接完成，第二轮是第二个数字和第一个比较，若小于则不动，若大于则相互交换位置，第二轮完成，第三轮第三个数字先和第二个数字比较，若大于第二个数字则交换位置，随后第二个数字再和第一个比较，若大于第一个数字继续调换位置，否则第三轮结束，依次比较下去即可完成排序。</p><p>插入排序的次数为<code>1+2+3+...(n-2)+(n-1)=(n²-n)/2</code>约为n²&#x2F;2，所以插入排序的时间复杂度也是O(n²)。</p><h3 id="堆排序"><a href="#堆排序" class="headerlink" title="堆排序"></a>堆排序</h3><p>堆排序利用堆数据结构的特性，因为堆数据结构的顶层节点是所有数据的最大值（或最小值），将数据存入堆数据结构中再依次取出来即可达到排序的效果。</p><p>插入或者弹出一个数据都与堆的高度有关，n条数据的堆高度为log2n，插入一条数据的时间复杂度为O(logn)，那么插入n条数据的时间复杂度就是O(nlogn)，由此可以得出堆排序的时间复杂度为O(nlogn)。</p><p>虽说相比于冒泡排序、选择排序和插入排序，堆排序的时间复杂度最小，但是实现起来较为复杂。可以通过在数组中使用一定的规则达到堆排序的效果。</p><h3 id="归并排序"><a href="#归并排序" class="headerlink" title="归并排序"></a>归并排序</h3><p>归并排序是一种先切分再合并的排序方法，在合并的过程中对数据进行比较调换位置，最后达到为序列排序的效果。具体操作方法是先将序列对半分割，然后再分别对两个序列进行对半分割，直至不可分割为止，接下来对分割完的数据进行合并。合并的时候比较两个子序列的首位数字，将较小的放在前面，较小的放在第二位，接下来再比较子序列的首位数字，重复上面的操作，确保两个子序列合并后序列的顺序是按从小到大排序的，依次向上合并排序后，序列的排序也就完成了。</p><p>归并排序中，无论分割到哪一步都是操作n个数据，所以每行的运行时间都是O(n)，而序列长度为n的数字全部切分为单个数字，都需要分割log2n次（参考堆数据结构的高度），那么所需的时间就是O(logn)，加上每行的时间所需要的总时间就是O(nlogn)，和堆排序的时间复杂度是相同的。</p><h3 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h3><p>快速排序是编程中应用最广的，因为简单且快速。快排是在一组序列中选出一个基准值，然后大于基准值的放在右边，小于基准值的放在左边，接下来再分别对左右两侧的子序列进行上述操作，递归执行，直至子序列中只剩一个数字，同时序列也按从小到大的顺序排好了。</p><p>对于时间复杂度的计算可以参考归并排序，假设每次选择的基准值都将序列对半分，那么共需分log2n次，每次需要比较n次，那么所需总时间为O(nlogn)。但若是运气不好的话每次都是选择序列中的最小值，那么每个数字都要与基准值进行比较，达不到分而治之的效果，就需要分n次，所需的总时间就是O(n²)。如果每个数字被选为基准值的概率都相等，那么它的时间复杂度就是O(nlogn)。</p>]]></content>
      
      
      <categories>
          
          <category> Algorithm </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DataX简单使用</title>
      <link href="/2021/07/23/Software/DataX%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/"/>
      <url>/2021/07/23/Software/DataX%E7%AE%80%E5%8D%95%E4%BD%BF%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<p>DataX 是由阿里巴巴开源的离线数据同步框架，它支持的数据源是采用插件式管理，目前支持大多数关系型数据库以及非关系型数据库，查看 DataX 支持的数据源可以在 <a href="https://github.com/alibaba/datax">Github</a> 或者 <a href="https://codechina.csdn.net/mirrors/alibaba/datax/">CODE China</a> 查看，也可以对源码进行二次开发。</p><span id="more"></span><h3 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h3><p>常用的命令只有一个 data.py，使用它有两个目的，一个是查看指定的 reader 和 writer 的模板配置，它采用的是 json 形式的配置方式，简单易理解；另外一个就是根据配置信息启动一个离线同步任务。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">查看帮助</span></span><br><span class="line"><span class="meta">$</span><span class="language-bash">DATAX_HOME/bin/datax.py --<span class="built_in">help</span></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看模板配置文件，它会返回一个json模板以及github链接，里面有每个参数的配置信息</span></span><br><span class="line"><span class="meta">$</span><span class="language-bash">DATAX_HOME/bin/datax.py -r mysqlreader -w hdfswriter</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">开始一个同步任务，同步完成后会弹出统计信息，包括用时和成功失败行数等</span></span><br><span class="line"><span class="meta">$</span><span class="language-bash">DATAX_HOME/bin/datax.py <span class="variable">$DATAX_HOME</span>/job/job.json</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">传入参数到json文件中，json文件中则采用<span class="variable">$&#123;table&#125;</span>来接收参数</span></span><br><span class="line"><span class="meta">$</span><span class="language-bash">DATAX_HOME/bin/datax.py -p<span class="string">&quot;-Dtable=datax_test -Ddt=2020-10-31&quot;</span> <span class="variable">$DATAX_HOME</span>/job/job.json</span></span><br></pre></td></tr></table></figure><p>每个 reader 或者 writer 包含的依赖都在<code>$DATAX_HOM/plugin</code>目录下，在依赖版本落后时可以替换依赖来完成需求。每次执行任务的 log 可以在<code>$DATAX_HOM/log</code>目录下找到，它是按日期分到不同的文件夹，再按 json 配置文件名加时间来区分不同任务的。</p><p>-p 参数特别有用，在使用定时任务的时候就可以在 shell 中给命令传入不同的参数同步指定的数据。</p><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>这是一个 MySQL 到 HDFS 的示例， reader 可以使用 table、column 和 where 的方式，也可以采用 querySql 的方式，若采用 querySql 则 table、column 和 where 参数无效</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;job&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;content&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">            <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;reader&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;mysqlreader&quot;</span><span class="punctuation">,</span> </span><br><span class="line">                    <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;column&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;id&quot;</span><span class="punctuation">,</span> <span class="string">&quot;name&quot;</span><span class="punctuation">,</span> <span class="string">&quot;age&quot;</span><span class="punctuation">,</span> <span class="string">&quot;ctime&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                        <span class="attr">&quot;connection&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                            <span class="punctuation">&#123;</span></span><br><span class="line">                                <span class="attr">&quot;jdbcUrl&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;jdbc:mysql://localhost:3306/test&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span> </span><br><span class="line">                                <span class="attr">&quot;table&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;$&#123;table&#125;&quot;</span><span class="punctuation">]</span></span><br><span class="line">                                # <span class="attr">&quot;querySql&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span><span class="string">&quot;select * from $&#123;table&#125; where data(ctime) = &#x27;$&#123;dt&#125;&#x27;&quot;</span><span class="punctuation">]</span></span><br><span class="line">                            <span class="punctuation">&#125;</span></span><br><span class="line">                        <span class="punctuation">]</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;password&quot;</span><span class="punctuation">:</span> <span class="string">&quot;root&quot;</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;username&quot;</span><span class="punctuation">:</span> <span class="string">&quot;123456&quot;</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;where&quot;</span><span class="punctuation">:</span> <span class="string">&quot;date(ctime) = $&#123;dt&#125;&quot;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span> </span><br><span class="line">                <span class="attr">&quot;writer&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;name&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hdfswriter&quot;</span><span class="punctuation">,</span> </span><br><span class="line">                    <span class="attr">&quot;parameter&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;column&quot;</span><span class="punctuation">:</span> <span class="punctuation">[</span></span><br><span class="line">                            <span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span><span class="string">&quot;id&quot;</span><span class="punctuation">,</span><span class="attr">&quot;type&quot;</span><span class="punctuation">:</span><span class="string">&quot;int&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span><span class="string">&quot;name&quot;</span><span class="punctuation">,</span><span class="attr">&quot;type&quot;</span><span class="punctuation">:</span><span class="string">&quot;string&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span><span class="string">&quot;age&quot;</span><span class="punctuation">,</span><span class="attr">&quot;type&quot;</span><span class="punctuation">:</span><span class="string">&quot;int&quot;</span><span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="punctuation">&#123;</span><span class="attr">&quot;name&quot;</span><span class="punctuation">:</span><span class="string">&quot;ctime&quot;</span><span class="punctuation">,</span><span class="attr">&quot;type&quot;</span><span class="punctuation">:</span><span class="string">&quot;string&quot;</span><span class="punctuation">&#125;</span></span><br><span class="line">                        <span class="punctuation">]</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;compress&quot;</span><span class="punctuation">:</span> <span class="string">&quot;snappy&quot;</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;defaultFS&quot;</span><span class="punctuation">:</span> <span class="string">&quot;hdfs://node01:9000&quot;</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;fieldDelimiter&quot;</span><span class="punctuation">:</span> <span class="string">&quot;\t&quot;</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;fileName&quot;</span><span class="punctuation">:</span> <span class="string">&quot;datax_test&quot;</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;fileType&quot;</span><span class="punctuation">:</span> <span class="string">&quot;orc&quot;</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;path&quot;</span><span class="punctuation">:</span> <span class="string">&quot;/tmp/tmp/other/&quot;</span><span class="punctuation">,</span> </span><br><span class="line">                        <span class="attr">&quot;writeMode&quot;</span><span class="punctuation">:</span> <span class="string">&quot;append&quot;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">]</span><span class="punctuation">,</span> </span><br><span class="line">        <span class="attr">&quot;setting&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;speed&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;channel&quot;</span><span class="punctuation">:</span> <span class="string">&quot;5&quot;</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="调优"><a href="#调优" class="headerlink" title="调优"></a>调优</h3><p>局部调优</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">&quot;setting&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;speed&quot;</span><span class="punctuation">:</span> <span class="punctuation">&#123;</span></span><br><span class="line">                <span class="attr">&quot;channel&quot;</span><span class="punctuation">:</span> <span class="number">2</span><span class="punctuation">,</span>  # 此处为数据导入的并发度，建议根据服务器硬件进行调优</span><br><span class="line">                <span class="attr">&quot;record&quot;</span><span class="punctuation">:</span> <span class="number">-1</span><span class="punctuation">,</span>  # 此处解除对读取行数的限制</span><br><span class="line">                <span class="attr">&quot;byte&quot;</span><span class="punctuation">:</span> <span class="number">-1</span><span class="punctuation">,</span>  # 此处解除对字节的限制</span><br><span class="line">                <span class="attr">&quot;batchSize&quot;</span><span class="punctuation">:</span> <span class="number">2048</span>  # 每次读取batch的大小</span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ETL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>读后感-认知觉醒</title>
      <link href="/2021/06/11/Mind/%E8%AF%BB%E5%90%8E%E6%84%9F-%E8%AE%A4%E7%9F%A5%E8%A7%89%E9%86%92/"/>
      <url>/2021/06/11/Mind/%E8%AF%BB%E5%90%8E%E6%84%9F-%E8%AE%A4%E7%9F%A5%E8%A7%89%E9%86%92/</url>
      
        <content type="html"><![CDATA[<p>前几天刚读完一本书——认知觉醒，虽然如同大多数类似的书籍一样，用了比较多的文字来讲废话，但仍可以从中挑出一些干货，对自己的认知能力提升或许有很大帮助。</p><span id="more"></span><h3 id="内观自己-摆脱焦虑"><a href="#内观自己-摆脱焦虑" class="headerlink" title="内观自己 摆脱焦虑"></a>内观自己 摆脱焦虑</h3><h4 id="大脑认知"><a href="#大脑认知" class="headerlink" title="大脑认知"></a>大脑认知</h4><p>本书把大脑按逻辑分层分为三层，分别是本能脑、情绪脑和认知脑。其中本能脑负责身体各项机能的正常运转；情绪脑负责调动快乐、悲伤、激动这些情绪；认知脑则是后天经过思考学习使我们可以主动思考，反映的是理性思考部分。</p><p>而对于大脑而言它的本质是避难趋易急于求成，比如沉迷于玩游戏不想学习的时候，上班摸鱼不想工作的时候，手头的事情特别无聊急于完成好去干有意义的事情，或者直接心不在焉一心二用，有时候还会因为现实与理想差距过大去调用情绪脑产生一些负面的因素。这些都是本能脑在作祟，它喜欢简单的，容易满足的，即时的事情，但如同以上所列举的，在没有外界因素的干预下，它会将我们带入深渊。</p><p>认知脑则负责理性思考，是认知与智慧的体现，做出眼前的数学题，写一篇策划书，编写一段代码等等，都需要调用认知脑，它为我们的成长带来的是正向的，进步的因素。</p><p>这样说并不是要尊崇认知脑压抑本能脑，而是两部分可以协同作战，在你沉迷玩游戏的时候调用认知脑把你拉出来，告诉自己这样下去是有问题的，从而选择去做正确的事情；在学习了很长时间以后，可以适当的给本能脑一点甜头，看一会视频，吃一点甜品，必要的时候要及时做出改变。</p><h4 id="焦虑的根源"><a href="#焦虑的根源" class="headerlink" title="焦虑的根源"></a>焦虑的根源</h4><p>焦虑是当今社会发展的产物，以前大家都穷，只顾得上为生计奔波，当今社会欲望太多，你什么都想要，但实际你能力没达到那个地步，于是生活跟你的欲望不匹配就会造成落差，这种落差就是你焦虑的原因，说白了就是想要的太多，掌握的太少。然而这么简单个事情并不是每个人都可以很好的对待处理。</p><h4 id="耐心"><a href="#耐心" class="headerlink" title="耐心"></a>耐心</h4><p>在我看来焦虑就是没耐心，没耐心把眼前的事情做好而去考虑长远的事情，你越是幻想未来的美好与现实的差距就越大。而没耐心则是因为大脑总是避难趋易和即时满足的，它不愿做那些长远的但是有益的，只愿做容易的，能够快速带来反馈的。</p><p>保持耐心有两种比较适合我的方式，分别是后娱乐和阶段性奖励，后娱乐是把刷视频、看文章、聊天这种和正事无关的一切事情放在做完正事之后，做这些事情可以给你带来即时的满足感，越满足越想继续做，反而特别反感去做正事，要做的就是把娱乐行为放在做完事情之后，算作是一个小奖励，这样做也有利于精力的合理利用，有时候我们并不会因为娱乐了很久然后去开开心心的干正事，反而那时候你的精力已经被耗费的差不多了。</p><p>另外一个保持耐心的方法是阶段性奖励，就是把一件长远的事情分成若干份，每做完一份就给自己一次阶段性反馈或奖励，这个反馈最好是和当前事情有关的，目的是保持对整件事情的持续兴趣。</p><h4 id="合理利用潜意识"><a href="#合理利用潜意识" class="headerlink" title="合理利用潜意识"></a>合理利用潜意识</h4><p>下面来说一说潜意识，以前对潜意识的认识只是在特定的场景下，比如你想去做一件不太想做的事，“不想做”就是潜意识释放给你的信号。你的潜意识是由本能脑主导的，比如我特别想吃甜品，我不想做那件无聊的事，我现在就想看视频等等；认知和意识则是由认知脑负责，比如你在减肥不能吃甜品，这件无聊的事是你的工作你必须做，已经看了好几个小时视频了必须要去学习了。</p><p>本能脑和认知脑的责任不相同，所以释放给你的想法也不相同，而我们每天都是在完成两部分大脑给我们分配的任务，如果它俩分配的任务能被我们合理的错开完成那是再好不过了，不过现实是每天都会面临两个大脑同时释放给你信号，对应着相对的事情，因为本能脑相比认知脑做出选择往往会耗费较少的能量，而且能力也更强大，所以认知脑经常掰不过本能脑，在该工作的时候摸鱼，该减肥的时候吃甜品，以前会说这是自制力的问题，但自制是压抑本能的行为，这样做总会有反弹的一天。所以让你的意识意识到你的潜意识，影响潜意识做出正确的选择，随后再驱使意识去完成，这样就会合理利用潜意识而不是总拖你后腿。</p><h4 id="元认知能力"><a href="#元认知能力" class="headerlink" title="元认知能力"></a>元认知能力</h4><p>至于元认知能力我把它理解为认识并掌控自己的能力，前面说的几部分放在以前我可能会不屑于思考或者完全意识不到，生活工作中可能有很多不如意的地方但却没有认真去思考是什么造成现在的后果，而只是陷入对事情本身得与失的思考，很可能只是抱怨、生气、无能狂怒或是默默忍受，应该从事情本身跳出来，客观的从自身和他人的角度去思考问题到底出在哪里，一件不如意事情的发生必然有原因，应该找出这个原因谨防下次再次导致这个错误，或者因为这个原因解决其他可能存在的隐患，而不是考虑得失和谁的责任。</p><p>这就是我理解的元认知能力，它是一种自我迭代升级的过程，想要进步就得客观认识自己，消除模糊明确问题所在，越是自己骗自己越没有认识自己的机会，谈何进步。</p><h3 id="外观世界，借力前行"><a href="#外观世界，借力前行" class="headerlink" title="外观世界，借力前行"></a>外观世界，借力前行</h3><h4 id="专注"><a href="#专注" class="headerlink" title="专注"></a>专注</h4><p>讲完自我认知，接下来是行动部分，前面说了焦虑，说了耐心，和这节专注的关系就是专注是以耐心为目的，而耐心又是解决焦虑的一个方法。</p><p>这里的专注指两个方面，一是情绪关注，思想层面的，就是对待做一件事的态度，干一件事就专心做，不管是什么事情。生活中可能会在吃饭的时候看着视频，工作的时候桌面右下角看着电影，跑步的时候大脑天马行空飘向远方，大多数时候是因为手头的事情太过无聊，急需找一个相对有意义能给大脑带来感觉的事情同时去想，这样下去的结果就是自己本职的事情没做好，三心二意的事情也没做好，两件事都没有什么收获，一次两次不可怕，若是经常这样呢，每件事都这样呢，长此以往不管做什么事情都做不到专注而为了，所以做一件事想一件事，确保思想跟你手头的事情相匹配，这是达到专注的前提条件。</p><p>二是学习专注，在于行动方面，做一件事的时候使用番茄钟定时，比如学习或者工作五十分钟休息十分钟，这样的话你只需要确保自己在这五十分钟内绝对专注，在没有紧要事情的情况下只考虑手头的事情，而且这也会提高学习质量，相比啥时候想到了啥时候学，不想学则去娱乐的无监督行为有保障多了。</p><h4 id="学习"><a href="#学习" class="headerlink" title="学习"></a>学习</h4><p>再来说说学习，学习这件事对我而言毕业了才算开始，果然学校落下的以后都会以各种方式让你补回来。这里所说的学习是学习一项新技能，或是一个新技术，不单指学校里的听课读书，依然需要遵循几个标准，分别是消除模糊，建立阶段性目标，深度学习和劳逸结合。</p><p>又是消除模糊，大多数时候你的迷茫、焦虑都是由无知引起的，对自己对目标所知甚少又不愿思考。消除模糊的目的就是确定目标，对于考证考试这类的不存在模糊，而对于职场上的大多数学习而言目标是模糊的，比如我目前在学习数据仓库并期望明年换个工资还过得去的工作，我逛招聘网站，搜寻一切所需要技能的学习资料，但经常觉得自己和空气斗智斗勇，因为有时候我不知道招聘要求上掌握某项技能到底是怎么个标准，具体包括什么，目标不明确让我时常怀疑自己，走了不少弯路。</p><p>二是建立阶段性目标，并且完成阶段性目标后必须有反馈，说白了就是分解目标，假设你打算骑车从北京到西安，你不会直接导航带着行李干粮就出发吧，至少也该规划一下每天骑多少公里，到哪里补给哪里休息，预计几天能到达，今天骑的少了明天要赶回来等等，这样做第一不会让你直面遥远的大目标，完成一个小目标就会让自己的信心增加一分，第二是直接给你一个大方向太容易走错了，分解成小目标也是在降低你犯错的几率。学习也是这样，没有哪个人上来就说我要当部门经理然后就去埋头苦干了，这不是笑话嘛。</p><p>第三是深度学习，这点对我很重要，近一段时间学习的技能比较多，但很多知识感觉学了就忘了，回头一点印象都没有了，这是一方面，还有一方面就是浅尝辄止，一项技能学个皮毛，等到后面用到了又往前翻，从而导致学习效率低下。所以我的理解是首先注重技能之间的关联性，在做计划的时候尽量把相似可对比的放一起学，提升学习效率的同时学习也有深度，其次就是学习到什么程度，因为技能最终服务于工作，所以还是要以实际的标准来衡量学习的深度，仅仅跟着教程做一遍远远不够。</p><p>四是劳逸结合，前半年来我一直都很着急，压力比较大，总觉得时间紧迫，一大堆技能需要掌握，盲目提高学习效率的同时丧失了学习深度。谁知上周做了下安排，才知道时间还是比较充足的，最后两个月甚至都没计划安排，由此看来事情是在合理安排下有条不紊完成的，而不是被内心的恐慌和焦虑催促着完成的。劳逸结合也会让你保持头脑清醒，有时间精力反思自我检查学习情况。</p><h4 id="行动力"><a href="#行动力" class="headerlink" title="行动力"></a>行动力</h4><p>下面是行动力，有关行动力依然要注意几点，分别是消除模糊、果断出击和以行动检验真理。</p><p>首先又是消除模糊，这对于行动力依然重要，要行动就得准则，就得有标准，谁都想条理清晰去做一件事情，而不是像无头苍蝇一样乱撞；其次是果断出击，对于做事犹豫不决或者瞻前顾后的人来说这是个难题，有时候我想做一件事，总想着把它想明白了或者考虑的差不多了再开始行动，但热情就在探索中被消磨殆尽，最后也就不了了之了，在面对一件事情的时候确定了大方向就可以行动，行动的切实感受也可以让你对此事有更准确的了解；最后是实践检验真理，尤其是在欲望如此强烈的时候，总是急于掌握一项技能，有些技能看一遍教程读几篇文章或者敲几行代码就以为自己会了，这远远不够，不去行动，不去应用在实践中，它并不会变成你自己的知识，过一段时间就忘的干干净净，而且对于深入的知识点和可能存在的bug也不能吸收到，只想不做犹如纸上谈兵，很难感受到它真正带给你的感受。</p><h4 id="管理情绪"><a href="#管理情绪" class="headerlink" title="管理情绪"></a>管理情绪</h4><p>有关情绪吧，我也深有感触，因为自己的情绪时而稳定时而不稳定，给自己造成了很大的麻烦。我们每天的精力都有限，一定要合理利用，不要把时间和精力浪费在无意义的事情上，我把这也算做情绪管理，有时候不是不知道该干什么，而是管不住自己想要娱乐的心。</p><p>还有就是每隔一段时间总会有坏情绪占据你的内心，可能是这段时间工作太忙，可能是压力过大，当意识到自己出问题的时候一定要及时解决，可以找个机会冥想片刻，让自己静下心来，再以旁观者的角度分析是什么原因导致自己出了问题，当你直面它的时候往往就没那么可怕了。如果压力太大一定要注意休息，良好的休息会让自己的头脑保持清醒，办事效率也会提高不少。</p><p>最后不要钻牛角尖，说的就是我了，总会因为一些琐事陷入自我否定或是否定他人，不断在脑中思考得失，这个劲过去了又埋怨自己不该这样，我觉得这种情况比较难以改变，至少对我而言。当然我也给自己找了办法，当这种状态开始的时候意识到它的存在，并且迅速切换注意力去想其他事或者强迫自己跳出这个状态，主动影响而不是任由它肆意泛滥。</p><h4 id="早冥读写跑"><a href="#早冥读写跑" class="headerlink" title="早冥读写跑"></a>早冥读写跑</h4><p>最后这部分我可是来劲了，前面说了一大堆有的没的，终究是要落在行动上。对于推荐的这几项行动，我很是认同，因为我已经在行动了，而且深感其带来的正面反馈，分别是早起、冥想、读书、写作和跑步，这里面除了冥想我没有比较固定的计划外，其余几项都在做。</p><p>我采取的是早起加跑步，阅读放在上班和下班路上，写作则是争取一周至少产出一篇文章，冥想则是在我烦恼或是劳累的时候，冥想五到十分钟左右，这个没有固定时间。</p><p>跑步我已经坚持了两年多，最近发现早起跑步后白天也特别有精神，就放在一起了，每次则是六到八公里，每周三到五次左右；阅读的话我主要是读工具书、认识自我类的和近现代小说，比如张贤亮和王小波写的书，目前这一项带给我的好处还没有那么强烈，但读书嘛，太过功利性就不是我想要的读书了；写作是我今年年初才开始的，自己搭了个网站，自己写给自己看，就当是个记录，主要包括技术类的博客和对自己的思考反思，写作可以锻炼自己的思维能力和概括能力，有些东西你看过一遍感觉自己会了没用，不说实际应用的话至少还要用自己的语言讲给他人或者写下来才勉强算是自己的；再来说说冥想，我深知它所带来的好处，让混浊的思维变得清晰，但总抽不出合适的时间去执行，在以后的规划中还是要把它变得常态化。</p><h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>洋洋洒洒写了五千字左右，从读书时候的思如泉涌，到提笔时候的纠结困惑，再到写完后的如释重负，文章是写完了，但生活的压力永远不会让你如释重负，这只是刚刚开始。</p><p>再来说说写这篇文章的感受，理出框架的时候心里还有很多话想写，但到了动笔的时候发现自己的经历或者所理解的事情跟主题并不相符，只能硬着头皮往下写，中途一度想放弃，看来我写作的能力还有待提升，至少下次不会这么啰嗦了。</p>]]></content>
      
      
      <categories>
          
          <category> Mind </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mind </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SQL OVER窗口函数</title>
      <link href="/2021/06/04/Language/SQL%20OVER%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0/"/>
      <url>/2021/06/04/Language/SQL%20OVER%E7%AA%97%E5%8F%A3%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p>这里的窗口函数在 SQL 、Hive 和 Spark SQL 中的用法差不多都是一样的。</p><span id="more"></span><p>窗口函数在我的理解下首先是解决 OLAP 系统的复杂分类问题，它可以定制不同规模的窗口让聚合函数在窗口内执行并返回结果到当前行，理解窗口函数脑中需要有一张表，模拟函数在计算时数据的来源，也就是窗口的定义和界限，在最新的 SQL 中支持 over 窗口函数，我们一般所说的窗口函数也就是 over 函数。</p><p>over 开窗函数可以配合 sum，avg，count，max，min 等聚合函数，也可以配合 rank，dense_rank和row_number 等专用开窗函数。当 over 函数中未使用 partition 和 order 时，它的窗口就是所有数据，只使用 partition 则窗口为每个分组，聚合的是每个分组内的数据，只使用 order 则窗口为所有数据，计算的是从起始行到当前行的数据聚合结果。准备数据如下：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SET</span> NAMES utf8mb4;</span><br><span class="line"><span class="keyword">SET</span> FOREIGN_KEY_CHECKS <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- ----------------------------</span></span><br><span class="line"><span class="comment">-- Table structure for student</span></span><br><span class="line"><span class="comment">-- ----------------------------</span></span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">TABLE</span> IF <span class="keyword">EXISTS</span> `student`;</span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `student` (</span><br><span class="line">  `name` <span class="type">varchar</span>(<span class="number">255</span>) <span class="keyword">NOT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  `class` <span class="type">varchar</span>(<span class="number">255</span>) <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  `score` <span class="type">int</span>(<span class="number">255</span>) <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  `subject` <span class="type">varchar</span>(<span class="number">255</span>) <span class="keyword">DEFAULT</span> <span class="keyword">NULL</span>,</span><br><span class="line">  <span class="keyword">PRIMARY</span> KEY (`name`)</span><br><span class="line">) ENGINE<span class="operator">=</span>InnoDB <span class="keyword">DEFAULT</span> CHARSET<span class="operator">=</span>utf8;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- ----------------------------</span></span><br><span class="line"><span class="comment">-- Records of student</span></span><br><span class="line"><span class="comment">-- ----------------------------</span></span><br><span class="line"><span class="keyword">BEGIN</span>;</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> `student` <span class="keyword">VALUES</span> (<span class="string">&#x27;a&#x27;</span>, <span class="string">&#x27;2&#x27;</span>, <span class="number">94</span>, <span class="string">&#x27;英语&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> `student` <span class="keyword">VALUES</span> (<span class="string">&#x27;b&#x27;</span>, <span class="string">&#x27;4&#x27;</span>, <span class="number">99</span>, <span class="string">&#x27;英语&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> `student` <span class="keyword">VALUES</span> (<span class="string">&#x27;c&#x27;</span>, <span class="string">&#x27;4&#x27;</span>, <span class="number">90</span>, <span class="string">&#x27;政治&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> `student` <span class="keyword">VALUES</span> (<span class="string">&#x27;d&#x27;</span>, <span class="string">&#x27;3&#x27;</span>, <span class="number">99</span>, <span class="string">&#x27;数学&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> `student` <span class="keyword">VALUES</span> (<span class="string">&#x27;e&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="number">88</span>, <span class="string">&#x27;语文&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> `student` <span class="keyword">VALUES</span> (<span class="string">&#x27;f&#x27;</span>, <span class="string">&#x27;3&#x27;</span>, <span class="number">78</span>, <span class="string">&#x27;英语&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> `student` <span class="keyword">VALUES</span> (<span class="string">&#x27;g&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="number">89</span>, <span class="string">&#x27;政治&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> `student` <span class="keyword">VALUES</span> (<span class="string">&#x27;n&#x27;</span>, <span class="string">&#x27;4&#x27;</span>, <span class="number">99</span>, <span class="string">&#x27;数学&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> `student` <span class="keyword">VALUES</span> (<span class="string">&#x27;q&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="number">95</span>, <span class="string">&#x27;数学&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> `student` <span class="keyword">VALUES</span> (<span class="string">&#x27;r&#x27;</span>, <span class="string">&#x27;2&#x27;</span>, <span class="number">98</span>, <span class="string">&#x27;语文&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> `student` <span class="keyword">VALUES</span> (<span class="string">&#x27;s&#x27;</span>, <span class="string">&#x27;3&#x27;</span>, <span class="number">90</span>, <span class="string">&#x27;语文&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> `student` <span class="keyword">VALUES</span> (<span class="string">&#x27;t&#x27;</span>, <span class="string">&#x27;2&#x27;</span>, <span class="number">79</span>, <span class="string">&#x27;数学&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> `student` <span class="keyword">VALUES</span> (<span class="string">&#x27;v&#x27;</span>, <span class="string">&#x27;4&#x27;</span>, <span class="number">88</span>, <span class="string">&#x27;语文&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> `student` <span class="keyword">VALUES</span> (<span class="string">&#x27;w&#x27;</span>, <span class="string">&#x27;1&#x27;</span>, <span class="number">80</span>, <span class="string">&#x27;英语&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> `student` <span class="keyword">VALUES</span> (<span class="string">&#x27;x&#x27;</span>, <span class="string">&#x27;3&#x27;</span>, <span class="number">67</span>, <span class="string">&#x27;政治&#x27;</span>);</span><br><span class="line"><span class="keyword">INSERT</span> <span class="keyword">INTO</span> `student` <span class="keyword">VALUES</span> (<span class="string">&#x27;z&#x27;</span>, <span class="string">&#x27;2&#x27;</span>, <span class="number">89</span>, <span class="string">&#x27;政治&#x27;</span>);</span><br><span class="line"><span class="keyword">COMMIT</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">SET</span> FOREIGN_KEY_CHECKS <span class="operator">=</span> <span class="number">1</span>;</span><br></pre></td></tr></table></figure><p>开窗函数主要是结合<code>over(partition by ... order by ...)</code>的句式，配合其他函数计算一个组内或者窗口内的数据，这其中可以不断组合以满足业务需求，而且还可以配合window子句已完成更复杂的滑动窗口。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- over窗口函数就是给数据一个计算窗口，当不指定partition和order的时候，默认窗口就是所有记录</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    name,class,subject,score,</span><br><span class="line">    <span class="built_in">avg</span>(score) <span class="keyword">over</span>() scoreSum,</span><br><span class="line">    <span class="built_in">max</span>(score) <span class="keyword">over</span>() scoreMax</span><br><span class="line"><span class="keyword">from</span> student;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* 数据太多没复制完</span></span><br><span class="line"><span class="comment">a    2    英语    94    1422    99</span></span><br><span class="line"><span class="comment">b    4    英语    99    1422    99</span></span><br><span class="line"><span class="comment">c    4    政治    90    1422    99</span></span><br><span class="line"><span class="comment">d    3    数学    99    1422    99</span></span><br><span class="line"><span class="comment">e    1    语文    88    1422    99</span></span><br><span class="line"><span class="comment">f    3    英语    78    1422    99</span></span><br><span class="line"><span class="comment">g    1    政治    89    1422    99</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 输出按科目分组，score降序排序，每个学生的名次（输出结果第四列即为名次）</span></span><br><span class="line"><span class="comment">-- 如需名次之间连续，例如1223这样则将rank替换为dense_rank</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    name,class,subject,score,</span><br><span class="line">    <span class="built_in">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) scoreRank</span><br><span class="line"><span class="keyword">from</span> student;</span><br><span class="line"></span><br><span class="line"><span class="comment">/* </span></span><br><span class="line"><span class="comment">c    4    政治    90    1</span></span><br><span class="line"><span class="comment">g    1    政治    89    2</span></span><br><span class="line"><span class="comment">z    2    政治    89    2</span></span><br><span class="line"><span class="comment">x    3    政治    67    4</span></span><br><span class="line"><span class="comment">d    3    数学    99    1</span></span><br><span class="line"><span class="comment">n    4    数学    99    1</span></span><br><span class="line"><span class="comment">q    1    数学    95    3</span></span><br><span class="line"><span class="comment">t    2    数学    79    4 </span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 输出按科目分组，score降序排序，每个学生的连续名次、累加score以及每个科目的最大score（多个组合窗口函数）</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    name,class,subject,score,</span><br><span class="line">    <span class="built_in">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) scoreRank, </span><br><span class="line">    <span class="built_in">sum</span>(score) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) scoreSum,</span><br><span class="line">    <span class="built_in">max</span>(score) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) scoreMax</span><br><span class="line"><span class="keyword">from</span> student;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">c    4    政治    90    1    90    90</span></span><br><span class="line"><span class="comment">g    1    政治    89    2    268    90</span></span><br><span class="line"><span class="comment">z    2    政治    89    2    268    90</span></span><br><span class="line"><span class="comment">x    3    政治    67    3    335    90</span></span><br><span class="line"><span class="comment">d    3    数学    99    1    198    99</span></span><br><span class="line"><span class="comment">n    4    数学    99    1    198    99</span></span><br><span class="line"><span class="comment">q    1    数学    95    2    293    99</span></span><br><span class="line"><span class="comment">t    2    数学    79    3    372    99</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 不分组仅按score降序排序输出累加score（不使用partition，注意score相等的score和并不是你想的那样）</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    name,class,subject,score,</span><br><span class="line">    <span class="built_in">sum</span>(score) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) scoreSum,</span><br><span class="line">    <span class="built_in">max</span>(score) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) scoreMax</span><br><span class="line"><span class="keyword">from</span> student;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">b    4    英语    99    297    99</span></span><br><span class="line"><span class="comment">d    3    数学    99    297    99</span></span><br><span class="line"><span class="comment">n    4    数学    99    297    99</span></span><br><span class="line"><span class="comment">r    2    语文    98    395    99</span></span><br><span class="line"><span class="comment">q    1    数学    95    490    99</span></span><br><span class="line"><span class="comment">a    2    英语    94    584    99</span></span><br><span class="line"><span class="comment">c    4    政治    90    764    99</span></span><br><span class="line"><span class="comment">s    3    语文    90    764    99</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">下面引入window子句，preceding（往前）、following（往后）、current row（当前行）、unbounded preceding（从起点开始）、unbounded following（截止到最后一行）</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">-- 计算按班级排序当前score以及前面两个score的平均值和最大值（这里会用到滑动窗口）</span></span><br><span class="line"><span class="comment">-- 引入新的关键字rows和preceding，表示截止到当前两行，就是选择最近三行的</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    name,class,subject,score,</span><br><span class="line">    <span class="built_in">avg</span>(score) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> class <span class="keyword">rows</span> <span class="number">2</span> preceding) scoreSum,</span><br><span class="line">    <span class="built_in">max</span>(score) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> class <span class="keyword">rows</span> <span class="number">2</span> preceding) scoreMax</span><br><span class="line"><span class="keyword">from</span> student;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">e    1    语文    88    88.0000    88</span></span><br><span class="line"><span class="comment">g    1    政治    89    88.5000    89</span></span><br><span class="line"><span class="comment">q    1    数学    95    90.6667    95</span></span><br><span class="line"><span class="comment">w    1    英语    80    88.0000    95</span></span><br><span class="line"><span class="comment">a    2    英语    94    89.6667    95</span></span><br><span class="line"><span class="comment">r    2    语文    98    90.6667    98</span></span><br><span class="line"><span class="comment">t    2    数学    79    90.3333    98</span></span><br><span class="line"><span class="comment">z    2    政治    89    88.6667    98</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 下面把经常遇到的场景列举一遍，因为数据太少只用了order，一般实际场景使用partition结合order情况比较多</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    name,class,subject,score, </span><br><span class="line">    <span class="built_in">sum</span>(score) <span class="keyword">over</span>() sumAll,  <span class="comment">-- 所有数score的和</span></span><br><span class="line">    <span class="built_in">sum</span>(score) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> class) sumAsClass,  <span class="comment">-- 按class分组组内score的和</span></span><br><span class="line">    <span class="built_in">sum</span>(score) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) sumAsScore,  <span class="comment">-- 按score降序排序累加的和</span></span><br><span class="line">    <span class="built_in">sum</span>(score) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> class <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) sumAsClassScore,  <span class="comment">-- 按class分组，score降序排序累加的和</span></span><br><span class="line">    <span class="built_in">sum</span>(score) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span> <span class="keyword">rows</span> <span class="number">2</span> preceding) sumAsPreceding,  <span class="comment">-- 按score排序往前两条记录到当前共三条记录的和</span></span><br><span class="line">    <span class="built_in">sum</span>(score) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span> <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">2</span> preceding <span class="keyword">and</span> <span class="keyword">current</span> <span class="type">row</span>) sumPrecedingCurrent,  <span class="comment">-- 和上一条表达意思相同</span></span><br><span class="line">    <span class="built_in">sum</span>(score) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span> <span class="keyword">rows</span> <span class="keyword">between</span> <span class="number">1</span> preceding <span class="keyword">and</span> <span class="number">1</span> following) sumPrecedingFollowing,  <span class="comment">-- 按score排序往前一条往后一条加上当前共三条记录的和</span></span><br><span class="line">    <span class="built_in">sum</span>(score) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span> <span class="keyword">rows</span> <span class="keyword">between</span> <span class="keyword">current</span> <span class="type">row</span> <span class="keyword">and</span> unbounded following) sumUnboundPreFoll  <span class="comment">-- 按score排序当前记录到结尾记录的和</span></span><br><span class="line"><span class="keyword">from</span> student</span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">name    class    subject    score    sumAll    sumAsClass    sumAsScore    sumAsClassScore    sumAsPreceding    sumPrecedingCurrent    sumPrecedingFollowing    sumUnboundPreFoll</span></span><br><span class="line"><span class="comment">d    3    数学    99    1422    334    297    99    99    99    198    1422</span></span><br><span class="line"><span class="comment">b    4    英语    99    1422    376    297    198    198    198    297    1323</span></span><br><span class="line"><span class="comment">n    4    数学    99    1422    376    297    198    297    297    296    1224</span></span><br><span class="line"><span class="comment">r    2    语文    98    1422    360    395    98    296    296    292    1125</span></span><br><span class="line"><span class="comment">q    1    数学    95    1422    352    490    95    292    292    287    1027</span></span><br><span class="line"><span class="comment">a    2    英语    94    1422    360    584    192    287    287    279    932</span></span><br><span class="line"><span class="comment">s    3    语文    90    1422    334    764    189    279    279    274    838</span></span><br><span class="line"><span class="comment">c    4    政治    90    1422    376    764    288    274    274    269    748</span></span><br><span class="line"><span class="comment">z    2    政治    89    1422    360    942    281    268    268    266    569</span></span><br><span class="line"><span class="comment">g    1    政治    89    1422    352    942    184    269    269    268    658</span></span><br><span class="line"><span class="comment">v    4    语文    88    1422    376    1118    376    265    265    256    392</span></span><br><span class="line"><span class="comment">e    1    语文    88    1422    352    1118    272    266    266    265    480</span></span><br><span class="line"><span class="comment">w    1    英语    80    1422    352    1198    352    256    256    247    304</span></span><br><span class="line"><span class="comment">t    2    数学    79    1422    360    1277    360    247    247    237    224</span></span><br><span class="line"><span class="comment">f    3    英语    78    1422    334    1355    267    237    237    224    145</span></span><br><span class="line"><span class="comment">x    3    政治    67    1422    334    1422    334    224    224    145    67</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure><p>需要注意的是因为排名、求和等函数是在窗口内逐行计算的，所以在 over 函数内降序排序和升序排序会返回不同的结果，务必根据场景选择排序方式；而且使用窗口函数计算出来的数值也可以放进 where 子句或者使用 order 再次排序。</p><p>除了常见的开窗场景还有常见的聚合场景也说一下</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- row_number在排名问题上比较常用，它返回当前记录的行号，不受记录重复造成的影响，eg：1234</span></span><br><span class="line"><span class="comment">-- rank跟row_number的不同之处是对于重复记录时使用相同排名，然后会跳过当前排名返回行号，eg：1224</span></span><br><span class="line"><span class="comment">-- dense_rank和rank不同的地方是遇到重复记录时使用相同排名，然后会接着上面的排名数字返回，eg：1223</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    name,class,subject,score, </span><br><span class="line">    <span class="built_in">row_number</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) rowNumber,</span><br><span class="line">    <span class="built_in">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) `Rank`,</span><br><span class="line">    <span class="built_in">dense_rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> subject <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) denseRank</span><br><span class="line"><span class="keyword">from</span> student;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">name    class    subject    score    rowNumber    Rank    denseRank</span></span><br><span class="line"><span class="comment">c    4    政治    90    1    1    1</span></span><br><span class="line"><span class="comment">g    1    政治    89    2    2    2</span></span><br><span class="line"><span class="comment">z    2    政治    89    3    2    2</span></span><br><span class="line"><span class="comment">x    3    政治    67    4    4    3</span></span><br><span class="line"><span class="comment">d    3    数学    99    1    1    1</span></span><br><span class="line"><span class="comment">n    4    数学    99    2    1    1</span></span><br><span class="line"><span class="comment">q    1    数学    95    3    3    2</span></span><br><span class="line"><span class="comment">t    2    数学    79    4    4    3</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- lag和lead函数，在返回用户上一次购买时间的情况下特别好用，若没有写默认值则返回NULL</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    name,class,subject,score, </span><br><span class="line">    <span class="built_in">lag</span>(score,<span class="number">1</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> class <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) lagOne,</span><br><span class="line">    <span class="built_in">lag</span>(score,<span class="number">2</span>, <span class="number">0</span>) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> class <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) lagTwo</span><br><span class="line"><span class="keyword">from</span> student;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">name    class    subject    score    lagOne    lagTwo</span></span><br><span class="line"><span class="comment">q    1    数学    95    NULL    0</span></span><br><span class="line"><span class="comment">g    1    政治    89    95    0</span></span><br><span class="line"><span class="comment">e    1    语文    88    89    95</span></span><br><span class="line"><span class="comment">w    1    英语    80    88    89</span></span><br><span class="line"><span class="comment">r    2    语文    98    NULL    0</span></span><br><span class="line"><span class="comment">a    2    英语    94    98    0</span></span><br><span class="line"><span class="comment">z    2    政治    89    94    98</span></span><br><span class="line"><span class="comment">t    2    数学    79    89    94</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- first_value和last_value分别是返回截止到当前行的第一条记录和最后一条记录（最后一条记录即当前记录）</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    name,class,subject,score, </span><br><span class="line">    <span class="built_in">first_value</span>(score) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> class <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) firstValue,</span><br><span class="line">    <span class="built_in">last_value</span>(score) <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> class <span class="keyword">order</span> <span class="keyword">by</span> score <span class="keyword">desc</span>) lastValue</span><br><span class="line"><span class="keyword">from</span> student;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">name    class    subject    score    firstValue    lastValue</span></span><br><span class="line"><span class="comment">q    1    数学    95    95    95</span></span><br><span class="line"><span class="comment">g    1    政治    89    95    89</span></span><br><span class="line"><span class="comment">e    1    语文    88    95    88</span></span><br><span class="line"><span class="comment">w    1    英语    80    95    80</span></span><br><span class="line"><span class="comment">r    2    语文    98    98    98</span></span><br><span class="line"><span class="comment">a    2    英语    94    98    94</span></span><br><span class="line"><span class="comment">z    2    政治    89    98    89</span></span><br><span class="line"><span class="comment">t    2    数学    79    98    79</span></span><br><span class="line"><span class="comment">*/</span></span><br></pre></td></tr></table></figure><p>窗口函数在结果分类分组计算以及滚动聚合的情况下特别合适，赋予 SQL 极大地灵活性，适用于 OLAP 分析性数据库，而且在 Hive 中也支持窗口函数，所以掌握窗口函数是十分有必要的。</p>]]></content>
      
      
      <categories>
          
          <category> Language </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL </tag>
            
            <tag> Hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论压抑</title>
      <link href="/2021/06/02/Mind/%E8%AE%BA%E5%8E%8B%E6%8A%91/"/>
      <url>/2021/06/02/Mind/%E8%AE%BA%E5%8E%8B%E6%8A%91/</url>
      
        <content type="html"><![CDATA[<h3 id="产生压抑的原因"><a href="#产生压抑的原因" class="headerlink" title="产生压抑的原因"></a>产生压抑的原因</h3><p>截止到现在，我依然不是很清楚为什么要写这个话题，是因为它影响到了正常生活，还是它挡在了我前进的路上，总而言之不管从内部或是外部来看，压抑带来的总是负面的影响，当在心中积压到一定的程度，就会严重扰乱我的生活节奏。</p><span id="more"></span><p>我想产生压抑的因素有这么几点，第一是自身能力与内心欲望的不匹配，按理来说这会导致焦虑和烦躁，但到我这都会转换成压抑，我会主动的将焦虑和烦躁都压到心底，转而选择“合适的”，“正确的”方式去解决这些问题，久而久之，问题并没有被解决，反而因为压的太久导致内心低迷；第二是目标不明确，这里指的是人生目标，足以为之奋斗的东西，把它定为钱财太肤浅了，我尝试过去主动寻找，但最终都不如意或是变得现实；第三是学习过程中阶段性目标的模糊，比如现在我很确信我要学数据仓库并且要能够积累足够的经验，但我不知道具体该怎么做，我总想在网上找别人对此的看法想法，因为我不知道把数据仓库学好需要哪些衡量指标，说到底是无知引起的；第四是人际关系，和家庭、同事、朋友之间的关系我都没能很好的处理，有些时候我的期望过高，指望别人能完全理解我，有时候期望又太低，导致丧失了交流的欲望，我能体会到完全理解他人是一种奢求，但还是会产生这种欲望；最后是一些莫须有的，我也说不上来到底是什么，上班路上走的快了也会让我心里堵得慌，不知道该干什么的时候也会转向情绪的阴暗面。</p><h3 id="解决压抑的必要性"><a href="#解决压抑的必要性" class="headerlink" title="解决压抑的必要性"></a>解决压抑的必要性</h3><p>首先它是我做出改变的开始，前两天看了一本书，认知觉醒，在讲该怎样全方位认识自己，用一些相对合理的语言解释自己的行为，最后是合理利用大脑的各部分机制达成目标，这也促使我开始深刻认识自己。前面讲到导致压抑的一些原因，同时这也是制约进步的因素。所以我把认识压抑和解决压抑提上了日程，因为我想通过解决它给自己一个反馈，这是改变的开始。</p><p>其次它是我前进路上的绊脚石，现如今社会压力大是无法改变的，只能通过提升自身能力来化解，不管再怎么抱怨臆想都是无用功，那么解决压抑所带来的负面影响是很有必要的，不消除这些负面影响因素怎么会安下心来做出改变。而且当下社会行业内卷严重，与其做一个愤世嫉俗的咆哮者或是目光短浅的享受者，不如做一个主动出击的进步者。</p><h3 id="通过发泄或者疏导化解"><a href="#通过发泄或者疏导化解" class="headerlink" title="通过发泄或者疏导化解"></a>通过发泄或者疏导化解</h3><p>以往的策略是在内心十分压抑时试图放松自己，放空内心，把堵在心口的浊气排出去，但这终归是治标不治本，属于压抑到来后的被动防守功能，要做到从根本上解决问题。</p><p>最近我开始尝试通过拒绝和专注两个方法，拒绝就是拒绝与生活工作和学习的无意义事情，选择对你当下来说有意义的，有目的的事情去做，像早上的收能量，在地铁上玩游戏或者看B站，还有上班时候不必要的摸鱼行为（不必要指的是精力充沛且手头有事情的时候），这些事情浪费了太多精力。而且也不要给自己定太多的目标，拒绝那些无意义而且不适合你的目标；其次是专注，忘了从什么时候开始静下心来认真做一件事情越来越难了，长此以往以后想要专注会更加困难，主要从两个方面入手，提前规划和后娱乐，从最近一年到最近三个月再到最近一个月，一周，明天的事情，从模糊到具体考虑清楚并写下来，不能专注的一大原因就是模糊，因为没有具体明确所以无法准确落实。后娱乐则是延迟满足，干一件事之前总想刷会视频，看一眼公众号文章，早上分明可以早起却要玩半天手机，分明可以早早完成却在磨洋工，没什么事总想摆弄两下手机，以后可以以奖励的性质去娱乐，定个番茄钟，五十分钟高质量专注奖励十分钟娱乐时间，把这当成一个闭环去执行，提高自己的专注能力。</p><p>其实现在内心大多数负面情绪都可以归结为想的太多，做的太少，欲望太多，能力太低，所以使用合适的方法提升自己的能力都是必须的，满足自己的欲望只是顺带的事情。</p>]]></content>
      
      
      <categories>
          
          <category> Mind </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mind </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>几种主流的数据仓库建模方法</title>
      <link href="/2021/05/20/Data/%E5%87%A0%E7%A7%8D%E4%B8%BB%E6%B5%81%E7%9A%84%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%BB%BA%E6%A8%A1%E6%96%B9%E6%B3%95/"/>
      <url>/2021/05/20/Data/%E5%87%A0%E7%A7%8D%E4%B8%BB%E6%B5%81%E7%9A%84%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E5%BB%BA%E6%A8%A1%E6%96%B9%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>几种常用的数据仓库建模方法，重点讲了维度建模。</p><span id="more"></span><p>数据仓库建模在数据仓库建设中有很重要的地位，是继业务梳理后的第二大要点，是将概念模型转化为物理模型的一个过程。关于建模一向被吹得神乎其神，相关介绍文章以及招聘需求中对此都要求很高，那么了解主流的建模方法和各自的优劣势以及应用场景就变得至关重要了。</p><p>选择合适的建模方法要考虑几点要素，分别是：性能、成本、效率和质量，性能是能够快速的查询所需的数据，减少数据IO的吞吐；成本是减少不必要的冗余，实现计算结果的复用，减少大数据系统的计算成本和存储成本；效率则是改善使用数据的效率，计算、查询效率也算在内；质量是改善数据统计口径的不一致性，减少数据计算错误的可能性，提供高质量、一致性的数据访问平台。在实际应用中则会根据这几点要素所占权重的不同来选择合适的建模方法。</p><h3 id="ER-模型"><a href="#ER-模型" class="headerlink" title="ER 模型"></a>ER 模型</h3><p>将事务抽象为“实体”、“属性”和“关系”来表示数据的关联和事物的描述，这种对事务的抽象建模通常称为 E-R 实体关系模型。数据仓库之父<code>Bill Inmon</code>提出的建模方法，从全企业的高度设计一个 3NF 模型，用实体关系（ Entity Relationship ）模型来描述企业业务，满足3NF。</p><p>数据仓库的 3NF 与 OLTP 系统中的 3NF 的区别在于，它是站在企业角度面向主题的抽象，而不是针对某个具体的业务流程。采用 E-R 模型建设数据仓库模型的出发点是整合数据，对各个系统的数据以整个企业角度按主题进行相似的组合和合并，并进行一致性处理，为数据分析决策服务，但是并不能直接用于分析决策。</p><p>作为一种标准的数据建模方案，它的实施周期非常长，一致性和扩展性比较好，能经得起时间的考验。但是随着企业数据的高速增长、复杂化，数仓如果全部使用 E-R 模型进行建模就显得越来越不适合现代化复杂、多变的业务组织，因此一般只有在数仓底层 ODS、DWD 会采用 E-R 关系模型进行设计。</p><h3 id="维度建模"><a href="#维度建模" class="headerlink" title="维度建模"></a>维度建模</h3><p>维度建模是数据仓库领域另一位大师 <code>Ralph Kimball</code> 所倡导，是数据仓库工程领域最流行的数仓建模经典。维度建模以分析决策的需求出发构建模型，构建的数据模型为分析需求服务，因此它重点解决用户如何更快速完成分析需求，同时还有较好的大规模复杂查询的响应性能。维度建模是专门用于分析型数据库、数据仓库、数据集市建模的方法。</p><p>维度建模的主要构成是维度表和事实表。每一张维度表对应现实世界中的一个对象或者一个主题，例如：客户、产品、时间、地区等，通常是包含了多个属性的列，通常数据量不会太大；事实表则是描述业务的多条记录，包含了描述业务的度量值以及和维度表相关联的外键，外键和维度表通常是多对多的关系，数据量大而且经常发生变化。</p><p>维度建模一般包含三个，一般是根据业务需求和业务复杂性加以区分，有区分的方法但没有比较清晰地界限。分别是星型模型、雪花模型和星座模型。</p><h4 id="星型模型"><a href="#星型模型" class="headerlink" title="星型模型"></a>星型模型</h4><p>星型模式（Star Schema）是面向主题的常用模式，主要由一个事实表和多个维度表构成，不存在二级维度表</p><div align=center><img src="星型模式.png"></div><h4 id="雪花模型"><a href="#雪花模型" class="headerlink" title="雪花模型"></a>雪花模型</h4><p>雪花模型（Snowflake Schema）是在星型模型基础上将维表再次扩展，每个维表可以继续向外连接多个子维表。</p><div align=center><img src="雪花模型.png"></div><p>雪花模型相当于将星型模型的大维表拆分成小维表，满足了规范化设计，因为很少会有事实表只关联一层维度的，往往维度还会细分，钻取。然而这种模式在实际应用中很少见，因为跨表查询时效率很慢，所以现在的做法是将部分维度表整合到事实表中，形成宽表，在查询汇总的时候只需要<code>group by</code>就可以了，不需要再进行<code>join</code>操作。</p><h4 id="星座模型"><a href="#星座模型" class="headerlink" title="星座模型"></a>星座模型</h4><p>星座模型（Fact Constellations Schema）也是星型模型的扩展，存在多个事实表且可共用同一个维表。实际上数仓模型建设后期，大部分维度建模都是星座模型。</p><div align=center><img src="星座模型.png"></div><p>前面介绍的两种维度建模方法都是多维表对应单事实表，但在很多时候维度空间内的事实表不止一个，而一个维表可能被多个事实表用到。在业务发展的后期，绝大部分维度建模都采用的都是星座模型。</p><h3 id="Data-Vault模型"><a href="#Data-Vault模型" class="headerlink" title="Data Vault模型"></a>Data Vault模型</h3><p>Data Vault 是 <code>Dan Linstedt</code> 发起创建的一种模型，它是 E-R 模型的衍生，其设计的出发点也是为了实现数据的整合，但不能直接用于数据分析决策。主要在对自然界中发现的复杂网络建模。</p><p>Data Vault 是面向细节的，可追踪历史的，一组有连接关系的规范化的表的集合。这些表可以支持一个或多个业务功能。从建模风格上看，它采用了一种由第三范式方法（3NF）与维度建模方法混合而成的方式，以二者的独特组合来满足企业需求。</p><p>同时它基于主题概念将企业数据进行结构化组织，并引入了更进一步的范式处理来优化模型，以应对源系统变更的扩展性。 Data Vault 型由以下几部分组成：</p><ul><li><strong>Hub - 中心表</strong>：是企业的核心业务实体，由实体 Key 、数仓序列代理键、装载时间、数据来源组成，不包含非键值以外的业务数据属性本身。</li><li><strong>Link - 链接表</strong>：代表 Hub 之间的关系，一个链接表意味着两个或多个中心表之间有关联。这里与 ER 模型最大的区别是将关系作为一个独立的单元抽象，可以提升模型的扩展性。它可以直接描述 1:1、1:2 和 n:n 的关系，而不需要做任何变更。它由 Hub的代理键、装载时间、数据来源组成。</li><li><strong>Satellite - 卫星表</strong>：数仓中数据的主要载体，包括对链接表、中心表的数据描述、数值度量等信息。Anchor 模型</li></ul><div align=center><img src="Data Vault.png"></div><p>Data Vault 模型比 E-R 模型更容易设计和产出，它的 ETL 加工可实现配置化。我们可以将 Hub 想象成人的骨架，那么 Link 就是连接骨架的韧带，而 SateIIite 就是骨架上面的血肉。</p><h3 id="Anchor模型"><a href="#Anchor模型" class="headerlink" title="Anchor模型"></a>Anchor模型</h3><p><code>Anchor</code> 是对 <code>Data Vault</code> 模型做了进一步的规范化处理，它的核心思想是所有的扩展只是添加而不是修改，因此将模型规范到 6NF，基本变成了 k-v 结构化模型。</p><ul><li><strong>Anchors</strong> ：类似于 <code>Data Vault</code> 的 Hub ，代表业务实体，且只有主键。</li><li><strong>Attributes</strong> ：功能类似于 <code>Data Vault</code> 的 Satellite，但是它更加规范化，将其全部 k-v 结构化， 一个表只有一个 Anchors 的属性描述。</li><li><strong>Ties</strong> ：就是 <code>Anchors</code> 之间的关系，单独用表来描述，类似于 <code>Data Vault</code> 的 Link ，可以提升整体模型关系的扩展能力。</li><li><strong>Knots</strong> ：代表那些可能会在 <code>Anchors</code> 中公用的属性的提炼，比如性别、状态等这种枚举类型且被公用的属性。</li></ul><p>由于过度规范化，使用中牵涉到太多的 Join 操作，这里我们就仅作了解。</p><h3 id="MOLAP"><a href="#MOLAP" class="headerlink" title="MOLAP"></a>MOLAP</h3><p><code>MOLAP</code>区别于以上几种建模方法，适用于 ADS 层。它是将数据进行预结算，并将结果存到 CUBE 模型中，在查询的时候效率就会快很多。<code>CUBE</code>模型以多维数组的形式，存储在系统中，加快后续的查询，但是需要耗费巨大的时间和空间成本，维度预处理可能会导致数据膨胀。</p><div align=center><img src="CUBE.png"></div><p>常见的<code>MOLAP</code>产品有<code>Kylin</code>和<code>Druid</code>，其中<code>Kylin</code>使用的会比较多一点，它是将数据仓库中的数据抽取过来进行维度组合，加工成<code>CUBE</code>后存储到<code>Hbase</code>数据库（Hbase 具有并发能力，所以查询性能会很高）中，业务端或者前端需要数据的时候<code>Kylin</code>则会从<code>Hbase</code>中将数据取出并返回。</p><div align=center><img src="kylin_diagram.png"></div><p>以上就是几种主流的建模方向，针对每一种建模方法还会有不少的细节问题，留到后面详细讨论。学习一项新技术就是要由上而下，由总体到局部抽丝剥茧，化繁为简。</p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据建模 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库架构</title>
      <link href="/2021/05/17/Data/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84/"/>
      <url>/2021/05/17/Data/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%9E%B6%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<p>将维度建模分为 ODS 、DW 以及 ADS 三层，实际生产环境中应按照业务需求以及现有技术进行合理调整。</p><span id="more"></span><p>这节来说说数据仓库的架构，关于架构并没有一个统一的标准，按照数据量以及使用环境可以搭建适合当下场景的数据仓库，下面主要是介绍比较大众或者说比较通用的数据仓库架构。传统的数据仓库架构按层级可分为 ETL、ODS、DWD 、DWS 和 ADS 五个层级，层次结构如下图所示。</p><div align=center><img src="数据仓库架构.png"></div><p>这五个层级针对不同的需求解决各阶段各自的问题，在数据量少的情况下个别层次可以继续简化，也可以在层次之间增加数据中间件，让整个过程更加规范化，对于层级结构没必要细究，重点是每个层级所解决的问题以及在整个链路所处的位置。</p><h3 id="ETL"><a href="#ETL" class="headerlink" title="ETL"></a>ETL</h3><p>ETL（Extract Transform Load）是抽取、转换、加载三个过程的缩写，目的是将数据从业务、文件或其他数据源抽取出来，经过简单转换后加载到数据仓库系统中。这个过程中有两个重点，从哪里抽取以及抽取方式。</p><p>抽取的数据源可以是结构化数据、非结构化数据和半结构化数据，这其中结构化数据一般来源自 MySQL、Oracle 等数据库，需要考虑的问题是通过 JDBC 还是数据库日志进行抽取，通过 JDBC 则会对数据库产生额外的开销，跟业务数据库抢资源，数据量不大的情况下也选择在半夜进行，若是数据量大可以通过数据库日志进行同步更新，不会对数据库产生额外开销，而且效率会快不少。非结构化或者半结构化就要简单一些了，无非是在转换的时候需要下点功夫解析数据。</p><p>抽取方式的话分为全量抽取和增量抽取，全量一般发生在第一次将数据加载到数据仓库中的时候，全量查询即可，增量则是会对业务数据库中有更新的数据抽取出来更新到数据仓库中，可以监控数据库日志对数据仓库进行操作。</p><h3 id="数据运营层（ODS）"><a href="#数据运营层（ODS）" class="headerlink" title="数据运营层（ODS）"></a>数据运营层（ODS）</h3><p>ODS（Operation Data Store）数据准备区，数据仓库会将业务数据或者日志数据原封不动的存储一份，一般通过增加字段进行处理管理，是后续数据仓库加工的来源。这一层及的重点是保证数据的完整性和实效性。</p><p>在离线数仓中，业务数据定期通过 ETL 加载进 ODS 中，导入方式有全量和增量两种，全量是数据第一次导入时选择的方式，增量则是非第一次导入，只需导入新增和更改的数据，建议使用外连接&amp;全覆盖的方式（将新增、更新的数据与数据仓库中的数据进行外连接，在内存中修改数据仓库中需要更新的数据再导入数据仓库中，将原先的数据全部覆盖）。</p><h3 id="数据仓库层（DW）"><a href="#数据仓库层（DW）" class="headerlink" title="数据仓库层（DW）"></a>数据仓库层（DW）</h3><p>DW（Data Warehouse）数据分层从下往上分为 DWD 和 DWS</p><ul><li>DWD（Data Warehouse Details）：数据明细层，是数据仓库与业务层的隔离层，主要对 ODS 做一些数据清洗和规范化操作</li><li>DWS（Data Warehouse Service）：数据服务层，基于 DWD 上的数据，按主题整合汇总数据，就是做宽表，用于后续业务查询支持，OLAP 分析等</li></ul><h3 id="数据应用层（ADS）"><a href="#数据应用层（ADS）" class="headerlink" title="数据应用层（ADS）"></a>数据应用层（ADS）</h3><p>ADS（Application Data Service）应用服务层，该层级主要存储数据产品和数据分析需要的数据，一般注重查询速度，所以需要将数据存储到指定的 ADS 应用或是 MySQL 数据库，或者存到 ES 以供前端搜索查询使用。</p><p>数据仓库的架构如此，但并不代表每个层级就得使用一个工具，例如 Hive 就包揽了好几个层级，ETL 层级也可能包含多个工具。在数据仓库的搭建环节，重点是做好仓库的架构以及数据在每个层级的产出是什么状态的，接下来是每个层级内部实行操作的细节，最后才是利用什么工具，耗费多少资源，切莫本末倒置，工具学得再好没有理论的支持也不知道在什么场合下使用。</p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据仓库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数据仓库概述</title>
      <link href="/2021/05/14/Data/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%A6%82%E8%BF%B0/"/>
      <url>/2021/05/14/Data/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%A6%82%E8%BF%B0/</url>
      
        <content type="html"><![CDATA[<p>介绍了数据仓库的产生和发展，以及从技术方面分为 MPP 数据库架构和大数据仓库架构。</p><span id="more"></span><p>最近开始接触数据仓库，要了解一项技术或者一个工具，就得先去了解它在某个链路中所处的位置，以及它的出现是为了解决什么问题，那么就有了这篇文章，从数据仓库的诞生环境、所能解决的问题以及主流的上层架构，简单介绍了数据仓库这项技术。</p><h3 id="产生背景"><a href="#产生背景" class="headerlink" title="产生背景"></a>产生背景</h3><p>数据仓库的诞生离不开环境的因素，在数据量加剧的当今社会和数据分析挖掘需求日益增加的环境下，传统面向业务的 MySQL、Oracle 等 OLTP 型数据库已经不能满足当下一些特殊的需求。OLTP 型数据库主要是解决业务需求，构造也具有遵循 3NF 范式，避免冗余等特点，但这些特点在面向分析的时候却成了导致效率低下和诸多限制的瓶颈。那么数据仓库这种 OLAP 型数据库就应运而生了，它不同于传统 OLTP 数据库，构造思想是反范式并且有冗余的，并且是一个面向主题的、集成的、非易失的且随时间变化的数据集合，目的是为了在面向分析或者数据挖掘的场景下提高效率或者简单易操作。</p><h3 id="数据仓库概述"><a href="#数据仓库概述" class="headerlink" title="数据仓库概述"></a>数据仓库概述</h3><p>既然数据仓库诞生的目的是为了解决主题分析或者数据挖掘这类问题，那么分析或是挖掘就是数据仓库的终点，起点则是所需要的各种数据，这些数据可能来自不同的地方，有 MySQL、Oracle 等结构化数据，也可能有 ES、MongoDB 等非结构化数据，也有可能是文件等等。数据经过起点，通过抽取、清洗转换、加载（ETL）之后进入数据仓库，最后再由分析或是挖掘需求进行消费，形成了数据产出、存储、加工、消费的一条链路。</p><p>数据库是面向事务的，属于 OLTP（在线事务处理），主要操作是随机读写以及事务支持，在设计时尽量避免冗余，常采用符合范式的规范来设计；而数据仓库是面向主题的，属于 OLAP (在线分析处理），主要操作是批量读写，关注数据整合，以及分析、处理性能，会有意引入冗余，采用反范式设计。关于数据库和数据仓库的比较如下表所示：</p><table><thead><tr><th align="center"></th><th align="center">数据库</th><th align="center">数据仓库</th></tr></thead><tbody><tr><td align="center"><strong>面向</strong></td><td align="center">事务</td><td align="center">分析</td></tr><tr><td align="center"><strong>数据类型</strong></td><td align="center">细节、业务</td><td align="center">综合、处理过的数据</td></tr><tr><td align="center"><strong>数据特点</strong></td><td align="center">当前的、最新的</td><td align="center">历史的、跨时间维度</td></tr><tr><td align="center"><strong>目的</strong></td><td align="center">日常操作</td><td align="center">长期信息需求、决策支持</td></tr><tr><td align="center"><strong>设计模型</strong></td><td align="center">基于ER模型、面向对象</td><td align="center">星型&#x2F;雪花模型，面向主题</td></tr><tr><td align="center"><strong>操作</strong></td><td align="center">读&#x2F;写</td><td align="center">大多为读</td></tr><tr><td align="center"><strong>数据规模</strong></td><td align="center">GB到TB</td><td align="center">&gt;&#x3D; TB</td></tr></tbody></table><h3 id="数据仓库技术实现"><a href="#数据仓库技术实现" class="headerlink" title="数据仓库技术实现"></a>数据仓库技术实现</h3><p>对于数据仓库的选型一般得考虑适用环境、数据规模、数据来源、吞吐量和数据消费者的类型等，这些都是决定选取哪种架构的数据仓库的因素。一般从数据规模来分有两种技术实现方法，一种是传统数据仓库– MPP（大规模并行处理）集群，一种是大数据数据仓库。</p><h4 id="MPP-数据库"><a href="#MPP-数据库" class="headerlink" title="MPP 数据库"></a>MPP 数据库</h4><p>MPP 数据库是一款 Shared Nothing 架构的分布式并行结构化数据库集群，具备高性能、高可用、高扩展特性，可以为超大规模数据管理提供高性价比的通用计算平台，并广泛地用于支撑各类数据仓库系统、BI 系统和决策支持系统。</p><p>MPP 数据库有对 SQL 的完整兼容和一些事务的处理能力，对于用户来说，在实际的使用场景中，如果数据扩展需求不是特别大，需要的处理节点不多，数据都是结构化的数据，习惯使用传统的 RDBMS 的很多特性的场景，可以考虑 MPP。常见产品有：Oracle RAC、DB2、Teradata 和 Greenplum 等。</p><h5 id="实现原理"><a href="#实现原理" class="headerlink" title="实现原理"></a>实现原理</h5><p>MPP 数据库是将单机数据库中的业务数据同步到其他节点，一个节点数据达到一定阈值继续往下一个节点传输，这样就形成了一个由多个独立的数据节点组成的集群，客户端的查询操作也是由各个数据节点完成，并且最后统一汇总。</p><h5 id="优劣势"><a href="#优劣势" class="headerlink" title="优劣势"></a>优劣势</h5><p>优势是数据迁移成本小，对于业务上使用的 RDBMS 数据库来说不需要进行大规模的数据迁移；对于小规模数据既可以满足业务需求又可以满足分析需求。</p><p>劣势是可扩展性有限，当集群达到一定规模的时候，集群中单个节点出现故障的概率就会越大，而且数据没有备份，若是单个节点宕机将会导致整个集群瘫痪；还有一个就是热点问题，数据是顺序性的存入一个个节点之中的，可能某个查询任务都会落在某一个节点上，而其他节点都在空闲状态，虽然有一个解决方案是数据加盐，打乱数据同步到各节点，但这样就会导致查询语句需要在多个节点执行，在没有元数据得知哪些数据被存入哪个节点的前提下，只能在所有节点执行查询语句并把结果汇总，效率会大大降低。</p><h4 id="大数据仓库"><a href="#大数据仓库" class="headerlink" title="大数据仓库"></a>大数据仓库</h4><p>大数据仓库实际上是依托于大数据技术而产生的一种数仓，底层的数据传输、存储和计算利用大数据技术，在上面加一层 SQL 支持，即可实现对数据的查询。在实现层面为了避免数据的移动而产生的网络和磁盘开销，一般是移动计算，将任务分发到数据所在的节点完成查询操作。常见产品有：Hive、Spark SQL、HBase、Impala、HAWQ 和 TIDB 等。</p><h5 id="实现原理-1"><a href="#实现原理-1" class="headerlink" title="实现原理"></a>实现原理</h5><p>这里就用HIVE来介绍，技术实现方法是将数据从数据源写入文件之中，并将文件按指定大小（一般是 128M ）分片，再将这些分片按照一定的规则分发到不同的节点当中，并且每个分片有至少三个备份文件分别存储在不同的节点当中，需要查询的时候将 SQL 语句转换成大数据计算引擎识别的程序，然后根据元数据去数据所在的节点执行操作完成数据的查询工作，最后再将结果汇总返回。</p><h5 id="优劣势-1"><a href="#优劣势-1" class="headerlink" title="优劣势"></a>优劣势</h5><p>优势是扩展性高，横向添加廉价的机器即可，计算的时候也会根据各个节点的资源情况选择较为空闲的机器执行操作；安全性高，多个副本的机制保障了数据的可靠性。</p><p>劣势是 SQL 支持率有待提升，这个问题在技术的进步下也在慢慢解决，例如 Hive 的 HQL 和 Spark SQL ；缺少事务支持，数据量庞大加上数据存储在不同的节点让支持事务变的比较困难，但对于面向分析的数仓来说支持事务显得并不是那么重要；在数据量未达到一定量级的时候效率反而不如单机或是 MMP 数据库，这是因为数据的转换、任务的调度、数据的汇总耗费了很多的时间和资源，在数据量不足的情况下这反而是一种劣势。</p>]]></content>
      
      
      <categories>
          
          <category> Data </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数据仓库 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>R语言环境空间理解</title>
      <link href="/2021/05/13/Language/R%E8%AF%AD%E8%A8%80%E7%8E%AF%E5%A2%83%E7%A9%BA%E9%97%B4%E7%90%86%E8%A7%A3/"/>
      <url>/2021/05/13/Language/R%E8%AF%AD%E8%A8%80%E7%8E%AF%E5%A2%83%E7%A9%BA%E9%97%B4%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p>环境空间 ( environment ) 对于刚接触 R 语言的我来说，是比较陌生的。虽然不了解它的运行原理，但也不影响我使用 R 语言。环境空间是 R 语言中关于计算机方面的底层设计，主要用于R语言是环境加载器。通过环境空间，封装了加载器的运行过程，让使用者在不知道底层细节的情况下，可以任意加载使用到的第三方的 R 语言程序包。</p><span id="more"></span><h3 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h3><p>在R语言中，不管是变量，对象，或者函数，都存在于 R 的环境空间中，R程序在运行时都自己的运行时空间。R 语言的环境 (environment) 是由内核定义的一个数据结构，由一系列的、有层次关系的框架 (frame) 组成，每个环境对应一个框架，用来区别不同的运行时空间 (scope) 。</p><p>环境空间有一些特征，比如 每个环境空间要有唯一的名字；环境空间是引入类型的，非赋值类型；环境空间都有父环境空间，空环境是最顶层的环境空间，没有父空间；子环境空间会继承父环境空间的变量等。</p><h3 id="环境的理解"><a href="#环境的理解" class="headerlink" title="环境的理解"></a>环境的理解</h3><p>关于环境的理解，先从新建一个环境开始：<code>new.env(hash = TRUE, parent = parent.frame(), size = 29L)</code></p><ul><li>hash 默认TRUE ，指使用 Hash Table 的结构</li><li>parent 默认当前环境，指定要创建环境的父环境</li><li>size 初始化环境大小</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 擦混关键环境env</span></span><br><span class="line">env <span class="operator">&lt;-</span> new.env<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line">env</span><br><span class="line"><span class="comment">## [1] &lt;environment: 0x3d7eef0&gt;</span></span><br><span class="line"><span class="built_in">class</span><span class="punctuation">(</span>env<span class="punctuation">)</span></span><br><span class="line"><span class="comment">## [1] &quot;environment&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义变量</span></span><br><span class="line">env<span class="operator">$</span>a <span class="operator">&lt;-</span> 123</span><br><span class="line">env<span class="operator">$</span>b <span class="operator">&lt;-</span> <span class="string">&#x27;qwe&#x27;</span></span><br><span class="line">ls<span class="punctuation">(</span>env<span class="punctuation">)</span></span><br><span class="line"><span class="comment">## [1] &quot;a&quot; &quot;b&quot;</span></span><br></pre></td></tr></table></figure><p>以上就是新建了一个父环境为当前环境的环境空间，在这个环境空间中可以定义自己的变量，并且可以使用<code>环境$变量</code>的方式访问环境内的变量。</p><h4 id="环境空间的层次结构"><a href="#环境空间的层次结构" class="headerlink" title="环境空间的层次结构"></a>环境空间的层次结构</h4><p>那么所谓创建环境时的父环境又是什么呢，要理解这个就得来理解环境空间的层次结构了。R 语言中有五种环境的定义：当前环境、内部环境、父环境、空环境和包环境。</p><ul><li>当前环境，即用户环境，用户运行程序时的环境（&lt;environment: R_GlobalEnv&gt;）</li><li>内部环境，构造出来的环境，使用<code>new.env()</code>构造出来的环境</li><li>父环境，即环境空间的上一层环境，R语言中除了空环境之外所有环境都有父环境</li><li>空环境，即顶层环境，没有父环境空间</li><li>包环境，包封装的环境空间</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 当前环境，或者使用globalenv</span></span><br><span class="line">environment<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment">## [1] &lt;environment: R_GlobalEnv&gt;</span></span><br><span class="line"><span class="comment"># 内部环境</span></span><br><span class="line">new.env<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment">## [1] &lt;environment: R_GlobalEnv&gt;</span></span><br><span class="line"><span class="comment"># 父环境</span></span><br><span class="line">parent.env<span class="punctuation">(</span>env<span class="punctuation">)</span></span><br><span class="line"><span class="comment">## [1] &lt;environment: R_GlobalEnv&gt;</span></span><br><span class="line"><span class="comment"># 空环境</span></span><br><span class="line"><span class="built_in">emptyenv</span><span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment">## [1] &lt;environment: R_EmptyEnv&gt;</span></span><br><span class="line"><span class="comment"># 包环境</span></span><br><span class="line"><span class="built_in">baseenv</span><span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment">## [1] &lt;environment: base&gt;</span></span><br></pre></td></tr></table></figure><p>既然环境空间是有层次的，那可以将从 env 到空环境的层次结构全部打印出来</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 递归打印环境空间，identical比较两者是否相同</span></span><br><span class="line">parent.call <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>env<span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line">  print<span class="punctuation">(</span>env<span class="punctuation">)</span></span><br><span class="line">  <span class="keyword">if</span><span class="punctuation">(</span><span class="built_in">is.environment</span><span class="punctuation">(</span>env<span class="punctuation">)</span> <span class="operator">&amp;</span> <span class="operator">!</span>identical<span class="punctuation">(</span>env<span class="punctuation">,</span> <span class="built_in">emptyenv</span><span class="punctuation">(</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="punctuation">&#123;</span> </span><br><span class="line">    parent.call<span class="punctuation">(</span>parent.env<span class="punctuation">(</span>env<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"></span><br><span class="line">parent.call<span class="punctuation">(</span>env<span class="punctuation">)</span></span><br><span class="line"><span class="comment">## &lt;environment: 0x366bf18&gt;</span></span><br><span class="line"><span class="comment">## &lt;environment: R_GlobalEnv&gt;</span></span><br><span class="line"><span class="comment">## &lt;environment: package:purrr&gt;</span></span><br><span class="line"><span class="comment">## attr(,&quot;name&quot;)</span></span><br><span class="line"><span class="comment">## [1] &quot;package:purrr&quot;</span></span><br><span class="line"><span class="comment">## attr(,&quot;path&quot;)</span></span><br><span class="line"><span class="comment">## [1] &quot;/home/dingtao/R/x86_64-pc-linux-gnu-library/3.3/purrr&quot;</span></span><br><span class="line"><span class="comment">## &lt;environment: package:stats&gt;</span></span><br><span class="line"><span class="comment">## attr(,&quot;name&quot;)</span></span><br><span class="line"><span class="comment">## [1] &quot;package:stats&quot;</span></span><br><span class="line"><span class="comment">## attr(,&quot;path&quot;)</span></span><br><span class="line"><span class="comment">## [1] &quot;/usr/lib/R/library/stats&quot;</span></span><br><span class="line"><span class="comment">## &lt;environment: package:graphics&gt;</span></span><br><span class="line"><span class="comment">## attr(,&quot;name&quot;)</span></span><br><span class="line"><span class="comment">## [1] &quot;package:graphics&quot;</span></span><br><span class="line"><span class="comment">## attr(,&quot;path&quot;)</span></span><br><span class="line"><span class="comment">## [1] &quot;/usr/lib/R/library/graphics&quot;</span></span><br><span class="line"><span class="comment">## &lt;environment: package:grDevices&gt;</span></span><br><span class="line"><span class="comment">## attr(,&quot;name&quot;)</span></span><br><span class="line"><span class="comment">## [1] &quot;package:grDevices&quot;</span></span><br><span class="line"><span class="comment">## attr(,&quot;path&quot;)</span></span><br><span class="line"><span class="comment">## [1] &quot;/usr/lib/R/library/grDevices&quot;</span></span><br><span class="line"><span class="comment">## &lt;environment: package:utils&gt;</span></span><br><span class="line"><span class="comment">## attr(,&quot;name&quot;)</span></span><br><span class="line"><span class="comment">## [1] &quot;package:utils&quot;</span></span><br><span class="line"><span class="comment">## attr(,&quot;path&quot;)</span></span><br><span class="line"><span class="comment">## [1] &quot;/usr/lib/R/library/utils&quot;</span></span><br><span class="line"><span class="comment">## &lt;environment: package:datasets&gt;</span></span><br><span class="line"><span class="comment">## attr(,&quot;name&quot;)</span></span><br><span class="line"><span class="comment">## [1] &quot;package:datasets&quot;</span></span><br><span class="line"><span class="comment">## attr(,&quot;path&quot;)</span></span><br><span class="line"><span class="comment">## [1] &quot;/usr/lib/R/library/datasets&quot;</span></span><br><span class="line"><span class="comment">## &lt;environment: package:methods&gt;</span></span><br><span class="line"><span class="comment">## attr(,&quot;name&quot;)</span></span><br><span class="line"><span class="comment">## [1] &quot;package:methods&quot;</span></span><br><span class="line"><span class="comment">## attr(,&quot;path&quot;)</span></span><br><span class="line"><span class="comment">## [1] &quot;/usr/lib/R/library/methods&quot;</span></span><br><span class="line"><span class="comment">## &lt;environment: 0x20cb5d0&gt;</span></span><br><span class="line"><span class="comment">## attr(,&quot;name&quot;)</span></span><br><span class="line"><span class="comment">## [1] &quot;Autoloads&quot;</span></span><br><span class="line"><span class="comment">## &lt;environment: base&gt;</span></span><br><span class="line"><span class="comment">## &lt;environment: R_EmptyEnv&gt;</span></span><br></pre></td></tr></table></figure><p>可以看到先是打印出来<code>&lt;environment: 0x366bf18&gt;</code>这是 env 的环境空间；接下来是作为 env 父空间的当前空间；再接下来是我导入的 purrr 包，其实这里还有很多，我给省略了，然后是六个基础包、Autoloads、base 包，最后是空环境。那么从R的导入顺序来看就是反过来的，先加载空环境，然后再一步步导入所需要的包，最后到达当前环境。</p><p>那么我们就可以访问到父环境以及爷爷环境等等中的变量，前提是未在当前环境中找到，它就会去父环境中寻找，一直找到空环境，若是还没有找到就会报错；但若是当前环境空间中已有相同名称的变量，那么你要么使用 rm 删除当前环境空间中的变量，要么指定环境变量访问变量，否则是访问不到指定环境空间中的变量的。</p><h3 id="环境的操作"><a href="#环境的操作" class="headerlink" title="环境的操作"></a>环境的操作</h3><p>主要是一些命令的使用</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 基础命令</span></span><br><span class="line">env1 <span class="operator">&lt;-</span> new.env<span class="punctuation">(</span><span class="punctuation">)</span>  <span class="comment"># 创建一个新环境</span></span><br><span class="line"><span class="built_in">is.environment</span><span class="punctuation">(</span>env1<span class="punctuation">)</span>  <span class="comment"># 判断是否是一个环境空间</span></span><br><span class="line">environment<span class="punctuation">(</span><span class="punctuation">)</span>  <span class="comment"># 查看当前环境空间</span></span><br><span class="line">environment<span class="punctuation">(</span>ls<span class="punctuation">)</span>  <span class="comment"># 查看函数中的环境空间</span></span><br><span class="line">environmentName<span class="punctuation">(</span><span class="built_in">baseenv</span><span class="punctuation">(</span><span class="punctuation">)</span><span class="punctuation">)</span>  <span class="comment"># 查看环境空间的名字</span></span><br><span class="line">environmentName<span class="punctuation">(</span>environment<span class="punctuation">(</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="built_in">attr</span><span class="punctuation">(</span>env1<span class="punctuation">,</span> <span class="string">&#x27;name&#x27;</span><span class="punctuation">)</span> <span class="operator">&lt;-</span> <span class="string">&#x27;env1&#x27;</span>  <span class="comment"># 设置环境空间的名字，默认创建出来是没有名字的</span></span><br><span class="line">env.profile<span class="punctuation">(</span>env1<span class="punctuation">)</span>  <span class="comment"># 查看环境空间的属性值</span></span><br><span class="line">rm<span class="punctuation">(</span><span class="built_in">list</span><span class="operator">=</span>ls<span class="punctuation">(</span><span class="punctuation">)</span><span class="punctuation">)</span>  <span class="comment"># 清空当前环境空间定义的所有对象</span></span><br><span class="line">rm<span class="punctuation">(</span>x<span class="punctuation">,</span> envir<span class="operator">=</span>env1<span class="punctuation">)</span>  <span class="comment"># 删除env1中的变量</span></span><br><span class="line">ls<span class="punctuation">(</span>env1<span class="punctuation">)</span>  <span class="comment"># 查看env1环境空间中的变量</span></span><br><span class="line">x <span class="operator">&lt;-</span> 1.5; y <span class="operator">&lt;-</span> 2<span class="operator">:</span><span class="number">10</span>  <span class="comment"># 定义环境空间中的变量</span></span><br><span class="line">env1 <span class="operator">&lt;-</span> new.env<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line">env1<span class="operator">$</span>x <span class="operator">&lt;-</span> <span class="operator">-</span><span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 环境空间变量取值</span></span><br><span class="line">get<span class="punctuation">(</span><span class="string">&#x27;x&#x27;</span><span class="punctuation">,</span> envir<span class="operator">=</span>env1<span class="punctuation">)</span>  <span class="comment"># 取env1环境空间中的x值</span></span><br><span class="line">get<span class="punctuation">(</span><span class="string">&#x27;y&#x27;</span><span class="punctuation">,</span> envir<span class="operator">=</span>env1<span class="punctuation">)</span>  <span class="comment"># 从env1环境空间中取从当前环境中继承的y值</span></span><br><span class="line"><span class="comment"># get(&#x27;y&#x27;, envir=env1, inherits=FALSE)  # 禁止环境空间的继承，会报错</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新赋值</span></span><br><span class="line">assign<span class="punctuation">(</span><span class="string">&#x27;x&#x27;</span><span class="punctuation">,</span> <span class="number">77</span><span class="punctuation">)</span>; x  <span class="comment"># 重新赋值当前环境空间中的x值</span></span><br><span class="line">assign<span class="punctuation">(</span><span class="string">&#x27;x&#x27;</span><span class="punctuation">,</span> <span class="number">99</span><span class="punctuation">,</span> envir<span class="operator">=</span>env1<span class="punctuation">)</span>; env1<span class="operator">$</span>x  <span class="comment"># 重新赋值环境空间中的x值</span></span><br><span class="line">assign<span class="punctuation">(</span><span class="string">&#x27;y&#x27;</span><span class="punctuation">,</span> <span class="number">333</span><span class="punctuation">,</span> envir<span class="operator">=</span>env1<span class="punctuation">,</span> inherits<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">)</span>  <span class="comment"># 在没有继承的情况下给环境空间中的y赋值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 变量在环境空间中是否存在</span></span><br><span class="line">exists<span class="punctuation">(</span><span class="string">&#x27;x&#x27;</span><span class="punctuation">)</span></span><br><span class="line">exists<span class="punctuation">(</span><span class="string">&#x27;x&#x27;</span><span class="punctuation">,</span> envir<span class="operator">=</span>env1<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># exists(&#x27;y&#x27;, envir=env1, isherits=FALSE)  # 在没有继承的情况下y是否存在，会报错</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看函数的环境空间，来自pryr包</span></span><br><span class="line">where<span class="punctuation">(</span><span class="string">&#x27;mean&#x27;</span><span class="punctuation">)</span></span><br><span class="line">where<span class="punctuation">(</span><span class="string">&#x27;where&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h3 id="函数的环境空间"><a href="#函数的环境空间" class="headerlink" title="函数的环境空间"></a>函数的环境空间</h3><p>在 R 语言中，变量、对象、函数都存在于环境空间中，而函数又可以有自己的环境空间，我们可以在函数内再定义变量、对象和函数，循环往复就形成了我们现在用的R语言环境系统。</p><p>一般情况，我们可以通过<code>new.env()</code>去创建一个环境空间，但更多的时候，我们使用的是函数环境空间。函数环境空间包括4方面的内容：</p><ul><li>封闭环境，每个函数都有一个且只有一个封闭环境空间，指向函数定义的环境空间</li><li>绑定环境，给函数指定一个名字，绑定到函数变量，<code>如 fun1&lt;-function()&#123;1&#125;</code></li><li>运行环境，当函数运行时，在内存中动态产生的环境空间，运行结束后，会自动销毁</li><li>调用环境，是指在哪个环境中进行的方法调用，如<code>fun1&lt;-function()&#123;fun2()&#125;</code>，函数fun2在函数fun1被调用</li></ul><h4 id="参数寻址补充"><a href="#参数寻址补充" class="headerlink" title="参数寻址补充"></a>参数寻址补充</h4><p>函数中的参数是惰性求值，不执行则不运算，只有在执行或者被调用的时候才会进行运算；函数体执行运算用到变量时若函数体中未定义，参数环境中也未定义，则会向全局环境中寻址；函数在被调用时若有缺省值，则局部参数被调用时会优先使用传入的参数，若未传入参数才会使用缺省值。</p>]]></content>
      
      
      <categories>
          
          <category> Language </category>
          
      </categories>
      
      
        <tags>
            
            <tag> R </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>R语言函数式编程理解</title>
      <link href="/2021/05/11/Language/R%E8%AF%AD%E8%A8%80%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%E7%90%86%E8%A7%A3/"/>
      <url>/2021/05/11/Language/R%E8%AF%AD%E8%A8%80%E5%87%BD%E6%95%B0%E5%BC%8F%E7%BC%96%E7%A8%8B%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p>介绍 R 语言的函数式编程，主要包含三个模块：匿名函数，闭包（由函数编写的函数）和函数列表。</p><span id="more"></span><p>R 语言结合了面向对象编程语言和函数式编程语言的特性，由于拥有函数式编程的特性，R 的每一个运算符，实际上也是函数，同样，面向对象的特性决定了你接触到的 R 中所有东西(从数字到字符串到矩阵等)都是对象。这些综合的特质决定了 R 这门语言的特殊性，最大的特点就是开源。</p><p>之前简单了解了 R 语言的 S3 对象以及泛型函数，下面介绍 R 语言的函数式编程，主要包含三个模块：匿名函数，闭包（由函数编写的函数）和函数列表。其主要目的还是为了减少代码量或提高效率。</p><h3 id="匿名函数"><a href="#匿名函数" class="headerlink" title="匿名函数"></a>匿名函数</h3><p>在 R 语言中，函数本身就是一个对象，使用&lt;-符号只是将这个对象绑定到一个符号上以供重复使用，但若是只使用一次或者是传入参数类型必须是函数类型时，匿名函数就可以派上用场了。R 函数不会自动绑定到名称，与许多语言（例如 C ， C ++，Python 和 Ruby ）不同，R 没有用于创建命名函数的特殊语法：创建函数时，可以使用常规赋值运算符为其命名。不命名的话你得到的就是一个匿名函数。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 比如这些场景</span></span><br><span class="line">lapply<span class="punctuation">(</span>mtcars<span class="punctuation">,</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> <span class="built_in">length</span><span class="punctuation">(</span>unique<span class="punctuation">(</span>x<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">Filter<span class="punctuation">(</span><span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> <span class="operator">!</span><span class="built_in">is.numeric</span><span class="punctuation">(</span>x<span class="punctuation">)</span><span class="punctuation">,</span> mtcars<span class="punctuation">)</span></span><br><span class="line">integrate<span class="punctuation">(</span><span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> <span class="built_in">sin</span><span class="punctuation">(</span>x<span class="punctuation">)</span> <span class="operator">^</span> <span class="number">2</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">,</span> <span class="built_in">pi</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 与R中的所有函数一样，匿名函数具有formals()，abody()和父级environment()</span></span><br><span class="line">formals<span class="punctuation">(</span><span class="keyword">function</span><span class="punctuation">(</span>x <span class="operator">=</span> <span class="number">4</span><span class="punctuation">)</span> g<span class="punctuation">(</span>x<span class="punctuation">)</span> <span class="operator">+</span> h<span class="punctuation">(</span>x<span class="punctuation">)</span><span class="punctuation">)</span>  <span class="comment"># 函数形参</span></span><br><span class="line">`## $x</span><br><span class="line">## [1] 4`</span><br><span class="line">body<span class="punctuation">(</span><span class="keyword">function</span><span class="punctuation">(</span>x <span class="operator">=</span> <span class="number">4</span><span class="punctuation">)</span> g<span class="punctuation">(</span>x<span class="punctuation">)</span> <span class="operator">+</span> h<span class="punctuation">(</span>x<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">`## [1] g(x) + h(x)`</span><br><span class="line">environment<span class="punctuation">(</span><span class="keyword">function</span><span class="punctuation">(</span>x <span class="operator">=</span> <span class="number">4</span><span class="punctuation">)</span> g<span class="punctuation">(</span>x<span class="punctuation">)</span> <span class="operator">+</span> h<span class="punctuation">(</span>x<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">`## [1] &lt;environment: R_GlobalEnv&gt;`</span><br></pre></td></tr></table></figure><p>在 map 或者 lapply 这种可以并行处理或者批处理函数中的 f 参数中使用匿名函数将会特别有用，当然，匿名函数的最大功能还是创建闭包。</p><h3 id="闭包"><a href="#闭包" class="headerlink" title="闭包"></a>闭包</h3><p>“对象是具有功能的数据。闭包是具有数据的功能。” —约翰·库克（John D. Cook） 闭包之所以得名，是因为它们封闭了父函数的环境并且可以访问其所有变量。这非常有用，因为它允许我们拥有两个级别的参数：控制操作的父级别和完成工作的子级别。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 一个小例子</span></span><br><span class="line">func <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line">  <span class="keyword">function</span><span class="punctuation">(</span>y<span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line">    paste<span class="punctuation">(</span>x<span class="punctuation">,</span> y<span class="punctuation">)</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">func1 <span class="operator">&lt;-</span> func<span class="punctuation">(</span><span class="string">&#x27;吃饭&#x27;</span><span class="punctuation">)</span></span><br><span class="line">func2 <span class="operator">&lt;-</span> func<span class="punctuation">(</span><span class="string">&#x27;睡觉&#x27;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用匿名函数</span></span><br><span class="line">func1<span class="punctuation">(</span><span class="string">&#x27;小明&#x27;</span><span class="punctuation">)</span></span><br><span class="line">`## [1] 小明 吃饭`</span><br><span class="line">func2<span class="punctuation">(</span><span class="string">&#x27;小明&#x27;</span><span class="punctuation">)</span></span><br><span class="line">`## [1] 小明 睡觉`</span><br></pre></td></tr></table></figure><p>以上就是闭包的一般应用场景了，我们可以在父函数中定义一系列操作，创建闭包后子函数可以访问到父函数的所有变量，再执行子函数的操作。那么闭包究竟是怎样工作的呢？</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先我们打印闭包，得到的只有子函数的函数体</span></span><br><span class="line">func1</span><br><span class="line"><span class="comment">## [1] function(y) &#123;</span></span><br><span class="line">                paste<span class="punctuation">(</span>x<span class="punctuation">,</span> y<span class="punctuation">)</span></span><br><span class="line">            <span class="punctuation">&#125;</span></span><br><span class="line">        <span class="operator">&lt;</span>environment<span class="operator">:</span> <span class="number">0xe3f6028</span><span class="operator">&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 闭包的父环境是创建它的函数的执行环境</span></span><br><span class="line">func <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span><span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line">  print<span class="punctuation">(</span>environment<span class="punctuation">(</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">  <span class="keyword">function</span><span class="punctuation">(</span><span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line">    <span class="number">0</span></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">func1 <span class="operator">&lt;-</span> func<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment">## [1] &lt;environment: 0xce6b3e8&gt;</span></span><br><span class="line">environment<span class="punctuation">(</span>func1<span class="punctuation">)</span></span><br><span class="line"><span class="comment">## [1] &lt;environment: 0xce6b3e8&gt;</span></span><br></pre></td></tr></table></figure><p>以上可以看出来闭包函数实际上还是那个子函数，只不过它所在的是创建它的父函数的执行环节。函数返回值后，执行环境通常会消失。但是，闭包捕获了父函数的执行环境。这意味着当函数 func 返回函数 func1 时，函数 func1 捕获并存储函数 func 的执行环境，并且它不会消失。</p><p>下面介绍函数父环境和子环境（创建环境和执行环境）在函数调用之时保持联系的状态。在不同环境级别上管理变量的关键是双箭头分配运算符（<code>&lt;&lt;-</code>）。与通常<code>&lt;-</code>在当前环境中分配对象的单箭头不同，双箭头运算符将继续查找父级环境链，直到找到匹配的名称。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 首先是在闭包中应用双箭头分配运算符</span></span><br><span class="line">func <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span><span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line">    i <span class="operator">&lt;-</span> 0</span><br><span class="line">    <span class="keyword">function</span><span class="punctuation">(</span><span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line">        i <span class="operator">&lt;&lt;-</span> i <span class="operator">+</span> <span class="number">1</span></span><br><span class="line">        i</span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"></span><br><span class="line">func1 <span class="operator">&lt;-</span> func<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line">func1<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment">## [1] 1</span></span><br><span class="line">func1<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment">## [1] 2</span></span><br><span class="line">func1<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment">## [1] 3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 再创建一个闭包</span></span><br><span class="line">func2 <span class="operator">&lt;-</span> func<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line">func2<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment">## [1] 1</span></span><br><span class="line">func2<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment">## [1] 2</span></span><br><span class="line">func2<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment">## [1] 3</span></span><br></pre></td></tr></table></figure><p>func1 和 func2 函数通过不修改其本地环境中的变量来规避“改变同一个变量”限制。由于更改是在不变的父（或封闭）环境中进行的，因此可以在函数调用之间保留更改。那么 func1 和 func2 之间为什么改变各自的封闭环境变量不受影响呢，因为func1 和 func2 在创建闭包的时候使用的已经不是同一个 func 函数了，而是复制出来一个父环境以供当前闭包使用，如需测试在 func 函数中加上打印 environment 就可以了。</p><p>还有一个例子比较直观，以下的两个函数，分别调用 func1 创建闭包 func2 哪个会对本地环境的i做出改变呢？答案是 func1 ，因为 func2 中闭包的父环境是 func2 的运行环境，一旦在父环境中找到了 i 变量就不会再去父环境的父环境中查找了，若是删掉 func2 中的 i &lt;- 0 这行代码则会改变本地环境中的i。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">i <span class="operator">&lt;-</span> 0</span><br><span class="line">func1 <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span><span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line">  i <span class="operator">&lt;&lt;-</span> i <span class="operator">+</span> <span class="number">1</span></span><br><span class="line">  i</span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">func2 <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span><span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line">  i <span class="operator">&lt;-</span> 0</span><br><span class="line">  <span class="keyword">function</span><span class="punctuation">(</span><span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line">    i <span class="operator">&lt;&lt;-</span> i <span class="operator">+</span> <span class="number">1</span></span><br><span class="line">    i</span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="函数列表"><a href="#函数列表" class="headerlink" title="函数列表"></a>函数列表</h3><p>关于函数列表理解起来就比较简单了，也有适用的环境。在 R 中，函数可以存储在列表中。这使得与相关函数组一起使用更容易，就像数据框使与相关向量组一起使用一样。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设要计算一组数据的聚合数据</span></span><br><span class="line">arg <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">,</span><span class="number">3</span><span class="punctuation">,</span><span class="number">4</span><span class="punctuation">,</span><span class="number">5</span><span class="punctuation">)</span></span><br><span class="line">mean<span class="punctuation">(</span>arg<span class="punctuation">)</span></span><br><span class="line"><span class="comment">## [1] 3</span></span><br><span class="line"><span class="built_in">max</span><span class="punctuation">(</span>arg<span class="punctuation">)</span></span><br><span class="line"><span class="comment">## [1] 5</span></span><br><span class="line"><span class="built_in">min</span><span class="punctuation">(</span>arg<span class="punctuation">)</span></span><br><span class="line"><span class="comment">## [1] 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果适用函数列表的方法</span></span><br><span class="line">func <span class="operator">&lt;-</span> <span class="built_in">list</span><span class="punctuation">(</span></span><br><span class="line">  me <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> mean<span class="punctuation">(</span>x<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">  ma <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> <span class="built_in">max</span><span class="punctuation">(</span>x<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">  mi <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> <span class="built_in">min</span><span class="punctuation">(</span>x<span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">)</span></span><br><span class="line">lapply<span class="punctuation">(</span>func<span class="punctuation">,</span> <span class="keyword">function</span><span class="punctuation">(</span>f<span class="punctuation">)</span> f<span class="punctuation">(</span>arg<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">map<span class="punctuation">(</span>func<span class="punctuation">,</span> <span class="keyword">function</span><span class="punctuation">(</span>f<span class="punctuation">)</span> f<span class="punctuation">(</span>arg<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment">## [[1]]</span></span><br><span class="line"><span class="comment">## [1] 3</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## [[2]]</span></span><br><span class="line"><span class="comment">## [1] 5</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## [[3]]</span></span><br><span class="line"><span class="comment">## [1] 1</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Language </category>
          
      </categories>
      
      
        <tags>
            
            <tag> R </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>R语言基于S3的面向对象编程</title>
      <link href="/2021/05/07/Language/R%E8%AF%AD%E8%A8%80%E5%9F%BA%E4%BA%8ES3%E7%9A%84%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B/"/>
      <url>/2021/05/07/Language/R%E8%AF%AD%E8%A8%80%E5%9F%BA%E4%BA%8ES3%E7%9A%84%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%BC%96%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<p>对于R语言的面向对象编程，不同于其他编程语言，R语言提供了3种底层对象类型，一种是S3类型，一种是S4类型，还有一种是RC类型。<br>S3对象简单，具有动态性，结构化特征不明显，S4对象结构化，功能强大。RC对象是R2.12版本后使用的新类型，从底层上改变了原有S3和S4对象系统的设计，去掉了泛型函数，真正地以类为基础实现面向对象的特征。</p><span id="more"></span><h3 id="S3对象的介绍"><a href="#S3对象的介绍" class="headerlink" title="S3对象的介绍"></a><strong>S3对象的介绍</strong></h3><p>在R语言中，基于S3对象的面向对象编程，是一种基于泛型函数的实现方式。泛型函数是一种特殊的函数，根据传入对象的类型决定调用那个具体的方法。基于S3对象实现面向对象编程，不同其他语言的面型对象编程，是一种动态函数调用的模拟实现。S3对象被广泛应用于R的早期的开发包中。</p><h3 id="创建S3对象"><a href="#创建S3对象" class="headerlink" title="创建S3对象"></a>创建S3对象</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过变量创建S3对象</span></span><br><span class="line">x <span class="operator">&lt;-</span> 1</span><br><span class="line"><span class="built_in">attr</span><span class="punctuation">(</span>x<span class="punctuation">,</span> <span class="string">&#x27;class&#x27;</span><span class="punctuation">)</span> <span class="operator">&lt;-</span> <span class="string">&#x27;foo&#x27;</span></span><br><span class="line"><span class="built_in">attr</span><span class="punctuation">(</span>x<span class="punctuation">,</span> <span class="string">&#x27;class&#x27;</span><span class="punctuation">)</span>  </span><br><span class="line">`## [1] &quot;foo&quot;`</span><br><span class="line"></span><br><span class="line"><span class="comment"># 通过structure函数创建</span></span><br><span class="line">x <span class="operator">&lt;-</span> structure<span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span> <span class="built_in">class</span><span class="operator">=</span><span class="string">&#x27;foo&#x27;</span><span class="punctuation">)</span></span><br><span class="line"><span class="built_in">class</span><span class="punctuation">(</span>x<span class="punctuation">)</span></span><br><span class="line">`## [1] &quot;foo&quot;`</span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建一个多类型的S3对象，S3对象的class属性可以是一个向量，包括多种类型</span></span><br><span class="line">x <span class="operator">&lt;-</span> structure<span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span> <span class="built_in">class</span><span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;foo&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;bar&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="built_in">class</span><span class="punctuation">(</span>x<span class="punctuation">)</span></span><br><span class="line">`## [1] &quot;foo&quot; &quot;bar&quot;`</span><br></pre></td></tr></table></figure><h3 id="泛型函数和方法调用"><a href="#泛型函数和方法调用" class="headerlink" title="泛型函数和方法调用"></a>泛型函数和方法调用</h3><p>对于S3对象的使用，通常用UseMethod()函数来定义一个泛型函数的名称，通过传入参数的class属性，来确定方法调用。</p><p>定义一个teacher泛型函数</p><ul><li>用UseMethod()定义teacher泛型函数</li><li>用teacher.xxx的语法格式定义teacher对象的行为</li><li>其中teacher.default是默认行为</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 用UseMethod()定义teacher泛型函数</span></span><br><span class="line">teacher <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>…<span class="punctuation">)</span> <span class="built_in">UseMethod</span><span class="punctuation">(</span><span class="string">&quot;teacher&quot;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义teacher内部函数</span></span><br><span class="line">teacher.lecture <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>…<span class="punctuation">)</span> print<span class="punctuation">(</span><span class="string">&quot;讲课&quot;</span><span class="punctuation">)</span></span><br><span class="line">teacher.assignment <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>…<span class="punctuation">)</span> print<span class="punctuation">(</span><span class="string">&quot;布置作业&quot;</span><span class="punctuation">)</span></span><br><span class="line">teacher.correcting <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>…<span class="punctuation">)</span> print<span class="punctuation">(</span><span class="string">&quot;批改作业&quot;</span><span class="punctuation">)</span></span><br><span class="line">teacher.default <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>…<span class="punctuation">)</span> print<span class="punctuation">(</span><span class="string">&quot;你不是teacher&quot;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>方法调用通过传入参数的class属性，来确定不同方法调用</p><ul><li>定义一个变量a，并设置a的class属性为lecture</li><li>把变量a传入到teacher泛型函数中</li><li>函数teacher.lecture()函数的行为被调用</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义变量并设置属性</span></span><br><span class="line">a <span class="operator">&lt;-</span> structure<span class="punctuation">(</span><span class="string">&#x27;qwe&#x27;</span><span class="punctuation">,</span> <span class="built_in">class</span><span class="operator">=</span><span class="string">&#x27;lecture&#x27;</span><span class="punctuation">)</span></span><br><span class="line">b <span class="operator">&lt;-</span> structure<span class="punctuation">(</span><span class="string">&#x27;qwe&#x27;</span><span class="punctuation">,</span> <span class="built_in">class</span><span class="operator">=</span><span class="string">&#x27;qwe&#x27;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用teacher泛型函数并返回对应类型的函数</span></span><br><span class="line">teacher<span class="punctuation">(</span>a<span class="punctuation">)</span></span><br><span class="line">`## [1] &quot;讲课&quot;`</span><br><span class="line">teacher<span class="punctuation">(</span>b<span class="punctuation">)</span></span><br><span class="line">`## [1] &quot;你不是teacher&quot;`</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用methods查看S3对象的函数</span></span><br><span class="line">methods<span class="punctuation">(</span>teacher<span class="punctuation">)</span></span><br><span class="line">`## [1] teacher.assignment teacher.correcting teacher.default teacher.lecture`</span><br></pre></td></tr></table></figure><h3 id="S3对象的继承关系"><a href="#S3对象的继承关系" class="headerlink" title="S3对象的继承关系"></a>S3对象的继承关系</h3><p>S3独享有一种非常简单的继承方式，用NextMethod()函数来实现。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个xx泛型函数</span></span><br><span class="line">xx <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> <span class="built_in">UseMethod</span><span class="punctuation">(</span><span class="string">&#x27;xx&#x27;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义xx内部函数</span></span><br><span class="line">xx.default <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> <span class="built_in">return</span><span class="punctuation">(</span><span class="string">&#x27;default&#x27;</span><span class="punctuation">)</span></span><br><span class="line">xx.son <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> <span class="built_in">return</span><span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;son&#x27;</span><span class="punctuation">,</span> NextMethod<span class="punctuation">(</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">xx.father <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> <span class="built_in">return</span><span class="punctuation">(</span><span class="string">&#x27;father&#x27;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义变量并设置son和father属性</span></span><br><span class="line">y <span class="operator">&lt;-</span> structure<span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span> <span class="built_in">class</span><span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;son&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;father&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">xx<span class="punctuation">(</span>y<span class="punctuation">)</span></span><br><span class="line">`## [1] &quot;son&quot; &quot;father&quot;`</span><br></pre></td></tr></table></figure><p>通过对xx()函数传入y的参数，xx.son()先被执行，然后通过NextMethod()函数继续执行了xx.father()函数。这样其实就模拟了子函数调用父函数的过程，实现了面向对象编程中的继承。</p><h3 id="S3对象的缺点"><a href="#S3对象的缺点" class="headerlink" title="S3对象的缺点"></a>S3对象的缺点</h3><p>从上面S3对象的介绍上来看，S3对象并不是完全的面向对象实现，而是一种通过泛型函数模拟的面向对象的实现。</p><ul><li>S3用起来简单，但在实际的面向对象编程的过程中，当对象关系有一定的复杂度，S3对象所表达的意义就变得不太清楚</li><li>S3封装的内部函数，可以绕过泛型函数的检查，以直接被调用（直接调用xx.father）</li><li>S3参数的class属性，可以被任意设置，没有预处理的检查（若没有对应属性的函数返回default函数）</li><li>S3参数，只能通过调用class属性进行函数调用，其他属性则不会被class()函数执行</li><li>S3参数的class属性有多个值时，调用时会被按照程序赋值顺序来调用第一个合法的函数</li></ul><p>所以，S3只是R语言面向对象的一种简单的实现。</p><p>写于2021年5月7日。</p>]]></content>
      
      
      <categories>
          
          <category> Language </category>
          
      </categories>
      
      
        <tags>
            
            <tag> R </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>HDFS架构及基础命令</title>
      <link href="/2021/05/04/Software/HDFS%E6%9E%B6%E6%9E%84%E5%8F%8A%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4/"/>
      <url>/2021/05/04/Software/HDFS%E6%9E%B6%E6%9E%84%E5%8F%8A%E5%9F%BA%E7%A1%80%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<p>HDFS (Hadoop Distribluted File System)是Apache Hadoop项目的一个子项目。 Hadoop非常适于存储大型数据(比如TB和PB)，其就是使用HDFS作为存储系统HDFS使用多台计算机存储文件，并且提供统一的访问接口，像是访问一个普通文件系统一样使用分布式文件系统。</p><span id="more"></span><h3 id="HDFS的应用场景"><a href="#HDFS的应用场景" class="headerlink" title="HDFS的应用场景"></a>HDFS的应用场景</h3><h4 id="适合的应用场景"><a href="#适合的应用场景" class="headerlink" title="适合的应用场景"></a>适合的应用场景</h4><ul><li>适合的应用场景存储非常大的文件：这里非常大指的是几百M、G、或者TB级别，需要高吞吐量，对延时没有要求</li><li>采用流式的数据访问方式：即一次写入、多次读取，数据集经常从数据源生成或者拷贝一次,然后在其上做很多分析工作</li><li>运行于商业硬件上：Hadoop不需要特别贵的机器，可运行于普通廉价机器，节约成本</li><li>需要高容错性</li><li>为数据存储提供所需的扩展能力</li></ul><h4 id="不适合的应用场景"><a href="#不适合的应用场景" class="headerlink" title="不适合的应用场景"></a>不适合的应用场景</h4><ul><li>低延时的数据访问对延时要求在毫秒级别的应用，不适合采用HDFS。HDFS是为高吞吐数据传输设计的，因此可能牺牲延时</li><li>大量小文件文件的元数据保存在NameNode的内存中，整个文件系统的文件数量会受限于NameNode的内存大小。经验而言，一个文件&#x2F;目录&#x2F;文件块一般占有150字节的元数据内存空间。如果有100万个文件,每个文件占用1个文件块，则需要大约300M的内存。因此十亿级别的文件数量在现有商用机器上难以支持。</li><li>多方读写，需要任意的文件修改HDFS采用追加(append-only)的方式写入数据、不支持文件任意offset的修改。不支持多个写入器(writer)</li></ul><h3 id="HDFS的架构"><a href="#HDFS的架构" class="headerlink" title="HDFS的架构"></a>HDFS的架构</h3><p>HDFS是一个主从（master&#x2F;slave）体系结构</p><p>HDFS主要由四部分构成，分别是：HDFS Client、Name node、Secondary Name node和Data node</p><p><img src="https://wangxukun.top/wp-content/uploads/2021/05/hdfs-Architecture.jpg"></p><h4 id="Client"><a href="#Client" class="headerlink" title="Client"></a>Client</h4><ul><li>文件切分。文件上传HDFS的时候，Client将文件切分成一个一个的Block，然后进行存储</li><li>与NameNode交互，获取文件的位置信息</li><li>与DataNode交互，读取或者写入数据</li><li>Client提供一些命令来管理和访问HDFS，比如启动或者关闭HDFS，读写文件等</li></ul><h4 id="NameNode"><a href="#NameNode" class="headerlink" title="NameNode"></a>NameNode</h4><ul><li>管理HDFS的名称空间</li><li>管理数据块(Block)映射信息</li><li>配置副本策略</li><li>处理客户端读写请求</li></ul><h4 id="DataNode"><a href="#DataNode" class="headerlink" title="DataNode"></a>DataNode</h4><ul><li>存储实际的数据块</li><li>执行数据块的读&#x2F;写操作</li></ul><h4 id="Secondary-NameNode"><a href="#Secondary-NameNode" class="headerlink" title="Secondary NameNode"></a>Secondary NameNode</h4><ul><li>并非NameNode的热备。当NameNode挂掉的时候，它并不能马上替换NameNode并提供服务</li><li>辅助NameNode，分担其工作量</li><li>定期合并fsimage和fsedits，并推送给NameNode</li><li>在紧急情况下，可辅助恢复NameNode</li></ul><h4 id="NameNode和DataNode"><a href="#NameNode和DataNode" class="headerlink" title="NameNode和DataNode"></a>NameNode和DataNode</h4><p><strong>NameNode和DataNode</strong>是HDFS中最重要的两个组件，对于各自的属性以及作用有以下理解</p><p><img src="https://wangxukun.top/wp-content/uploads/2021/05/hdfs-NameNode-DataNode.jpg"></p><h4 id="NameNode作用"><a href="#NameNode作用" class="headerlink" title="NameNode作用"></a>NameNode作用</h4><ul><li>NameNode在内存中保存着整个文件系统的名称空间和文件数据块的地址映射</li><li>整个HDFS可存储的文件数受限于NameNode的内存大小</li><li>NameNode元数据信息：文件名，文件目录结构，文件属性(生成时间，副本数，权限)，每个文件的块列表。以及列表中的块与块所在的DataNode之间的地址映射关系。在内存中加载文件系统中每个文件和每个数据块的引用关系(文件、block，datanode之间的映射信息)，数据会定期保存到本地磁盘(fslmage文件和edits文件)</li><li>NameNode文件操作：NameNode负责文件元数据的操作DataNode负责处理文件内容的读写请求，数据流不经过NameNode，会询问它跟那个DataNode联系</li><li>NameNode副本：文件数据块到底存放到哪些DataNode上，是由NameNode决定的，NameNode根据全局情况做出放置副本的决定</li><li>NameNode心跳机制：全权管理数据块的复制，周期性的接受心跳和块的状态报告信息(包含该DataNode上所有数据块的列表)若接受到心跳信息，NameNode认为DataNode工作正常，如果在10分钟后还接受到不到DN的心跳，那么NameNode认为DataNode已经宕机，这时候NN准备要把DN上的数据块进行重新复制。块的状态报告包含了一个DN上所有数据块的列表，blocks report每个1小时发送一次</li></ul><h4 id="DataNode作用"><a href="#DataNode作用" class="headerlink" title="DataNode作用"></a>DataNode作用</h4><ul><li>Data Node以数据块的形式奇储HDFS文件</li><li>Data Node响应HDFS客户端读写请求</li><li>Data Node周期性的向NameNode汇报心跳信息</li><li>Data Node周期性的向NameNode汇报数据块信息</li><li>Data Node周期性的向NameNode汇报缓存数据块信息（针对访问频率较高的数据块将它放在内存中，叫做块缓存）</li></ul><h3 id="HDFS基础命令"><a href="#HDFS基础命令" class="headerlink" title="HDFS基础命令"></a>HDFS基础命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh  <span class="comment"># 启动HDFS集群</span></span><br><span class="line">stop-dfs.sh  <span class="comment"># 关闭HDFS集群</span></span><br><span class="line"></span><br><span class="line">hdfs dfsadmin -report  <span class="comment"># 报告HDFS使用情况</span></span><br><span class="line">hdfs dfs -count -q -h -v /  <span class="comment"># 统计目录文件夹数量、文件数量、空间使用量</span></span><br><span class="line"></span><br><span class="line">hdfs dfs -<span class="built_in">df</span> -h /  <span class="comment"># 统计文件系统的可用空间信息</span></span><br><span class="line">hdfs dfs -<span class="built_in">ls</span> /  <span class="comment"># 查看/路径下的文件或者文件夹</span></span><br><span class="line">hdfs dfs -lsr /  <span class="comment"># 递归查看/路径下的文件或者文件夹</span></span><br><span class="line">hdfs dfs -<span class="built_in">stat</span> /a.txt  <span class="comment"># 显示文件所占块数，文件名，块大小，复制数，修改时间等信息</span></span><br><span class="line">hdfs dfs -<span class="built_in">mkdir</span> -p /a/b/c  <span class="comment"># 递归创建文件夹</span></span><br><span class="line">hdfs dfs -moveFronLocal localpath hdfspath  <span class="comment"># 从本地上传文件到hdfs中（本地文件会被删除）</span></span><br><span class="line">hdfs dfs -<span class="built_in">mv</span> oldpath newpath  <span class="comment"># 移动文件</span></span><br><span class="line">hdfs dfs -put localpath hdfspath  <span class="comment"># 从本地上传文件到hdfs中（文件还在本地）</span></span><br><span class="line">hdfs dfs -appendToFile a.txt b.txt /c.txt  <span class="comment"># 追加本地文件a、b到hdfs文件c中</span></span><br><span class="line">hdfs dfs -touchz /empty.txt  <span class="comment"># 创建一个空文件</span></span><br><span class="line">hdfs dfs -<span class="built_in">cat</span> /c.txt  <span class="comment"># 打印文件到shell</span></span><br><span class="line">hdfs dfs -<span class="built_in">cp</span> oldpath newpath  <span class="comment"># 拷贝文件</span></span><br><span class="line">hdfs dfs -<span class="built_in">rm</span> path  <span class="comment"># 删除文件夹或者文件（如需递归加参数r）</span></span><br><span class="line">hdfs dfs -<span class="built_in">chmod</span> -R 777 filepath  <span class="comment"># 更改文件访问权限</span></span><br><span class="line">hdfs dfs -<span class="built_in">chown</span> -R elk:elk filepath  <span class="comment"># 更改文件用户组和用户</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume深入理解</title>
      <link href="/2021/04/14/Software/Flume%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/"/>
      <url>/2021/04/14/Software/Flume%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h3 id="事务性"><a href="#事务性" class="headerlink" title="事务性"></a>事务性</h3><p>Flume 为了保持数据传递的可靠性使用两个独立的事务分别负责从<code>soucrce</code>到<code>channel</code>（Put事务），以及从<code>channel</code>到<code>sink</code>（Take事务）的事件传递。在这两个事务中，任何一个下游组件（channel或sink）未能正确的接受数据都会触发事务回滚，标志这次数据传输失败，尝试重新传输或者将数据保存在上游队列等待重新传输。</p><span id="more"></span><p>比如：<code>spooling directory source</code> 为文件的每一行创建一个事件，一旦事务中所有的事件全部传递到<code>channel</code>且提交成功，那么<code>source</code>就将该文件标记为完成（在传输完成的文件后加后缀名.COMPLETED）。同理，事务以类似的方式处理从<code>channel</code>到<code>sink</code>的传递过程，如果因为某种原因使得事件无法记录，那么事务将会回滚。且所有的事件都会保持到<code>channel</code>中，等待重新传递。</p><h3 id="Flume-Agent-连接"><a href="#Flume-Agent-连接" class="headerlink" title="Flume Agent  连接"></a>Flume Agent  连接</h3><p>为了跨多个 agent 或 flume 传输数据，前一个agent的<code>sink</code>和当前 flume 的<code>source</code>需要是<code>avro</code>类型，<code>sink</code>指向<code>source</code>的主机名(或IP地址)和端口，后一个<code>source</code>则使用<code>avro</code>类型的<code>source</code>监听本机端口。</p><div align=center><img src="http://flume.apache.org/_images/UserGuide_image03.png"></div><p>有了这个基础的 Flume Agent 串联才有了后续的聚合。</p><h3 id="Flume-Agent-聚合"><a href="#Flume-Agent-聚合" class="headerlink" title="Flume Agent  聚合"></a>Flume Agent  聚合</h3><p>Flume Agent 聚合的应用场景在于将多个源的数据聚合在一起传输到一个<code>sink</code>中，例如：Agent1 2 3 是部署在三台生成log的机器上，用来收集各自的 log 日志，收集完成后再统一发布到 Agent 4 机器的端口上，使用的就是<code>avro</code>类型的<code>source</code>和<code>sink</code>，这里有两种选择方式，第一种是 Agent 1 2 3 所有的<code>sink</code>发送到 Agent 4 机器上的同一个端口，这样 Agent 4 只需要监听一个端口；第二种是 Agent 1 2 3 各自的sink发送到 Agent 4 不同的端口上，Agent 4 配置<code>sources</code>同时监听三个端口，再将所有数据传输给 Agent 4 的<code>channel</code>，第一种是在 Agent 4 的<code>source</code>部分完成数据合并，第二种则是在<code>channel</code>部分完成合并。</p><div align=center><img src="http://flume.apache.org/_images/UserGuide_image02.png"></div><h3 id="Channel-选择器"><a href="#Channel-选择器" class="headerlink" title="Channel 选择器"></a>Channel 选择器</h3><p>数据通过<code>sources</code>组件后，会进入<code>channel</code>组件，在这个过程中有几个步骤，分别是拦截器、Channel 选择器和 Put 事务，其中 Channel 选择器的作用是将<code>sources</code>组件传过来的数据分配到不同的<code>channel</code>中，官方有两种类型：<code>Replicating Channel Selector (default)</code>和<code>Multiplexing Channel Selector</code>。</p><p>**Replicating Channel Selector ** 是默认选项，它会将<code>sources</code>传过来的数据发送到所有<code>channel</code>，第一个单纯复制的 Channel 选择器。</p><p><strong>Multiplexing Channel Selector</strong>  则是多路复用<code>channel</code>选择器，它通过从<code>sources</code>传输过来的数据所携带的 header 信息判断会将数据发送给哪个<code>channel</code>，需要自定义 sources header。</p><div align=center><img src="http://flume.apache.org/_images/UserGuide_image01.png"></div><h3 id="Sink-组故障转移和负载均衡"><a href="#Sink-组故障转移和负载均衡" class="headerlink" title="Sink 组故障转移和负载均衡"></a>Sink 组故障转移和负载均衡</h3><p>Sink 组主要是解决数据从<code>channel</code>到<code>sink</code>的故障转移和负载均衡需求，官方有两种类型：<code>Failover Sink Processor</code>和<code>Load balancing Sink Processor</code>。</p><p><strong>Failover Sink Processor</strong>根据一个按优先级排列的sink列表，确保只要有一个<code>sink</code>可用，事件就会被处理(交付)。故障转移机制的工作原理是将失败的<code>sink</code>转移到失败池中，在失败池中为它们分配一个冷却期，在这个冷却期到达之前不会尝试发送到这个<code>sink</code>。一旦<code>sink</code>成功地发送了一个事件，它就会被恢复到活动池。每个<code>sink</code>有一个优先级，数值越大，优先级越高。如果一个<code>sink</code>在发送事件时失败，下一个具有最高优先级的<code>sink</code>将被尝试下一步发送事件。</p><p><strong>Load balancing Sink Processor</strong> 提供了在多个 sink 上负载平衡的能力。它根据一个活动接收的<code>sink</code>列表进行分发负载。通过设置<code>processor.selector</code>参数可以指定通过轮训或者随机分发的方式传输到每个<code>sink</code>。</p><h3 id="Flume-数据可靠性"><a href="#Flume-数据可靠性" class="headerlink" title="Flume 数据可靠性"></a>Flume 数据可靠性</h3><h4 id="File-Channel"><a href="#File-Channel" class="headerlink" title="File Channel"></a>File Channel</h4><p>Channel 组件的类型有 memory、file 和 kafka ，暂且不论 kafka ，<code>file channel</code>对于生产环境来说是一个相对可靠的选择，避免宕机后<code>memory</code>中的数据都会丢失，<code>file channel</code>在数据未被<code>sink</code>消费时可以保存在硬盘上，服务恢复后将<code>sink</code>的<code>channel</code>绑定为<code>file channel</code>就可以取到未被<code>sink</code>消费的数据了。</p><h4 id="数据持久化"><a href="#数据持久化" class="headerlink" title="数据持久化"></a>数据持久化</h4><p>虽然 Flume 在安全性和可靠性的地方已经做得很好了，有 File Channel、Put 和 Take 事务和 Sink 组等等，但为了数据的安全性考虑还是需要考虑持久化操作，其实做起来也很简单，就是将<code>sources</code>传输过来的数据分别发送给<code>memory channel</code>和<code>file channel</code>，然后<code>memory channel</code>对接 HDFS 或者 hive sink，<code>file channel</code>对接<code>file sink</code>，这样<code>file channel</code>和<code>file sink</code>就是对数据的双重保障，</p><h3 id="Flume-监控"><a href="#Flume-监控" class="headerlink" title="Flume 监控"></a>Flume 监控</h3><p>对于 Flume 的运行，我们需要一个能展示 Flume 实时收集数据动态信息的界面或者统计信息，包括 Flume 成功收集的日志数量、成功发送的日志数量、Flume 启动时间、停止时间、以及 Flume 一些具体的配置信息，像通道容量等，于是顺利成章的监控能帮我们做到这些，有了这些数据，在遇到数据收集瓶颈或者数据丢失的时候，通过分析监控数据来分析、解决问题。</p><h3 id="http-监控"><a href="#http-监控" class="headerlink" title="http 监控"></a>http 监控</h3><p>一般来说使用自带的 http 监控就可以看到大部分有用的数据。使用这种监控方式，只需要在启动 Flume 的时候在启动参数上面加上监控配置，例如这样：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent --conf conf --conf-file conf/flume_conf.properties --name collect -Dflume.monitoring.type=http -Dflume.monitoring.port=1234</span><br></pre></td></tr></table></figure><p>其中<code>-Dflume.monitoring.type=http</code>表示使用 http 方式来监控，后面的<code>-Dflume.monitoring.port=1234</code>表示我们需要启动的监控服务的端口号为 1234，这个端口号可以自己随意配置。然后启动 Flume 之后，通过<code>http://ip:1234/metrics</code>就可以得到 Flume 的一个 json 格式的监控数据。</p><h4 id="ganglia-监控"><a href="#ganglia-监控" class="headerlink" title="ganglia 监控"></a>ganglia 监控</h4><p>这种监控方式需要先安装 ganglia 然后启动 ganglia ，然后在启动 Flume 的时候加上监控配置，例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">bin/flume-ng agent --conf conf --conf-file conf/producer.properties --name collect -Dflume.monitoring.type=ganglia -Dflume.monitoring.hosts=ip:port</span><br></pre></td></tr></table></figure><p>其中<code>-Dflume.monitoring.type=ganglia</code>表示使用 ganglia 的方式来监控，而<code>-Dflume.monitoring.hosts=ip:port</code>表示 ganglia 安装的 ip 和启动的端口号。</p><h3 id="拦截器"><a href="#拦截器" class="headerlink" title="拦截器"></a>拦截器</h3><p>拦截器是简单的插件式组件，设置在 source 和 channel 之间。source 接收到的事件，在写入 channel 之前，拦截器都可以进行转换或者删除这些事件。每个拦截器只处理同一个 source 接收到的事件。可以自定义拦截器。Flume 内置了很多拦截器，并且会定期的添加一些拦截器，在这里列出一些 Flume 内置的，经常使用的拦截器。</p><p><strong>1、Timestamp Interceptor (时间戳拦截器)</strong></p><p>Flume 中一个最经常使用的拦截器 ，该拦截器的作用是将时间戳插入到 Flume 的事件报头中。如果不使用任何拦截器，Flume 接受到的只有 message 。时间戳拦截器<code>type</code>参数默认值为<code>timestamp</code>，<code>preserveExisting</code>默认为<code>false</code>，如果设置为<code>true</code>，若事件中报头已经存在，不会替换时间戳报头的值。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># source连接到时间戳拦截器的配置</span><br><span class="line">a1.sources.r1.interceptors = timestamp</span><br><span class="line">a1.sources.r1.interceptors.timestamp.type=timestamp</span><br><span class="line">a1.sources.r1.interceptors.timestamp.preserveExisting=false</span><br></pre></td></tr></table></figure><p><strong>2、Host Interceptor (主机拦截器)</strong></p><p>主机拦截器插入服务器的 ip 地址或者主机名，agent 将这些内容插入到事件的报头中。时间报头中的 key 使用 hostHeader 配置，默认是 host。主机拦截器<code>type</code>参数默认值为 host，host.useIP 如果设置为 false，host 则放入主机名，<code>timestamp.preserveExisting</code>默认为<code>false</code>，如果设置为<code>true</code>，若事件中报头已经存在，不会替换时间戳报头的值。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># source连接到主机拦截器的配置</span><br><span class="line">a1.sources.r1.interceptors = host</span><br><span class="line">a1.sources.r1.interceptors.host.type=host</span><br><span class="line">a1.sources.r1.interceptors.host.useIP=false</span><br><span class="line">a1.sources.r1.interceptors.timestamp.preserveExisting=true</span><br></pre></td></tr></table></figure><p><strong>3、Static Interceptor (静态拦截器)</strong></p><p>静态拦截器的作用是将 k&#x2F;v 插入到事件的报头中，channel 或者 sink 可以根据指定的 k&#x2F;v 值操作数据。配置如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># source连接到****静态拦截器****的配置</span><br><span class="line">a1.sources.r1.interceptors = static</span><br><span class="line">a1.sources.r1.interceptors.static.type=static</span><br><span class="line">a1.sources.r1.interceptors.static.key=logs</span><br><span class="line">a1.sources.r1.interceptors.static.value=logFlume</span><br><span class="line">a1.sources.r1.interceptors.static.preserveExisting=false</span><br></pre></td></tr></table></figure><p><strong>4、Regex Filtering Interceptor (正则过滤拦截器)</strong></p><p>在日志采集的时候，可能有一些数据是我们不需要的，这样添加过滤拦截器，可以过滤掉不需要的日志，也可以根据需要收集满足正则条件的日志。配置如下：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># source连接到正则过滤拦截器的配置，这样配置的拦截器就只会接收日志消息中带有rm 或者kill的日志：</span><br><span class="line">a1.sources.r1.interceptors = regex</span><br><span class="line">a1.sources.r1.interceptors.regex.type=REGEX_FILTER</span><br><span class="line">a1.sources.r1.interceptors.regex.regex=(rm)|(kill)</span><br><span class="line">a1.sources.r1.interceptors.regex.excludeEvents=false</span><br></pre></td></tr></table></figure><p><strong>5、Regex Extractor Interceptor</strong></p><p>通过正则表达式来在 header 中添加指定的 key , value 则为正则匹配的部分。</p><p><strong>6、UUID Interceptor</strong></p><p>用于在每个 events header 中生成一个 UUID 字符串，例如：b5755073-77a9-43c1-8fad-b7a586fc1b97。生成的 UUID 可以在 sink 中读取并使用。</p><h3 id="配置文件示例"><a href="#配置文件示例" class="headerlink" title="配置文件示例"></a>配置文件示例</h3><p>本想着分开来写，但想到针对每个功能都要写好几个配置文件，所以就写了个大杂烩，生产环境中就别这样配了，没什么用。其中包含静态筛选器、多路复用 Channel 选择器、File Channel 以及数据持久化，Flume Agent 聚合以及 Sink 组故障转移需要多个 Agent 来完成，就没在文件中配置，生产中使用 avro source 和 sink 连接多个 Agent 就行了，其他的详见<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#flume-sink-processors">官方文档</a>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Name</span></span><br><span class="line">a1.sources = r1 r2</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"></span><br><span class="line"><span class="comment"># hive source</span></span><br><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line">a1.sources.r1.positionFile = /usr/local/flume/data/multiplexing/position_hive.json</span><br><span class="line">a1.sources.r1.filegroups = f1</span><br><span class="line">a1.sources.r1.filegroups.f1 = /usr/local/flume/data/multiplexing/hive.log</span><br><span class="line">a1.sources.r1.fileHeader = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># nginx source</span></span><br><span class="line">a1.sources.r2.type = TAILDIR</span><br><span class="line">a1.sources.r2.positionFile = /usr/local/flume/data/multiplexing/position_nginx.json</span><br><span class="line">a1.sources.r2.filegroups = f2</span><br><span class="line">a1.sources.r2.filegroups.f2 = /usr/local/flume/data/multiplexing/nginx.log</span><br><span class="line">a1.sources.r2.fileHeader = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># channel selector</span></span><br><span class="line">a1.sources.r1.selector.type = multiplexing</span><br><span class="line">a1.sources.r1.selector.header = <span class="built_in">type</span></span><br><span class="line">a1.sources.r1.selector.mapping.hive = c1</span><br><span class="line">a1.sources.r1.selector.mapping.nginx = c2</span><br><span class="line"><span class="comment"># a1.sources.r1.selector.default = c3</span></span><br><span class="line"></span><br><span class="line">a1.sources.r2.selector.type = multiplexing</span><br><span class="line">a1.sources.r2.selector.header = <span class="built_in">type</span></span><br><span class="line">a1.sources.r2.selector.mapping.hive = c1</span><br><span class="line">a1.sources.r2.selector.mapping.nginx = c2</span><br><span class="line"><span class="comment"># a1.sources.r2.selector.default = c3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sources interceptors</span></span><br><span class="line">a1.sources.r1.interceptors = static</span><br><span class="line">a1.sources.r1.interceptors.static.type = static</span><br><span class="line">a1.sources.r1.interceptors.static.key = <span class="built_in">type</span></span><br><span class="line">a1.sources.r1.interceptors.static.value = hive</span><br><span class="line">a1.sources.r1.interceptors.static.preserveExisting = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line">a1.sources.r2.interceptors = static</span><br><span class="line">a1.sources.r2.interceptors.static.type = static</span><br><span class="line">a1.sources.r2.interceptors.static.key = <span class="built_in">type</span></span><br><span class="line">a1.sources.r2.interceptors.static.value = nginx</span><br><span class="line">a1.sources.r2.interceptors.static.preserveExisting = <span class="literal">false</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line">a1.sinks.k2.type = file_roll</span><br><span class="line">a1.sinks.k2.sink.rollInterval = 30</span><br><span class="line">a1.sinks.k2.sink.directory = /usr/local/flume/data/file_roll</span><br><span class="line"></span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory and file</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line"></span><br><span class="line">a1.channels.c2.type = file</span><br><span class="line">a1.channels.c2.checkpointDir = /usr/local/flume/data/filechannel/checkpoint</span><br><span class="line">a1.channels.c2.dataDirs = /usr/local/flume/data/filechannel/data</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sources.r2.channels = c1 c2</span><br><span class="line"></span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ETL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Flume简单应用</title>
      <link href="/2021/04/13/Software/Flume%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8/"/>
      <url>/2021/04/13/Software/Flume%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<p>Apache Flume 是一个分布式的、可靠的、可用的系统，可以有效地收集、聚合和将大量的日志数据从许多不同的源移动到一个集中的数据存储中。它的 SOURCE、SINK、CHANNEL 三大组件这种模式，来完成数据的接收、缓存、发送这个过程，拥有非常完美的契合度。 Flume agent 是一个(JVM)进程，它承载着从 source 到下一个 sink 的事件流。</p><span id="more"></span><h3 id="Flume架构"><a href="#Flume架构" class="headerlink" title="Flume架构"></a><strong>Flume架构</strong></h3><div align=center><img src="http://flume.apache.org/_images/UserGuide_image00.png"></div><p>Flume source 使用由外部源(如 web 服务器)交付给它的事件。外部源以目标 Flume source 能够识别的格式向 Flume 发送事件。例如，Avro Flume source 可用于从 Avro 客户端或流中的其他 Flume agent 接收来自 Avro 的事件。channel 是一个被动存储事件，直到它被 Flume sink 使用。file channel 就是一个例子——它由本地文件系统支持。sink 将事件从 channel 中移除，并将其放入外部存储库，如HDFS(通过Flume HDFS sink)，或将其转发到流中的下一个 Flume agent 的 Flume source。</p><p>事件在每个 agent的 channel 中暂存。然后事件被交付到 sink 中的下一个 agent 或终端存储库(如 HDFS )。事件仅在存储在下一个 agent 的 channel 或终端存储库中之后才会从当前 channel 中删除，当然 file channel 例外 它会将数据持久化在磁盘，而过不被 sink 消费就一直存在。这就是 Flume 中如何提供流的端到端的可靠性。</p><h3 id="Flume-配置文件"><a href="#Flume-配置文件" class="headerlink" title="Flume 配置文件"></a><strong>Flume 配置文件</strong></h3><p>Flume 的启动配置文件一般只需要修改 Flume 根目录下 conf 目录中的 flume-env.sh 文件，修改内容如下</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 指定JAVA_HOME</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/local/jdk1.8.0_181/</span><br><span class="line"><span class="comment"># 可以将启动Flume时固定配置的参数放在JAVA_OPTS中（指定从终端打印数据并且指定监控数据访问端口http://本地ip:1234/metrics），可以不设置</span></span><br><span class="line">JAVA_OPTS=<span class="string">&quot;-Dflume.root.logger=INFO,console -Dflume.monitoring.type=http -Dflume.monitoring.port=1234&quot;</span></span><br></pre></td></tr></table></figure><p>Flume 的启动命令一般包括特定的几个参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/flume/bin/flume-ng agent \  <span class="comment"># Flume启动命令</span></span><br><span class="line">    --conf /usr/local/flume/conf \  <span class="comment"># 指定配置文件目录</span></span><br><span class="line">    --conf-file /usr/local/flume/config/spooldir.conf \  <span class="comment"># 指定agent配置文件</span></span><br><span class="line">    --name a1 \  <span class="comment"># 指定agent名称</span></span><br><span class="line">    -Dflume.root.logger=INFO,console \  <span class="comment"># 在控制台打印数据</span></span><br><span class="line">    -Dflume.monitoring.type=http \  <span class="comment"># 使用http监控Flume传输数据</span></span><br><span class="line">    -Dflume.monitoring.port=1234  <span class="comment"># 访问监控数据的本地端口（http://本地ip:1234/metrics）</span></span><br></pre></td></tr></table></figure><p>Flume 每个 agent 的配置包括 SOURCE、SINK、CHANNEL 三大组件的配置，每个 agent 配置文件单独放在单独的 conf 文件中，启动 Flume 的时候指定既可，以下是一个官方示例，监听本地端口 4444 并将数据暂存在 memory channel 中由终端消费：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">_<span class="comment"># example.conf: A single-node Flume configuration_ </span></span><br><span class="line"></span><br><span class="line">_<span class="comment"># Name the components on this agent_ </span></span><br><span class="line">a1.sources **=** r1 </span><br><span class="line">a1.sinks **=** k1 </span><br><span class="line">a1.channels **=** c1 </span><br><span class="line"></span><br><span class="line">_<span class="comment"># Describe/configure the source_ </span></span><br><span class="line">a1.sources.r1.type **=** netcat </span><br><span class="line">a1.sources.r1.bind **=** localhost </span><br><span class="line">a1.sources.r1.port **=** 44444 </span><br><span class="line"></span><br><span class="line">_<span class="comment"># Describe the sink_ </span></span><br><span class="line">a1.sinks.k1.type **=** logger </span><br><span class="line"></span><br><span class="line">_<span class="comment"># Use a channel which buffers events in memory_ </span></span><br><span class="line">a1.channels.c1.type **=** memory </span><br><span class="line">a1.channels.c1.capacity **=** 1000 </span><br><span class="line">a1.channels.c1.transactionCapacity **=** 100 </span><br><span class="line"></span><br><span class="line">_<span class="comment"># Bind the source and sink to the channel_ </span></span><br><span class="line">a1.sources.r1.channels **=** c1 </span><br><span class="line">a1.sinks.k1.channel **=** c1</span><br></pre></td></tr></table></figure><p>常见的 source 有 exec（执行一段代码）、spooldir（监控一个目录的新文件）、taildir（监控一个或多个文件）以及 netcat（监听一个服务器端口），其他的查询<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#flume-sources">官方文档</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># exec</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sources.r1.type = <span class="built_in">exec</span></span><br><span class="line">a1.sources.r1.command = <span class="built_in">tail</span> -f /usr/local/flume/data/exec.log</span><br><span class="line"></span><br><span class="line"><span class="comment"># spooldir</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sources.r1.type = spooldir</span><br><span class="line">a1.sources.r1.spoolDir = /usr/local/flume/data/spooldir</span><br><span class="line">a1.sources.r1.fileHeader = <span class="literal">true</span></span><br><span class="line">a1.sources.r1.fileSuffix = .COMPLETED</span><br><span class="line"></span><br><span class="line"><span class="comment"># taildir</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line">a1.sources.r1.positionFile = /usr/local/flume/data/taildir/taildir_position.json</span><br><span class="line">a1.sources.r1.filegroups = f1 f2</span><br><span class="line">a1.sources.r1.filegroups.f1 = /usr/local/flume/data/taildir/file1</span><br><span class="line">a1.sources.r1.filegroups.f2 = /usr/local/flume/data/taildir/.*txt</span><br><span class="line">a1.sources.r1.fileHeader = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># netcat</span></span><br><span class="line">a1.sources.r1.type **=** netcat </span><br><span class="line">a1.sources.r1.bind **=** localhost </span><br><span class="line">a1.sources.r1.port **=** 44444 </span><br></pre></td></tr></table></figure><p>常见的 channel 有 memory 、file 以及 kafaka ，其他的查看<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#flume-channels">官方文档</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># memory</span></span><br><span class="line">a1.channels **=** c1 </span><br><span class="line">a1.channels.c1.type **=** memory </span><br><span class="line">a1.channels.c1.capacity **=** 10000 </span><br><span class="line">a1.channels.c1.transactionCapacity **=** 10000 a1.channels.c1.byteCapacityBufferPercentage **=** 20 </span><br><span class="line">a1.channels.c1.byteCapacity **=** 800000</span><br><span class="line"></span><br><span class="line"><span class="comment"># kafka</span></span><br><span class="line">a1.channels **=** c1 </span><br><span class="line">a1.channels.c1.type **=** org.apache.flume.channel.kafka.KafkaChannel </span><br><span class="line">a1.channels.c1.kafka.bootstrap.servers **=** kafka-1:9092,kafka-2:9092,kafka-3:9092 </span><br><span class="line">a1.channels.c1.kafka.topic **=** c1 </span><br><span class="line">a1.channels.c1.kafka.consumer.group.id **=** flume-consumer</span><br><span class="line"></span><br><span class="line"><span class="comment"># file</span></span><br><span class="line">a1.channels **=** c1 </span><br><span class="line">a1.channels.c1.type = file</span><br><span class="line">a1.channels.c1.checkpointDir = /usr/local/flume/data/filechannel/checkpoint</span><br><span class="line">a1.channels.c1.dataDirs = /usr/local/flume/data/filechannel/data</span><br></pre></td></tr></table></figure><p>常见的 sink 有 logger（终端）、file_roll（滚动文件）、avro（端口）、hdfs、hive、kafka、ES、HBASE 等等，其他的查看<a href="http://flume.apache.org/releases/content/1.9.0/FlumeUserGuide.html#flume-sinks">官方文档</a></p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># logger</span></span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line"></span><br><span class="line"><span class="comment"># file_roll</span></span><br><span class="line">a1.sinks = k1</span><br><span class="line">a1.sinks.k1.type = file_roll</span><br><span class="line">a1.sinks.k1.sink.rollInterval = 30</span><br><span class="line">a1.sinks.k1.sink.directory = /usr/local/flume/data/file_roll</span><br><span class="line"></span><br><span class="line"><span class="comment"># avro</span></span><br><span class="line">a1.sinks **=** k1 </span><br><span class="line">a1.sinks.k1.type **=** avro </span><br><span class="line">a1.sinks.k1.channel **=** c1 </span><br><span class="line">a1.sinks.k1.hostname **=** 10.10.10.10 </span><br><span class="line">a1.sinks.k1.port **=** 4545</span><br><span class="line"></span><br><span class="line"><span class="comment"># hive以及hdfs参数有点多，详情查看官方文档</span></span><br></pre></td></tr></table></figure><p>最后放一个做了数据持久化的发送到hdfs的示例</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Name the components on this agent</span></span><br><span class="line">a1.sources = r1</span><br><span class="line">a1.sinks = k1 k2</span><br><span class="line">a1.channels = c1 c2</span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe/configure the source</span></span><br><span class="line">a1.sources.r1.type = TAILDIR</span><br><span class="line">a1.sources.r1.positionFile = /usr/local/flume/data/taildir/taildir_position.json</span><br><span class="line">a1.sources.r1.filegroups = f1 f2 f3</span><br><span class="line">a1.sources.r1.filegroups.f1 = /usr/local/flume/data/taildir/file1</span><br><span class="line">a1.sources.r1.filegroups.f2 = /usr/local/flume/data/taildir/.*txt</span><br><span class="line">a1.sources.r1.filegroups.f3 = /usr/local/flume/data/taildir/.*<span class="built_in">log</span></span><br><span class="line">a1.sources.r1.fileHeader = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Describe the sink</span></span><br><span class="line">a1.sinks.k1.type = logger</span><br><span class="line">a1.sinks.k2.type **=** hdfs </span><br><span class="line">a1.sinks.k2.channel **=** c1 </span><br><span class="line">a1.sinks.k2.hdfs.path **=** hdfs://hadoop102:9000/flume/events/%y-%m-%d/%H%M/%S </span><br><span class="line">a1.sinks.k2.hdfs.filePrefix **=** events- </span><br><span class="line">a1.sinks.k2.hdfs.round **=** <span class="literal">true</span> </span><br><span class="line">a1.sinks.k2.hdfs.roundValue **=** 10 </span><br><span class="line">a1.sinks.k2.hdfs.roundUnit **=** minute</span><br><span class="line">a1.sinks.k2.hdfs.useLocalTimeStamp = <span class="literal">true</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Use a channel which buffers events in memory and file</span></span><br><span class="line">a1.channels.c1.type = memory</span><br><span class="line">a1.channels.c1.capacity = 1000</span><br><span class="line">a1.channels.c1.transactionCapacity = 100</span><br><span class="line">a1.channels.c2.type = file</span><br><span class="line">a1.channels.c2.checkpointDir = /usr/local/flume/data/filechannel/checkpoint</span><br><span class="line">a1.channels.c2.dataDirs = /usr/local/flume/data/filechannel/data</span><br><span class="line"></span><br><span class="line"><span class="comment"># Bind the source and sink to the channel</span></span><br><span class="line">a1.sources.r1.channels = c1 c2</span><br><span class="line">a1.sinks.k1.channel = c1</span><br><span class="line">a1.sinks.k2.channel = c2</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ETL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Dockerfile简单应用</title>
      <link href="/2021/04/11/Software/Dockerfile%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8/"/>
      <url>/2021/04/11/Software/Dockerfile%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<p>Dockerfile 是一个用来构建镜像的文本文件，结合Docker使用，构建自己的镜像，文本内容包含了一条条构建镜像所需的指令和说明。</p><span id="more"></span><h4 id="FROM"><a href="#FROM" class="headerlink" title="FROM"></a><strong>FROM</strong></h4><p>基础镜像可以为任意镜像。如果基础镜像没有被发现，Docker将试图从Docker image index来查找该镜像。FROM命令必须是Dockerfile的首个命令。如果同一个DockerFile创建多个镜像时，可使用多个FROM指令（每个镜像一次）</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Usage: FROM [image name]</span><br><span class="line">FROM ubuntu </span><br></pre></td></tr></table></figure><h4 id="MAINTAINER"><a href="#MAINTAINER" class="headerlink" title="MAINTAINER"></a><strong>MAINTAINER</strong></h4><p>指定维护者的信息，并应该放在FROM的后面。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Usage: MAINTAINER [name]</span><br><span class="line">MAINTAINER authors_name </span><br></pre></td></tr></table></figure><h4 id="RUN"><a href="#RUN" class="headerlink" title="RUN"></a><strong>RUN</strong></h4><p>RUN命令是Dockerfile执行命令的核心部分。它接受命令作为参数并用于创建镜像。不像CMD命令，RUN命令用于创建镜像（在之前commit的层之上形成新的层）。<br>格式为Run 或者Run [“executable” ,”Param1”, “param2”]<br>前者在shell终端上运行，即&#x2F;bin&#x2F;sh -C，后者使用exec运行。例如：RUN [“&#x2F;bin&#x2F;bash”, “-c”,”echo hello”]<br>每条run指令在当前基础镜像执行，并且提交新镜像。当命令比较长时，可以使用“&#x2F;”换行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Usage: RUN [command]</span><br><span class="line">RUN apt-get update </span><br></pre></td></tr></table></figure><h4 id="USER"><a href="#USER" class="headerlink" title="USER"></a><strong>USER</strong></h4><p>格式为 USER daemon 。<br>指定运行容器时的用户名或UID，后续的 RUN 也会使用指定用户。<br>当服务不需要管理员权限时，可以通过该命令指定运行用户。并且可以在之前创建所需要的用户，例如： RUN groupadd -r postgres &amp;&amp; useradd -r -g postgres postgres 。要临时获取管理员权限可以使用 gosu ，而不推荐 sudo 。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Usage: USER [UID]</span><br><span class="line">USER 751</span><br></pre></td></tr></table></figure><h4 id="VOLUME"><a href="#VOLUME" class="headerlink" title="VOLUME"></a><strong>VOLUME</strong></h4><p>VOLUME命令用于让你的容器访问宿主机上的目录。<br>格式为 VOLUME [“&#x2F;data”] 。<br>创建一个可以从本地主机或其他容器挂载的挂载点，一般用来存放数据库和需要保持的数据等。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Usage: VOLUME [&quot;/dir_1&quot;, &quot;/dir_2&quot; ..]</span><br><span class="line">VOLUME [&quot;/my_files&quot;, &quot;/app_files&quot;]</span><br></pre></td></tr></table></figure><h4 id="WORKDIR"><a href="#WORKDIR" class="headerlink" title="WORKDIR"></a><strong>WORKDIR</strong></h4><p>WORKDIR命令用于设置CMD指明的命令的运行目录。<br>格式为 WORKDIR &#x2F;path&#x2F;to&#x2F;workdir 。<br>为后续的 RUN 、 CMD 、 ENTRYPOINT 指令配置工作目录。<br>可以使用多个 WORKDIR 指令，后续命令如果参数是相对路径，则会基于之前命令指定的路径。例如:</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">WORKDIR /a </span><br><span class="line">WORKDIR b </span><br><span class="line">WORKDIR c </span><br><span class="line">RUN pwd </span><br><span class="line"># 最终路径为 /a/b/c </span><br></pre></td></tr></table></figure><h4 id="CMD"><a href="#CMD" class="headerlink" title="CMD"></a><strong>CMD</strong></h4><p>支持三种格式：<br>CMD [“executable” ,”Param1”, “param2”]使用exec执行，推荐<br>CMD command param1 param2，在&#x2F;bin&#x2F;sh上执行<br>CMD [“Param1”, “param2”] 提供给ENTRYPOINT做默认参数。<br>每个容器只能执行一条CMD命令，多个CMD命令时，只最后一条被执行。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Usage 1: CMD application &quot;argument&quot;, &quot;argument&quot;, ..</span><br><span class="line">CMD &quot;echo&quot; &quot;Hello docker!&quot;</span><br></pre></td></tr></table></figure><h4 id="ENV"><a href="#ENV" class="headerlink" title="ENV"></a><strong>ENV</strong></h4><p>格式为 ENV 。 指定一个环境变量，会被后续 RUN 指令使用，并在容器运行时保持。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ENV TZ &quot;Asia/Shanghai&quot;</span><br><span class="line">ENV LANG en_US.UTF-8</span><br><span class="line">ENV LC_ALL en_US.UTF-8</span><br></pre></td></tr></table></figure><h4 id="ADD"><a href="#ADD" class="headerlink" title="ADD"></a><strong>ADD</strong></h4><p>ADD命令有两个参数，源和目标。它的基本作用是从源系统的文件系统上复制文件到目标容器的文件系统。如果源是一个URL，那该URL的内容将被下载并复制到容器中。如果文件是可识别的压缩格式，则docker会帮忙解压缩。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Usage: ADD [source directory or URL] [destination directory]</span><br><span class="line">ADD /my_app_folder /my_app_folder </span><br></pre></td></tr></table></figure><h4 id="COPY（与ADD没有区别）"><a href="#COPY（与ADD没有区别）" class="headerlink" title="COPY（与ADD没有区别）"></a><strong>COPY（与ADD没有区别）</strong></h4><p>COPY 将文件从路径 &lt;src复制添加到容器内部路径 <dest>。</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">COPY &lt;src&gt; &lt;dest&gt;</span><br></pre></td></tr></table></figure><h4 id="EXPOSE"><a href="#EXPOSE" class="headerlink" title="EXPOSE"></a><strong>EXPOSE</strong></h4><p>指定在docker允许时指定的端口进行转发</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">EXPOSE &lt;port&gt;[&lt;port&gt;...]</span><br></pre></td></tr></table></figure><h4 id="ENTRYPOINT"><a href="#ENTRYPOINT" class="headerlink" title="ENTRYPOINT"></a><strong>ENTRYPOINT</strong></h4><p>两种格式：<br>ENTRYPOINT [“executable”, “param1”, “param2”]<br>ENTRYPOINT command param1 param2（shell中执行）。<br>配置容器启动后执行的命令，并且不可被 docker run 提供的参数覆盖。<br>每个 Dockerfile 中只能有一个 ENTRYPOINT，当指定多个时，只有最后一个起效。</p><h4 id="以构建Flume为示例"><a href="#以构建Flume为示例" class="headerlink" title="以构建Flume为示例"></a><strong>以构建Flume为示例</strong></h4><figure class="highlight dockerfile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">FROM</span> centos</span><br><span class="line"></span><br><span class="line">_<span class="comment"># install java_</span></span><br><span class="line"><span class="keyword">ADD</span><span class="language-bash"> _https://repo.huaweicloud.com/java/jdk/8u181-b13/jdk-8u181-linux-x64.tar.gz_ _/usr/local_</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> _tar_ _-zxvf_ _/usr/local/jdk-8u181-linux-x64.tar.gz_ _-C_ _/usr/local/_</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> _rm_ _-rf_ _/usr/local/jdk-8u181-linux-x64.tar.gz_</span></span><br><span class="line"><span class="keyword">ENV</span> JAVA_HOME _/usr/local/jdk1.<span class="number">8.0</span>_181_</span><br><span class="line"><span class="keyword">ENV</span> PATH $JAVA_HOME_/bin:_$PATH</span><br><span class="line"><span class="keyword">ENV</span> CLASSPATH _.:_$JAVA_HOME_/lib_</span><br><span class="line"></span><br><span class="line">_<span class="comment"># install Flume_</span></span><br><span class="line"><span class="keyword">ADD</span><span class="language-bash"> _https://mirrors.tuna.tsinghua.edu.cn/apache/flume/1.9.0/apache-flume-1.9.0-bin.tar.gz_ _/usr/local_</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> _tar_ _-zxvf_ _/usr/local/apache-flume-1.9.0-bin.tar.gz_ _-C_ _/usr/local/_</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> _rm_ _-rf_ _/usr/local/apache-flume-1.9.0-bin.tar.gz_</span></span><br><span class="line"><span class="keyword">RUN</span><span class="language-bash"> _mv_ _/usr/local/apache-flume-1.9.0-bin_ _/usr/local/flume_</span></span><br><span class="line"><span class="keyword">WORKDIR</span><span class="language-bash"> _/usr/local/flume_</span></span><br><span class="line"><span class="keyword">ADD</span><span class="language-bash"> _flume-env.sh_ _/usr/local/flume/conf_</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># export port</span></span><br><span class="line"><span class="keyword">EXPOSE</span> _1999_</span><br></pre></td></tr></table></figure><h4 id="Docker镜像打包"><a href="#Docker镜像打包" class="headerlink" title="Docker镜像打包"></a><strong>Docker镜像打包</strong></h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 以打包flume为例，先cd到Dockerfile所在的目录</span></span><br><span class="line"><span class="comment"># 发布到Docker Hub</span></span><br><span class="line">docker build -t flume .</span><br><span class="line">docker tag flume wxk749/flume:latest  <span class="comment"># 给镜像打标签</span></span><br><span class="line">docker login  <span class="comment"># 输入用户名和密码登录到Docker Hub</span></span><br><span class="line">docker push wxk749/flume</span><br><span class="line">docker pull wxk749/flume</span><br><span class="line"></span><br><span class="line"><span class="comment"># 接发布到阿里云私人仓库，私人仓库需要先登录后推送和拉取</span></span><br><span class="line">docker login --username=wxk74963 registry.cn-beijing.aliyuncs.com</span><br><span class="line">docker tag flume registry.cn-beijing.aliyuncs.com/wangxukun/flume:latest</span><br><span class="line">docker push registry.cn-beijing.aliyuncs.com/wangxukun/flume:latest</span><br><span class="line">docker pull registry.cn-beijing.aliyuncs.com/wangxukun/flume:latest</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Logstash简单应用</title>
      <link href="/2021/04/08/Software/Logstash%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8/"/>
      <url>/2021/04/08/Software/Logstash%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<p><strong>Elasticsearch</strong>是当前主流的分布式大数据存储和搜索引擎，可以为用户提供强大的全文本检索能力，广泛应用于日志检索，全站搜索等领域。<strong>Logstash</strong>作为Elasicsearch常用的实时数据采集引擎，可以采集来自不同数据源的数据，并对数据进行处理后输出到多种输出源，是Elastic Stack 的重要组成部分。</p><span id="more"></span><h3 id="Logstash工作原理"><a href="#Logstash工作原理" class="headerlink" title="Logstash工作原理"></a><strong>Logstash工作原理</strong></h3><p><strong>处理过程</strong></p><p><img src="https://main.qcloudimg.com/raw/7367005a46890c48d27e8518a14a1772.png"></p><p>如上图，Logstash的数据处理过程主要包括：Inputs, Filters, Outputs 三部分， 另外在Inputs和Outputs中可以使用Codecs对数据格式进行处理。这四个部分均以插件形式存在，用户通过定义pipeline配置文件，设置需要使用的input，filter，output, codec插件，以实现特定的数据采集，数据处理，数据输出等功能</p><p>（1）Inputs：用于从数据源获取数据，常见的插件如file, syslog, redis, beats 等[<a href="https://www.elastic.co/guide/en/logstash/current/input-plugins.html">详细参考</a>]<br>（2）Filters：用于处理数据如格式转换，数据派生等，常见的插件如grok, mutate, drop, clone, geoip等[<a href="https://www.elastic.co/guide/en/logstash/current/filter-plugins.html">详细参考</a>]<br>（3）Outputs：用于数据输出，常见的插件如elastcisearch，file, graphite, statsd等[<a href="https://www.elastic.co/guide/en/logstash/current/output-plugins.html">详细参考</a>]<br>（4）Codecs：Codecs不是一个单独的流程，而是在输入和输出等插件中用于数据转换的模块，用于对数据进行编码处理，常见的插件如json，multiline[<a href="https://www.elastic.co/guide/en/logstash/current/codec-plugins.html">详细参考</a>]</p><h4 id="执行模型"><a href="#执行模型" class="headerlink" title="执行模型"></a><strong>执行模型</strong></h4><p>（1）每个Input启动一个线程，从对应数据源获取数据<br>（2）Input会将数据写入一个队列：默认为内存中的有界队列（意外停止会导致数据丢失）。为了防止数丢失Logstash提供了两个特性：<br><a href="https://www.elastic.co/guide/en/logstash/current/persistent-queues.html">Persistent Queues</a>：通过磁盘上的queue来防止数据丢失<br><a href="https://www.elastic.co/guide/en/logstash/current/dead-letter-queues.html">Dead Letter Queues</a>：保存无法处理的event（仅支持Elasticsearch作为输出源）<br>（3）Logstash会有多个pipeline worker, 每一个pipeline worker会从队列中取一批数据，然后执行filter和output（worker数目及每次处理的数据量均由配置确定）</p><h4 id="简单应用"><a href="#简单应用" class="headerlink" title="简单应用"></a><strong>简单应用</strong></h4><p>首先是Logstash的配置文件，也可以不用配置，不会对数据采集和传输产生很大影响，当需要精细化的参数调整时再配置指定的参数即可。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">xpack.management.enabled: <span class="literal">false</span></span><br><span class="line">xpack.monitoring.elasticsearch.hosts: [<span class="string">&quot;node1:9200&quot;</span>,<span class="string">&quot;node2:9201&quot;</span>]</span><br><span class="line">xpack.monitoring.enabled: <span class="literal">true</span></span><br></pre></td></tr></table></figure><p>接下来就是配置数据传输的conf文件，需要制定inputs、filter和outputs三个模块。首先是从文件收集数据，filter解析后传入ElasticSearch。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">  file &#123;</span><br><span class="line">    path =&gt; [<span class="string">&quot;/usr/share/logstash/data/file/CouponProduct.csv&quot;</span>]  <span class="comment"># 必须使用绝对路径</span></span><br><span class="line">    <span class="comment"># sincedb记录现在有一个与之关联的最后活动时间戳。如果在最近N天内没有在跟踪文件中检测到任何更改，则它的sincedb跟踪记录将过期，并且不会被持久保存。默认14天</span></span><br><span class="line">    sincedb_clean_after =&gt; <span class="string">&quot;2 weeks&quot;</span></span><br><span class="line">    sincedb_path =&gt; <span class="string">&quot;/usr/share/logstash/data/file/sincedb&quot;</span>  <span class="comment"># sincedb存储地址</span></span><br><span class="line">    start_position =&gt; <span class="string">&quot;beginning&quot;</span>  <span class="comment"># 选择Logstash最初开始读取文件的位置:开始或结束。默认行为将文件视为实时流（end），因此从末尾开始。</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">filter&#123;</span><br><span class="line">  csv &#123;</span><br><span class="line">    separator =&gt; <span class="string">&quot;,&quot;</span>  <span class="comment"># 分隔符，默认是逗号</span></span><br><span class="line">    <span class="comment"># quote_char =&gt; &quot;\&quot;&quot;  # 定义用于引用CSV字段的字符，默认是双引号</span></span><br><span class="line">    columns =&gt; [<span class="string">&quot;id&quot;</span>, <span class="string">&quot;code&quot;</span>, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;enname&quot;</span>, <span class="string">&quot;points&quot;</span>, <span class="string">&quot;creater&quot;</span>, <span class="string">&quot;updater&quot;</span>, <span class="string">&quot;money&quot;</span>, <span class="string">&quot;createtime&quot;</span>, <span class="string">&quot;updatetime&quot;</span>]  <span class="comment"># 指定列名</span></span><br><span class="line">    autodetect_column_names =&gt; <span class="literal">false</span>  <span class="comment"># 是否检测列名，默认为false</span></span><br><span class="line">    skip_header =&gt; <span class="literal">false</span>  <span class="comment"># 是否跳过第一行，和autodetect_column_names一起设置，全为true或false</span></span><br><span class="line">    autogenerate_column_names =&gt; <span class="literal">false</span>  <span class="comment"># 是否设置默认的列名，默认为true</span></span><br><span class="line">    convert =&gt; &#123;  <span class="comment"># 指定数据类型</span></span><br><span class="line">      <span class="string">&quot;money&quot;</span> =&gt; <span class="string">&quot;integer&quot;</span></span><br><span class="line">      <span class="string">&quot;createtime&quot;</span> =&gt; <span class="string">&quot;date&quot;</span></span><br><span class="line">      <span class="string">&quot;updatetime&quot;</span> =&gt; <span class="string">&quot;date&quot;</span></span><br><span class="line">    &#125;</span><br><span class="line">    skip_empty_columns =&gt; <span class="literal">false</span>  <span class="comment"># 跳过空列，默认false</span></span><br><span class="line">    skip_empty_rows =&gt; <span class="literal">false</span>  <span class="comment"># 跳过空行，默认false</span></span><br><span class="line">  &#125;</span><br><span class="line">  mutate &#123;</span><br><span class="line">    add_field =&gt; &#123;<span class="string">&quot;insert_time&quot;</span> =&gt; <span class="string">&quot;%&#123;@timestamp&#125;&quot;</span>&#125;  <span class="comment"># 新增字段，%&#123;已有字段&#125;可以引用现有字段</span></span><br><span class="line">    remove_field =&gt; [<span class="string">&quot;message&quot;</span>,<span class="string">&quot;@version&quot;</span>,<span class="string">&quot;updater&quot;</span>,<span class="string">&quot;host&quot;</span>,<span class="string">&quot;@timestamp&quot;</span>]  <span class="comment"># 删除字段</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">output &#123;</span><br><span class="line">  stdout&#123;</span><br><span class="line">  &#125;</span><br><span class="line">  elasticsearch &#123;</span><br><span class="line">    hosts =&gt; [<span class="string">&quot;node1:9200&quot;</span>,<span class="string">&quot;node2:9201&quot;</span>]  <span class="comment"># 输出到ES，和logstash.yml保持一致</span></span><br><span class="line">    index =&gt; <span class="string">&quot;coupon_product&quot;</span>  <span class="comment"># 索引名称</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>以下是传输从filebeat收集的数据，对数据内容使用grok解析，再对filebeat传过来的log.file.path和host.name使用ruby代码进行解析并传入数据。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">  beats &#123;</span><br><span class="line">    port =&gt; 5044  <span class="comment"># 指定filebeat的端口</span></span><br><span class="line">    host =&gt; logstash  <span class="comment"># 指定filebeat的主机</span></span><br><span class="line">    add_hostname =&gt; <span class="literal">true</span>  <span class="comment"># 将主机名添加到数据中，默认为false</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">filter &#123;</span><br><span class="line">  grok &#123;  <span class="comment"># 使用正则表达式解析日志</span></span><br><span class="line">    match =&gt; &#123;<span class="string">&quot;message&quot;</span> =&gt; <span class="string">&quot;%&#123;IP:ip&#125; %&#123;WORD:method&#125; %&#123;URIPATHPARAM:request&#125; %&#123;NUMBER:bytes&#125; %&#123;NUMBER:duration&#125;&quot;</span>&#125;</span><br><span class="line">  &#125;</span><br><span class="line">  ruby &#123;  <span class="comment"># 运行ruby代码，解析filebeat传过来的log.file.path和host.name</span></span><br><span class="line">    code =&gt; <span class="string">&quot;</span></span><br><span class="line"><span class="string">      path = event.get(&#x27;log&#x27;)[&#x27;file&#x27;][&#x27;path&#x27;]</span></span><br><span class="line"><span class="string">      hostname = event.get(&#x27;host&#x27;)[&#x27;name&#x27;]</span></span><br><span class="line"><span class="string">      event.set(&#x27;path&#x27;, path)</span></span><br><span class="line"><span class="string">      event.set(&#x27;hostname&#x27;, hostname)</span></span><br><span class="line"><span class="string">    &quot;</span></span><br><span class="line">  &#125;</span><br><span class="line">  mutate &#123;</span><br><span class="line">    add_field =&gt; &#123;<span class="string">&quot;insert_time&quot;</span> =&gt; <span class="string">&quot;%&#123;@timestamp&#125;&quot;</span>&#125;</span><br><span class="line">    remove_field =&gt; [<span class="string">&quot;ecs&quot;</span>,<span class="string">&quot;host&quot;</span>,<span class="string">&quot;@timestamp&quot;</span>,<span class="string">&quot;agent&quot;</span>,<span class="string">&quot;log&quot;</span>,<span class="string">&quot;@version&quot;</span>,<span class="string">&quot;input&quot;</span>,<span class="string">&quot;tags&quot;</span>,<span class="string">&quot;message&quot;</span>]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">output &#123;</span><br><span class="line">  stdout&#123;</span><br><span class="line">  &#125;</span><br><span class="line">  elasticsearch &#123;</span><br><span class="line">    hosts =&gt; [<span class="string">&quot;node1:9200&quot;</span>,<span class="string">&quot;node2:9201&quot;</span>]  <span class="comment"># 输出到ES，和logstash.yml保持一致</span></span><br><span class="line">    index =&gt; <span class="string">&quot;filebeat_logstash&quot;</span>  <span class="comment"># 索引名称</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>关于一个Logstash进程的运行则需要使用bin目录下的logstash命令加-f参数制定配置文件进行。当我们修改配置文件之后需要杀死进程重新启动Logstash才可以更新处理流程，–config.reload.automatic参数可以使Logstash自动嗅探配置文件的变化并对此进程的收集和处理进行纠正，默认嗅探时间是15s。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">logstash -f ./config/filebeat.conf --config.reload.automatic</span><br></pre></td></tr></table></figure><h3 id="Logstash和Flume相比"><a href="#Logstash和Flume相比" class="headerlink" title="Logstash和Flume相比"></a><strong>Logstash和Flume相比</strong></h3><ul><li>Logstash比较偏重于字段的预处理，在异常情况下可能会出现数据丢失，只是在运维日志场景下，一般认为这个可能不重要；而Flume偏重数据的传输，几乎没有数据的预处理，仅仅是数据的产生，封装成event然后传输；传输的时候flume比logstash多考虑了一些可靠性。因为数据会持久化在channel中，数据只有存储在下一个存储位置（可能是最终的存储位置，如HDFS；也可能是下一个Flume节点的channel），数据才会从当前的channel中删除。这个过程是通过事务来控制的，这样就保证了数据的可靠性。</li><li>Logstash有几十个插件，配置比较灵活；flume强调用户自定义开发，开发难度相对较高。</li><li>Logstash的input和filter还有output之间都存在buffer，进行缓冲；Flume直接使用channel做持久化。</li><li>Logstash性能以及资源消耗比较严重，且不支持缓存。</li><li>综上所述，Logstash偏重于与ELK Stack结合使用，当然也可以output到kafka再做后续使用，侧重点是数据的预处理；而Flume虽然开发难度较高，基本不存在数据预处理，但在生产环境中比较安全，有分布式和channel的双重保障，侧重点是数据的传输。两者根据不同的使用环境做不同的选择。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ETL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker WordPress的Nginx应用</title>
      <link href="/2021/03/31/Software/Docker%20WordPress%E7%9A%84Nginx%E5%BA%94%E7%94%A8/"/>
      <url>/2021/03/31/Software/Docker%20WordPress%E7%9A%84Nginx%E5%BA%94%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<p>前一段时间用 Docker 在服务器部署了一个 WordPress 博客，部署起来倒很简单，Docker 镜像都是封装好拿来即用的，问题出现在前两天想在 Docker 中部署 ssl 的时候。</p><span id="more"></span><p>我先是在阿里云申请了域名和免费一年的ssl，随后部署的时候就出现问题了，首先我用 Nginx 在服务器上直接部署 ssl，它显示 WordPress 和 Nginx 监听的端口冲突，失败；接下来我直接在 WordPress 所在的容器进行 ssl 部署，用的是容器自带的 Apache，由于配置文件的原因和我所申请的 ssl 是指定的域名导致一直不能成功，失败；然后我又想着把 WordPress 和 Nginx 全部装在服务器上，在服务器完成搭建 WordPress 和配置 ssl 的过程，结果直接卡在了 WordPress 搭建这里，失败；无奈最后服务器端 iptables 和 Docker 等一众服务全都出现问题，不能重启也找不到 docker. service，只能将云盘清空重新将 WordPress 部署在 Docker 中，再将以前备份的数据库恢复，万幸是恢复如初了，再没敢动配置 ssl 这个心思。</p><p>昨晚把这个事仔细想了想，有两个方法可以实现这个需求，第一是使用 Docker 内部的 Apache 继续配置，在 Docker 中就只是传入 pem 和 key 文件再配置 conf 文件就行了，上次是由于没了解各参数所代表的意思，跟着网上的教程乱配一通：还有一种方法是再开一个 Nginx 容器，监听服务器的 80 和 443 端口，再转发给 WordPress 对外暴露的 3344 和 4455 端口，实现一个服务器内部端口转发的功能，这样只暴露 Nginx 的 80 和 443 端口就可以了，其他端口都是内部使用。</p><p>当然这只是个人想法，还不确定能不能实现，我对 Nginx 这个做法比较感兴趣，但确定的是再也不敢直接在服务器端尝试了，有时间了重新搭建一个 WordPress 容器测试没问题了再应用在生产中，不然又得经历一次清盘操作了。</p>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Filebeat简单应用</title>
      <link href="/2021/03/31/Software/Filebeat%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8/"/>
      <url>/2021/03/31/Software/Filebeat%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<p>Filebeat 是一个轻量级的部署应用简单的数据收集工具，一般应用在日志收集场景下。从配置文件的构成来看，分为了三部分，inputs、outputs和其他全局配置，其中 inputs 模块接受的数据来源常见的有 kafka、redis、本地文件和 TCP（监听一个服务器端口传送过来的数据），outputs 模块则可以支持传输到 ES、Logstash、kafka、redis、命令行和本地文件。</p><span id="more"></span><p>Filebeat 由两个主要组件构成： prospector 和 harvesters。这两类组件一起协同完成 Filebeat 的工作，从指定文件中把数据读取出来，然后发送事件数据到配置的 output中。Harvesters 负责进行单个文件的内容收集，在运行过程中，每一个 Harvester 会对一个文件逐行进行内容读取，并且把读写到的内容发送到配置的 output 中。Prospector 负责管理 Harvsters，并且找到所有需要进行读取的数据源。如果 input type 配置的是 log 类型，Prospector 将会去配置度路径下查找所有能匹配上的文件，然后为每一个文件创建一个 Harvster。</p><p>其中 Harvesters 会根据文件的状态来读取增量数据，所谓的状态就是每次读取之后会记录文件的偏移量，以保证下次直接从最新的数据行开始读取。同时还支持断点续传，在定义的输出被阻塞并且没有确认所有事件的情况下，Filebeat 将继续尝试发送事件，直到输出确认它已经接收到事件。如果 Filebeat 在发送事件的过程中关闭，它在关闭之前不会等待输出确认所有事件。任何发送到输出但在 Filebeat 关闭之前未确认的事件，将在 Filebeat 重新启动时再次发送。</p><p>本地日志传输到 ElasticSearch 或者 logstash（也可以去<a href="https://www.elastic.co/guide/en/beats/filebeat/current/index.html">官方文档</a>或者其他人整理好的<a href="https://cloud.tencent.com/developer/article/1006051">博客</a>查看）</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">filebeat.inputs:</span><br><span class="line">- <span class="built_in">type</span>: <span class="built_in">log</span></span><br><span class="line">  enabled: <span class="literal">true</span></span><br><span class="line">  paths:</span><br><span class="line">   - /usr/share/filebeat/data/*.<span class="built_in">log</span></span><br><span class="line"></span><br><span class="line">setup.dashboards.enabled: <span class="literal">true</span></span><br><span class="line">setup.kibana:</span><br><span class="line">  host: <span class="string">&quot;kibana:5601&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ElasticSearch</span></span><br><span class="line">output.elasticsearch:</span><br><span class="line">  hosts: [<span class="string">&quot;node1:9200&quot;</span>, <span class="string">&quot;node2:9201&quot;</span>, <span class="string">&quot;node3:9202&quot;</span>]</span><br><span class="line"><span class="comment"># Logstash</span></span><br><span class="line">output.logstash:</span><br><span class="line">  hosts: [<span class="string">&quot;localhost:5044&quot;</span>]</span><br></pre></td></tr></table></figure><p>传输到 ElasticSearch 的默认索引为 “%{[fields.log_type]}-%{[agent.version]}-%{+yyyy.MM.dd}” ，但是需要同时设置 <code>setup.template.name</code> 和 <code>setup.template.pattern</code> 参数，我还没搞明白咋设置。</p>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ETL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Docker Compose简单应用</title>
      <link href="/2021/03/20/Software/Docker%20Compose%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8/"/>
      <url>/2021/03/20/Software/Docker%20Compose%E7%AE%80%E5%8D%95%E5%BA%94%E7%94%A8/</url>
      
        <content type="html"><![CDATA[<p>Compose 是用于定义和运行多容器 Docker 应用程序的工具。通过 Compose，您可以使用 YML 文件来配置应用程序需要的所有服务。然后，使用一个命令，就可以从 YML 文件配置中创建并启动所有服务。常用的容器编码配置文件存放在<a href="https://gitee.com/wang749/docker-compose">Git仓库</a>。</p><span id="more"></span><h3 id="命令选项"><a href="#命令选项" class="headerlink" title="命令选项"></a><strong>命令选项</strong></h3><ul><li><code>-f, --file FILE</code> 指定使用的 Compose 模板文件，默认为 <code>docker-compose.yml</code>，可以多次指定。</li><li><code>-p, --project-name NAME</code> 指定项目名称，默认将使用所在目录名称作为项目名。</li><li><code>--verbose</code> 输出更多调试信息。</li><li><code>-v, --version</code> 打印版本并退出。</li></ul><h3 id="命令使用说明"><a href="#命令使用说明" class="headerlink" title="命令使用说明"></a><strong>命令使用说明</strong></h3><h4 id="build"><a href="#build" class="headerlink" title="build"></a><strong>build</strong></h4><p><code>docker-compose build [options] [SERVICE...]</code></p><p>构建（重新构建）项目中的服务容器。</p><p>服务容器一旦构建后，将会带上一个标记名，例如对于 web 项目中的一个 db 容器，可能是 web_db。</p><p>可以随时在项目目录下运行 <code>docker-compose build</code> 来重新构建服务。</p><p>选项包括：</p><ul><li><code>--force-rm</code> 删除构建过程中的临时容器。</li><li><code>--no-cache</code> 构建镜像过程中不使用 cache（这将加长构建过程）。</li><li><code>--pull</code> 始终尝试通过 pull 来获取更新版本的镜像。</li></ul><h4 id="config"><a href="#config" class="headerlink" title="config"></a>config</h4><p><code>docker-compose config docker-compose.yml</code></p><p>验证 Compose 文件格式是否正确，若正确则显示配置，若格式错误显示错误原因。</p><h4 id="down"><a href="#down" class="headerlink" title="down"></a>down</h4><p><code>docker-compose down</code></p><p>此命令将会停止 <code>up</code> 命令所启动的容器，并移除网络</p><h4 id="exec"><a href="#exec" class="headerlink" title="exec"></a>exec</h4><p><code>docker-compose exec container /bin/bash</code></p><p>进入指定的容器。</p><h4 id="help"><a href="#help" class="headerlink" title="help"></a>help</h4><p>获得一个命令的帮助。</p><h4 id="images"><a href="#images" class="headerlink" title="images"></a>images</h4><p><code>docker-compose images</code></p><p>列出 Compose 文件中包含的镜像。</p><h4 id="logs"><a href="#logs" class="headerlink" title="logs"></a>logs</h4><p><code>docker-compose logs --tail 100</code></p><p>查看服务容器的输出。默认情况下，docker-compose 将对不同的服务输出使用不同的颜色来区分。可以通过 <code>--no-color</code> 来关闭颜色。</p><h4 id="pause"><a href="#pause" class="headerlink" title="pause"></a>pause</h4><p><code>docker-compose pause</code></p><p>暂停一个服务容器。</p><h4 id="port"><a href="#port" class="headerlink" title="port"></a>port</h4><p><code>docker-compose port [options] SERVICE PRIVATE_PORT</code></p><p>打印某个容器端口所映射的公共端口。</p><ul><li><code>--protocol=proto</code> 指定端口协议，tcp（默认值）或者 udp。</li><li><code>--index=index</code> 如果同一服务存在多个容器，指定命令对象容器的序号（默认为 1）。</li></ul><h4 id="ps"><a href="#ps" class="headerlink" title="ps"></a>ps</h4><p><code>docker-compose ps [options] [SERVICE...]</code></p><p>列出所包含容器运行的所有进程。</p><h4 id="pull"><a href="#pull" class="headerlink" title="pull"></a>pull</h4><p><code>docker-compose pull [options] [SERVICE...]</code></p><p>拉取服务依赖的镜像。</p><ul><li><code>--ignore-pull-failures</code> 忽略拉取镜像过程中的错误。</li></ul><h4 id="push"><a href="#push" class="headerlink" title="push"></a>push</h4><p>推送服务依赖的镜像到 Docker 镜像仓库。</p><h4 id="restart"><a href="#restart" class="headerlink" title="restart"></a>restart</h4><p><code>docker-compose restart</code></p><p>重启项目中的服务。</p><h4 id="rm"><a href="#rm" class="headerlink" title="rm"></a>rm</h4><p><code>docker-compose rm [options] [SERVICE...]</code></p><p>删除所有（停止状态的）服务容器。推荐先执行 <code>docker-compose stop</code> 命令来停止容器，不用的话直接使用<code>down</code>。</p><ul><li><code>-f, --force</code> 强制直接删除，包括非停止状态的容器。一般尽量不要使用该选项。</li><li><code>-v</code> 删除容器所挂载的数据卷。</li></ul><h4 id="run"><a href="#run" class="headerlink" title="run"></a>run</h4><p><code>docker-compose run [options] [-p PORT...] [-e KEY=VAL...] SERVICE [COMMAND] [ARGS...]</code></p><p>在指定服务上执行一个命令。</p><p>例如：</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker-compose run ubuntu ping docker.com</span><br></pre></td></tr></table></figure><p>将会启动一个 ubuntu 服务容器，并执行 <code>ping docker.com</code> 命令。</p><p>默认情况下，如果存在关联，则所有关联的服务将会自动被启动，除非这些服务已经在运行中。</p><p>该命令类似启动容器后运行指定的命令，相关卷、链接等等都将会按照配置自动创建。</p><p>两个不同点：</p><ul><li>给定命令将会覆盖原有的自动运行命令；</li><li>不会自动创建端口，以避免冲突。</li></ul><p>如果不希望自动启动关联的容器，可以使用 <code>--no-deps</code> 选项，例如</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker-compose run --no-deps web python manage.py shell</span><br></pre></td></tr></table></figure><p>将不会启动 web 容器所关联的其它容器。</p><p>选项：</p><ul><li><code>-d</code> 后台运行容器。</li><li><code>--name NAME</code> 为容器指定一个名字。</li><li><code>--entrypoint CMD</code> 覆盖默认的容器启动指令。</li><li><code>-e KEY=VAL</code> 设置环境变量值，可多次使用选项来设置多个环境变量。</li><li><code>-u, --user=&quot;&quot;</code> 指定运行容器的用户名或者 uid。</li><li><code>--no-deps</code> 不自动启动关联的服务容器。</li><li><code>--rm</code> 运行命令后自动删除容器，<code>d</code> 模式下将忽略。</li><li><code>-p, --publish=[]</code> 映射容器端口到本地主机。</li><li><code>--service-ports</code> 配置服务端口并映射到本地主机。</li><li><code>-T</code> 不分配伪 tty，意味着依赖 tty 的指令将无法运行。</li></ul><h4 id="scale"><a href="#scale" class="headerlink" title="scale"></a>scale</h4><p><code>docker-compose scale [options] [SERVICE=NUM...]</code>。</p><p>设置指定服务运行的容器个数。</p><p>通过 <code>service=num</code> 的参数来设置数量。例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ docker-compose scale web=3 db=2</span><br></pre></td></tr></table></figure><p>将启动 3 个容器运行 web 服务，2 个容器运行 db 服务。</p><p>一般的，当指定数目多于该服务当前实际运行容器，将新创建并启动容器；反之，将停止容器。</p><p>选项：</p><ul><li><code>-t, --timeout TIMEOUT</code> 停止容器时候的超时（默认为 10 秒）。</li></ul><h4 id="start"><a href="#start" class="headerlink" title="start"></a>start</h4><p><code>docker-compose start [SERVICE...]</code></p><p>启动已经存在的服务容器。</p><h4 id="stop"><a href="#stop" class="headerlink" title="stop"></a>stop</h4><p><code>docker-compose stop [options] [SERVICE...]</code></p><p>停止已经处于运行状态的容器，但不删除它。通过 <code>docker-compose start</code> 可以再次启动这些容器。</p><h4 id="top"><a href="#top" class="headerlink" title="top"></a>top</h4><p>查看各个服务容器内运行的进程。</p><h4 id="unpause"><a href="#unpause" class="headerlink" title="unpause"></a>unpause</h4><p><code>docker-compose unpause [SERVICE...]</code></p><p>恢复处于暂停状态中的服务。</p><h4 id="up"><a href="#up" class="headerlink" title="up"></a>up</h4><p><code>docker-compose up -d</code></p><p>该命令十分强大，它将尝试自动完成包括构建镜像，（重新）创建服务，启动服务，并关联服务相关容器的一系列操作。</p><p>链接的服务都将会被自动启动，除非已经处于运行状态。</p><p>可以说，大部分时候都可以直接通过该命令来启动一个项目。</p><p>默认情况，<code>docker-compose up</code> 启动的容器都在前台，控制台将会同时打印所有容器的输出信息，可以很方便进行调试。</p><p>当通过 <code>Ctrl-C</code> 停止命令时，所有容器将会停止。</p><p>如果使用 <code>docker-compose up -d</code>，将会在后台启动并运行所有的容器。一般推荐生产环境下使用该选项。</p><p>默认情况，如果服务容器已经存在，<code>docker-compose up</code> 将会尝试停止容器，然后重新创建（保持使用 <code>volumes-from</code> 挂载的卷），以保证新启动的服务匹配 <code>docker-compose.yml</code> 文件的最新内容。如果用户不希望容器被停止并重新创建，可以使用 <code>docker-compose up --no-recreate</code>。这样将只会启动处于停止状态的容器，而忽略已经运行的服务。如果用户只想重新部署某个服务，可以使用 <code>docker-compose up --no-deps -d &lt;SERVICE_NAME&gt;</code> 来重新创建服务并后台停止旧服务，启动新服务，并不会影响到其所依赖的服务。</p><p>选项：</p><ul><li><code>-d</code> 在后台运行服务容器。</li><li><code>--no-color</code> 不使用颜色来区分不同的服务的控制台输出。</li><li><code>--no-deps</code> 不启动服务所链接的容器。</li><li><code>--force-recreate</code> 强制重新创建容器，不能与 <code>--no-recreate</code> 同时使用。</li><li><code>--no-recreate</code> 如果容器已经存在了，则不重新创建，不能与 <code>--force-recreate</code> 同时使用。</li><li><code>--no-build</code> 不自动构建缺失的服务镜像。</li><li><code>-t, --timeout TIMEOUT</code> 停止容器时候的超时（默认为 10 秒）。</li></ul><h4 id="version"><a href="#version" class="headerlink" title="version"></a>version</h4><p><code>docker-compose version</code></p><p>打印版本信息。</p>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>别绷着了</title>
      <link href="/2021/03/17/Mind/%E5%88%AB%E7%BB%B7%E7%9D%80%E4%BA%86/"/>
      <url>/2021/03/17/Mind/%E5%88%AB%E7%BB%B7%E7%9D%80%E4%BA%86/</url>
      
        <content type="html"><![CDATA[<p>在往地铁站走的路上，我想了一大堆描述我此时状态的话语，结果开始写的时候却无法把它归纳好写出来，但我不喜欢现在的状态是真的。</p><span id="more"></span><p>忘了是从什么时候开始，或许是来了北京之后吧，一直有一种被追着跑的感觉，仿佛自己一天不“努力”，就离自己想要的生活遥远了几分，就不能过自己想过的日子，就不能过旭旭想要的日子，但我现在能想到的好日子完全是建立在金钱之上的，至于自己未来的状态，我完全想象不出来。</p><p>变成现在这样有一部分原因是因为生活节奏快，工作虽然不多，但在家里和在路上却很忙，有一大堆要做的事，去了公司也要在工作之余学习，当然这些都不重要，重要的是自己的心态有问题。第二个原因就是自己的心态了，我总是很着急，上班路上想着走快点，到了公司却是先摸鱼；工作想着干快点，却总是因为急躁没找对合适的解决办法，白白浪费了时间；学习更想快点，但却是眼前的还没学扎实，就想着去学自己感兴趣的技术；最重要的是嗓子眼一直绷着一根筋，仿佛这根筋断了或者松了就会要了我的命，哪怕没什么事要做的时候也得绷着，绷得我叫苦不堪，绷得我精疲力竭。</p><p>说了这么多，大体上就一个原因：想的太多。不能说我做的少，我还绷着呢，但这种绷着的状态实际上给自己带来了多少收获，取得了哪些进步，却没什么指标去衡量，所以我认为这种打着“努力”的旗号实则以透支自己的精力为代价最终还没多少进步的行为就是病态的，有问题的，需要纠正，需要停止。</p><p>人生不是拉力赛，人生是场马拉松，慢点，再慢点，松掉绷着的那根弦，晚几分钟到公司没什么，工作多做会也没什么，今天少学一个章节这些都没什么，活好当下，关注内心的成长，注重效率的同时别忘了质量。</p>]]></content>
      
      
      <categories>
          
          <category> Mind </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mind </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>努力了然后呢</title>
      <link href="/2021/03/15/Mind/%E5%8A%AA%E5%8A%9B%E4%BA%86%E7%84%B6%E5%90%8E%E5%91%A2/"/>
      <url>/2021/03/15/Mind/%E5%8A%AA%E5%8A%9B%E4%BA%86%E7%84%B6%E5%90%8E%E5%91%A2/</url>
      
        <content type="html"><![CDATA[<p>今天看到一篇文章《该摸鱼摸鱼，该拼命拼命》，看完了后有点想法，我在拼命的时候仍然想着摸鱼，摸鱼的时候总会有点负罪感，想着自己是不是该去拼命了。</p><span id="more"></span><p>所以有了这篇文章，我一直在努力，旭旭也这么觉得，我比大多数人在这方面付出的总多那么一点，但收获着实不太满意。以前没想过这个事情，甚至连收获如何也没有一个标准，努力已经那么辛苦了，还想那么多干嘛。</p><p>直到今天看了这篇文章，我才知道其中原因所在，产生这种状况的原因是因为内卷，想要的太多，欲望太多，就想赶快把所有需要的都装进脑子，在装箱的过程中你会想方设法的去摸鱼，这是因为压力太大了，你太累了，总觉得摸会鱼就是赚了，而没去关注努力的成效如何。所以第一要衡量你努力的成果，所做的这些是否有效；第二要想明白你需要的不是利用上下班路上和上班之余挤时间去学习去拼命，而是常态化，并且有成效，如若不然，那你的努力只是隐式的自我安慰罢了。</p><p>这就引出了另外一个重要的因素–专注，专注去做手头的事，专注努力，把现在正在做的做好，再去想其他的事情；第二个因素–复盘，这件事我起码尝试了三次，但每次都无疾而终，不能把它当做习惯融入自己的行为当中，那就降低要求，只做到衡量成果，就是衡量努力的成效，然后促使自己对努力的过程做出改变。</p>]]></content>
      
      
      <categories>
          
          <category> Mind </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mind </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常见错误统计</title>
      <link href="/2020/11/07/Language/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E7%BB%9F%E8%AE%A1/"/>
      <url>/2020/11/07/Language/%E5%B8%B8%E8%A7%81%E9%94%99%E8%AF%AF%E7%BB%9F%E8%AE%A1/</url>
      
        <content type="html"><![CDATA[<p>常见错误统计，目前主要是Hive。</p><span id="more"></span><h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3><h4 id="Tez计算引擎导致的错误"><a href="#Tez计算引擎导致的错误" class="headerlink" title="Tez计算引擎导致的错误"></a>Tez计算引擎导致的错误</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Status: Failed</span><br><span class="line">Invalid event on Vertex vertex_1622365407812_0028_1_00 [Map 1]</span><br><span class="line">FAILED: Execution Error, <span class="built_in">return</span> code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Invalid event on Vertex vertex_1622365407812_0028_1_00 [Map 1]</span><br></pre></td></tr></table></figure><p>当在运行sql脚本或者sql语句的时候发生以上错误，而且yarn和nodemanageer以及任务日志基本看不出来哪里出了问题的时候，把Hive配置文件hive-site.xml中的计算引擎由tez改为mr（MapReduce）。</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.execution.engine<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>mr<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="MySQL驱动版本不一致"><a href="#MySQL驱动版本不一致" class="headerlink" title="MySQL驱动版本不一致"></a>MySQL驱动版本不一致</h4><p><code>Loading class &#39;com.mysql.jdbc.Driver&#39;. This is deprecated. The new driver class is &#39;com.mysql.cj.jdbc.Driver&#39;. </code></p><p>一般是链接MySQL的驱动版本太高而配置文件中的配置又太低的缘故，建议在Hive2.x中仍然使用<code>mysql-connect-5.1.x</code>版本的驱动，并且在配置文件中使用<code>com.mysql.jdbc.Driver</code>，Hive3.x版本使用<code>mysql-connect-8.0.x</code>版本的驱动，并且在配置文件中使用<code>com.mysql.cj.jdbc.Driver</code>，必须要对应，不然会报错</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">&lt;!-- Hive2.x对应mysql-connect-5.1.x驱动 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- Hive3.x对应mysql-connect-8.0.x驱动 --&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.cj.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Driver class name for a JDBC metastore<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="连接DataGrip错误"><a href="#连接DataGrip错误" class="headerlink" title="连接DataGrip错误"></a>连接DataGrip错误</h4><p>新版DataGrip连接的时候不需要输入用户名和密码，网上搜出来的都是旧版的，可以参考所需的JAR包，一般出现错误都是JAR包不兼容，在Hive主机上使用beeline测试Hive可以连接后就不是Hive的问题了。不要使用DataGrip建议的包，自己下载需要的JAR包并导入，支持Hive1.2.1的包在服务器上。</p><h4 id="拼接两个无关的查询结果"><a href="#拼接两个无关的查询结果" class="headerlink" title="拼接两个无关的查询结果"></a>拼接两个无关的查询结果</h4><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 前提是两个查询结果的行数相同，而且必须group by，否则会报错</span></span><br><span class="line"><span class="keyword">select</span></span><br><span class="line">    t1.id, </span><br><span class="line">    t1.name,</span><br><span class="line">    t2.id, </span><br><span class="line">    t2.name</span><br><span class="line"><span class="keyword">from</span> t1 </span><br><span class="line"><span class="keyword">left</span> <span class="keyword">join</span> t2 </span><br><span class="line"><span class="keyword">on</span> <span class="number">1</span><span class="operator">=</span><span class="number">1</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> t2.id,t2.name;</span><br></pre></td></tr></table></figure><h4 id="When-importing-query-results-in-parallel…"><a href="#When-importing-query-results-in-parallel…" class="headerlink" title="When importing query results in parallel…"></a>When importing query results in parallel…</h4><p><code>When importing query results in parallel, you must specify --split-by.</code></p><p>当<code>--num-mappers</code>设置的值大于1时，<code>--split-by</code>必须设置字段（需要是 int 类型的字段），如果不是 int类型的字段，则需要加上参数，例如<code>--split-by columnName</code></p><h4 id="Zero-date-value-prohibited"><a href="#Zero-date-value-prohibited" class="headerlink" title="Zero date value prohibited"></a>Zero date value prohibited</h4><p>这是因为数据库中的<code>0000-00-00 00:00:00</code>在Hive中是被禁止的，所以需要在JDBC中加入参数<code>zeroDateTimeBehavior=convertToNull</code></p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">报错</span></span><br><span class="line">Caused by: com.mysql.cj.exceptions.DataReadException: Zero date value prohibited</span><br><span class="line"><span class="meta"># </span><span class="language-bash">修改参数</span></span><br><span class="line">jdbc:mysql://192.168.1.111:3306/shanhy_demo?zeroDateTimeBehavior=convertToNull</span><br></pre></td></tr></table></figure><h4 id="java-sql-SQLException-HOUR-OF-DAY-0-gt-1"><a href="#java-sql-SQLException-HOUR-OF-DAY-0-gt-1" class="headerlink" title="java.sql.SQLException: HOUR_OF_DAY: 0 -&gt; 1"></a>java.sql.SQLException: HOUR_OF_DAY: 0 -&gt; 1</h4><p>查了半天也没查出来原因，应该是时区的原因，但是有解决办法</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">报错</span></span><br><span class="line">Caused by: java.sql.SQLException: HOUR_OF_DAY: 0 -&gt; 1</span><br><span class="line">Caused by: com.mysql.cj.exceptions.WrongArgumentException: HOUR_OF_DAY: 0 -&gt; 1</span><br><span class="line">Caused by: java.lang.IllegalArgumentException: HOUR_OF_DAY: 0 -&gt; 1</span><br><span class="line"><span class="meta"># </span><span class="language-bash">修改参数</span></span><br><span class="line">jdbc:mysql://192.168.1.111:3306/shanhy_demo?zeroDateTimeBehavior=convertToNull&amp;serverTimezone=GMT%2B8</span><br></pre></td></tr></table></figure><h4 id="ParseException-line-1-40-cannot-recognize-input-near…"><a href="#ParseException-line-1-40-cannot-recognize-input-near…" class="headerlink" title="ParseException line 1:40 cannot recognize input near…"></a>ParseException line 1:40 cannot recognize input near…</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">报错</span></span><br><span class="line">FAILED: ParseException line 1:40 cannot recognize input near &#x27;with&#x27; &#x27;tb&#x27; &#x27;as&#x27; in statement</span><br><span class="line"><span class="meta"># </span><span class="language-bash">SQL语法问题，with新建临时表之后，后面必须接select语句，而且with as语句后不能加分号，否则就会报错</span></span><br><span class="line">with mm as (</span><br><span class="line">  select </span><br><span class="line">    &#x27;$do_date&#x27; report_time, </span><br><span class="line">    source, </span><br><span class="line">    count(1) member_all, </span><br><span class="line">    sum(if(newoldmember=&#x27;新会员&#x27;,1,0)) new_member_all, </span><br><span class="line">    sum(if(newoldmember=&#x27;老会员&#x27;,1,0)) old_member_all</span><br><span class="line">  from dws_Member</span><br><span class="line">  group by source</span><br><span class="line">)</span><br><span class="line">select * from mm;</span><br></pre></td></tr></table></figure><h4 id="return-code-1-from-org-apache-hadoop-hive-ql-exec-tez-TezTask"><a href="#return-code-1-from-org-apache-hadoop-hive-ql-exec-tez-TezTask" class="headerlink" title="return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask"></a>return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask</h4><p>这是使用tez引擎时出现的错误，产生这种问题的原因有很多</p><ul><li>tez-site.xml文件有放在hadoop配置文件目录下的，有让放在hive配置文件目录下的，最后还是两个文件夹都放了才解决这个问题。</li><li>hive、tez和hadoop中的jar包彼此冲突，解决方法：将tez目录中lib目录下的两个hadoop相关的包改成和hadoop目录下一样的，然后再将tez目录下tez相关的包复制到hive安装目录下的lib目录下。</li><li>hdfs根目录下的tmp文件夹最好给上所有权限</li></ul><h4 id="java-lang-NoSuchMethodError-com-google-common-base-Preconditions-checkArgument"><a href="#java-lang-NoSuchMethodError-com-google-common-base-Preconditions-checkArgument" class="headerlink" title="java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument"></a>java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument</h4><p>错误原因：系统找不到这个类所在的jar包或者jar包的版本不一样系统不知道使用哪个。hive启动报错的原因是后者</p><p>解决办法：<br>1、com.google.common.base.Preconditions.checkArgument这个类所在的jar包为：guava.jar<br>2、hadoop-3.2.1（路径：hadoop\share\hadoop\common\lib）中该jar包为 guava-27.0-jre.jar；而hive-3.1.2(路径：hive&#x2F;lib)中该jar包为guava-19.0.1.jar<br>3、将jar包变成一致的版本：删除hive中低版本jar包，将hadoop中高版本的复制到hive的lib中。</p><p>再次启动问题得到解决！</p><h4 id="User-root-is-not-allowed-to-impersonate-root"><a href="#User-root-is-not-allowed-to-impersonate-root" class="headerlink" title="User: root is not allowed to impersonate root"></a>User: root is not allowed to impersonate root</h4><p>beeline通过jdbc登录hive的时候需要输入服务器用户名和密码，报这个错需要在hadoop中core-site.xml配置文件中修改两个参数，而且记得要分发到所有集群并重启hadoop集群</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.hosts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.proxyuser.root.groups<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">      <span class="tag">&lt;<span class="name">value</span>&gt;</span>*<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="错误-找不到或无法加载主类-org-apache-hadoop-mapreduce-v2-app-MRAppMaster"><a href="#错误-找不到或无法加载主类-org-apache-hadoop-mapreduce-v2-app-MRAppMaster" class="headerlink" title="错误: 找不到或无法加载主类 org.apache.hadoop.mapreduce.v2.app.MRAppMaster"></a>错误: 找不到或无法加载主类 org.apache.hadoop.mapreduce.v2.app.MRAppMaster</h4><p>输入命令 hadoop classpath，将输出的内容直接复制到yarn-site.xml文件中：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.application.classpath<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/hadoop3.1.2/etc/hadoop:/usr/local/hadoop3.1.2/share/hadoop/common/lib/*:/usr/local/hadoop3.1.2/share/hadoop/common/*:/usr/local/hadoop3.1.2/share/hadoop/hdfs:.....<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="Hive元数据中文乱码"><a href="#Hive元数据中文乱码" class="headerlink" title="Hive元数据中文乱码"></a>Hive元数据中文乱码</h4><p>因为初始化 MySQL 元数据库的时候默认的编码格式是 latin1，必须修改为 UTF-8 才可以正常显示，但是只需修改特定的字段即可。修改之前的数据还是乱码，不起作用。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#修改字段注释字符集</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> COLUMNS_V2 modify <span class="keyword">column</span> COMMENT <span class="type">varchar</span>(<span class="number">256</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line">#修改表注释字符集</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> TABLE_PARAMS modify <span class="keyword">column</span> PARAM_VALUE <span class="type">varchar</span>(<span class="number">20000</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line">#修改分区参数，支持分区建用中文表示</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> PARTITION_PARAMS modify <span class="keyword">column</span> PARAM_VALUE <span class="type">varchar</span>(<span class="number">20000</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> PARTITION_KEYS modify <span class="keyword">column</span> PKEY_COMMENT <span class="type">varchar</span>(<span class="number">20000</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line">#修改索引名注释，支持中文表示</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> INDEX_PARAMS modify <span class="keyword">column</span> PARAM_VALUE <span class="type">varchar</span>(<span class="number">4000</span>) <span class="type">character</span> <span class="keyword">set</span> utf8;</span><br><span class="line">#修改视图，支持视图中文</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> TBLS modify <span class="keyword">COLUMN</span> VIEW_EXPANDED_TEXT mediumtext <span class="type">CHARACTER</span> <span class="keyword">SET</span> utf8;</span><br><span class="line"><span class="keyword">ALTER</span> <span class="keyword">TABLE</span> TBLS modify <span class="keyword">COLUMN</span> VIEW_ORIGINAL_TEXT mediumtext <span class="type">CHARACTER</span> <span class="keyword">SET</span> utf8;</span><br></pre></td></tr></table></figure><h4 id="FAILED-Execution-Error-return-code-2-from-org-apache-hadoop-hive-ql-exec-mr-MapRedTask"><a href="#FAILED-Execution-Error-return-code-2-from-org-apache-hadoop-hive-ql-exec-mr-MapRedTask" class="headerlink" title="FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask"></a>FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask</h4><p>执行 hive mapreduce 任务报这个错误，查看 hive 日志发现其实主要原因是<code>java.lang.OutOfMemoryError: Java heap space</code>这个错误，找了一圈发现设置<code>set io.sort.mb=10;</code>即可解决，默认给的是100，指定了排序使用的内存，大的内存可以加快 job 的处理速度。</p><h4 id="Hive-建表后直接将数据导入-HDFS-查不到数据（DataX-导入到-HDFS）"><a href="#Hive-建表后直接将数据导入-HDFS-查不到数据（DataX-导入到-HDFS）" class="headerlink" title="Hive 建表后直接将数据导入 HDFS 查不到数据（DataX 导入到 HDFS）"></a>Hive 建表后直接将数据导入 HDFS 查不到数据（DataX 导入到 HDFS）</h4><p>如果有分区的话，将数据导入分区所在目录下，再使用<code>alter table test if not exists add partition(year=&#39;2021&#39;,month=&#39;10&#39;,day=&#39;12&#39;)</code>新增分区，其实就是同步到元数据库。如果表没有分区，那么就需要使用语句将导入进来的数据再插入到表中：<code>insert overwrite table test select * from test;</code>，如果不是全量数据，则需要加上查询语句：<code>insert into table test select * from test where ctime = &#39;2021-10-12&#39;;</code></p><h3 id="Linux"><a href="#Linux" class="headerlink" title="Linux"></a>Linux</h3><h4 id="x2F-bin-x2F-bash-M-坏的解释器"><a href="#x2F-bin-x2F-bash-M-坏的解释器" class="headerlink" title="&#x2F;bin&#x2F;bash^M: 坏的解释器"></a>&#x2F;bin&#x2F;bash^M: 坏的解释器</h4><p>一个linux的shell脚本在执行的时候出现错误：<code>/bin/bash^M: 坏的解释器: 没有那个文件或目录</code></p><p>原因：这个文件在Windows 下编辑过，在Windows下每一行结尾是\n\r，而Linux下则是\n，所以才会有 多出来的\r。</p><p>解决：使用命令<code>sed -i &#39;s/\r$//&#39; 名称.sh</code>，上面的指令会把 <code>名称.sh</code> 中的\r 替换成空白！</p><h3 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h3><h4 id="datetime默认值设置0000-00-00失败"><a href="#datetime默认值设置0000-00-00失败" class="headerlink" title="datetime默认值设置0000-00-00失败"></a>datetime默认值设置0000-00-00失败</h4><p>mysql5.7之后版本的sql_mode默认使用：<br>ONLY_FULL_GROUP_BY, STRICT_TRANS_TABLES, NO_ZERO_IN_DATE, NO_ZERO_DATE, ERROR_FOR_DIVISION_BY_ZERO, NO_AUTO_CREATE_USER, and NO_ENGINE_SUBSTITUTION</p><p>其中NO_ZERO_IN_DATE, NO_ZERO_DATE两个选项禁止了0000这样的日期和时间。因此在mysql的配置文件中，重新设置sql_mode，去掉这两项就可以了。</p><p>修改my.cnf文件，在[mysqld]中添加</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sql-mode=ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION</span><br></pre></td></tr></table></figure><h3 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h3><h4 id="本地编译出现异常：com-sun-tools-javac-code-TypeTags"><a href="#本地编译出现异常：com-sun-tools-javac-code-TypeTags" class="headerlink" title="本地编译出现异常：com.sun.tools.javac.code.TypeTags"></a>本地编译出现异常：com.sun.tools.javac.code.TypeTags</h4><p>找到问题是因为jdk版本是11，lombok版本太低导致的，将lombok升级即可</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">dependency</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">groupId</span>&gt;</span>org.projectlombok<span class="tag">&lt;/<span class="name">groupId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">artifactId</span>&gt;</span>lombok<span class="tag">&lt;/<span class="name">artifactId</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">version</span>&gt;</span>1.18.4<span class="tag">&lt;/<span class="name">version</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">dependency</span>&gt;</span></span><br></pre></td></tr></table></figure><h4 id="IDEA-中-pom-xml-多个插件变红"><a href="#IDEA-中-pom-xml-多个插件变红" class="headerlink" title="IDEA 中 pom.xml 多个插件变红"></a>IDEA 中 pom.xml 多个插件变红</h4><p>检查 Maven 配置文件没问题之后，命令行输入<code>mvn -U idea:idea</code></p>]]></content>
      
      
      <categories>
          
          <category> Language </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Other </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>常用软件命令</title>
      <link href="/2020/10/18/Language/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%91%BD%E4%BB%A4/"/>
      <url>/2020/10/18/Language/%E5%B8%B8%E7%94%A8%E8%BD%AF%E4%BB%B6%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<p>常用软件的安装方法以及命令操作。</p><span id="more"></span><h2 id="MySQL"><a href="#MySQL" class="headerlink" title="MySQL"></a>MySQL</h2><h3 id="MySQL安装"><a href="#MySQL安装" class="headerlink" title="MySQL安装"></a>MySQL安装</h3><h4 id="删除老版本"><a href="#删除老版本" class="headerlink" title="删除老版本"></a>删除老版本</h4><ol><li><p>检查是否有MySQL相关包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa|grep -i mysql</span><br></pre></td></tr></table></figure></li><li><p>删除之前安装的MySQL<br>删除命令：rpm -e –nodeps 包名；<br>如果提示依赖包错误，则使用以下命令尝试：rpm -ev 包名 –nodeps；<br>如果提示错误：error: %preun(xxxxxx) scriptlet failed, exit status 1，则用以下命令尝试：rpm -e –noscripts 包名。</p></li><li><p>删除以前的MySQL目录、文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">find / -name mysql</span><br></pre></td></tr></table></figure><p>删除对应目录；<br>注意：卸载后&#x2F;etc&#x2F;my.cnf不会删除，需要进行手工删除：rm -rf &#x2F;etc&#x2F;my.cnf。</p></li><li><p>再次查找是否存在MySQL包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -qa|grep -i mysql</span><br></pre></td></tr></table></figure></li></ol><h4 id="安装前的准备"><a href="#安装前的准备" class="headerlink" title="安装前的准备"></a>安装前的准备</h4><ol><li><p>下载包</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">wget https://downloads.mysql.com/archives/get/p/23/file/mysql-8.0.21-linux-glibc2.12-x86_64.tar.xz</span><br></pre></td></tr></table></figure></li><li><p>解压包<br>解压包到指定位置并修改文件名：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">tar -xvf mysql-8.0.21-linux-glibc2.12-x86_64.tar.xz -C /usr/local</span><br><span class="line">cd /usr/local</span><br><span class="line">mv mysql-8.0.21-linux-glibc2.12-x86_64 mysql</span><br><span class="line">mkdir -p /usr/local/mysql/data</span><br></pre></td></tr></table></figure></li><li><p>创建用户组分配权限</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">groupadd mysql</span><br><span class="line">useradd -r -g mysql mysql</span><br><span class="line">chown mysql.mysql -R /usr/local/mysql</span><br></pre></td></tr></table></figure></li><li><p>配置文件<br>vim &#x2F;etc&#x2F;my.profile后添加以下内容，路径随安装路径改变</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">bind-address=0.0.0.0</span><br><span class="line">port=3306</span><br><span class="line">user=mysql</span><br><span class="line">basedir=/usr/local/mysql</span><br><span class="line">datadir=/usr/local/mysql/data</span><br><span class="line">socket=/tmp/mysql.sock</span><br><span class="line">log-error=/usr/local/mysql/data/mysql.err</span><br><span class="line">pid-file=/usr/local/mysql/data/mysql.pid</span><br><span class="line"><span class="meta">#</span><span class="language-bash">character config</span></span><br><span class="line">character_set_server=utf8mb4</span><br><span class="line">symbolic-links=0</span><br><span class="line">explicit_defaults_for_timestamp=true</span><br></pre></td></tr></table></figure></li></ol><h4 id="初始化登录"><a href="#初始化登录" class="headerlink" title="初始化登录"></a>初始化登录</h4><ol><li><p>初始化<br>首先安装依赖libaio，随后cd到mysql&#x2F;bin目录下初始化<br>初始化完成后需要去查看临时密码以供登录</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">yum install -y libaio</span><br><span class="line">cd /usr/local/mysql/bin</span><br><span class="line"><span class="meta"># </span><span class="language-bash">初始化</span></span><br><span class="line">./mysqld --defaults-file=/etc/my.cnf --basedir=/usr/local/mysql/ --datadir=/usr/local/mysql/data/ --user=mysql --initialize</span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看临时密码，在最后一行</span></span><br><span class="line">cat /usr/local/mysql/data/mysql.err</span><br><span class="line"><span class="meta"># </span><span class="language-bash">将mysql.server放置到 /etc/init.d/mysql中</span></span><br><span class="line">cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysql</span><br><span class="line"><span class="meta"># </span><span class="language-bash">启动Mysql</span></span><br><span class="line">service mysql start</span><br><span class="line"><span class="meta"># </span><span class="language-bash">使用临时密码登录MySQL</span></span><br><span class="line">./mysql -uroot -p</span><br></pre></td></tr></table></figure></li><li><p>登录修改密码</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># 修改密码</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">user</span> <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>；</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">user</span> <span class="string">&#x27;root&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> password expire never;</span><br><span class="line">flush privileges;</span><br><span class="line">quit;</span><br></pre></td></tr></table></figure><p>这时候需要退出登录，重新使用修改后的密码登录</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 使root可以在任何host访问</span><br><span class="line"><span class="keyword">update</span> <span class="keyword">user</span> <span class="keyword">set</span> host <span class="operator">=</span> <span class="string">&#x27;%&#x27;</span> <span class="keyword">where</span> <span class="keyword">user</span> <span class="operator">=</span> <span class="string">&#x27;root&#x27;</span>;</span><br><span class="line">flush privileges;</span><br><span class="line">quit;</span><br></pre></td></tr></table></figure></li></ol><h3 id="远程连接服务器MySQL"><a href="#远程连接服务器MySQL" class="headerlink" title="远程连接服务器MySQL"></a>远程连接服务器MySQL</h3><p>以阿里云Centos8云服务器为例，需要注意的是需要使用公网ip，若是个人电脑则需要两台电台在同一个网段下才可以。</p><ol><li><p>服务器端MySQL设置</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 登录mysql</span><br><span class="line">use mysql;</span><br><span class="line"># 更新host允许外部访问</span><br><span class="line"><span class="keyword">update</span> <span class="keyword">user</span> <span class="keyword">set</span> host<span class="operator">=</span><span class="string">&#x27;%&#x27;</span> <span class="keyword">where</span> <span class="keyword">user</span> <span class="operator">=</span><span class="string">&#x27;root&#x27;</span>;</span><br><span class="line"># 刷新权限</span><br><span class="line">flush privileges;</span><br></pre></td></tr></table></figure><p>以上是root账号和密码，若需新增用户需要分配对应的读写权限。</p></li><li><p>服务器端开放端口</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">查看防火墙状态</span></span><br><span class="line">systemctl status firewalld</span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看3306端口是否开放</span></span><br><span class="line">firewall-cmd --query-port=3306/tcp</span><br><span class="line"><span class="meta"># </span><span class="language-bash">开放防火墙端口3306</span></span><br><span class="line">firewall-cmd --zone=public --add-port=3306/tcp --permanent</span><br><span class="line"><span class="meta"># </span><span class="language-bash">重启防火墙</span></span><br><span class="line">systemctl restart firewalld</span><br></pre></td></tr></table></figure><p>这是Centos8的设置方法，若是其他系统可能方式不一样。</p></li><li><p>最重要的步骤<br>阿里云服务器需要在控制台设置安全组，开放3306端口，否则还是无法远程访问。</p></li></ol><h3 id="MySQL主从复制"><a href="#MySQL主从复制" class="headerlink" title="MySQL主从复制"></a>MySQL主从复制</h3><p>前提条件是从MySQL（slave）可以远程访问主MySQL（master）</p><h4 id="配置master机器"><a href="#配置master机器" class="headerlink" title="配置master机器"></a><strong>配置master机器</strong></h4><p>首先需要在master开启binlog日志，其次指定binlog日志目录（如果已有就不要改了）和server-id&#x3D;1；</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/my.cnf</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">添加以下代码到文件最后</span></span><br><span class="line">log-bin = &quot;/usr/local/mysql/bin-log&quot;</span><br><span class="line">server_id = 1</span><br><span class="line">binlog-do-db = mydb  # 复制哪个数据库</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">配置完成后需要重启MySQL</span></span><br><span class="line">service mysql restart</span><br></pre></td></tr></table></figure><p>接下来需要分配一个账号并给replacation slave权限（这里容易出错，要测试账号），查看master状态并记住File和Position对应的值，后面会用到。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># 查看log_bin状态</span><br><span class="line"><span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;%log_bin%&#x27;</span>;</span><br><span class="line"># 新建账号并分配权限</span><br><span class="line"></span><br><span class="line"># 查看master状态，注意File和Position</span><br><span class="line"><span class="keyword">show</span> master status;</span><br></pre></td></tr></table></figure><p>最后需要把现有数据库的数据迁移到slave数据库，position是偏移量，从现有数据库状态的基础上记录日志的行数位置，这里需要同版本同系统，若不相同就不要使用Mysqldump方法了</p><h4 id="配置slave"><a href="#配置slave" class="headerlink" title="配置slave"></a><strong>配置slave</strong></h4><p>首先需要在slave开启binlog日志，这里和master一样，其次指定binlog日志目录（如果已有就不要改了）和server-id&#x3D;2</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/my.cnf</span><br><span class="line"><span class="meta"># </span><span class="language-bash">添加以下代码到文件最后</span></span><br><span class="line">log-bin = &quot;/usr/local/mysql/bin-log&quot;</span><br><span class="line">server_id = 2</span><br><span class="line"><span class="meta"># </span><span class="language-bash">配置完成后需要重启MySQL</span></span><br><span class="line">service mysql restart</span><br></pre></td></tr></table></figure><p>接下来登录MySQL进行配置</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 关闭slave服务后再进行操作</span><br><span class="line">stop slave;</span><br><span class="line"># master_log_file和master_log_pos对应刚才master的file和position</span><br><span class="line">change master <span class="keyword">to</span> master_host<span class="operator">=</span><span class="string">&#x27;8.131.90.208&#x27;</span>,master_user<span class="operator">=</span><span class="string">&#x27;root&#x27;</span>,master_password<span class="operator">=</span><span class="string">&#x27;Wang1024...&#x27;</span>,master_log_file<span class="operator">=</span><span class="string">&#x27;mysql-bin.000002&#x27;</span>,master_log_pos<span class="operator">=</span><span class="number">775</span>;</span><br><span class="line"></span><br><span class="line"># 启用slave并检查状态</span><br><span class="line"><span class="keyword">start</span> slave;</span><br><span class="line"><span class="keyword">show</span> slave status;</span><br></pre></td></tr></table></figure><p>主从复制配置到这里就结束了，需要注意的地方有master账号的权限，尽量不要使用root账号；其次是server-id的设置；再然后是复制的前提是从服务器已存在目标数据库；最后是file和position的正确配置。</p><h4 id="故障处理"><a href="#故障处理" class="headerlink" title="故障处理"></a><strong>故障处理</strong></h4><p><strong>问题一</strong>：如果是表结构的问题，可以先停止服务，修改表结构和master相同再start slave</p><p><strong>问题二</strong>：出现“log event entry exceeded max_allowed_pack”错误。</p><p>如果在应用中使用大的BLOG列或者长字符串，那么在从服务器上回复时可能会出现“log event entryexceeded max_allowed_pack”错误，这是因为含有大文本的记录无法通过网络进行传输，解决方法是在主从服务器上添加max_allowed_packet参数（默认设置是1MB）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> variables <span class="keyword">like</span><span class="string">&#x27;MAX_ALLOWED_PACKET&#x27;</span>;</span><br><span class="line"><span class="keyword">set</span> @<span class="variable">@global</span>.max_allowed_packet<span class="operator">=</span><span class="number">16777216</span>;  # <span class="number">16</span>M</span><br></pre></td></tr></table></figure><p>同时，在my.cnf里设置max_allowed_packet&#x3D;16MB，数据库重新启动之后该参数将有效。</p><p><strong>问题三</strong>：多主复制时的自增长变量冲突问题。</p><p>大多数情况下使用一台主服务器对一台或者多台从服务器，但是在某些情况下可能会将多个服务器配置为复制主服务器，所以使用auto_increment时应采取特殊步骤以防止键值冲突，否则插入行时多个主服务器会试图使用相同的auto_increment值。</p><p>服务器变量auto_increment_increment和auto_increment_offset可以协调多主服务器复制和auto_increment列。</p><p>在多主服务器复制到从服务器过程中，迟早会发生主键冲突，为了解决这种情况，可以重新设置不同主服务器的这两个参数，比如在A数据库服务器上设置auto_increment_increment&#x3D;1、auto_increment_offset&#x3D;1，在B数据库服务器上设置auto_increment_increment&#x3D;1、auto_increment_offset&#x3D;0。</p><p><strong>问题四</strong>：利用pos偏移量解决小问题</p><p>不建议使用这种方法，可能会造成数据丢失，而且bin-log数据文件可能因为过大或者重启服务已变更为新的文件。</p><h4 id="切换主从服务器"><a href="#切换主从服务器" class="headerlink" title="切换主从服务器"></a><strong>切换主从服务器</strong></h4><p>在实际工作环境中，有时候遇到这样的问题：在一个工作环境中，有一个主数据库服务器A，两个从数据库服务器B、C同时指向主数据库服务器，当主数据库服务器A发生故障时，需要将其中的一个从数据库B服务器切换成主数据库，同时修改数据库C服务器的配置，使其指向新的主数据库B。</p><ol><li><p>首先要确保所有的从数据库都已经执行了relay log中的全部更新，查看从数据库的状态是否是Has read all relay log（是否更新都已经执行完成）。</p></li><li><p>在从数据库B上停止slave服务，然后执行reset master，重置成主数据库。（前提是确保&#x2F;etc&#x2F;my.cnf中设置了bin-log地址，并且重启服务器）</p></li><li><p>在从数据库B上添加具有replication权限的用户rep1，查询主数据库状态。在从数据库C上配置复制的参数。在从数据库C上执行show slave status命令，查看从数据库服务是否成功启动。</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># 查看relay log是否全部更新，查看Slave_SQL_Running_Statu字段是否为Slave has read <span class="keyword">all</span> relay log</span><br><span class="line"><span class="keyword">show</span> slave status;</span><br><span class="line"># B服务器停止slave，配置bin<span class="operator">-</span>log地址，重置成主数据库</span><br><span class="line">stop slave;</span><br><span class="line">reset master;</span><br><span class="line"><span class="keyword">show</span> master status;</span><br><span class="line"># 需要重新配置数据库C，方法就没什么差别了，停止服务，配置slave参数，启动服务</span><br><span class="line">change master <span class="keyword">to</span> master_host<span class="operator">=</span><span class="string">&#x27;8.131.90.208&#x27;</span>,master_user<span class="operator">=</span><span class="string">&#x27;root&#x27;</span>,master_password<span class="operator">=</span><span class="string">&#x27;Wang1024...&#x27;</span>,master_log_file<span class="operator">=</span><span class="string">&#x27;mysql-bin.000002&#x27;</span>,master_log_pos<span class="operator">=</span><span class="number">775</span>;</span><br></pre></td></tr></table></figure></li></ol><h4 id="多源复制"><a href="#多源复制" class="headerlink" title="多源复制"></a>多源复制</h4><p>MySQL 8.0添加了多源复制功能，可以实现多主服务器和一从服务器的复制。</p><ol><li>如果在主服务器进行了分库分表的操作，可以在从服务器进行数据汇总。为了实现后期的一些数据统计功能，往往需要把数据汇总在一起再统计。</li><li>在从服务器时对主服务器的数据进行备份，在MySQL 8.0之前每一个主服务器都需要一个从服务器，很容易造成资源浪费，同时也加大了数据库管理员的维护成本；MySQL 8.0则引入了多源复制，可以把多个主服务器的数据同步到一个从服务器进行备份。</li></ol><h2 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h2><h3 id="Redis安装"><a href="#Redis安装" class="headerlink" title="Redis安装"></a>Redis安装</h3><ol><li><p>下载解压</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">wget http://download.redis.io/releases/redis-6.0.9.tar.gz  # 下载</span><br><span class="line">tar -zxvf redis-6.0.9.tar.gz  # 解压</span><br><span class="line">make PREFIX=/usr/local/redis install  # 安装到/usr/local/redis</span><br><span class="line">cd /usr/local/redis/bin</span><br></pre></td></tr></table></figure><p>目录结构对应关系</p><table><thead><tr><th align="center">文件</th><th align="center">功能</th></tr></thead><tbody><tr><td align="center">redis-check-aof</td><td align="center">AOF文件修复工具</td></tr><tr><td align="center">redis-check-rdb</td><td align="center">redis-check-rdb</td></tr><tr><td align="center">redis-cli</td><td align="center">redis命令行客户端</td></tr><tr><td align="center">redis.conf</td><td align="center">redis配置文件</td></tr><tr><td align="center">redis-sentinal</td><td align="center">redis集群管理工具</td></tr><tr><td align="center">redis-server</td><td align="center">redis服务进程</td></tr><tr><td align="center">redis-benchmark</td><td align="center">redis性能测试工具</td></tr></tbody></table></li><li><p>基本配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">vim /usr/local/redis/bin/redis.conf</span><br><span class="line"><span class="meta"># </span><span class="language-bash">找到daemonize=no并改为<span class="built_in">yes</span></span></span><br></pre></td></tr></table></figure></li><li><p>启动Redis</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">启动Redis</span></span><br><span class="line">cd /usr/local/redis/bin</span><br><span class="line">./redis-server ./redis.conf</span><br><span class="line"><span class="meta"># </span><span class="language-bash">连接Redis</span></span><br><span class="line">./redis-cli</span><br><span class="line"><span class="meta"># </span><span class="language-bash">关闭Redis</span></span><br><span class="line">./redis-cli shutdown</span><br><span class="line"><span class="meta"># </span><span class="language-bash">强行终止Redis</span></span><br><span class="line">pkill redis-server</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">设置开机自启动</span></span><br><span class="line">vim /etc/rc.local</span><br><span class="line"><span class="meta"># </span><span class="language-bash">添加</span></span><br><span class="line">/usr/local/redis/bin/redis-server /usr/local/redis/bin/redis-conf</span><br></pre></td></tr></table></figure></li></ol><h3 id="Redis配置"><a href="#Redis配置" class="headerlink" title="Redis配置"></a>Redis配置</h3><ol><li><p>基础参数配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">是否在后台执行，<span class="built_in">yes</span>：后台运行；no：不是后台运行（老版本默认）</span></span><br><span class="line">daemonize yes</span><br><span class="line"><span class="meta"># </span><span class="language-bash">是否开启保护模式（默认开启）</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">要是配置里没有指定<span class="built_in">bind</span>和密码。开启该参数后，redis只会本地进行访问，拒绝外部访问。要是开启了密码  和<span class="built_in">bind</span>，可以开启。否  则最好关闭，设置为no。</span></span><br><span class="line">protected-mode yes</span><br><span class="line"><span class="meta"># </span><span class="language-bash">redis的进程文件</span></span><br><span class="line">pidfile /var/run/redis/redis-server.pid</span><br><span class="line"><span class="meta"># </span><span class="language-bash">redis监听的端口号</span></span><br><span class="line">port 6379</span><br><span class="line"><span class="meta"># </span><span class="language-bash">此参数确定了TCP连接中已完成队列的长度(默认511)</span></span><br><span class="line">tcp-backlog 511</span><br><span class="line"><span class="meta"># </span><span class="language-bash">指定redis只接收指定IP地址的请求</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">如需处理所有请求（远程访问） <span class="built_in">bind</span> 0.0.0.0</span></span><br><span class="line">bind 127.0.0.1</span><br><span class="line"><span class="meta"># </span><span class="language-bash">配置unix socket来让redis支持监听本地连接。</span></span><br><span class="line">unixsocket /var/run/redis/redis.sock</span><br><span class="line"><span class="meta"># </span><span class="language-bash">配置unix socket使用文件的权限</span></span><br><span class="line">unixsocketperm 700</span><br><span class="line"><span class="meta"># </span><span class="language-bash">此参数为设置客户端空闲超过<span class="built_in">timeout</span>，服务端会断开连接，为0则服务端不会主动断开连接，不能小于0。</span></span><br><span class="line">timeout 0</span><br><span class="line"><span class="meta"># </span><span class="language-bash">tcp keepalive参数。如果设置不为0，就使用配置tcp的SO_KEEPALIVE值，使用keepalive有两个好处:检测挂掉的对端。降低中间设备出问题而导致网络看似连接却已经与对端端口的问题。在Linux内核中，设置了keepalive，redis会定时给对端发送ack。检测到对端关闭需要两倍的设置值。</span></span><br><span class="line">tcp-keepalive 0</span><br><span class="line"><span class="meta"># </span><span class="language-bash">指定了服务端日志的级别</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">debug（适合开发、测试环境）</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">verbose（较少于debug级别 适合开发、测试环境）</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">notice（适合生产环境）</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">warn（只有非常重要的信息）</span></span><br><span class="line">loglevel notice</span><br><span class="line"><span class="meta"># </span><span class="language-bash">指定了记录日志的文件。空字符串的话，日志会打印到标准输出设备。后台运行的redis标准输出是/dev/null。</span></span><br><span class="line">logfile /var/log/redis/redis-server.log</span><br><span class="line"><span class="meta"># </span><span class="language-bash">是否打开记录syslog功能</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">syslog-enabled no</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">syslog的标识符</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">syslog-ident redis</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">日志的来源、设备</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">syslog-facility local0</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">数据库的数量（默认16）</span></span><br><span class="line">databases 16</span><br></pre></td></tr></table></figure></li><li><p>持久化配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">注释掉<span class="string">&quot;save&quot;</span>这一行配置项就可以让保存数据库功能失效</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">设置redis进行数据库镜像的频率。</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">900秒（15分钟）内至少1个key值改变（则进行数据库保存--持久化）</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">300秒（5分钟）内至少10个key值改变（则进行数据库保存--持久化）</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">60秒（1分钟）内至少10000个key值改变（则进行数据库保存--持久化）</span></span><br><span class="line">save 900 1</span><br><span class="line">save 300 10</span><br><span class="line">save 60 10000</span><br><span class="line"><span class="meta"># </span><span class="language-bash">当rdb持久化出现错误后，是否依然进行继续进行工作，</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash"><span class="built_in">yes</span>：不能进行工作，</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">no：可以继续进行工作，</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">可以通过info中的rdb_last_bgsave_status了解RDB持久化是否有错误</span></span><br><span class="line">stop-writes-on-bgsave-error yes</span><br><span class="line"><span class="meta"># </span><span class="language-bash">使用压缩rdb文件</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash"><span class="built_in">yes</span>：压缩，但是需要一些cpu的消耗</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">no：不压缩，需要更多的磁盘空间</span></span><br><span class="line">rdbcompression yes</span><br><span class="line"><span class="meta"># </span><span class="language-bash">是否校验rdb文件</span></span><br><span class="line">rdbchecksum yes</span><br><span class="line"><span class="meta"># </span><span class="language-bash">rdb文件的名称</span></span><br><span class="line">dbfilename dump.rdb</span><br><span class="line"><span class="meta"># </span><span class="language-bash">数据目录，数据库的写入会在这个目录。rdb、aof文件也会写在这个目录</span></span><br><span class="line">dir /var/lib/redis</span><br><span class="line"><span class="meta"># </span><span class="language-bash">默认redis使用的是rdb方式持久化，这种方式在许多应用中已经足够用了。但是redis如果中途宕机，会导致可能有几分钟的数据丢失，根据save来策略进行持久化，Append Only File是另一种持久化方式，可以提供更好的持久化特性。Redis会把每次写入的数据在接收后都写入 appendonly.aof 文件，每次启动时Redis都会先把这个文件的数据读入内存里，先忽略RDB文件。</span></span><br><span class="line">appendonly no</span><br><span class="line"><span class="meta"># </span><span class="language-bash">aof文件名</span></span><br><span class="line">appendfilename &quot;appendonly.aof&quot;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">aof持久化策略的配置</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">no表示不执行fsync，由操作系统保证数据同步到磁盘，速度最快。</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">always表示每次写入都执行fsync，以保证数据同步到磁盘。</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">everysec表示每秒执行一次fsync，可能会导致丢失这1s数据。</span></span><br><span class="line">appendfsync everysec</span><br><span class="line"><span class="meta"># </span><span class="language-bash">在aof重写或者写入rdb文件的时候，会执行大量IO，此时对于everysec和always的aof模式来说，执行fsync会造成阻塞过长时间，no-appendfsync-on-rewrite字段设置为默认设置为no。如果对延迟要求很高的应用，这个字段可以设置为<span class="built_in">yes</span>，否则还是设置为no，这样对持久化特性来说这是更安全的选择。设置为<span class="built_in">yes</span>表示rewrite期间对新写操作不fsync,暂时存在内存中,等rewrite完成后再写入，默认为no，建议<span class="built_in">yes</span>。Linux的默认fsync策略是30秒。可能丢失30秒数据。</span></span><br><span class="line">no-appendfsync-on-rewrite no</span><br><span class="line"><span class="meta"># </span><span class="language-bash">aof自动重写配置。当目前aof文件大小超过上一次重写的aof文件大小的百分之多少进行重写，即当aof文件增长到一定大小的时候Redis能够调用bgrewriteaof对日志文件进行重写。当前AOF文件大小是上次日志重写得到AOF文件大小的二倍（设置为100）时，自动启动新的日志重写过程。</span></span><br><span class="line">auto-aof-rewrite-percentage 100</span><br><span class="line"><span class="meta"># </span><span class="language-bash">设置允许重写的最小aof文件大小，避免了达到约定百分比但尺寸仍然很小的情况还要重写</span></span><br><span class="line">auto-aof-rewrite-min-size 64mb</span><br><span class="line"><span class="meta"># </span><span class="language-bash">aof文件可能在尾部是不完整的，当redis启动的时候，aof文件的数据被载入内存。重启可能发生在redis所在的主机操作系统宕机后，尤其在ext4文件系统没有加上data=ordered选项（redis宕机或者异常终止不会造成尾部不完整现象。）出现这种现象，可以选择让redis退出，或者导入尽可能多的数据。如果选择的是<span class="built_in">yes</span>，当截断的aof文件被导入的时候，会自动发布一个<span class="built_in">log</span>给客户端然后load。如果是no，用户必须手动redis-check-aof修复AOF文件才可以。</span></span><br><span class="line">aof-load-truncated yes</span><br></pre></td></tr></table></figure></li><li><p>主从配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">复制选项，slave复制对应的master</span></span><br><span class="line">slaveof &lt;masterip&gt; &lt;masterport&gt;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">如果master设置了requirepass，那么slave要连上master，需要有master的密码才行。masterauth就是用来配置master的密码，这样可以在连上master后进行认证</span></span><br><span class="line">masterauth &lt;master-password&gt;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">当slave同master失去连接或者复制正在进行，slave的运行方式</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">1.如果slave-serve-stale-data设置为<span class="built_in">yes</span>(默认设置)，slave会继续响应客户端的请求。</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">2.如果slave-serve-stale-data设置为no，除去INFO和SLAVOF命令之外的任何请求都会返回一个错误”SYNC with master <span class="keyword">in</span> progress”。</span></span><br><span class="line">slave-serve-stale-data yes</span><br><span class="line"><span class="meta"># </span><span class="language-bash">slave服务器读写配置</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">默认情况下是只读的（<span class="built_in">yes</span>）</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">修改成no，可读可写（不建议）</span></span><br><span class="line">slave-read-only yes</span><br><span class="line"><span class="meta"># </span><span class="language-bash">是否使用socket方式复制数据。目前redis复制提供两种方式，disk和socket。如果新的slave连上来或者重连的slave无法部分同步，就会执行全量同步，master会生成rdb文件。有2种方式：disk方式是master创建一个新的进程把rdb文件保存到磁盘，再把磁盘上的rdb文件传递给slave。socket是master创建一个新的进程，直接把rdb文件以socket的方式发给slave。disk方式的时候，当一个rdb保存的过程中，多个slave都能共享这个rdb文件。socket的方式就的一个个slave顺序复制。在磁盘速度缓慢，网速快的情况下推荐用socket方式。</span></span><br><span class="line">repl-diskless-sync no</span><br><span class="line"><span class="meta"># </span><span class="language-bash">diskless复制的延迟时间，防止设置为0。一旦复制开始，节点不会再接收新slave的复制请求直到下一个rdb传输。所以最好等待一段时间，等更多的slave连上来。</span></span><br><span class="line">repl-diskless-sync-delay 5</span><br><span class="line"><span class="meta"># </span><span class="language-bash">slave根据指定的时间间隔向服务器发送ping请求</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">时间间隔可以通过 repl_ping_slave_period 来设置，默认10秒。</span></span><br><span class="line">repl-ping-slave-period 10</span><br><span class="line"><span class="meta"># </span><span class="language-bash">复制连接超时时间。master和slave都有超时时间的设置。master检测到slave上次发送的时间超过repl-timeout，即认为slave离线，清除该slave信息。slave检测到上次和master交互的时间超过repl-timeout，则认为master离线。需要注意的是repl-timeout需要设置一个比repl-ping-slave-period更大的值，不然会经常检测到超时。</span></span><br><span class="line">repl-timeout 60</span><br><span class="line"><span class="meta"># </span><span class="language-bash">是否禁止复制tcp链接的tcp nodelay参数，可传递<span class="built_in">yes</span>或者no。默认是no，即使用tcp nodelay。如果master设置了<span class="built_in">yes</span>来禁止tcp nodelay设置，在把数据复制给slave的时候，会减少包的数量和更小的网络带宽。但是这也可能带来数据的延迟。默认我们推荐更小的延迟，但是在数据量传输很大的场景下，建议选择<span class="built_in">yes</span>。</span></span><br><span class="line">repl-disable-tcp-nodelay no</span><br><span class="line"><span class="meta"># </span><span class="language-bash">复制缓冲区大小，这是一个环形复制缓冲区，用来保存最新复制的命令。这样在slave离线的时候，不需要完全复制master的数据，如果可以执行部分同步，只需要把缓冲区的部分数据复制给slave，就能恢复正常复制状态。缓冲区的大小越大，slave离线的时间可以更长，复制缓冲区只有在有slave连接的时候才分配内存。没有slave的一段时间，内存会被释放出来，默认1m。</span></span><br><span class="line">repl-backlog-size 5mb</span><br><span class="line"><span class="meta"># </span><span class="language-bash">master没有slave一段时间会释放复制缓冲区的内存，repl-backlog-ttl用来设置该时间长度。单位为秒。</span></span><br><span class="line">repl-backlog-ttl 3600</span><br><span class="line"><span class="meta"># </span><span class="language-bash">当master不可用，Sentinel会根据slave的优先级选举一个master。最低的优先级的slave，当选master。而配置成0，永远不会被选举。</span></span><br><span class="line">slave-priority 100</span><br><span class="line"><span class="meta"># </span><span class="language-bash">redis提供了可以让master停止写入的方式，如果配置了min-slaves-to-write，健康的slave的个数小于N，mater就禁止写入。master最少得有多少个健康的slave存活才能执行写命令。这个配置虽然不能保证N个slave都一定能接收到master的写操作，但是能避免没有足够健康的slave的时候，master不能写入来避免数据丢失。设置为0是关闭该功能。</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">min-slaves-to-write 3</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">延迟小于min-slaves-max-lag秒的slave才认为是健康的slave。</span></span><br><span class="line">min-slaves-max-lag 10</span><br><span class="line"><span class="meta"># </span><span class="language-bash">设置1或另一个设置为0禁用这个特性。</span></span><br><span class="line">min-slaves-max-lag is set to 10</span><br></pre></td></tr></table></figure></li><li><p>安全配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">配置redis连接密码（默认未启用，建议启用）</span></span><br><span class="line">requirepass foobared</span><br><span class="line"><span class="meta"># </span><span class="language-bash">把危险的命令给修改成其他名称。比如CONFIG命令可以重命名为一个很难被猜到的命令，这样外部连接不能使用，而服务器内部连接工具还能继续使用。</span></span><br><span class="line">rename-command CONFIG cmd</span><br><span class="line"><span class="meta"># </span><span class="language-bash">设置成一个空的值，可以禁止一个命令</span></span><br><span class="line">rename-command CONFIG &quot;&quot;</span><br></pre></td></tr></table></figure></li><li><p>连接配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">设置能连上redis的最大客户端连接数量。默认是10000个客户端连接。由于redis不区分连接是客户端连接还是内部打开文件或者和slave连接等，所以maxclients最小建议设置到32。如果超过了maxclients，redis会给新的连接发送’max number of clients reached’，并关闭连接。</span></span><br><span class="line">maxclients 10000</span><br><span class="line"><span class="meta"># </span><span class="language-bash">redis配置的最大内存容量。当内存满了，需要配合maxmemory-policy策略进行处理。注意slave的输出缓冲区是不计算在maxmemory内的。所以为了防止主机内存使用完，建议设置的maxmemory需要更小一些。</span></span><br><span class="line">maxmemory &lt;bytes&gt;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">内存容量超过maxmemory后的处理策略。</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">volatile-lru：利用LRU算法移除设置过过期时间的key。</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">volatile-random：随机移除设置过过期时间的key。</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">volatile-ttl：移除即将过期的key，根据最近过期时间来删除（辅以TTL）</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">allkeys-lru：利用LRU算法移除任何key。</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">allkeys-random：随机移除任何key。</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">noeviction：不移除任何key，只是返回一个写错误。</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">上面的这些驱逐策略，如果redis没有合适的key驱逐，对于写命令，还是会返回错误。redis将不再接收写请求，只接收get请求。</span></span><br><span class="line">maxmemory-policy noeviction</span><br><span class="line"><span class="meta"># </span><span class="language-bash">lru检测的样本数。使用lru或者ttl淘汰算法，从需要淘汰的列表中随机选择sample个key，选出闲置时间最长的key移除。</span></span><br><span class="line">maxmemory-samples 5</span><br><span class="line"><span class="meta"># </span><span class="language-bash">如果达到最大时间限制（毫秒），redis会记个<span class="built_in">log</span>，然后返回error。当一个脚本超过了最大时限。只有SCRIPT KILL和SHUTDOWN NOSAVE可以用。第一个可以杀没有调write命令的东西。要是已经调用了write，只能用第二个命令杀。</span></span><br><span class="line">lua-time-limit 5000</span><br></pre></td></tr></table></figure></li><li><p>集群配置</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">集群开关，默认是不开启集群模式。</span></span><br><span class="line">cluster-enabled yes</span><br><span class="line"><span class="meta"># </span><span class="language-bash">集群配置文件的名称，每个节点都有一个集群相关的配置文件，持久化保存集群的信息。这个文件并不需要手动配置，这个配置文件有Redis生成并更新，每个Redis集群节点需要一个单独的配置文件，请确保与实例运行的系统中配置文件名称不冲突</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">cluster-config-file nodes-a.conf</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">节点互连超时的阀值。集群节点超时毫秒数</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">cluster-node-timeout 15000</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">在进行故障转移的时候，全部slave都会请求申请为master，但是有些slave可能与master断开连接一段时间了，导致数据过于陈旧，这样的slave不应该被提升为master。该参数就是用来判断slave节点与master断线的时间是否过长。判断方法是：</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">比较slave断开连接的时间和(node-timeout * slave-validity-factor) +repl-ping-slave-period</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">如果节点超时时间为三十秒, 并且slave-validity-factor为10,假设默认的repl-ping-slave-period是10秒，即如果超过310秒slave将不会尝试进行故障转移</span></span><br><span class="line">cluster-slave-validity-factor 10</span><br><span class="line"><span class="meta"># </span><span class="language-bash">master的slave数量大于该值，slave才能迁移到其他孤立master上，如这个参数若被设为2，那么只有当一个主节点拥有2 个可工作的从节点时，它的一个从节点会尝试迁移。</span></span><br><span class="line">cluster-migration-barrier 1</span><br><span class="line"><span class="meta"># </span><span class="language-bash">默认情况下，集群全部的slot有节点负责，集群状态才为ok，才能提供服务。设置为no，可以在slot没有全部分配的时候提供服务。不建议打开该配置，这样会造成分区的时候，小分区的master一直在接受写请求，而造成很长时间数据不一致。</span></span><br><span class="line">cluster-require-full-coverage yes</span><br></pre></td></tr></table></figure></li><li><p><a href="https://www.jianshu.com/p/8d64c6e849d9">其他配置</a></p></li></ol><h3 id="Redis基础语法"><a href="#Redis基础语法" class="headerlink" title="Redis基础语法"></a>Redis基础语法</h3><h4 id="shell"><a href="#shell" class="headerlink" title="shell"></a>shell</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">./redis-server ./redis.conf  # 从redis.conf读取配置并启动redis服务</span><br><span class="line">./redis-cli -h 127.0.0.1 -p 6379  # 从默认端口启动redis服务</span><br><span class="line">./redis-cli ping  # 返回PONG则说明客户端与redis服务的连接正常</span><br><span class="line">./redis-cli shutdown  # 关闭redis服务</span><br></pre></td></tr></table></figure><h4 id="string"><a href="#string" class="headerlink" title="string"></a>string</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">set foo 1  # 赋值</span><br><span class="line">get foo  # 字符串回复，当请求一个字符串类型键的键值或一个其他类型键中的某个元素</span><br><span class="line">keys *  # keys命令的作用是获取数据库中符合指定规则的键名，支持glob风格通配符格式（?,*,[],\x）</span><br><span class="line">exists key  # exists判断key键是否存在，返回1和0</span><br><span class="line">del key1 key2 ...  # del删除一个或多个键</span><br><span class="line">type key  # type获得key值的数据类型</span><br><span class="line">incr foo  # 增键值的INCR命令会以整数形式返回递增后的键值</span><br><span class="line">incrby foo 2  # 增加指定的整数</span><br><span class="line">incrbyfloat foo 0.1  # 增加指定的浮点数</span><br><span class="line">decr foo  # 自减1</span><br><span class="line">decrby foo 2  # 减少指定的整数</span><br><span class="line">append key &quot;value&quot;  # 在key键原有的值尾部追加“value”</span><br><span class="line">strlen key  # strlen返回字符串长度</span><br><span class="line">mget key1 key2...  # 同时获取多个键值</span><br><span class="line">mset key1 value1 key2 value2...  # 同时赋值多个键值对</span><br></pre></td></tr></table></figure><h4 id="hash"><a href="#hash" class="headerlink" title="hash"></a>hash</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hset key field1 value1  # 对键、字段、value赋值，若字段已存在则更新value</span><br><span class="line">hmset key field1 value1 field2 value2...  # 赋值多个字段和value</span><br><span class="line">hsetnx key field1 value1...  # 若字段不存在则新建赋值，已存在则不进行任何操作（nx：if not exists）</span><br><span class="line">hget key field1  # 返回键、字段对应的value</span><br><span class="line">hmget key field1 field2...  # 返回键对应的多个字段value</span><br><span class="line">hgetall key  # 返回键对应的所有字段和value，返回的结果是字段和字段值遍历组成的列表，不是很直观</span><br><span class="line">hkeys key  # 只获取键下的字段名</span><br><span class="line">hvals key  # 只获取键下所有的value</span><br><span class="line">hlen key  # 获取键长度，也就是字段数量</span><br><span class="line">hexists key field1  # 判断键对应的字段是否存在，返回1和0</span><br><span class="line">hincrbr key field1 2  # 对value进行自增指定数字，若不存在则会新建</span><br><span class="line">hdel key field1 field2...  # 删除键下的一个或多个字段</span><br></pre></td></tr></table></figure><h4 id="list"><a href="#list" class="headerlink" title="list"></a>list</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">lpush key value1 value2...  # 向列表左边添加一个或多个元素，会首先添加value1</span><br><span class="line">rpush key value1 value2...  # 向列表右边添加一个或多个元素</span><br><span class="line">lpop key  # 从列表左边弹出一个元素并从列表中删除这个元素</span><br><span class="line">rpop key  # 从列表右边弹出一个元素并从列表中删除这个元素</span><br><span class="line">brpop key... timeout  # brpop和rpop相似，区别是当列表中没有元素时brpop命令会一直阻塞住连接，直到有新元素加入，timeout指定超时时间，单位是秒。当超过了此时间仍然没有获得新元素的话就会返回nil，若为0则不限制等待时间</span><br><span class="line">llen key  # 列表中元素的个数</span><br><span class="line">lrange key start stop  # 获得列表从start到stop的所有的列表元素，返回的元素从上到下即从左到右</span><br><span class="line">lrange key 0 -1  # 获得列表中的所有元素</span><br><span class="line">lrem key count value  # 删除前count个值为value的元素（当count＞0时从列表左边开始删除前count个值为value的元素；当count＜0时从列表右边开始删除前|count|个值为value的元素；当count = 0时会删除所有值为value的元素）</span><br><span class="line">lindex key index  # 获得列表中指定index的value，index从左往右从0开始依次排列，或者从右至左从-1开始</span><br><span class="line">lset key index value  # 设置列表中指定index的值为value，若该index位置已存在值，则替换原有值</span><br><span class="line">linsert key before|after pivot value  # 在指定值前/后插入value。首先在列表中从左到右查找值为pivot的元素，然后根据第二个参数是BEFORE还是AFTER来决定将value插入到该元素的前面还是后面。</span><br><span class="line">rpoplpush key key1  # 将元素从一个列表转移到另外一个列表。先执行rpop命令再执行lpush命令。先从key列表类型键的右边弹出一个元素，然后将其加入到key1列表类型键的左边，并返回这个元素的值。</span><br></pre></td></tr></table></figure><h4 id="set"><a href="#set" class="headerlink" title="set"></a>set</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sadd key member1 member2...  # 向集合中添加一个或多个元素，返回实际修改元素数量，若已存在则忽略</span><br><span class="line">srem key member1 member2...  # 从集合中删除一个或多个元素，并返回删除成功的个数</span><br><span class="line">smembers key  # 获得集合中的所有元素</span><br><span class="line">sismember key member1  # 判断元素member1是否存在于集合key中，返回1和0</span><br><span class="line">sdiff A B  # 返回A与B的差集，A-B，支持多个集合</span><br><span class="line">sinter A B  # 返回A与B的交集，A∩B，支持多个集合</span><br><span class="line">sunion A B  # 返回A与B的并集，A∪B，支持多个集合</span><br><span class="line">scard key  # 返回集合长度，也就是元素数量</span><br><span class="line">sdiffstore new key1 key2...  # 计算两个或多个集合的差集并存储到new中</span><br><span class="line">sinterstore new key1 key2...  # 计算两个或多个集合的交集并存储到new中</span><br><span class="line">sunion new key1 key2...  # 计算两个或多个集合的并集并存储到new中</span><br><span class="line">spop key  # 从集合中随机弹出一个元素</span><br><span class="line">srandmember key [count]  # 随机获得集合中的元素。count参数来一次随机获得多个元素：当count为正数时，随机从集合里获得count个不重复的元素。如果count的值大于集合中的元素个数，则会返回集合中的全部元素；当count为负数时，会随机从集合里获得|count|个的元素，这些元素有可能相同。</span><br></pre></td></tr></table></figure><p>sorted set</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">zadd key score member [score member...]  # 新建/添加/修改有序索引，score为分数，member为元素</span><br><span class="line">zscore key member  # 获得一个member的score</span><br><span class="line">zrange key start stop [withscores]  # 从start stop指定下标从key取出元素，withscores指定是否输出score</span><br><span class="line">zrevrange key start stop [withscores]  # 与zrange不同之处是会按score从大到小排列输出元素</span><br><span class="line">zrangebyscore key min max [withscores] [limit offset count]  # 获得指定分数范围的元素，如果希望分数范围不包含端点值，可以在分数前加上“(”符号；min和max还支持无穷大，同ZADD命令一样，-inf和+inf分别表示负无穷和正无穷；在本命令中limit offset count 与SQL中的用法基本相同，即在获得的元素列表的基础上向后偏移offset个元素，并且只获取前count个元素。</span><br><span class="line">zincrby key increment member  # 增加某个元素的分数，若指定的元素不存在，会先建立它并将它的分数赋为0再操作。</span><br><span class="line">zcard key  # 获得集合中元素的数量</span><br><span class="line">zcount key min max  # 返回指定分数范围内的元素个数</span><br><span class="line">zremrangebyrank key min max  # 按照排名范围删除元素，按照元素分数从小到大的顺序（即索引0表示最小的值）删除处在指定排名范围内的所有元素，并返回删除的元素数量</span><br><span class="line">zrank key member  # 返回member元素的排名（从小到大排列）</span><br><span class="line">zrevrank key member  # 返回member元素的排名（从大到小排列）</span><br></pre></td></tr></table></figure><h4 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">事务 multi开始，<span class="built_in">exec</span>结束</span></span><br><span class="line">multi</span><br><span class="line">sadd key1 1</span><br><span class="line">sadd key2 2</span><br><span class="line">exec</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">watch命令可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行。</span></span><br><span class="line">watch key</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">生存时间</span></span><br><span class="line">expire key 900  # 指定key在15分钟（900s）后删除，再次指定可重新设置生存时间，覆盖已有生存时间</span><br><span class="line">expireat key 1351858600  # 同样是生存时间，不过单位是以秒为单位的Unix时间戳</span><br><span class="line">paxpireat key 1351858600000  # 单位是以毫秒为单位的Unix时间戳</span><br><span class="line">ttl key  # 查看键在多少秒后删除</span><br><span class="line">persist key  # 取消键的生存时间设置（即将键恢复成永久的）</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">排序</span></span><br><span class="line">sort key [desc]  # 按照key值对key进行从小到大排列，若是有序集合则忽略score对value进行排序，desc从大到小</span><br><span class="line">sort key alpha [desc]  # 若value是字符串，则使用alpha指定按第一个字母排序，desc从大到小</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">按key排序</span></span><br><span class="line">lpush fdf 3 5 6  # 需要排序的列表</span><br><span class="line">set dd:3 10  # 新建三个dd:v 普通类型数据，v值对应列表中的value值</span><br><span class="line">set dd:6 7</span><br><span class="line">set dd:5 5</span><br><span class="line">sort fdf by dd:* desc  # 则sort fdf会按dd:v对应的值对列表中的value值进行排序，结果为3 6 5</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">SORT是Redis中最强大最复杂的命令之一，如果使用不好很容易成为性能瓶颈。</span></span><br></pre></td></tr></table></figure><h3 id="Redis基础知识"><a href="#Redis基础知识" class="headerlink" title="Redis基础知识"></a>Redis基础知识</h3><h4 id="多数据库"><a href="#多数据库" class="headerlink" title="多数据库"></a>多数据库</h4><p>Redis是一个字典结构的存储服务器，而实际上一个Redis实例提供了多个用来存储数据的字典，客户端可以指定将数据存储在哪个字典中。这与我们熟知的在一个关系数据库实例中可以创建多个数据库类似，所以可以将其中的每个字典都理解成一个独立的数据库。</p><p>每个数据库对外都是以一个从0开始的递增数字命名，Redis默认支持16个数据库，可以通过配置参数databases来修改这一数字。客户端与Redis建立连接后会自动选择0号数据库，不过可以随时使用SELECT命令更换数据库。</p><blockquote><p>然而这些以数字命名的数据库又与我们理解的数据库有所区别。首先Redis不支持自定义数据库的名字，每个数据库都以编号命名，开发者必须自己记录哪些数据库存储了哪些数据。另外Redis也不支持为每个数据库设置不同的访问密码，所以一个客户端要么可以访问全部数据库，要么连一个数据库也没有权限访问。最重要的一点是多个数据库之间并不是完全隔离的，比如FLUSHALL命令可以清空一个Redis实例中所有数据库中的数据。综上所述，这些数据库更像是一种命名空间，而不适宜存储不同应用程序的数据。比如可以使用0号数据库存储某个应用生产环境中的数据，使用1号数据库存储测试环境中的数据，但不适宜使用0号数据库存储A应用的数据而使用1号数据库存储B应用的数据，不同的应用应该使用不同的Redis实例存储数据。由于Redis非常轻量级，一个空Redis实例占用的内存只有1MB左右，所以不用担心多个Redis实例会额外占用很多内存。</p></blockquote><h4 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h4><ol><li><p>字符串类型<br>字符串类型是Redis中最基本的数据类型，它能存储任何形式的字符串，包括二进制数据。你可以用其存储用户的邮箱、JSON化的对象甚至是一张图片。一个字符串类型键允许存储的数据的最大容量是512 MB</p></li><li><p>哈希类型<br>哈希类型（hash）的键值也是一种字典结构，其存储了字段（field）和字段值的映射，但字段值只能是字符串，不支持其他数据类型，换句话说，哈希类型不能嵌套其他的数据类型。一个哈希类型键可以包含至多232-1个字段。</p><blockquote><p>除了哈希类型，Redis的其他数据类型同样不支持数据类型嵌套。比如集合类型的每个元素都只能是字符串，不能是另一个集合或哈希表等。</p></blockquote></li><li><p>列表类型<br>列表类型（list）可以存储一个有序的字符串列表，常用的操作是向列表两端添加元素，或者获得列表的某一个片段。</p><p>列表类型内部是使用双向链表（double linked list）实现的，所以向列表两端添加元素的时间复杂度为O(1)，获取越接近两端的元素速度就越快。这意味着即使是一个有几千万个元素的列表，获取头部或尾部的10条记录也是极快的。</p></li><li><p>集合类型<br>在集合中的每个元素都是不同的，且没有顺序。一个集合类型（set）键可以存储至多232 -1个字符串。</p><p>集合类型的常用操作是向集合中加入或删除元素、判断某个元素是否存在等，由于集合类型在Redis内部是使用值为空的哈希表（hash table）实现的，所以这些操作的时间复杂度都是O(1)。最方便的是多个集合类型键之间还可以进行并集、交集和差集运算。</p></li><li><p>有序集合<br>在集合类型的基础上有序集合类型为集合中的每个元素都关联了一个分数，这使得我们不仅可以完成插入、删除和判断元素是否存在等集合类型支持的操作，还能够获得分数最高（或最低）的前N个元素、获得指定分数范围内的元素等与分数有关的操作。<br>虽然集合中每个元素都是不同的，但是它们的分数却可以相同。有序集合类型在某些方面和列表类型有些相似。（1）二者都是有序的。<br>（2）二者都可以获得某一范围的元素。<br>但是二者有着很大的区别，这使得它们的应用场景也是不同的。<br>（1）列表类型是通过链表实现的，获取靠近两端的数据速度极快，而当元素增多后，访问中间数据的速度会较慢，所以它更加适合实现如“新鲜事”或“日志”这样很少访问中间元素的应用。<br>（2）有序集合类型是使用散列表和跳跃表（Skip list）实现的，所以即使读取位于中间部分的数据速度也很快（时间复杂度是O(log(N))）。<br>（3）列表中不能简单地调整某个元素的位置，但是有序集合可以（通过更改这个元素的分数）。<br>（4）有序集合要比列表类型更耗费内存。</p></li></ol><h4 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h4><p>Redis也支持事务，由multi开始，exec结束，中间是要运行的代码。Redis保证一个事务中的所有命令要么都执行，要么都不执行。如果在发送EXEC命令前客户端断线了，则Redis会清空事务队列，事务中的所有命令都不会执行。</p><p>语法错误。语法错误指命令不存在或者命令参数的个数不对。而只要有一个命令有语法错误，执行EXEC命令后Redis就会直接返回错误，连语法正确的命令也不会执行。</p><p>运行错误。运行错误指在命令执行时出现的错误，比如使用散列类型的命令操作集合类型的键，这种错误在实际执行之前Redis是无法发现的，所以在事务里这样的命令是会被Redis接受并执行的。如果事务里的一条命令出现了运行错误，事务里其他的命令依然会继续执行（包括出错命令之后的命令）。</p><p>Redis的事务没有关系数据库事务提供的回滚（rollback）[插图]功能。为此开发者必须在事务执行出错后自己收拾剩下的摊子（将数据库复原回事务执行前的状态等）。</p><h4 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h4><p>Redis的强劲性能很大程度上是由于其将所有数据都存储在了内存中，为了使Redis在重启之后仍能保证数据不丢失，需要将数据从内存中以某种形式同步到硬盘中，这一过程就是持久化。Redis支持两种方式的持久化，一种是RDB方式，一种是AOF方式。可以单独使用其中一种或将二者结合使用。</p><ol><li><p>RDB方式的持久化是通过快照（snapshotting）完成的，当符合一定条件时Redis会自动将内存中的所有数据进行快照并存储在硬盘上。进行快照的条件可以由用户在配置文件中自定义，由两个参数构成：时间和改动的键的个数。当在指定的时间内被更改的键的个数大于指定的数值时就会进行快照。RDB是Redis默认采用的持久化方式。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">save参数指定了快照条件，可以存在多个条件，条件之间是“或”的关系。save 900 1的意思是在15分钟（900秒钟）内有至少一个键被更改则进行快照。</span></span><br><span class="line">save 900 1</span><br><span class="line">save 300 10</span><br><span class="line">sava 6010000</span><br></pre></td></tr></table></figure><p>Redis默认会将快照文件存储在当前目录的dump.rdb文件中，可以通过配置dir和dbfilename两个参数分别指定快照文件的存储路径和文件名。<br>理清Redis实现快照的过程对我们了解快照文件的特性有很大的帮助。快照的过程如下。<br>（1）Redis使用fork函数复制一份当前进程（父进程）的副本（子进程）；<br>（2）父进程继续接收并处理客户端发来的命令，而子进程开始将内存中的数据写入硬盘中的临时文件；<br>（3）当子进程写入完所有数据后会用该临时文件替换旧的RDB文件，至此一次快照操作完成。</p><p>通过上述过程可以发现Redis在进行快照的过程中不会修改RDB文件，只有快照结束后才会将旧的文件替换成新的，也就是说任何时候RDB文件都是完整的。这使得我们可以通过定时备份RDB文件来实现Redis数据库备份。RDB文件是经过压缩（可以配置rdbcompression参数以禁用压缩节省CPU占用）的二进制格式，所以占用的空间会小于内存中的数据大小，更加利于传输。</p><p>除了自动快照，还可以手动发送SAVE或BGSAVE命令让Redis执行快照，两个命令的区别在于，前者是由主进程进行快照操作，会阻塞住其他请求，后者会通过fork子进程进行快照操作。Redis启动后会读取RDB快照文件，将数据从硬盘载入到内存。根据数据量大小与结构和服务器性能不同，这个时间也不同。通常将一个记录一千万个字符串类型键、大小为1GB的快照文件载入到内存中需要花费20～30秒钟。</p><p>通过RDB方式实现持久化，一旦Redis异常退出，就会丢失最后一次快照以后更改的所有数据。这就需要开发者根据具体的应用场合，通过组合设置自动快照条件的方式来将可能发生的数据损失控制在能够接受的范围。如果数据很重要以至于无法承受任何损失，则可以考虑使用AOF方式进行持久化。</p></li><li><p>AOF（append only file）方式的持久化</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">默认情况下Redis没有开启AOF（append only file）方式的持久化，可以通过appendonly参数开启</span></span><br><span class="line">appendonly yes</span><br><span class="line"><span class="meta"># </span><span class="language-bash">开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬盘中的AOF文件。文件位置和RDB文件的位置相同，都是通过<span class="built_in">dir</span>参数设置的，默认的文件名是appendonly.aof，可通过appendfilename name修改</span></span><br><span class="line">appendfilename appendonly.aof</span><br></pre></td></tr></table></figure><p>可见AOF文件是纯文本文件，其内容正是Redis客户端向Redis发送的原始通信协议的内容（Redis的通信协议会在7.4节中介绍，为了便于阅读，这里将实际的命令部分以粗体显示），从中可见Redis确实只记录了前3条命令。然而这时有一个问题是前2条命令其实都是冗余的，因为这两条的执行结果会被第三条命令覆盖。随着执行的命令越来越多，AOF文件的大小也会越来越大，即使内存中实际的数据可能并没有多少。很自然地，我们希望Redis可以自动优化AOF文件，就上例而言，就是将前两条无用的记录删除，只保留第三条。实际上Redis也正是这样做的，每当达到一定条件时Redis就会自动重写AOF文件，这个条件可以在配置文件中设置。</p><p>Redis允许同时开启AOF和RDB，既保证了数据安全又使得进行备份等操作十分容易。此时重新启动Redis后Redis会使用AOF文件来恢复数据，因为AOF方式的持久化可能丢失的数据更少。</p></li></ol><h4 id="主从复制"><a href="#主从复制" class="headerlink" title="主从复制"></a>主从复制</h4><p>在Redis中使用复制功能非常容易，只需要在从数据库的配置文件中加入“slaveof主数据库IP主数据库端口”即可，主数据库无需进行任何配置。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">redis-server --slaveof 8.131.90.208 6379</span><br></pre></td></tr></table></figure><p>从数据库持久化</p><p>另一个相对耗时的操作是持久化，为了提高性能，可以通过复制功能建立一个（或若干个）从数据库，并在从数据库中启用持久化，同时在主数据库禁用持久化。当从数据库崩溃时重启后主数据库会自动将数据同步过来，所以无需担心数据丢失。而当主数据库崩溃时，需要在从数据库中使用SLAVEOF NO ONE命令将从数据库提升成主数据库继续服务，并在原来的主数据库启动后使用SLAVEOF命令将其设置成新的主数据库的从数据库，即可将数据同步回来。</p><h2 id="Hadoop"><a href="#Hadoop" class="headerlink" title="Hadoop"></a>Hadoop</h2><h3 id="编译Hadoop"><a href="#编译Hadoop" class="headerlink" title="编译Hadoop"></a>编译Hadoop</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">下载相关包（一定要用java1.8）</span></span><br><span class="line">wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.1/hadoop-3.2.1-src.tar.gz  # Hadoop源码包</span><br><span class="line">wget https://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz  # Maven包</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">下载ProtocolBuffer2.5.0，MapReduce和HDFS用protocol buffer来压缩和交换数据</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">百度网盘：链接：https://pan.baidu.com/s/1ljoP-tLCPkVbtssEdiGTFQ 提取码：q5fe</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">安装依赖库</span></span><br><span class="line">yum -y updates</span><br><span class="line">yum -y install kernel-devel  </span><br><span class="line">yum -y install gcc* </span><br><span class="line">yum -y install cmake3  # 必须是最新的</span><br><span class="line">ln -s /usr/bin/cmake3 /usr/bin/cmake  # 最后将cmake3做一个软链cmake</span><br><span class="line">yum -y install glibc-headers  </span><br><span class="line">yum -y install gcc-c++  </span><br><span class="line">yum -y install zip-devel  </span><br><span class="line">yum -y install openssl-devel  </span><br><span class="line">yum -y install svn  </span><br><span class="line">yum -y install git  </span><br><span class="line">yum -y install ncurses-devel  </span><br><span class="line">yum -y install lzo-devel  </span><br><span class="line">yum -y install autoconf  </span><br><span class="line">yum -y install libtool  </span><br><span class="line">yum -y install automake  </span><br><span class="line">yum -y install patch</span><br><span class="line">yum -y install doxygen</span><br><span class="line">yum -y install protobuf</span><br><span class="line">yum install -y graphviz</span><br><span class="line">yum install -y protobuf-devel</span><br><span class="line">yum -y install build-essential libtool zlib1g-dev pkg-config libssl-dev libsasl2-dev</span><br><span class="line">yum install -y cyrus-sasl* </span><br><span class="line">yum install -y libgsasl-devel*</span><br></pre></td></tr></table></figure><p>编译前的准备</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">Maven配置</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">解压Maven包并把路径添加到环境变量中</span></span><br><span class="line">export PATH=&quot;/usr/local/servers/apache-maven-3.6.3/bin:$PATH&quot;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">将Maven镜像更换为阿里云中央仓库</span></span><br><span class="line">vim /usr/local/servers/apache-maven-3.6.3/conf/settings.xml</span><br><span class="line"><span class="meta"># </span><span class="language-bash">在&lt;mirrors&gt;节点中添加</span></span><br><span class="line">&lt;mirror&gt;</span><br><span class="line">    &lt;id&gt;alimaven&lt;/id&gt;</span><br><span class="line">    &lt;name&gt;aliyun maven&lt;/name&gt;</span><br><span class="line">    &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;</span><br><span class="line">    &lt;mirrorOf&gt;central&lt;/mirrorOf&gt;        </span><br><span class="line">&lt;/mirror&gt;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">安装ProtocolBuffer</span></span><br><span class="line">tar -zxf protobuf-2.5.0.tar.gz</span><br><span class="line">cd protobuf-2.5.0/bin</span><br><span class="line">./configure --prefix=/usr/local/servers/protobuf-2.5.0</span><br><span class="line">make &amp;&amp; make install</span><br><span class="line"><span class="meta"># </span><span class="language-bash">在/etc/profile中添加bin路径到path中</span></span><br><span class="line">export LD_LIBRARY_PATH=&quot;/usr/local/servers/protobuf-2.5.0/lib&quot;</span><br><span class="line">export PATH=&quot;/usr/local/servers/protobuf-2.5.0/bin:$PATH&quot;</span><br><span class="line"></span><br><span class="line">source /etc/profile</span><br><span class="line">protoc --version  # 安装完成查看版本 </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">设置Hadoop编译时下载为阿里云maven镜像</span></span><br><span class="line">cd hadoop-3.2.1-src</span><br><span class="line">vim pom.xml</span><br><span class="line"><span class="meta"># </span><span class="language-bash">在repositories下添加如下内容</span></span><br><span class="line">&lt;repositories&gt;  </span><br><span class="line">    &lt;repository&gt;  </span><br><span class="line">        &lt;id&gt;alimaven&lt;/id&gt;  </span><br><span class="line">        &lt;name&gt;aliyun maven&lt;/name&gt;  </span><br><span class="line">        &lt;url&gt;http://maven.aliyun.com/nexus/content/groups/public/&lt;/url&gt;  </span><br><span class="line">        &lt;releases&gt;  </span><br><span class="line">            &lt;enabled&gt;true&lt;/enabled&gt;  </span><br><span class="line">        &lt;/releases&gt;  </span><br><span class="line">        &lt;snapshots&gt;  </span><br><span class="line">            &lt;enabled&gt;false&lt;/enabled&gt;  </span><br><span class="line">        &lt;/snapshots&gt;  </span><br><span class="line">    &lt;/repository&gt;  </span><br><span class="line">&lt;/repositories&gt; </span><br><span class="line"><span class="meta"># </span><span class="language-bash">注释掉不必要的代码</span></span><br><span class="line">vim /usr/local/softwares/hadoop-3.2.1-src/hadoop-client-modules/pom.xml</span><br><span class="line"></span><br><span class="line">&lt;module&gt;hadoop-client-minicluster&lt;/module&gt;</span><br><span class="line">&lt;!-- Checks invariants above --&gt;</span><br><span class="line">&lt;module&gt;hadoop-client-check-invariants&lt;/module&gt;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">修改这里注释掉这一行</span></span><br><span class="line">&lt;!-- &lt;module&gt;hadoop-client-check-test-invariants&lt;/module&gt;--&gt;</span><br><span class="line">&lt;!-- Attempt to use the created libraries --&gt;</span><br><span class="line">&lt;module&gt;hadoop-client-integration-tests&lt;/module&gt;</span><br></pre></td></tr></table></figure><p>开始编译，有三种编译方式</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash"><span class="built_in">cd</span>到Hadoop源码包中</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">仅编译正常源码部分，对于native部分不进行编译，最终结果打包</span></span><br><span class="line">mvn package -Pdist -DskipTests -Dtar -Dmaven.javadoc.skip=true </span><br><span class="line"><span class="meta"># </span><span class="language-bash">编译正常部分源码、native依赖库以及帮助文档，最终结果打包</span></span><br><span class="line">mvn package -Pdist,native,docs -DskipTests -Dtar </span><br><span class="line"><span class="meta"># </span><span class="language-bash">一般使用下面这种方法</span></span><br><span class="line">mvn -X package -Pdist,native,docs -DskipTests -Dtar -Dmaven.skip.test=true -Dmaven.javadoc.skip=true </span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看编译好的包（hadoop-3.2.1和hadoop-3.2.1.tar.gz一样）</span></span><br><span class="line">ls /usr/local/softwares/hadoop-3.2.1-src/hadoop-dist/target/</span><br></pre></td></tr></table></figure><h3 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h3><h4 id="安装Hadoop"><a href="#安装Hadoop" class="headerlink" title="安装Hadoop"></a>安装Hadoop</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">将编译好的安装包解压到/usr/local/servers目录下，设置Hadoop_home</span></span><br><span class="line">vim /etc/profile</span><br><span class="line">export HADOOP_HOME=/usr/local/servers/hadoop-3.2.1</span><br><span class="line">export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin</span><br><span class="line">source /etc/profile</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">新建使用的文件夹</span></span><br><span class="line">cd /usr/local/servers/hadoop-3.2.1</span><br><span class="line">mkdir -p data/dfs/name</span><br><span class="line">mkdir -p data/dfs/name2</span><br><span class="line">mkdir -p data/dfs/data</span><br><span class="line">mkdir -p data/dfs/data2</span><br><span class="line">mkdir -p data/jobhistory/donedatas</span><br><span class="line">mkdir -p data/jobhistory/intermediate</span><br><span class="line">mkdir -p data/nn/edits</span><br><span class="line">mkdir -p data/nodemanager/data</span><br><span class="line">mkdir -p data/nodemanager/logs</span><br><span class="line">mkdir -p data/remote/logs</span><br><span class="line">mkdir -p data/snn/name</span><br><span class="line">mkdir -p data/snn/edits</span><br></pre></td></tr></table></figure><p>配置文件（一共修改七个）</p><p>​    core-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定NameNode的地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://node01:9009<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!-- 指定hadoop数据的存储目录 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/servers/hadoop-3.2.1/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--  缓冲区大小，实际工作中根据服务器性能动态调整--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>io.file.buffer.size<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>8192<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="comment">&lt;!--  开启hdfs的垃圾桶机制，删除掉的数据可以从垃圾桶中回收，默认10080分钟--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.trash.interval<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>10080<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>hdfs-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.secondary.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node03:50090<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--  指定namenode的访问端口和端口  --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.http-address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01:50070<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--  指定namenode存储元数据的地址  --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///usr/local/servers/hadoop-3.2.1/data/dfs/name,file:///usr/local/servers/hadoop-3.2.1/data/dfs/name2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--  指定datanode存储数据的地址  --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///usr/local/servers/hadoop-3.2.1/data/dfs/data,file:///usr/local/servers/hadoop-3.2.1/data/dfs/data2<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--  指定存放日志文件的文件地址  --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.edits.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///usr/local/servers/hadoop-3.2.1/data/nn/edits<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--  检查点路径  --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///usr/local/servers/hadoop-3.2.1/data/snn/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span>    </span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--  检查点日志路径  --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.checkpoint.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///usr/local/servers/hadoop-3.2.1/data/snn/edits<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--  文件切片的副本个数  --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>3<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--  设置hdfs的文件权限  --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.permissions<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--  设置一个文件切片的大小：128M  --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.blocksize<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>134217728<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>mapred-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定MapReduce程序运行在Yarn上 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>Xmx512M<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.memory.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.java.opts<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>Xmx512M<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.task.io.sort.mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>256<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.task.io.sort.factor<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>100<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.reduce.shuffle.parallelcopies<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>25<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 历史服务器端地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01:10020<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 历史服务器web端地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01:19888<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.intermediate-done-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/servers/hadoop-3.2.1/data/jobhistory/intermediate<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.jobhistory.done-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/usr/local/servers/hadoop-3.2.1/data/jobhistory/donedatas<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.app.mapreduce.am.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=/usr/local/servers/hadoop-3.2.1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.map.env<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>HADOOP_MAPRED_HOME=/usr/local/servers/hadoop-3.2.1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>yarn-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定MR走shuffle --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 指定ResourceManager的地址--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.hostname<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01:8032<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.scheduler.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01:8030<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01:8031<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.admin.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01:8033<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.resourcemanager.webapp.address<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>node01:8088<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 开启日志聚集功能 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation-enable<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置日志聚集服务器地址 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log.server.url<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>http://node01:19888/jobhistory/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 设置日志保留时间为7天 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>604800<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 环境变量的继承 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.env-whitelist<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- yarn容器允许分配的最大最小内存 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.minimum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.scheduler.maximum-allocation-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2048<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- yarn容器允许管理的物理内存大小 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.memory-mb<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>1024<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-pmem-ratio<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>2.1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!-- 关闭yarn对物理内存和虚拟内存的限制检查 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.pmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.vmem-check-enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>false<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.resource.detect-hardware-capabilities<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.local-dirs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///usr/local/servers/hadoop-3.2.1/dfs/nodemanager/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.log-dirs<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///usr/local/servers/hadoop-3.2.1/dfs/nodemanager/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.remote-app-log-dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:///usr/local/servers/hadoop-3.2.1/dfs/remote/logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.remote-app-log-dir-suffix<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>logs<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>18144000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="comment">&lt;!--  --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.log-aggregation.retain-check-interval-seconds<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>86400<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><p>yarn-env.sh</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在最后添加语句</span></span><br><span class="line"><span class="built_in">export</span> JAVA_HOME=/usr/local/servers/jdk8</span><br></pre></td></tr></table></figure><p>mapred-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">在最后添加语句</span></span><br><span class="line">export JAVA_HOME=/usr/local/servers/jdk8</span><br><span class="line">export HADOOP_JOB_HISTORYSERVER_HEAPSIZE=1000</span><br><span class="line">export HADOOP_MAPRED_ROOT_LOGGER=INFO,RFA</span><br></pre></td></tr></table></figure><p>hadoop-env.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">在最后添加语句</span></span><br><span class="line">export JAVA_HOME=/usr/local/servers/jdk8</span><br><span class="line">export HDFS_NAMENODE_OPTS=&quot;-XX:+UseParallelGC -Xmx4g&quot;</span><br><span class="line">export NAMENODE_HEAPSIZE=&quot;-Xmx204m&quot;</span><br></pre></td></tr></table></figure><h4 id="安装zookeeper"><a href="#安装zookeeper" class="headerlink" title="安装zookeeper"></a>安装zookeeper</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">下载安装包</span></span><br><span class="line">cd /usr/local/softwares</span><br><span class="line">wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.6.2/apache-zookeeper-3.6.2-bin.tar.gz</span><br><span class="line">tar -zxvf apache-zookeeper-3.6.2-bin.tar.gz -C ../servers</span><br><span class="line">cd ../servers</span><br><span class="line">mv apache-zookeeper-3.6.2-bin zookeeper-3.6.2</span><br><span class="line">cd zookeeper-3.6.2</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">配置文件修改</span></span><br><span class="line">mkdir ./data</span><br><span class="line">cp ./conf/zoo_sample.cfg ./conf/zoo.cfg</span><br><span class="line">vim ./conf/zoo.cfg</span><br><span class="line"><span class="meta"># </span><span class="language-bash">在最后添加以下语句</span></span><br><span class="line">dataDir=/usr/local/servers/zookeeper-3.6.2/data</span><br><span class="line">server.1=node01:2888:3888</span><br><span class="line">server.2=node02:2888:3888</span><br><span class="line">server.3=node03:2888:3888</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">创建myid文件</span></span><br><span class="line">touch ./data/myid</span><br><span class="line">echo &quot;1&quot; &gt;&gt; ./data/myid  # 在scp到其他机器上后要修改myid为2、3、4...</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">将用户修改为elk，复制到其他主机并启动</span></span><br><span class="line">chown -R elk. /usr/local/servers/zookeeper-3.6.2</span><br><span class="line">scp -r zookeeper-3.6.2 node02:$PWD  # 记得修改myid</span><br><span class="line">scp -r zookeeper-3.6.2 node03:$PWD  # 记得修改myid</span><br><span class="line">su -elk  # 切换用户并启动</span><br><span class="line">cd /usr/local/servers/zookeeper-3.6.2/bin</span><br><span class="line">./zkServer.start  # 其他主机也是一样</span><br><span class="line">./zkServer.status  # 查看主机是leader或者follower</span><br><span class="line">./zkServer.stop  # 停止zookeeper</span><br></pre></td></tr></table></figure><h4 id="安装Hive"><a href="#安装Hive" class="headerlink" title="安装Hive"></a>安装Hive</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">下载解压</span></span><br><span class="line">cd /usr/local/softwares</span><br><span class="line">wget http://archive.apache.org/dist/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz</span><br><span class="line">tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /usr/local/servers/</span><br><span class="line">cd /usr/local/servers</span><br><span class="line">mv apache-hive-3.1.2-bin hive-3.1.2</span><br><span class="line">cd hive-3.1.2/conf</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">修改配置</span></span><br><span class="line">cp ./hive-env.sh.template hive-env.sh</span><br><span class="line">mkdir -p /data/hive/iotmp</span><br><span class="line">vim hive-env.sh  # 修改HADOOP_HOME和HIVE_CONF_DIR（hive的conf目录）</span><br><span class="line">vim hive-site.xml  # 新建配置文件，内容如下</span><br><span class="line">cd ../lib</span><br><span class="line">wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.23.tar.gz  # 下载连接mysql所需的jar包并放入lib文件夹里</span><br><span class="line">tar -zxvf mysql-connector-java-8.0.23.tar.gz </span><br><span class="line">cd mysql-connector-java-8.0.23/</span><br><span class="line">mv mysql-connector-java-8.0.23.jar ../</span><br><span class="line">cd ..</span><br><span class="line">rm -rf mysql-connector-java-8.0.23.tar.gz  mysql-connector-java-8.0.23</span><br><span class="line">chown -R elk. /usr/local/servers/hive-3.1.2</span><br><span class="line">chown -R elk. /data/hive</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">启动前的准备（检查Hadoop和hive的guava.jar版本是否一致，如不一致删除低版本的）</span></span><br><span class="line">vim /etc/profile  # 确保hive和hadoop home在环境变量中</span><br><span class="line"></span><br><span class="line">export  HADOOP_HOME=&quot;/usr/local/servers/hadoop-3.2.1&quot;</span><br><span class="line">export HIVE_HOME=&quot;/usr/local/servers/hive-3.1.2&quot;</span><br><span class="line">export PATH=&quot;$PATH:$HIVE_HOME/bin</span><br><span class="line">export PATH=&quot;$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin&quot;</span><br><span class="line"></span><br><span class="line">ls /usr/local/servers/hive-3.1.2/lib/</span><br><span class="line">ls /usr/local/servers/hadoop-3.2.1/share/hadoop/common/lib</span><br><span class="line">cp /usr/local/servers/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar /usr/local/servers/hive-3.1.2/lib/  # 将高版本的jar包复制过去</span><br><span class="line">rm -rf /usr/local/servers/hive-3.1.2/lib/guava-19.0.jar  # 删除低版本的jar包</span><br><span class="line">su - elk</span><br><span class="line">cd /usr/local/servers/hive-3.1.2</span><br><span class="line">./bin/schematool -dbType mysql -initSchema # schematool初始化当前Hive版本的Metastore架构</span><br><span class="line">./bin/hive  # 启动hive</span><br></pre></td></tr></table></figure><p>hive-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!--连接MySQL的链接--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionURL<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>jdbc:mysql://node01:3306/hive?createDatabaseIfNotExist=true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定驱动类型--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionDriverName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>com.mysql.cj.jdbc.Driver<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--指定MySQL用户名--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionUserName<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>root<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--连接MySQL密码--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>javax.jdo.option.ConnectionPassword<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>123456<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--Hive表存放位置--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.metastore.warehouse.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/warehouse<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span>     #会在hdfs生成相应路径</span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.querylog.location<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/user/hive/log<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.exec.local.scratchdir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/hive/iotmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Local scratch space for Hive jobs<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">name</span>&gt;</span>hive.downloaded.resources.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">value</span>&gt;</span>/data/hive/iotmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">description</span>&gt;</span>Temporary local directory for added resources in the remote file system.<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><h3 id="Zookeeper"><a href="#Zookeeper" class="headerlink" title="Zookeeper"></a>Zookeeper</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">cd /usr/local/servers/zookeeper-3.6.2/bin</span><br><span class="line">./zkServer.sh start  # 启动zkserver，关闭则改为stop</span><br><span class="line">./zkClient.sh  # 进入client交互模式</span><br><span class="line">ls /  # 查看节点、文件（-s返回详细信息，-R返回递归目录）</span><br><span class="line"><span class="meta"># </span><span class="language-bash">新建节点并写入hello内容（-s顺序节点，-e临时节点；也可在已有节点下创建子节点，不可以在上层节点不存在的情况下创建子节点）</span></span><br><span class="line">create -s -e /tmp hello</span><br><span class="line">get /tmp  # 获取节点数据（-s获取详细内容，包括创建修改时间、子节点数量等）</span><br><span class="line">get -w /app1  # watch监控（内容修改时会发送消息）</span><br><span class="line">set /tmp world  # 修改节点内容</span><br><span class="line"><span class="meta"># </span><span class="language-bash">删除节点</span></span><br><span class="line">delete /tmp  # 若节点下有子节点则删除失败</span><br><span class="line">deleteall /app1  # 递归删除（删除节点及子节点）</span><br></pre></td></tr></table></figure><h3 id="HDFS"><a href="#HDFS" class="headerlink" title="HDFS"></a>HDFS</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh  # 启动HDFS集群</span><br><span class="line">stop-dfs.sh  # 关闭HDFS集群</span><br><span class="line"></span><br><span class="line">hdfs dfs -ls /  # 查看路径下的文件或者文件夹</span><br><span class="line">hdfs dfs -mkdir -p /a/b/c  # 递归创建文件夹</span><br><span class="line">hdfs dfs -moveFronLocal localpath hdfspath  # 从本地上传文件到hdfs中（本地文件会被删除）</span><br><span class="line">hdfs dfs -mv oldpath newpath  # 移动文件</span><br><span class="line">hdfs dfs -put localpath hdfspath  # 从本地上传文件到hdfs中（文件还在本地）</span><br><span class="line">hdfs dfs -appendToFile a.txt b.txt /c.txt  # 追加本地文件a、b到hdfs文件c中</span><br><span class="line">hdfs dfs -cat /c.txt  # 打印文件到shell</span><br><span class="line">hdfs dfs -cp oldpath newpath  # 拷贝文件</span><br><span class="line">hdfs dfs -rm path  # 删除文件夹或者文件（如需递归加参数r）</span><br><span class="line"></span><br><span class="line">hdfs dfs -chmod -R 777 filepath  # 更改文件访问权限</span><br><span class="line">hdfs dfs -chown -R elk:elk filepath  # 更改文件用户组和用户</span><br></pre></td></tr></table></figure><h3 id="MapReduce"><a href="#MapReduce" class="headerlink" title="MapReduce"></a>MapReduce</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">start-yarn.sh # 启动Map Reduce</span><br><span class="line">stop-yarn.sh # 停止Map Reduce</span><br><span class="line">mr-jobhistory-daemon.sh start historyserver  # 启动Map Reduce历史记录</span><br><span class="line">mr-jobhistory-daemon.sh stop historyserver  # 停止Map Reduce历史记录</span><br></pre></td></tr></table></figure><h2 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h2><p>Hive基本操作如下。Hive真正的难点在自定义函数，详见&#x3D;&#x3D;<a href="https://cwiki.apache.org/confluence/display/HIve/HivePlugins">官方文档</a>&#x3D;&#x3D;。</p><h3 id="初始化启动"><a href="#初始化启动" class="headerlink" title="初始化启动"></a>初始化启动</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">Hive启动一（会话关闭Hive关闭，<span class="built_in">log</span>打印到控制台）</span></span><br><span class="line">hive --service metastore &amp;  # 启动Hive元数据服务</span><br><span class="line">netstat -atunlp | grep 9083</span><br><span class="line">hive --service hiveserver2 &amp;  # 启动Hive的服务端，供外部连接使用</span><br><span class="line">netstat -atunlp | grep 10000</span><br><span class="line"><span class="meta"># </span><span class="language-bash">Hive启动二（会话关闭Hive关闭，<span class="built_in">log</span>重定向到文件）</span></span><br><span class="line">hive --service metastore &gt;&gt; /tmp/root/hivemetastore.log 2&gt;&amp;1 &amp;</span><br><span class="line">hive --service hiveserver2 &gt;&gt; /tmp/root/hiveserver2.log 2&gt;&amp;1 &amp;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">Hive启动三（Hive不受会话关闭影响，<span class="built_in">log</span>重定向到文件）</span></span><br><span class="line">nohup hive --service metastore &gt;&gt; /tmp/root/hivemetastore.log 2&gt;&amp;1 &amp;</span><br><span class="line">nohup hive --service hiveserver2 &gt;&gt; /tmp/root/hiveserver2.log 2&gt;&amp;1 &amp;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">初始化MySQL元数据库</span></span><br><span class="line">schematool -initSchema -dbType mysql -verbose</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">Hive关闭</span></span><br><span class="line">jps  # 找到RunJar对应的端口号，一般有两个</span><br><span class="line">kill -9 端口号</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">Hive参数</span></span><br><span class="line">hive -e &quot;select * from mall.user;&quot;  # 执行sql语句</span><br><span class="line">hive -f ./xxx.sql  # 执行本地sql文件</span><br><span class="line">hive -f hdfs://tmp/xxx.sql  # 执行hdfs sql文件</span><br><span class="line">hive -hiveconf proprepty=value  # 给指定参数传入值，在当前会话有效</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">beeline连接Hive</span></span><br><span class="line">beeline</span><br><span class="line">!connect jdbc:hive2://localhost:10000</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">测试tez</span></span><br><span class="line"><span class="meta">$</span><span class="language-bash">HADOOP_HOME/bin/yarn jar <span class="variable">$TEZ_HOME</span>/tez-examples-0.10.1-SNAPSHOT.jar orderedwordcount /tez/word.txt /tez/output/</span></span><br></pre></td></tr></table></figure><h3 id="常用语法"><a href="#常用语法" class="headerlink" title="常用语法"></a>常用语法</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># HQL语法</span><br><span class="line"><span class="keyword">create</span> database dbname location <span class="string">&#x27;/hivedb&#x27;</span>;  # 指定新建数据库的存放位置，默认在xml中配置</span><br><span class="line"><span class="keyword">alter</span> database dbname <span class="keyword">set</span> dbpropreties(<span class="string">&#x27;createtime&#x27;</span><span class="operator">=</span><span class="string">&#x27;20210101&#x27;</span>);  # 修改数据库基本信息</span><br><span class="line"><span class="keyword">desc</span> database [extended] dbname;  # 查看数据库基本信息，extended参数查看详细内容</span><br><span class="line"><span class="keyword">desc</span> [formatted] tb;  # 查看表信息，formatted查看详细信息</span><br><span class="line"><span class="keyword">drop</span> database dbname [cascade];  # 删除数据库，cascade如果其中有表强制删除</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tb <span class="keyword">values</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>);  # 插入数据，会很慢，因为使用mapreduce</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tb <span class="keyword">as</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> othertb;  # 复制表结构和表内容</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tb <span class="keyword">like</span> othertb;  # 复制表结构</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> oldname rename <span class="keyword">to</span> newname;  # 更改表名</span><br><span class="line"><span class="keyword">show</span> functions;  # 查看内置函数</span><br><span class="line"><span class="keyword">desc</span> <span class="keyword">function</span> [extended] upper; # 查看函数的用法，extended返回详细内容</span><br><span class="line"></span><br><span class="line"># 创建表（内部表：会将数据移动到数据仓库指向的路径；外部表：仅记录数据所在的路径，不改变数据位置，删除表时，内部表的元数据和数据一起被删除，而外部表只会删除元数据，不会删除数据）</span><br><span class="line"><span class="keyword">CREATE</span> [<span class="keyword">EXTERNAL</span>] <span class="keyword">TABLE</span> [IF <span class="keyword">NOT</span> <span class="keyword">EXISTS</span>] table_name  # <span class="keyword">EXTERNAL</span>表示创建外部表</span><br><span class="line">[(col_name data_type [COMMENT col_comment], ...)]  # 表字段</span><br><span class="line">[COMMENT table_comment]  # 为表与字段增加注释</span><br><span class="line">[PARTITIONED <span class="keyword">BY</span> (col_name data_type [COMMENT col_comment], ...)]  # 分区，一个表可以有多个分区，每个分区以文件夹的方式单独存储在表的目录下</span><br><span class="line">[CLUSTERED <span class="keyword">BY</span> (col_name, col_name, ...)  # 分桶，获得更高的查询效率</span><br><span class="line">[SORTED <span class="keyword">BY</span> (col_name [<span class="keyword">ASC</span><span class="operator">|</span><span class="keyword">DESC</span>], ...)] <span class="keyword">INTO</span> num_buckets BUCKETS]  # 排序</span><br><span class="line">[<span class="type">ROW</span> FORMAT row_format]  # 字段之间的分隔符</span><br><span class="line">[STORED <span class="keyword">AS</span> file_format]  # 存储类型，一般是textfile纯文本，需要压缩SEQUENCE</span><br><span class="line">[LOCATION hdfs_path]  # 指定存储位置</span><br></pre></td></tr></table></figure><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"># 创建外部表示例</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">external</span> <span class="keyword">table</span> movies(id <span class="type">int</span>,content string,genre string) </span><br><span class="line">comment <span class="string">&#x27;This is movies.&#x27;</span> </span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span> </span><br><span class="line">stored <span class="keyword">as</span> textfile;  # 创建外部表示例</span><br><span class="line"># 从本地加载数据到hive，数据上传到movies表目录下，overwrite覆盖原数据（数据不要有标题）</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/tmp/win/movies.csv&#x27;</span> [overwrite] <span class="keyword">into</span> <span class="keyword">table</span> movies;</span><br><span class="line"># 从hdfs文件系统加载数据到hive，会将文件移动到movies表目录下</span><br><span class="line">load data inpath <span class="string">&#x27;/user/elk/tmp/movies.csv&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> movies;</span><br><span class="line"></span><br><span class="line"># 创建分区表，实则就是在表下面用一个类型标识这部分数据并分开存储在不同的目录下，一般用时间标识</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tb(id <span class="type">int</span>, name string) </span><br><span class="line">partitioned <span class="keyword">by</span> (<span class="keyword">year</span> string)  # 可支持多个分区</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;  # 创建分区表</span><br><span class="line"># 加载数据并标识数据为<span class="number">2020</span>，继续加载其他年份的数据改变<span class="keyword">year</span>值即可，会在表目录下生成不同的文件夹</span><br><span class="line">load data <span class="keyword">local</span> inpath <span class="string">&#x27;/tmp/win/tb.csv&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> movies <span class="keyword">partition</span>(<span class="keyword">year</span><span class="operator">=</span><span class="string">&#x27;2020&#x27;</span>);</span><br><span class="line"># 如果不加<span class="keyword">where</span>条件会查询出所有的内容，如需查询单个分区则需加上<span class="keyword">where</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tb <span class="keyword">where</span> <span class="keyword">year</span> <span class="operator">=</span><span class="operator">=</span> <span class="string">&#x27;2020&#x27;</span>;</span><br><span class="line"># 若需查询多个分区的内容，各查询之间用<span class="keyword">union</span>连接</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tb <span class="keyword">where</span> <span class="keyword">year</span> <span class="operator">=</span><span class="operator">=</span> <span class="string">&#x27;2020&#x27;</span> <span class="keyword">union</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tb <span class="keyword">where</span> <span class="keyword">year</span> <span class="operator">=</span><span class="operator">=</span> <span class="string">&#x27;2021&#x27;</span>;</span><br><span class="line"><span class="keyword">show</span> <span class="keyword">partition</span> tb;  # 查看所有分区</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb <span class="keyword">add</span> <span class="keyword">partition</span>(<span class="keyword">year</span><span class="operator">=</span><span class="string">&#x27;2019&#x27;</span>)  # 增加分区</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb <span class="keyword">drop</span> <span class="keyword">partition</span>(<span class="keyword">year</span><span class="operator">=</span><span class="string">&#x27;2019&#x27;</span>)  # 删除分区</span><br><span class="line"></span><br><span class="line"># 创建分桶表（按照数据字段将数据划分到多个文件中去）</span><br><span class="line"><span class="keyword">set</span> hive.enforce.bucketing<span class="operator">=</span><span class="literal">true</span>;  # 开启分桶功能</span><br><span class="line"><span class="keyword">set</span> mapreduce.job.reduces<span class="operator">=</span><span class="number">3</span>;  # 设置reduce个数</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tb(id <span class="type">int</span>, name string, type string) </span><br><span class="line">clustered <span class="keyword">by</span> (type) <span class="keyword">into</span> <span class="number">3</span> buckets  # 可支持多个分区</span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;  # 创建分桶表</span><br><span class="line"># 加载数据不用以前的方法，使用<span class="keyword">insert</span> overwrite</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tb2(id <span class="type">int</span>, name string, type string) </span><br><span class="line"><span class="type">row</span> format delimited fields terminated <span class="keyword">by</span> <span class="string">&#x27;,&#x27;</span>;  # 创建普通表用来辅助</span><br><span class="line">load data inpath <span class="string">&#x27;/user/elk/tmp/movies.csv&#x27;</span> <span class="keyword">into</span> <span class="keyword">table</span> tb2;  # 加载数据到辅助表</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> tb <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tb2 cluster <span class="keyword">by</span>(type);  # 通过cluster <span class="keyword">by</span>分桶加载</span><br></pre></td></tr></table></figure><h2 id="Docker"><a href="#Docker" class="headerlink" title="Docker"></a>Docker</h2><h3 id="安装-1"><a href="#安装-1" class="headerlink" title="安装"></a>安装</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">Linux安装起来比较麻烦</span></span><br><span class="line">yum install -y yum-utils</span><br><span class="line">yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo</span><br><span class="line">yum install -y docker-ce docker-ce-cli containerd.io</span><br><span class="line">systemctl start docker</span><br></pre></td></tr></table></figure><h3 id="镜像命令"><a href="#镜像命令" class="headerlink" title="镜像命令"></a>镜像命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">查看镜像</span></span><br><span class="line">docker images</span><br><span class="line">docker images [-aq]  # 加参数aq只显示镜像id</span><br><span class="line"><span class="meta"># </span><span class="language-bash">搜索docker hub中的镜像</span></span><br><span class="line">docker search mysql</span><br><span class="line"><span class="meta"># </span><span class="language-bash">拉取镜像</span></span><br><span class="line">docker pull mysql</span><br><span class="line">docker pull mysql:5.7  # 拉取指定版本镜像</span><br><span class="line"><span class="meta"># </span><span class="language-bash">删除镜像</span></span><br><span class="line">docker rmi -f 镜像ID [镜像ID] [镜像ID]</span><br><span class="line">docker rmi -f $(docker images -aq)  # 删除所有镜像</span><br></pre></td></tr></table></figure><h3 id="容器命令"><a href="#容器命令" class="headerlink" title="容器命令"></a>容器命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">容器运行</span></span><br><span class="line">docker fun [可选参数] image</span><br><span class="line">--name=&quot;name&quot;  # 容器名字，用来区分容器</span><br><span class="line">-d  # 后台运行</span><br><span class="line">-it  # 使用交互式运行，进入容器查看内容</span><br><span class="line">-p  # 指定容器端口 -p 8080:8080</span><br><span class="line">    -p ip:主机端口：容器端口</span><br><span class="line">    -p 主机端口：容器端口（常用）</span><br><span class="line">    -p 容器端口</span><br><span class="line">    -p  # 不加参数随机指定端口</span><br><span class="line">docker run -it ubuntu /bin/bash  # 启动并进入容器</span><br><span class="line">docker run -d --name mysql01 -p:3344:3306 mysql  # 后台启动一个MySQL镜像命名为mysql01并将容器内的3306端口映射到主机3344端口上，访问主机3344就可以访问到mysql01</span><br><span class="line">docker run -it --rm tomcat  # 用完即删，用作测试，不建议使用</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">win10启动容器（必须在docker desktop setting/Resources/FILE SHARING中增加文件共享目录e：workspace/docker/volume）</span></span><br><span class="line">docker run -d -it --name test -v /e/workspace/docker/volume/test:/root centos </span><br><span class="line"></span><br><span class="line">exit  # 退出容器，容器也会停止（Ctrl + P + Q退出容器不停止容器）</span><br><span class="line">docker ps  # 查看正在运行的容器</span><br><span class="line">docker ps -a  # 查看以前运行过的所有容器</span><br><span class="line">docker rm 容器id  # 删除没有正在运行的容器</span><br><span class="line">docker rm -f 容器id  # 强制删除容器</span><br><span class="line">docker rm -f $(docker ps -aq)  # 删除所有容器</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">启动和停止容器</span></span><br><span class="line">docker start 容器id  # 启动容器</span><br><span class="line">docker restart 容器id  # 重启容器</span><br><span class="line">docker stop 容器id  # 停止容器</span><br><span class="line">docker kill 容器id  # 杀死容器</span><br><span class="line"><span class="meta"># </span><span class="language-bash">只要容器在数据就在，不会丢失，重新启动即可（使用docker ps -a查看已关闭的容器）</span></span><br></pre></td></tr></table></figure><h3 id="其他命令"><a href="#其他命令" class="headerlink" title="其他命令"></a>其他命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">查看日志、元数据</span></span><br><span class="line">docker -tfn 20 容器ID  # 查看容器日志，t表示显示时间戳，f跟踪日志输出，n指定输出日志数量</span><br><span class="line">docker top 容器ID  # 查看容器内进程</span><br><span class="line">docker inspect 容器ID  # 查看容器元数据</span><br><span class="line">docker stats [容器ID] # 查看所有/指定容器内存使用情况</span><br><span class="line">docker exec -it 容器ID /bin/bash  # 进入正在运行的容器（进入容器开启一个新的终端，常用）</span><br><span class="line">docker attach 容器ID  # 进入正在运行的容器（进入容器正在执行的终端）</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">拷贝容器内文件到当前主机</span></span><br><span class="line">docker cp 容器ID:/root/test /home/wxk</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">内存管理</span></span><br><span class="line">docker stats 容器ID  # 查看容器占用内存情况</span><br><span class="line">docker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e ES_JAVA_OPTS=&quot;-Xms64m -Xmx512m&quot; elasticsearch  # 通过-e修改配置文件，最大最小内存</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">Commit镜像，保存当前容器的所有状态，打包成一个镜像</span></span><br><span class="line">docker commit -a &quot;作者&quot; -m &quot;提交信息说明&quot; 容器ID 镜像名称:镜像版本</span><br><span class="line">docker commit -a &quot;wxk&quot; -m &quot;add test&quot; 38ff0892001a ubuntu01:1.0</span><br></pre></td></tr></table></figure><h3 id="容器数据卷"><a href="#容器数据卷" class="headerlink" title="容器数据卷"></a>容器数据卷</h3><p>容器之间的一个共享技术，Docker容器中产生的数据，可以同步到本地作持久化，将容器内的目录挂载到Linux中，跟容器是否启动无关，这样在容器删了之后不会面对数据也不存在的情况。而且容器之间也是可以数据共享的。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">新建数据卷</span></span><br><span class="line">docker run -d -v 本机目录:容器目录 [-v 本机目录:容器目录] 容器ID  # 指定路径挂载，将两个/多个目录连接起来</span><br><span class="line">docker run -d -v 容器目录 容器ID  # 匿名挂载，只有容器内的路径</span><br><span class="line">docker run -d -v 卷名:容器目录 容器ID  # 具名挂载，给数据卷起个名字（常用）</span><br><span class="line">docker run -d -v 卷名:容器目录:ro/rw 容器ID  # ro表示容器内只读，只能主机写；默认rw，主机和容器都可读可写</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看数据卷情况</span></span><br><span class="line">docker volume ls  # 查看所有数据卷</span><br><span class="line">docker colume inspect 卷名  # 查看指定卷名的情况，包括本地路径</span><br></pre></td></tr></table></figure><h3 id="Dockerfile、Docker网络"><a href="#Dockerfile、Docker网络" class="headerlink" title="Dockerfile、Docker网络"></a>Dockerfile、Docker网络</h3><p>Dockerfile就是用来构造docker镜像的构造文件，通过脚本可以生成自己的镜像，指路<a href="https://www.bilibili.com/video/BV1og4y1q7M4?p=26">教程</a>。<br>Docker网络在容器之间的通信和集群的搭建需要考虑，同样指路上方教程。</p><h3 id="发布镜像"><a href="#发布镜像" class="headerlink" title="发布镜像"></a>发布镜像</h3><p>可以发布镜像到自己的<a href="https://cr.console.aliyun.com/cn-beijing/instances/repositories">阿里云容器镜像服务</a>，相当于是一个仓库，创建自己的镜像，以供自己和他人随时随地进行拉取。新建仓库push自己的镜像，使用的时候直接使用公网地址pull即可。</p><h3 id="Docker网络"><a href="#Docker网络" class="headerlink" title="Docker网络"></a>Docker网络</h3><p>每启动一个Docker容器，就会给容器分配一个IP，使用的是evth-pair桥接模式，在宿主机使用ip-addr查看docker0以及桥接的网络。<br>evth-pair就是一对的虚拟接口设备，他们都是成对出现的，一端连着协议，一端连着彼此，正因为有这个特性，evth-pair充当一个桥梁，连接各种虚拟网络设备。</p><p>Tomcat01和Tomcat02共用同一个路由器docker0，所有容器不指定网络的情况下，都是由docker0路由的，docker会给容器分配一个默认的可用IP，当然每一次启动都会重新分配新的IP。<br>而Docker使用的是Linux的桥接，宿主机是一个Docker容器的网桥docker0。Docker中所有的网络接口都是虚拟的，转发效率高。<br>Docker0不支持容器名连接，只能使用link，但是会有局限性，所以一般会搭建自己的网络。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">查看容器网络配置信息</span></span><br><span class="line">docker network ls  # 查看所有容器网络</span><br><span class="line">docker network inspect 网络ID  # 查看容器网络配置的详细信息</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">创建自己的网络</span></span><br><span class="line">docker run -d -it --net bridge centos  # 默认创建容器时使用bridge桥接</span><br><span class="line">docker network create --driver bridge --subnet 192.168.0.0/16 --gateway 192.168.0.1 mynet  # 创建自己的网络，使用docker network create --help查看使用方法</span><br><span class="line">docker run -d -it --name ubuntu01 --net mynet ubuntu  # 使用自己的网络启动容器</span><br><span class="line">docker run -d -it --name ubuntu02 --net mynet ubuntu </span><br><span class="line">docker exec -it ubuntu01 ping ubuntu02  # 网络下启动的所有容器彼此可以通信，重要的是可以直接使用容器名（当然你得先在容器中安装iputils-ping）</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">连接一个容器到一个网络（一个容器两个地址，解决不同网络的容器互连问题）</span></span><br><span class="line">docker run -d -it --name ubuntu-docker0 ubuntu  # 使用默认网络bridge</span><br><span class="line">docker run -d -it --name ubuntu-mynet --net mynet ubuntu  # 使用自己的网络mynet</span><br><span class="line">docker network connect mynet ubuntu-docker0  # 连接一个容器到一个网络中，这样ubuntu-docker0和ubuntu-mynet就可以互相通信了</span><br></pre></td></tr></table></figure><h3 id="Docker-Compose"><a href="#Docker-Compose" class="headerlink" title="Docker-Compose"></a>Docker-Compose</h3><p><a href="https://docs.docker.com/compose/">Docker Compose</a>是一个用来定义和运行复杂应用的Docker工具。使用Compose，你可以在一个文件中定义一个多容器应用，然后使用一条命令来启动并管理你的应用，完成一切准备工作。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">安装docker-compose</span></span><br><span class="line">yum install docker-compose </span><br><span class="line">docker-compose version</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">docker-compose基本命令</span></span><br><span class="line">build 构建或重建服务</span><br><span class="line">help 命令帮助</span><br><span class="line">kill 杀掉容器</span><br><span class="line">logs 显示容器的输出内容</span><br><span class="line">port 打印绑定的开放端口</span><br><span class="line">ps 显示容器</span><br><span class="line">pull 拉取服务镜像</span><br><span class="line">restart 重启服务</span><br><span class="line">rm 删除停止的容器</span><br><span class="line">run 运行一个一次性命令</span><br><span class="line">scale 设置服务的容器数目</span><br><span class="line">start 开启服务</span><br><span class="line">stop 停止服务</span><br><span class="line">up 创建并启动容器</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">使用docker-compose构建wordpress</span></span><br><span class="line">mkdir wordpress</span><br><span class="line">cd wordpress</span><br><span class="line">vim docker-compose.yml  # 内容如下</span><br><span class="line">docker-compose -f docker-compose.wordpress.yml up -d  # 后台运行</span><br><span class="line"><span class="meta"># </span><span class="language-bash">后台访问地址：IP:3344/wp-admin</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">基本操作（需要在docker-compose.yml目录下操作）</span></span><br><span class="line">docker-compose ps  # 查看容器运行情况</span><br><span class="line">docker-compose logs  # 返回容器log</span><br><span class="line">docker-compose stop  # 停止相关的容器</span><br><span class="line">docker-compose start  # 启动相关的容器</span><br></pre></td></tr></table></figure><p>docker-compose.yml</p><figure class="highlight yml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr">version:</span> <span class="string">&#x27;3.3&#x27;</span></span><br><span class="line"><span class="attr">services:</span></span><br><span class="line">  <span class="attr">db:</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">mysql:5.7</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">db:/var/lib/mysql</span>  <span class="comment"># 数据卷，使用docker volume ls &amp; docker inspect 卷名查看挂载地址</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="attr">MYSQL_ROOT_PASSWORD:</span> <span class="string">wordpress</span></span><br><span class="line">      <span class="attr">MYSQL_DATABASE:</span> <span class="string">wordpress</span></span><br><span class="line">      <span class="attr">MYSQL_USER:</span> <span class="string">wordpress</span></span><br><span class="line">      <span class="attr">MYSQL_PASSWORD:</span> <span class="string">wordpress</span></span><br><span class="line">  <span class="attr">wordpress:</span></span><br><span class="line">    <span class="attr">depends_on:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">db</span>  <span class="comment"># 依赖</span></span><br><span class="line">    <span class="attr">image:</span> <span class="string">wordpress:latest</span></span><br><span class="line">    <span class="attr">ports:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">&quot;3344:80&quot;</span></span><br><span class="line">    <span class="attr">restart:</span> <span class="string">always</span></span><br><span class="line">    <span class="attr">environment:</span></span><br><span class="line">      <span class="attr">WORDPRESS_DB_HOST:</span> <span class="string">db:3306</span></span><br><span class="line">      <span class="attr">WORDPRESS_DB_USER:</span> <span class="string">wordpress</span></span><br><span class="line">      <span class="attr">WORDPRESS_DB_PASSWORD:</span> <span class="string">wordpress</span></span><br><span class="line">      <span class="attr">WORDPRESS_DB_NAME:</span> <span class="string">wordpress</span></span><br><span class="line">    <span class="attr">volumes:</span></span><br><span class="line">      <span class="bullet">-</span> <span class="string">wordpress:/var/www/html</span>  <span class="comment"># 数据卷</span></span><br><span class="line"><span class="attr">volumes:</span>  <span class="comment"># 使数据卷生效</span></span><br><span class="line">  <span class="attr">db:</span></span><br><span class="line">  <span class="attr">wordpress:</span></span><br></pre></td></tr></table></figure><h2 id="MongoDB"><a href="#MongoDB" class="headerlink" title="MongoDB"></a>MongoDB</h2><h4 id="命令行"><a href="#命令行" class="headerlink" title="命令行"></a>命令行</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">数据库集合相关</span></span><br><span class="line">show dbs;  # 显示所有数据库</span><br><span class="line">use mydb;  # 进入数据库</span><br><span class="line">db;  # 显示当前数据库名称</span><br><span class="line">db.dropDatabase();  # 删除当前进入的数据库，并且释放磁盘</span><br><span class="line">show collections;  # 查看所有集合，也可以用tables</span><br><span class="line">db.fruit.drop();  # 删除一个集合</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">插入数据</span></span><br><span class="line">db.fruit.insetOne(&#123;name : &quot;apple&quot;&#125;);  # 插入一条数据</span><br><span class="line">db.fruit.insetMany([  # 插入多条数据</span><br><span class="line">    &#123;name : &quot;pair&quot;&#125;,</span><br><span class="line">    &#123;name : &quot;orange&quot;&#125;</span><br><span class="line">]);</span><br><span class="line">db.fruit.insertOne(  # 插入多层次的数据</span><br><span class="line">    &#123;name:&quot;app&quot;,</span><br><span class="line">    from:&#123;</span><br><span class="line">        country:&quot;china&quot;,</span><br><span class="line">        province:&quot;beijing&quot;&#125;</span><br><span class="line">    &#125;</span><br><span class="line">)</span><br><span class="line">db.fruit.insert([  # 使用insert也可以插入多条，插入一个数组</span><br><span class="line">    &#123;name:&quot;ppp&quot;,color:[&quot;black&quot;,&quot;yeelow&quot;]&#125;,</span><br><span class="line">    &#123;name:&quot;www&quot;,color:[&quot;white&quot;,&quot;blue&quot;]&#125;]</span><br><span class="line">)</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">查找数据</span></span><br><span class="line">db.moives.find();  # find查询语句</span><br><span class="line">db.users.findOne();  # 只返回一条记录</span><br><span class="line">db.fruit.find().pretty();  # 结构化显示查询结果</span><br><span class="line">db.moives.find(&#123;&quot;year&quot; : 1975, &quot;title&quot; : &quot;agg&quot;&#125;);  # 多条件and查询</span><br><span class="line">db.moives.find(&#123;$and : [&#123;&quot;year&quot;:1975&#125;, &#123;&quot;title&quot;:&quot;agg&quot;&#125;]&#125;);  # 多条件and</span><br><span class="line">db.moives.find(&#123;$or : [&#123;&quot;year&quot;:1975&#125;, &#123;&quot;title&quot;:&quot;agg&quot;&#125;]&#125;);  # 多条件or</span><br><span class="line">db.fruit.find(&#123;&quot;from.country&quot;:&quot;china&quot;&#125;);  # 查找多层次结构数据</span><br><span class="line">db.fruit.find(&#123;&#125;,&#123;&quot;_id&quot;:0,&quot;name&quot;:1&#125;);  # 第一个大括号内是条件，第二个指定不返回_id字段，只返回name字段</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">删除数据</span></span><br><span class="line">db.fruit.remove(&#123;&#125;);  # 删除集合内所有内容，相当于sql中的delete</span><br><span class="line">db.fruit.remove(&#123;a: &#123;$lt: 5&#125;&#125;)  # 删除a小于5的记录</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">更新数据</span></span><br><span class="line">db.fruit.updateOne(&#123;name:&quot;app&quot;&#125;,&#123;$set:&#123;from.province:&quot;xian&quot;&#125;&#125;);  # 更新一条记录，第一个大括号是条件，第二个是更新的内容，若更新字段不存在则自动新建，不管匹配多少条，默认更新第一条</span><br><span class="line">db.fruit.updateMany(&#123;name:&quot;app&quot;&#125;,&#123;$set:&#123;from.province:&quot;xian&quot;&#125;&#125;); # 更新匹配到的多条记录，必须有以下字段：</span><br><span class="line"><span class="meta"># </span><span class="language-bash"><span class="variable">$set</span>/<span class="variable">$unset</span></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">/<span class="variable">$push</span>（增加一个对象到数组底部）/<span class="variable">$pushAll</span>（增加多个对象到数组底部）/<span class="variable">$pop</span>（从数组底部删除一个对象）</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">/<span class="variable">$pull</span>（如果匹配指定的值则从数组中删除）/<span class="variable">$pullAll</span>（如果匹配任意的值则从数组中删除相应的对象）</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">/<span class="variable">$addToSet</span>（如果不存在则增加一个值到数组）</span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">聚合查询</span></span><br><span class="line">db.users.aggregate([#select username &quot;姓名&quot;,age &quot;年龄&quot; from users where phone=&quot;19991259321&quot; limit 1 1;</span><br><span class="line">    &#123;&quot;$match&quot;:&#123;&quot;phone&quot;:&quot;19991259321&quot;&#125;&#125;,</span><br><span class="line">    &#123;&quot;$skip&quot;:1&#125;,</span><br><span class="line">    &#123;&quot;$limit&quot;:1&#125;,</span><br><span class="line">    &#123;&quot;$project&quot;:&#123;&quot;姓名&quot;:&quot;$username&quot;,&quot;年龄&quot;:&quot;$age&quot;&#125;&#125;</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">user_coll.insert(  # 插入一条数组记录</span><br><span class="line">    &#123;&quot;username&quot;:&quot;wang&quot;,&quot;score&quot;:[</span><br><span class="line">        &#123;&quot;subject&quot;:&quot;语文&quot;,&quot;score&quot;:80&#125;,</span><br><span class="line">        &#123;&quot;subject&quot;:&quot;数学&quot;,&quot;score&quot;:90&#125;,</span><br><span class="line">        &#123;&quot;subject&quot;:&quot;英语&quot;,&quot;score&quot;:78&#125;</span><br><span class="line">]&#125;)</span><br><span class="line">db.users.aggregate([&#123;&quot;$unwind&quot;:&quot;$score&quot;&#125;])  # 将数组拆分展示</span><br><span class="line"></span><br><span class="line">db.users.insert([</span><br><span class="line">    &#123;&quot;name&quot;:&quot;wxk&quot;, &quot;price&quot;:90&#125;,</span><br><span class="line">    &#123;&quot;name&quot;:&quot;wx&quot;, &quot;price&quot;:56&#125;,</span><br><span class="line">    &#123;&quot;name&quot;:&quot;qqq&quot;, &quot;price&quot;:66&#125;,</span><br><span class="line">    &#123;&quot;name&quot;:&quot;www&quot;, &quot;price&quot;:56&#125;,</span><br><span class="line">    &#123;&quot;name&quot;:&quot;eee&quot;, &quot;price&quot;:96&#125;,</span><br><span class="line">    &#123;&quot;name&quot;:&quot;rrr&quot;, &quot;price&quot;:36&#125;,</span><br><span class="line">    &#123;&quot;name&quot;:&quot;ttt&quot;, &quot;price&quot;:86&#125;,</span><br><span class="line">    &#123;&quot;name&quot;:&quot;yyy&quot;, &quot;price&quot;:76&#125;</span><br><span class="line">])</span><br><span class="line">db.users.aggregate([  # 分桶计数</span><br><span class="line">    &#123;$bucket:&#123;</span><br><span class="line">        groupBy:&quot;$score&quot;,</span><br><span class="line">        boundaries:[0,30,60,80,90],</span><br><span class="line">        default:&quot;Other&quot;,</span><br><span class="line">        output:&#123;&quot;count&quot;:&#123;&quot;$sum&quot;:1&#125;&#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;])</span><br></pre></td></tr></table></figure><h4 id="SQL和MQL"><a href="#SQL和MQL" class="headerlink" title="SQL和MQL"></a>SQL和MQL</h4><table><thead><tr><th>SQL</th><th>MQL</th></tr></thead><tbody><tr><td>a &#x3D; 1</td><td>{a : 1}</td></tr><tr><td>a &lt;&gt; 1</td><td>{a : {$ne : 1}}</td></tr><tr><td>a &gt; 1</td><td>{a : {$gt : 1}}</td></tr><tr><td>a &gt;&#x3D; 1</td><td>{a : {$gte : 1}}</td></tr><tr><td>a &lt; 1</td><td>{a : {$lt : 1}}</td></tr><tr><td>a &lt;&#x3D; 1</td><td>{a : {$lte : 1}}</td></tr><tr><td>a &#x3D; 1 AND b &#x3D; 1</td><td>{a &#x3D; 1, b &#x3D; 1}或者{$and: [{a&#x3D;1}, {b&#x3D;1}]}</td></tr><tr><td>a &#x3D; 1 OR b &#x3D; 1</td><td>{$or: [{a&#x3D;1}, {b&#x3D;1}]}</td></tr><tr><td>a IS NULL</td><td>{a: {$exsits: false}}</td></tr><tr><td>a IN (1, 2, 3)</td><td>{a: {$in: [1, 2, 3]}}</td></tr><tr><td>WHERE</td><td>$match</td></tr><tr><td>AS</td><td>$project</td></tr><tr><td>ORDER BY</td><td>$sort</td></tr><tr><td>GROUP BY</td><td>$group</td></tr><tr><td>SKIP&#x2F;LIMIT</td><td>$sklp&#x2F;$limit</td></tr><tr><td>LEFT OUTER JOIN</td><td>$lookup</td></tr></tbody></table><h4 id="Python操作"><a href="#Python操作" class="headerlink" title="Python操作"></a>Python操作</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pymongo  <span class="comment"># pymongo包</span></span><br><span class="line"><span class="keyword">import</span> dns  <span class="comment"># dnspython包</span></span><br><span class="line"><span class="comment"># 建立连接，使用的是Atlas集群</span></span><br><span class="line">client = pymongo.MongoClient(</span><br><span class="line">    <span class="string">&quot;mongodb+srv://wang:wangxukun1024@wxk.t4vz2.mongodb.net/myFirstDatabase?        retryWrites=true&amp;w=majority&quot;</span>)</span><br><span class="line"><span class="comment"># client = pymongo.MongoClient(&quot;mongodb://localhost:27017&quot;)  # 本地连接方式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 新建数据库eshop和表users</span></span><br><span class="line">db = client[<span class="string">&#x27;eshop&#x27;</span>]</span><br><span class="line">user_coll = db[<span class="string">&#x27;users&#x27;</span>]</span><br><span class="line">user_coll.insert_many([  <span class="comment"># 插入数据</span></span><br><span class="line">    &#123;<span class="string">&quot;username&quot;</span>:<span class="string">&quot;wxk&quot;</span>,<span class="string">&quot;age&quot;</span>:<span class="number">26</span>,<span class="string">&quot;email&quot;</span>:<span class="string">&quot;w749@qq.com&quot;</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;username&quot;</span>:<span class="string">&quot;wx&quot;</span>,<span class="string">&quot;age&quot;</span>:<span class="number">26</span>,<span class="string">&quot;email&quot;</span>:<span class="string">&quot;31923@qq.com&quot;</span>&#125;</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 更新数据</span></span><br><span class="line">user_coll.update_one(&#123;<span class="string">&quot;username&quot;</span>:<span class="string">&quot;wxk&quot;</span>&#125;,&#123;<span class="string">&quot;$set&quot;</span>:&#123;<span class="string">&quot;phone&quot;</span>:<span class="string">&quot;19991259321&quot;</span>&#125;&#125;)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 聚合</span></span><br><span class="line">user_coll.aggregate([</span><br><span class="line">    &#123;<span class="string">&quot;$match&quot;</span>:&#123;<span class="string">&quot;phone&quot;</span>:<span class="string">&quot;19991259321&quot;</span>&#125;&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;$skip&quot;</span>:<span class="number">1</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;$limit&quot;</span>:<span class="number">1</span>&#125;,</span><br><span class="line">    &#123;<span class="string">&quot;$project&quot;</span>:&#123;<span class="string">&quot;姓名&quot;</span>:<span class="string">&quot;$username&quot;</span>,<span class="string">&quot;年龄&quot;</span>:<span class="string">&quot;$age&quot;</span>&#125;&#125;</span><br><span class="line">])</span><br></pre></td></tr></table></figure><h2 id="Spark"><a href="#Spark" class="headerlink" title="Spark"></a>Spark</h2><h3 id="安装-2"><a href="#安装-2" class="headerlink" title="安装"></a>安装</h3><ol><li>Local模式，直接下载安装包解压即可，甚至不用配置，直接运行bin目录下的spark-shell就可以启动一个交互式spark环境</li><li>Standalone模式，区别于Local模式，这是多台机器组成的集群，有Master用于管理任务和Worker用于运行任务，配置时修改conf目录下的spark-env.sh文件，添加JAVA_HOME、SPARK_MASTER_HOST和SPARK_MASTER_PORT三个环境变量即可，用以指定Master机器，随后修改workers文件，添加计算任务的机器IP。最后将安装包分发到其他机器上，在Master机器上启动sbin目录下的start-all.sh即可。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">spark-env.sh</span></span><br><span class="line">JAVA_HOME=/usr/lib/java/jdk1.8.0_181</span><br><span class="line">SPARK_MASTER_HOST=node01</span><br><span class="line">SPARK_MASTER_PORT=7077</span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">workers</span></span><br><span class="line">node01</span><br><span class="line">node02</span><br><span class="line">node03</span><br></pre></td></tr></table></figure><ol start="3"><li>Yarn on Spark模式，这种模式是将Spark托管在Hadoop下的Yarn用以资源调度，配合Hadoop使用，只需要在spark-env.sh再添加两个环境变量，在spark-defaults.conf中添加几项配置，然后将jars目录下的所有jar包打包放在hdfs下，这个位置需要在spark-defaults.conf中指定，需要特别注意lzo的配置，目前就卡在这了，要将所需jar包放在jars目录下。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">spark-env.sh</span></span><br><span class="line">JAVA_HOME=/usr/lib/java/jdk1.8.0_181</span><br><span class="line">HADOOP_CONF_DIR=/opt/app/hadoop-3.1.3/etc/hadoop</span><br><span class="line">YARN_CONF_DIR=/opt/app/hadoop-3.1.3/etc/hadoop</span><br><span class="line">SPARK_MASTER_HOST=node01</span><br><span class="line">SPARK_MASTER_PORT=7077</span><br><span class="line">SPARK_MASTER_WEBUI_PORT=8080</span><br><span class="line">SPARK_HISTORY_OPTS=&quot;-Dspark.history.fs.logDirectory=hdfs://node01:9000/sparklog/ -Dspark.history.fs.cleaner.enabled=true&quot;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">workers</span></span><br><span class="line">node01</span><br><span class="line">node02</span><br><span class="line">node03</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">spark-defaults.conf</span></span><br><span class="line">spark.eventLog.enabled           true  # 开启日志</span><br><span class="line">spark.eventLog.dir               hdfs://node01:9000/sparklog/  # 日志位置，hdfs提前建好目录</span><br><span class="line">spark.eventLog.compress          true</span><br><span class="line">spark.yarn.historyServer.address node01:18080</span><br><span class="line">spark.yarn.jars                  hdfs://node01:9000/spark/jars/* /opt/app/spark-3.1.2-bin-hadoop3.2/jars/</span><br></pre></td></tr></table></figure><h3 id="运行"><a href="#运行" class="headerlink" title="运行"></a>运行</h3><p>Spark任务一般使用spark-submit命令提交，有下面几个参数比较常用</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line">--class spark.core.WordCount  # Spark程序中包含主程序的类</span><br><span class="line">--master local[*]             # Spark程序运行的模式（环境）</span><br><span class="line">--executor-memory 1G          # 指定每个executor可用内存为1G（512m）</span><br><span class="line">--total-executor-cores 2      # 指定所有executor使用的cpu核数为两个</span><br><span class="line">--executor-cores 2            # 指定每个executor使用的cpu核数</span><br><span class="line">spark-wordcount-1.0.jar       # 打包好的包含依赖的jar包，本地或者hdfs</span><br></pre></td></tr></table></figure><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>Spark 框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构。如下图所示，它展示了一个 Spark 执行时的基本结构。图形中的 Driver 表示 master， 负责管理整个集群中的作业任务调度。图形中的 Executor 则是 slave，负责实际执行任务。</p><div align=center><img src="image-20210825151104511.png"></div><p><strong>Diver &amp; Executor</strong></p><ul><li><strong>Diver</strong><br>Spark驱动节点，用于执行Spark任务中的main方法，负责实际代码的执行工作。</li><li><strong>Executor</strong><br>Spark Executor 是集群中工作节点(Worker)中的一个 JVM 进程，负责在 Spark 作业中运行具体任务(Task)，任务彼此之间相互独立。Spark应用启动时，Executor节点被同时启动，并且始终伴随着整个Spark应用的生命周期而存在。如果有Executor节点发生了 故障或崩溃，Spark应用也可以继续执行，会将出错节点上的任务调度到其他 Executor节点上继续运行。</li></ul><p><strong>Master &amp; Worker</strong></p><p>Spark集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调 度的功能，所以环境中还有其他两个核心组件:Master和Worker，这里的Master是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于Yarn环境中的ResourceManager, 而Worker呢，也是进程，一个Worker运行在集群中的一台服务器上，由 Master分配资源对 数据进行并行的处理和计算，类似于 Yarn 环境中NodeManager。</p><p><strong>ApplicationMaster</strong><br>Hadoop用户向YARN 集群提交应用程序时,提交程序中应该包含ApplicationMaster，用 于向资源调度器申请执行任务的资源容器Container，运行用户自己的程序任务job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。说的简单点就是，ResourceManager(资源)和Driver(计算)之间的解耦合靠的就是ApplicationMaster。</p><h3 id="核心概念"><a href="#核心概念" class="headerlink" title="核心概念"></a>核心概念</h3><p><strong>Executor与Core</strong></p><p>Spark Executor 是集群中运行在工作节点(Worker)中的一个 JVM 进程，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资 源。这里的资源一般指的是工作节点 Executor 的内存大小和使用的虚拟 CPU 核(Core)数 量。<br>应用程序相关启动参数如下:<code>--num-executors</code>、<code>--executor-memory</code>、<code>--executor-cores</code></p><p><strong>并行度(Parallelism)</strong> </p><p>在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行<br>计算，所以能够真正地实现多任务并行执行，记住，这里是并行，而不是并发。这里我们将 整个集群并行执行任务的数量称之为并行度。那么一个作业到底并行度是多少呢?这个取决 于框架的默认配置。应用程序也可以在运行过程中动态修改。</p><p><strong>有向无环图(DAG)</strong></p><p>大数据计算引擎框架我们根据使用方式的不同一般会分为四类，其中第一类就是 Hadoop 所承载的 MapReduce,它将计算分为两个阶段，分别为 Map 阶段 和 Reduce 阶段。 对于上层应用来说，就不得不想方设法去拆分算法，甚至于不得不在上层应用实现多个 Job 的串联，以完成一个完整的算法，例如迭代计算。 由于这样的弊端，催生了支持DAG 框 架的产生。因此，支持 DAG 的框架被划分为第二代计算引擎。如 Tez 以及更上层的 Oozie。这里我们不去细究各种 DAG 实现之间的区别，不过对于当时的 Tez 和 Oozie 来 说，大多还是批处理的任务。接下来就是以 Spark 为代表的第三代的计算引擎。第三代计 算引擎的特点主要是 Job 内部的 DAG 支持(不跨越 Job)，以及实时计算。</p><p><strong>提交流程</strong></p><p>所谓的提交流程，其实就是我们开发人员根据需求写的应用程序通过 Spark 客户端提交 给 Spark 运行环境执行计算的流程。在不同的部署环境中，这个提交过程基本相同，但是又 有细微的区别，我们这里不进行详细的比较，但是因为国内工作中，将 Spark 引用部署到 Yarn 环境中会更多一些，所以本课程中的提交流程是基于 Yarn 环境的。</p><div align=center><img src="image-20210825154407863.png"></div><p>Spark 应用程序提交到 Yarn 环境中执行的时候，一般会有两种部署执行的方式:Client 和 Cluster。两种模式主要区别在于:Driver 程序的运行节点位置。</p><ol><li><strong>Yarn Client 模式</strong></li></ol><p>Client 模式将用于监控和调度的 Driver 模块在客户端执行，而不是在 Yarn 中，所以一<br>般用于测试。</p><ul><li>Driver在任务提交的本地机器上运行</li><li>Driver启动后会和ResourceManager通讯申请启动ApplicationMaster</li><li>ResourceManager分配container，在合适的NodeManager上启动ApplicationMaster，负责向 ResourceManager 申请 Executor 内存</li><li>ResourceManager接到ApplicationMaster的资源申请后会分配container，然后ApplicationMaster 在资源分配指定的 NodeManager 上启动 Executor 进程</li><li>Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行 main 函数</li></ul><p>之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分stage，每个stage生 成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行。</p><ol start="2"><li><strong>Yarn Cluster 模式</strong></li></ol><p>Cluster 模式将用于监控和调度的 Driver 模块启动在 Yarn 集群资源中执行。一般应用于<br>实际生产环境。</p><ul><li>在YARNCluster模式下，任务提交后会和ResourceManager通讯申请启动<br>ApplicationMaster，</li><li>随后ResourceManager分配container，在合适的NodeManager上启动ApplicationMaster，<br>此时的 ApplicationMaster 就是 Driver。</li><li>Driver启动后向ResourceManager申请Executor内存，ResourceManager接到ApplicationMaster 的资源申请后会分配 container，然后在合适的 NodeManager 上启动Executor 进程</li><li>Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main 函数</li><li>之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分stage，每个stage生<br>成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行。</li></ul>]]></content>
      
      
      <categories>
          
          <category> Language </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Other </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ELK命令语法详解</title>
      <link href="/2020/10/18/Software/ELK%E5%91%BD%E4%BB%A4%E8%AF%AD%E6%B3%95%E8%AF%A6%E8%A7%A3/"/>
      <url>/2020/10/18/Software/ELK%E5%91%BD%E4%BB%A4%E8%AF%AD%E6%B3%95%E8%AF%A6%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<p>Elastic Search 、Logstash 和 Kibana 的安装使用以及常用的 ES 语法</p><span id="more"></span><h3 id="架构"><a href="#架构" class="headerlink" title="架构"></a>架构</h3><p>反正给我自己看，就不粘贴那些乱七八糟的了</p><h3 id="Elastic-Search-安装"><a href="#Elastic-Search-安装" class="headerlink" title="Elastic Search 安装"></a>Elastic Search 安装</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">下载安装包</span></span><br><span class="line">wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.2.1-linux-x86_64.tar.gz</span><br><span class="line"><span class="meta"># </span><span class="language-bash">解压修改文件夹名</span></span><br><span class="line">tar -zxvf elasticsearch-7.2.1-linux-x86_64.tar.gz -C ../servers/</span><br><span class="line">mv elasticsearch-7.2.1/ elasticsearch</span><br><span class="line"><span class="meta"># </span><span class="language-bash">给文件夹分配用户组，所有节点都要进行</span></span><br><span class="line">mkdir -p ./elasticsearch/&#123;data,logs&#125;  # 数据和日志</span><br><span class="line">groupadd elk</span><br><span class="line">useradd elk -g elk</span><br><span class="line">chown -R elk. /data/elasticsearch/</span><br><span class="line">chown -R elk. /usr/local/servers/elasticsearch/</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">修改elasticsearch.yml配置文件，先备份默认的</span></span><br><span class="line">cp ./elasticsearch/config/elasticsearch.yml ./elasticsearch/config/elasticsearch_copy.yml</span><br><span class="line">vi /usr/local/servers/elasticsearch/config/elasticsearch.yml</span><br><span class="line"><span class="meta"># </span><span class="language-bash">直接粘贴配置，各节点修改node.name名称</span></span><br><span class="line">path.data: /data/elasticsearch/data  #数据</span><br><span class="line">path.logs: /data/elasticsearch/logs  #日志</span><br><span class="line">cluster.name: ELK  # 集群中的名称</span><br><span class="line">cluster.initial_master_nodes: [&quot;192.168.163.100&quot;, &quot;192.168.163.110&quot;, &quot;192.168.163.120&quot;]  #主节点</span><br><span class="line">node.name: node01  # 该节点名称，与前面配置hosts保持一致</span><br><span class="line">node.master: true  # 意思是该节点是否可选举为主节点</span><br><span class="line">node.data: true  # 表示这不是数据节点</span><br><span class="line">network.host: 0.0.0.0  # 监听全部ip，在实际环境中应为一个安全的ip</span><br><span class="line">http.port: 9200  # es服务的端口号</span><br><span class="line">http.cors.enabled: true</span><br><span class="line">http.cors.allow-origin: &quot;*&quot;</span><br><span class="line">discovery.zen.minimum_master_nodes: 2 #防止集群“脑裂”，需要配置集群最少主节点数目，通常为 (主节点数目/2) + 1</span><br><span class="line">discovery.seed_hosts: [&quot;192.168.163.100&quot;, &quot;192.168.163.110&quot;, &quot;192.168.163.120&quot;]</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">将此节点安装目录发送给其他节点，其他节点elasticsearch.yml文件修改node.name节点名称</span></span><br><span class="line">scp -r elasticsearch root@node02:/usr/local/servers</span><br></pre></td></tr></table></figure><h4 id="各节点启动"><a href="#各节点启动" class="headerlink" title="各节点启动"></a>各节点启动</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">su - elk  # 切换到指定用户</span><br><span class="line"><span class="meta"># </span><span class="language-bash">启动elasticsearch</span></span><br><span class="line">cd /usr/local/servers/elasticsearch/bin</span><br><span class="line">./elasticsearch -d</span><br><span class="line"><span class="meta"># </span><span class="language-bash">如有错误查看<span class="built_in">log</span></span></span><br><span class="line">cat /data/elasticsearch/logs/ELK.log</span><br><span class="line"><span class="meta"># </span><span class="language-bash">检查是否启动</span></span><br><span class="line">netstat -lntup|grep java  # 查看java相关端口是否启动</span><br><span class="line">curl 192.168.163.100:9200  # 查看节点状态</span><br></pre></td></tr></table></figure><h4 id="可能遇到的问题"><a href="#可能遇到的问题" class="headerlink" title="可能遇到的问题"></a>可能遇到的问题</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">问题一：max virtual memory areas vm.max_map_count [65530] likely too low, increase to at least [262144]</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">解决：切换到root用户修改配置sysctl.conf</span></span><br><span class="line">vi /etc/sysctl.conf </span><br><span class="line">vm.max_map_count=655360  # 添加这条配置</span><br><span class="line">sysctl -p  # 退出执行命令</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">问题二：ERROR: bootstrap checks failed</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">max file descriptors [4096] <span class="keyword">for</span> elasticsearch process likely too low, increase to at least [65536] max number of threads [1024] <span class="keyword">for</span> user [lishang] likely too low, increase to at least [2048]</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">解决：切换到root用户，编辑limits.conf 添加类似如下内容</span></span><br><span class="line">vi /etc/security/limits.conf </span><br><span class="line"><span class="meta"># </span><span class="language-bash">添加如下</span></span><br><span class="line">* soft nofile 65536</span><br><span class="line">* hard nofile 131072</span><br><span class="line">* soft nproc 2048</span><br><span class="line">* hard nproc 4096</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">问题三：max number of threads [1024] <span class="keyword">for</span> user [lish] likely too low, increase to at least [2048]</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">解决：切换到root用户，进入limits.d目录下修改配置文件。</span></span><br><span class="line">vi /etc/security/limits.d/90-nproc.conf </span><br><span class="line"><span class="meta"># </span><span class="language-bash">修改如下内容</span> </span><br><span class="line">* soft nproc 1024</span><br><span class="line"><span class="meta"># </span><span class="language-bash">修改为</span></span><br><span class="line">* soft nproc 2048</span><br></pre></td></tr></table></figure><h3 id="Logstash安装"><a href="#Logstash安装" class="headerlink" title="Logstash安装"></a>Logstash安装</h3><p>Logstash 用来清洗并导入数据到es，需要注意的是如果是一次性数据导入则导入完成后就关闭进程，否则它会一直监控目标文件，若内容有变动则会重新执行整个conf文件</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">下载</span></span><br><span class="line">wget https://artifacts.elastic.co/downloads/logstash/logstash-7.2.1.tar.gz</span><br><span class="line"><span class="meta"># </span><span class="language-bash">解压</span></span><br><span class="line">tar -zxvf logstash-7.2.1.tar.gz -C /usr/local/servers</span><br><span class="line"><span class="meta"># </span><span class="language-bash">新建数据日志文件夹，分配权限</span></span><br><span class="line">mkdir -p /data/logstash/&#123;logs,data&#125;</span><br><span class="line">chown -R elk. /data/logstash/</span><br><span class="line">chown -R elk. /usr/local/servers/logstash/</span><br><span class="line"><span class="meta"># </span><span class="language-bash">备份原有配置</span></span><br><span class="line">cp /usr/local/servers/logstash/config/logstash.yml /usr/local/servers/logstash/config/logstash_copy.yml</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">修改配置</span></span><br><span class="line">vi /usr/local/servers/logstash/config/logstash.yml</span><br><span class="line"><span class="meta"># </span><span class="language-bash">直接替换原有内容</span></span><br><span class="line">http.host: &quot;192.168.163.100&quot;</span><br><span class="line">path.data: /data/logstash/data</span><br><span class="line">path.logs: /data/logstash/logs</span><br><span class="line">xpack.monitoring.enabled: true #kibana监控插件中启动监控logstash</span><br><span class="line">xpack.monitoring.elasticsearch.hosts: [&quot;192.168.163.100&quot;, &quot;192.168.163.110&quot;, &quot;192.168.163.120&quot;]</span><br><span class="line"><span class="meta"># </span><span class="language-bash">创建配置文件</span></span><br><span class="line">vim /usr/local/servers/logstash/config/logstash.conf </span><br><span class="line"><span class="meta"># </span><span class="language-bash">输入下文</span></span><br><span class="line">input &#123;</span><br><span class="line">  beats &#123;</span><br><span class="line">    port =&gt; 5044</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">output &#123;</span><br><span class="line">  stdout &#123;</span><br><span class="line">    codec =&gt; rubydebug</span><br><span class="line">  &#125;</span><br><span class="line">  elasticsearch &#123;</span><br><span class="line">    hosts =&gt; [&quot;192.168.163.100&quot;, &quot;192.168.163.110&quot;, &quot;192.168.163.120&quot;]</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">切换到指定用户</span></span><br><span class="line">su - elk</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">启动logstash</span></span><br><span class="line">cd /usr/local/servers/logstash/bin</span><br><span class="line">./logstash</span><br><span class="line"><span class="meta"># </span><span class="language-bash">或者使用<span class="built_in">nohup</span>可以后台运行</span></span><br><span class="line">nohup /usr/local/servers/logstash/bin/logstash &amp;</span><br></pre></td></tr></table></figure><p>logstash用法是针对每一次数据传入编写一次conf配置文件，文件中需指定输入包括路径和开始位置等参数，接下来是filter清洗数据，最后是output指定输出es的地址，以下为例</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">input &#123;</span><br><span class="line">  file &#123;</span><br><span class="line">    path =&gt; &quot;/usr/local/servers/logstash/data/movies/movies.csv&quot;</span><br><span class="line">    start_position =&gt; &quot;beginning&quot;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line">filter &#123;</span><br><span class="line">  csv &#123;</span><br><span class="line">    separator =&gt; &quot;,&quot;</span><br><span class="line">    columns =&gt; [&quot;id&quot;,&quot;content&quot;,&quot;genre&quot;]</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  mutate &#123;</span><br><span class="line">    split =&gt; &#123; &quot;genre&quot; =&gt; &quot;|&quot; &#125;</span><br><span class="line">    remove_field =&gt; [&quot;path&quot;, &quot;host&quot;,&quot;@timestamp&quot;,&quot;message&quot;]</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  mutate &#123;</span><br><span class="line"></span><br><span class="line">    split =&gt; [&quot;content&quot;, &quot;(&quot;]</span><br><span class="line">    add_field =&gt; &#123; &quot;title&quot; =&gt; &quot;%&#123;[content][0]&#125;&quot;&#125;</span><br><span class="line">    add_field =&gt; &#123; &quot;year&quot; =&gt; &quot;%&#123;[content][1]&#125;&quot;&#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  mutate &#123;</span><br><span class="line">    convert =&gt; &#123;</span><br><span class="line">      &quot;year&quot; =&gt; &quot;integer&quot;</span><br><span class="line">    &#125;</span><br><span class="line">    strip =&gt; [&quot;title&quot;]</span><br><span class="line">    remove_field =&gt; [&quot;path&quot;, &quot;host&quot;,&quot;@timestamp&quot;,&quot;message&quot;,&quot;content&quot;]</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line">output &#123;</span><br><span class="line">   elasticsearch &#123;</span><br><span class="line">     hosts =&gt; [&quot;192.168.163.100:9200&quot;, &quot;192.168.163.110:9200&quot;, &quot;192.168.163.120:9200&quot;]</span><br><span class="line">     index =&gt; &quot;movies&quot;</span><br><span class="line">     document_id =&gt; &quot;%&#123;id&#125;&quot;</span><br><span class="line">   &#125;</span><br><span class="line">  stdout &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Kibana安装"><a href="#Kibana安装" class="headerlink" title="Kibana安装"></a>Kibana安装</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">下载</span></span><br><span class="line">wget https://artifacts.elastic.co/downloads/kibana/kibana-7.2.1-linux-x86_64.tar.gz</span><br><span class="line"><span class="meta"># </span><span class="language-bash">解压</span></span><br><span class="line">tar -zxvf kibana-7.2.1-linux-x86_64.tar.gz -C /usr/local/servers</span><br><span class="line"><span class="meta">#</span><span class="language-bash">创建日志目录，分配权限</span></span><br><span class="line">mkdir -p /data/kibana/logs/</span><br><span class="line">chown -R elk. /data/kibana/</span><br><span class="line">chown -R elk. /usr/local/servers/kibana/</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">备份原有配置</span></span><br><span class="line">cp /usr/local/servers/kibana/config/kibana.yml /usr/local/servers/kibana/config/kibana_copy.yml</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">修改kibana配置文件</span></span><br><span class="line">vi /usr/local/servers/kibana/config/kibana.yml</span><br><span class="line"><span class="meta"># </span><span class="language-bash">以下</span></span><br><span class="line">server.port: 5601 # 配置kibana的端口，windows5601端口有时会被占用</span><br><span class="line">server.host: 192.168.163.100 # 配置监听ip(设置本地ip使用nginx认证登录)</span><br><span class="line">elasticsearch.hosts: [&quot;http://192.168.163.100:9200&quot;,&quot;http://192.168.163.110:9200&quot;,&quot;http://192.168.163.120:9200&quot;] # 配置es服务器的ip</span><br><span class="line">logging.dest: /data/kibana/logs/kibana.log # 配置kibana的日志文件路径，默认messages</span><br><span class="line">i18n.locale: &quot;zh-CN&quot; #配置中文语言</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">切换到指定用户</span></span><br><span class="line">su - elk</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">启动kibana</span></span><br><span class="line">cd /usr/local/servers/kibana/bin</span><br><span class="line">./kibana</span><br><span class="line"><span class="meta"># </span><span class="language-bash">或者使用<span class="built_in">nohup</span>可以后台运行</span></span><br><span class="line">nohup /usr/local/servers/kibana/bin/kibana &amp;</span><br></pre></td></tr></table></figure><h3 id="停止服务"><a href="#停止服务" class="headerlink" title="停止服务"></a>停止服务</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">Kibana</span></span><br><span class="line">ps -ef|grep kibana</span><br><span class="line"><span class="meta"># </span><span class="language-bash">Elasticsearch</span></span><br><span class="line">jps | grep Elasticsearch</span><br><span class="line"><span class="meta"># </span><span class="language-bash">返回进程号并<span class="built_in">kill</span></span></span><br><span class="line">kill -9 进程号</span><br></pre></td></tr></table></figure><h3 id="ES语法"><a href="#ES语法" class="headerlink" title="ES语法"></a>ES语法</h3><h4 id="常用"><a href="#常用" class="headerlink" title="常用"></a>常用</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">功能性</span></span><br><span class="line">GET _cat/indices  # 查看所有索引及基础信息</span><br><span class="line">GET 索引名/_mapping  # 查看索引内的映射关系</span><br><span class="line">GET _cluster/health  # 查看集群状态</span><br><span class="line">GET _cluster/stats  # 查看集群详细状态</span><br><span class="line">GET /_cluster/settings  # 查看集群配置</span><br><span class="line">GET _cat/nodes?v  # 查看所有nodes</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">查询</span></span><br><span class="line">from &amp; size  # 从哪开始返回几条数据</span><br><span class="line">_source  # 指定要返回的字段</span><br><span class="line">size &amp; sort  # 返回按列排序的多少条数据</span><br><span class="line">query &amp; match  # 匹配某个字段等于某个值</span><br><span class="line">query &amp; range  # 匹配某个字段在某个范围内</span><br><span class="line">query &amp; fuzzy  # 模糊查询</span><br><span class="line">query &amp; bool &amp; must/must_not &amp; match/range  # 多个条件同时满足/不满足，bool下可同时指定must和must_not</span><br><span class="line">query &amp; bool &amp; filter &amp; mtach/range  # 和must差不多，但不算分，效率比must快</span><br><span class="line">query &amp; bool &amp; should &amp; mtach/range  # should内是或的关系</span><br><span class="line">_sql  # 直接传入sql语句，进行简单的查询</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">聚合</span></span><br><span class="line">aggs &amp; sum/max/avg/min/value_count  # 返回多个聚合结果</span><br><span class="line">aggs &amp; stats  # 返回上方所有的聚合结果，返回数据的描述性信息</span><br><span class="line">aggs &amp; terms  # 分组并统计每个类型出现的次数，类似于groupby然后count</span><br><span class="line">aggs &amp; terms &amp; aggs &amp; terms  # 嵌套达到groupby两个或多个字段并count的效果</span><br><span class="line">aggs &amp; terms &amp; aggs &amp; stats  # groupby后返回每个bucket指定字段的统计信息</span><br><span class="line">aggs &amp; top_hits  # 在top_hits指定sort参数达到返回按列排序的前几条数据，那么用size&amp;sort不香吗</span><br><span class="line">aggs &amp; range  # 对数值进行分桶并计算每个bucket中的数量，指定ranges参数</span><br><span class="line">aggs &amp; histogram  # 按同样的区间对字段进行分组统计数量，指定interval步长即可</span><br></pre></td></tr></table></figure><h4 id="功能性"><a href="#功能性" class="headerlink" title="功能性"></a>功能性</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">查看所有索引及基础信息</span></span><br><span class="line">GET _cat/indices</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看索引内的映射关系</span></span><br><span class="line">GET 索引名/_mapping</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看集群状态</span></span><br><span class="line">GET _cluster/health</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看集群详细状态</span></span><br><span class="line">GET _cluster/stats</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看集群配置</span></span><br><span class="line">GET /_cluster/settings</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看索引存放的shards</span></span><br><span class="line">GET _cat/shards</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看master机器</span></span><br><span class="line">GET _cat/master?v</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看所有nodes</span></span><br><span class="line">GET _cat/nodes?v</span><br></pre></td></tr></table></figure><h4 id="写入更新"><a href="#写入更新" class="headerlink" title="写入更新"></a>写入更新</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">写入单条数据</span></span><br><span class="line">PUT user/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;:&quot;wxk&quot;,</span><br><span class="line">  &quot;age&quot;:26,</span><br><span class="line">  &quot;isRich&quot;:&quot;true&quot;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">定义mapping，一般es会自动创建mapping，先上传测试数据，然后修改mapping再提交，最后再上传数据</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">第一步：插入一条测试数据</span></span><br><span class="line">PUT user/_doc/1</span><br><span class="line">&#123;</span><br><span class="line">  &quot;name&quot;:&quot;wxk&quot;,</span><br><span class="line">  &quot;age&quot;:26,</span><br><span class="line">  &quot;isRich&quot;:&quot;true&quot;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">第二步：查看自动创建的mapping并复制下来</span></span><br><span class="line">GET user/_mapping</span><br><span class="line"><span class="meta"># </span><span class="language-bash">第三步：删除测试索引</span></span><br><span class="line">DELETE user</span><br><span class="line"><span class="meta"># </span><span class="language-bash">第四步：将复制的mapping拿过来修改后PUT使用，最后再次上传所有数据</span></span><br><span class="line">PUT user</span><br><span class="line">&#123;    </span><br><span class="line">  &quot;mappings&quot; : &#123;</span><br><span class="line">    &quot;properties&quot; : &#123;</span><br><span class="line">      &quot;age&quot; : &#123;</span><br><span class="line">        &quot;type&quot; : &quot;long&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;isRich&quot; : &#123;</span><br><span class="line">        &quot;type&quot; : &quot;boolean&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">      &quot;name&quot; : &#123;</span><br><span class="line">        &quot;type&quot; : &quot;text&quot;,</span><br><span class="line">        &quot;fields&quot; : &#123;</span><br><span class="line">          &quot;keyword&quot; : &#123;</span><br><span class="line">            &quot;type&quot; : &quot;keyword&quot;,</span><br><span class="line">            &quot;ignore_above&quot; : 256</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">禁止索引，在mapping中指定，指定后不能通过该字段进行搜索</span></span><br><span class="line">PUT user</span><br><span class="line">&#123;</span><br><span class="line">    &quot;mappings&quot; : &#123;</span><br><span class="line">      &quot;properties&quot; : &#123;</span><br><span class="line">        &quot;age&quot; : &#123;</span><br><span class="line">          &quot;type&quot; : &quot;long&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;name&quot; : &#123;</span><br><span class="line">          &quot;type&quot; : &quot;text&quot;,</span><br><span class="line">          &quot;index&quot;: false</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"> # </span><span class="language-bash">查询空值，需要在mapping指定null_value，查询空值直接查询指定的null_value，注意只能在keyword类型使用</span></span><br><span class="line">PUT user</span><br><span class="line">&#123;</span><br><span class="line">    &quot;mappings&quot; : &#123;</span><br><span class="line">      &quot;properties&quot; : &#123;</span><br><span class="line">        &quot;age&quot; : &#123;</span><br><span class="line">          &quot;type&quot; : &quot;long&quot;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;name&quot; : &#123;</span><br><span class="line">          &quot;type&quot; : &quot;keyword&quot;,</span><br><span class="line">          &quot;null_value&quot;: &quot;null&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h4 id="全文查询"><a href="#全文查询" class="headerlink" title="全文查询"></a>全文查询</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">查看索引内所有内容</span></span><br><span class="line">GET kibana_sample_data_flights/_search</span><br><span class="line"><span class="meta"># </span><span class="language-bash">from,size从指定位置开始返回指定数量的记录</span></span><br><span class="line">GET kibana_sample_data_flights/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;from&quot;: 5,</span><br><span class="line">  &quot;size&quot;: 5</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">match匹配查询，直接匹配目标字符串，不做分词处理，需要注意索引内是否分词</span></span><br><span class="line">GET kibana_sample_data_flights/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;match&quot;: &#123;</span><br><span class="line">      &quot;Origin&quot;: &quot;Rajiv Gandhi International Airport&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">range查询范围</span></span><br><span class="line">GET kibana_sample_data_flights/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;range&quot;: &#123;</span><br><span class="line">      &quot;FlightTimeHour&quot;: &#123;</span><br><span class="line">        &quot;gte&quot;: 1,</span><br><span class="line">        &quot;lte&quot;: 3</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">_source返回自定字段</span></span><br><span class="line">GET kibana_sample_data_flights/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;_source&quot;: [&quot;OriginLocation&quot;, &quot;FlightTimeHour&quot;], </span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;range&quot;: &#123;</span><br><span class="line">      &quot;FlightTimeHour&quot;: &#123;</span><br><span class="line">        &quot;gte&quot;: 1,</span><br><span class="line">        &quot;lte&quot;: 3  </span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">term查询不会对输入进行分词处理，而是作为一个整体</span></span><br><span class="line">GET kibana_sample_data_flights/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;term&quot;: &#123;</span><br><span class="line">      &quot;Origin&quot;: &#123;</span><br><span class="line">        &quot;value&quot;: &quot;Rajiv Gandhi International Airport&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">terms则是可以输入多个对象来匹配，对象之间是或关系</span></span><br><span class="line">GET kibana_sample_data_flights/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;terms&quot;: &#123;</span><br><span class="line">      &quot;Origin&quot;: [</span><br><span class="line">        &quot;Rajiv Gandhi International Airport&quot;,</span><br><span class="line">        &quot;Chengdu Shuangliu International Airport&quot;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">wildcard使用通配符查询</span></span><br><span class="line">GET kibana_sample_data_flights/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;wildcard&quot;: &#123;</span><br><span class="line">      &quot;Origin&quot;: &#123;</span><br><span class="line">         &quot;value&quot;: &quot;*utm_source=*&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash"><span class="built_in">sort</span>对查询结果按字段进行排序，desc降序，asc升序</span></span><br><span class="line">GET kibana_sample_data_flights/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;_source&quot;: [&quot;Origin&quot;, &quot;OriginLocation&quot;, &quot;FlightTimeMin&quot;], </span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;terms&quot;: &#123;</span><br><span class="line">      &quot;Origin&quot;: [</span><br><span class="line">        &quot;Rajiv Gandhi International Airport&quot;,</span><br><span class="line">        &quot;Chengdu Shuangliu International Airport&quot;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;sort&quot;: [</span><br><span class="line">      &#123;</span><br><span class="line">        &quot;FlightTimeMin&quot;: &#123;</span><br><span class="line">          &quot;order&quot;: &quot;desc&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">constant_score不尽兴相关性算分，并把查询的数据进行缓存，提升效率</span></span><br><span class="line">GET kibana_sample_data_flights/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;_source&quot;: [&quot;Origin&quot;, &quot;OriginLocation&quot;, &quot;FlightTimeMin&quot;], </span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;constant_score&quot;: &#123;</span><br><span class="line">      &quot;filter&quot;: &#123;</span><br><span class="line">        &quot;match&quot;: &#123;</span><br><span class="line">          &quot;Origin&quot;: &quot;Rajiv Gandhi International Airport&quot;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;boost&quot;: 1.0</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">match_phrase将输入作为一个短语来匹配</span></span><br><span class="line">GET kibana_sample_data_flights/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;_source&quot;: [&quot;Origin&quot;, &quot;OriginLocation&quot;, &quot;FlightTimeMin&quot;], </span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;match_phrase&quot;: &#123;</span><br><span class="line">      &quot;Origin&quot;: &quot;Rajiv Gandhi International Airport&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">multi_match从多个字段中去匹配查询目标</span></span><br><span class="line">GET kibana_sample_data_flights/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;_source&quot;: [&quot;Origin&quot;, &quot;OriginLocation&quot;, &quot;FlightTimeMin&quot;], </span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;multi_match&quot;: &#123;</span><br><span class="line">      &quot;query&quot;: &quot;Rajiv Gandhi International Airport&quot;,</span><br><span class="line">      &quot;fields&quot;: [&quot;Origin&quot;, &quot;OriginCityName&quot;]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">query_string针对字符串的查询，字符串之间用AND和OR连接</span></span><br><span class="line">GET kibana_sample_data_flights/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;_source&quot;: [&quot;Origin&quot;, &quot;OriginLocation&quot;, &quot;FlightTimeMin&quot;], </span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;query_string&quot;: &#123;</span><br><span class="line">      &quot;default_field&quot;: &quot;FIELD&quot;,</span><br><span class="line">      &quot;query&quot;: &quot;this AND that OR thus&quot;</span><br><span class="line">      &quot;default_operator&quot;: &quot;OR&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">simple_query_string针对在多个字段中查找目标</span></span><br><span class="line">GET kibana_sample_data_flights/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;_source&quot;: [&quot;Origin&quot;, &quot;OriginLocation&quot;, &quot;FlightTimeMin&quot;], </span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;simple_query_string&quot;: &#123;</span><br><span class="line">      &quot;query&quot;: &quot;International Airport&quot;,</span><br><span class="line">      &quot;fields&quot;: [&quot;Origin&quot;, &quot;OriginLocation&quot;], </span><br><span class="line">      &quot;default_operator&quot;: &quot;OR&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">fuzzy模糊查询，按得分值降序</span></span><br><span class="line">GET movies/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;fuzzy&quot;: &#123;</span><br><span class="line">      &quot;title&quot;: &quot;Clara&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">fuzzy&amp;fuzziness控制对模糊值的改变次数来匹配目标最终文档，fuzzinaess的取值范围为0、1、2</span></span><br><span class="line">GET movies/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;fuzzy&quot;: &#123;</span><br><span class="line">      &quot;title&quot;: &#123;</span><br><span class="line">        &quot;value&quot;: &quot;Clara&quot;, </span><br><span class="line">        &quot;fuzziness&quot;: 1</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">bool&amp;must多条件查询</span></span><br><span class="line">GET movies/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;bool&quot;: &#123;</span><br><span class="line">      &quot;must&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;simple_query_string&quot;: &#123;</span><br><span class="line">          &quot;query&quot;: &quot;beautiful mind&quot;,</span><br><span class="line">          &quot;fields&quot;: [&quot;title&quot;]</span><br><span class="line">        &#125;&#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;range&quot;: &#123;</span><br><span class="line">            &quot;year&quot;: &#123;</span><br><span class="line">              &quot;gte&quot;: 1990,</span><br><span class="line">              &quot;lte&quot;: 1992</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">bool&amp;must&amp;must not多条件查询</span></span><br><span class="line">GET movies/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;bool&quot;: &#123;</span><br><span class="line">      &quot;must&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;simple_query_string&quot;: &#123;</span><br><span class="line">          &quot;query&quot;: &quot;beautiful mind&quot;,</span><br><span class="line">          &quot;fields&quot;: [&quot;title&quot;]</span><br><span class="line">        &#125;&#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;range&quot;: &#123;</span><br><span class="line">            &quot;year&quot;: &#123;</span><br><span class="line">              &quot;gte&quot;: 1990,</span><br><span class="line">              &quot;lte&quot;: 1992</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      ],</span><br><span class="line">      &quot;must_not&quot;: [</span><br><span class="line">        &#123;&quot;simple_query_string&quot;: &#123;</span><br><span class="line">          &quot;query&quot;: &quot;Story&quot;,</span><br><span class="line">          &quot;fields&quot;: [&quot;title&quot;]</span><br><span class="line">        &#125;&#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">bool&amp;filter筛选，和must差不多，支持多条件</span></span><br><span class="line">GET movies/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;bool&quot;: &#123;</span><br><span class="line">      &quot;filter&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;simple_query_string&quot;: &#123;</span><br><span class="line">          &quot;query&quot;: &quot;beautiful&quot;,</span><br><span class="line">          &quot;fields&quot;: [&quot;title&quot;]</span><br><span class="line">        &#125;&#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;range&quot;: &#123;</span><br><span class="line">            &quot;year&quot;: &#123;</span><br><span class="line">              &quot;gte&quot;: 1990,</span><br><span class="line">              &quot;lte&quot;: 1992</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">bool&amp;should多条件之间是或者的关系</span></span><br><span class="line">GET movies/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;bool&quot;: &#123;</span><br><span class="line">      &quot;should&quot;: [</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;simple_query_string&quot;: &#123;</span><br><span class="line">          &quot;query&quot;: &quot;beautiful&quot;,</span><br><span class="line">          &quot;fields&quot;: [&quot;title&quot;]</span><br><span class="line">        &#125;&#125;,</span><br><span class="line">        &#123;</span><br><span class="line">          &quot;range&quot;: &#123;</span><br><span class="line">            &quot;year&quot;: &#123;</span><br><span class="line">              &quot;gte&quot;: 1990,</span><br><span class="line">              &quot;lte&quot;: 1992</span><br><span class="line">            &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">使用sql语句进行查询</span></span><br><span class="line">GET _sql</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &quot;&quot;&quot;</span><br><span class="line">  SELECT sum(AvgTicketPrice) agg_sum FROM &quot;kibana_sample_data_flights&quot; where DestCountry = &#x27;US&#x27;</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">translate将SQL语句解析为es查询json</span></span><br><span class="line">GET _sql/translate</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &quot;&quot;&quot;</span><br><span class="line">  SELECT sum(AvgTicketPrice) agg_sum FROM &quot;kibana_sample_data_flights&quot; where DestCountry = &#x27;US&#x27;</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">format参数可返回多种形式的结果（json、yaml、txt、csv等）默认json</span></span><br><span class="line">GET _sql?format=csv</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &quot;&quot;&quot;</span><br><span class="line">  SELECT AvgTicketPrice,Cancelled,DestCountry FROM &quot;kibana_sample_data_flights&quot; where DestCountry = &#x27;US&#x27; limit 10</span><br><span class="line">  &quot;&quot;&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="聚合查询"><a href="#聚合查询" class="headerlink" title="聚合查询"></a>聚合查询</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">对某个字段求和（<span class="built_in">sum</span>）、平均（avg）、计数（value_count）、非重复计数（cardinality）</span></span><br><span class="line">GET employee/_search</span><br><span class="line">&#123;</span><br><span class="line">    &quot;size&quot;: 0,  # 只返回聚合结果，默认为20，返回20条原数据</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;sum_agg&quot;: &#123;  # 求和后的字段名称</span><br><span class="line">      &quot;sum&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;sal&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">一次进行多个聚合</span></span><br><span class="line">GET kibana_sample_data_flights/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;size&quot;: 0, </span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;max&quot;: &#123;</span><br><span class="line">      &quot;max&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;AvgTicketPrice&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;, </span><br><span class="line">    &quot;avg&quot;: &#123;</span><br><span class="line">      &quot;avg&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;AvgTicketPrice&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;min&quot;: &#123;</span><br><span class="line">      &quot;min&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;AvgTicketPrice&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">stats查看数据类型的数据最大最小平均值等描述性信息</span></span><br><span class="line">GET employee/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;size&quot;: 0,</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;NAME&quot;: &#123;</span><br><span class="line">      &quot;stats&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;sal&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">terms对filed中的每个类型进行分组并统计每个类型出现的次数，类似于groupby然后count</span></span><br><span class="line">GET kibana_sample_data_flights/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;size&quot;: 0,</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;NAME&quot;: &#123;</span><br><span class="line">      &quot;terms&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;DestCountry&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">terms&amp;terms嵌套达到groupby两个或多个字段并count的效果</span></span><br><span class="line">GET kibana_sample_data_flights/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;size&quot;: 0,</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;NAME&quot;: &#123;</span><br><span class="line">      &quot;terms&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;DestCountry&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;aggs&quot;: &#123;</span><br><span class="line">        &quot;NAME&quot;: &#123;</span><br><span class="line">          &quot;terms&quot;: &#123;</span><br><span class="line">            &quot;field&quot;: &quot;DestWeather&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">trems&amp;states查看某一字段中的不同类型对应的最大最小描述性信息（groupby后max、min、avg等）</span></span><br><span class="line">GET employee/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;size&quot;: 0, </span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;NAME&quot;: &#123;</span><br><span class="line">      &quot;terms&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;job&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;aggs&quot;: &#123;</span><br><span class="line">        &quot;NAME&quot;: &#123;</span><br><span class="line">          &quot;stats&quot;: &#123;</span><br><span class="line">            &quot;field&quot;: &quot;sal&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">top_hits&amp;size&amp;<span class="built_in">sort</span>返回按列排序后的前几条数据</span></span><br><span class="line">GET employee/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;size&quot;: 0,</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;NAME&quot;: &#123;</span><br><span class="line">      &quot;top_hits&quot;: &#123;</span><br><span class="line">        &quot;size&quot;: 2, </span><br><span class="line">        &quot;sort&quot;: [</span><br><span class="line">          &#123;</span><br><span class="line">            &quot;age&quot;: &#123;</span><br><span class="line">              &quot;order&quot;: &quot;desc&quot;</span><br><span class="line">              &#125;</span><br><span class="line">          &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">range对数值进行分组并对每个区间进行计数</span></span><br><span class="line">GET employee/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;size&quot;: 0,</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;NAME&quot;: &#123;</span><br><span class="line">      &quot;range&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;sal&quot;,</span><br><span class="line">        &quot;ranges&quot;: [</span><br><span class="line">          &#123;</span><br><span class="line">            &quot;key&quot;: &quot;0-10000&quot;, </span><br><span class="line">            &quot;to&quot;: 10001</span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">            &quot;key&quot;: &quot;10000-20000&quot;, </span><br><span class="line">            &quot;from&quot;: 10001, </span><br><span class="line">            &quot;to&quot;: 20001</span><br><span class="line">          &#125;,</span><br><span class="line">          &#123;</span><br><span class="line">            &quot;key&quot;: &quot;20000-30000&quot;, </span><br><span class="line">            &quot;from&quot;: 20001, </span><br><span class="line">            &quot;to&quot;: 30001</span><br><span class="line">          &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">histogram是对数值进行分区间，sep为固定值，并对每个区间进行计数</span></span><br><span class="line">GET employee/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;size&quot;: 0,</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;NAME&quot;: &#123;</span><br><span class="line">      &quot;histogram&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;sal&quot;,</span><br><span class="line">        &quot;interval&quot;: 10000</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">min_bucket筛选平均工资最低的工种</span></span><br><span class="line">GET employee/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;size&quot;: 0,</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;NAME1&quot;: &#123;</span><br><span class="line">      &quot;terms&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;job&quot;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;aggs&quot;: &#123;</span><br><span class="line">        &quot;NAME2&quot;: &#123;</span><br><span class="line">          &quot;avg&quot;: &#123;</span><br><span class="line">            &quot;field&quot;: &quot;sal&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;NAME&quot;: &#123;</span><br><span class="line">      &quot;min_bucket&quot;: &#123;</span><br><span class="line">        &quot;buckets_path&quot;: &quot;NAME1&gt;NAME2&quot;  # 这里为路径，按name指定</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">range&amp;avg先分组再计算组内平均值</span></span><br><span class="line">GET employee/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;size&quot;: 0,</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;NAME&quot;: &#123;</span><br><span class="line">      &quot;range&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;sal&quot;,</span><br><span class="line">        &quot;ranges&quot;: [</span><br><span class="line">          &#123;</span><br><span class="line">            &quot;key&quot;: &quot;大于30&quot;, </span><br><span class="line">            &quot;from&quot;: 30</span><br><span class="line">          &#125;</span><br><span class="line">        ]</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;aggs&quot;: &#123;</span><br><span class="line">        &quot;NAME&quot;: &#123;</span><br><span class="line">          &quot;avg&quot;: &#123;</span><br><span class="line">            &quot;field&quot;: &quot;sal&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">query&amp;aggs先筛选再聚合</span></span><br><span class="line">GET employee/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;match&quot;: &#123;</span><br><span class="line">      &quot;job&quot;: &quot;java&quot;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;size&quot;: 0, </span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;NAME&quot;: &#123;</span><br><span class="line">      &quot;stats&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;sal&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">针对两次聚合不同数据源的聚合，aggs下分name后filter接下来再聚合</span></span><br><span class="line">GET employee/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;size&quot;: 0,</span><br><span class="line">  &quot;aggs&quot;: &#123;</span><br><span class="line">    &quot;平均工资&quot;: &#123;</span><br><span class="line">      &quot;avg&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;sal&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;,</span><br><span class="line">    &quot;筛选大于30&quot;: &#123;</span><br><span class="line">      &quot;filter&quot;: &#123;</span><br><span class="line">        &quot;range&quot;: &#123;</span><br><span class="line">          &quot;age&quot;: &#123;</span><br><span class="line">            &quot;gte&quot;: 30</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;,</span><br><span class="line">      &quot;aggs&quot;: &#123;</span><br><span class="line">        &quot;大于30平均工资&quot;: &#123;</span><br><span class="line">          &quot;avg&quot;: &#123;</span><br><span class="line">            &quot;field&quot;: &quot;sal&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="搜索建议-x2F-高亮"><a href="#搜索建议-x2F-高亮" class="headerlink" title="搜索建议&#x2F;高亮"></a>搜索建议&#x2F;高亮</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">suggest搜索建议，针对未添加到索引中的文本</span></span><br><span class="line">GET movies/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;suggest&quot;: &#123;</span><br><span class="line">    &quot;NAME&quot;: &#123;</span><br><span class="line">      &quot;text&quot;: &quot;beauti&quot;,</span><br><span class="line">      &quot;term&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;title&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">suggest搜索建议，设置不管在不在索引中都提供搜索建议，设置suggest_mode为always</span></span><br><span class="line">GET movies/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;suggest&quot;: &#123;</span><br><span class="line">    &quot;NAME&quot;: &#123;</span><br><span class="line">      &quot;text&quot;: &quot;beauty&quot;,</span><br><span class="line">      &quot;term&quot;: &#123;</span><br><span class="line">        &quot;field&quot;: &quot;title&quot;,</span><br><span class="line">        &quot;suggest_mode&quot;:&quot;always&quot;  # 默认为missing，设置为popular为常见的建议</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">completion自动补全功能的实现（首先要将mapping需要补全的字段类型设置为completion，然后在传入数据）</span></span><br><span class="line">GET movies_completion/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;_source&quot;: [&quot;&quot;],  # 不显示其他字段内容，只显示匹配的title</span><br><span class="line">  &quot;suggest&quot;: &#123;</span><br><span class="line">    &quot;NAME&quot;: &#123;</span><br><span class="line">      &quot;prefix&quot;:&quot;bea&quot;,  # 前缀为bea字段为title的自动补全</span><br><span class="line">      &quot;completion&quot;: &#123;</span><br><span class="line">        &quot;field&quot;:&quot;title&quot;，</span><br><span class="line">        &quot;skip_duplicates&quot;:true， # 忽略重复值，返回唯一值</span><br><span class="line">        &quot;size&quot;:10  # 默认显示5条</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">highlight高亮查询出来匹配的文本</span></span><br><span class="line">GET movies/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;multi_match&quot;: &#123;</span><br><span class="line">      &quot;query&quot;: &quot;romance&quot;,</span><br><span class="line">      &quot;fields&quot;: [&quot;title&quot;, &quot;genre&quot;]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;, </span><br><span class="line">  &quot;highlight&quot;: &#123;</span><br><span class="line">    &quot;fields&quot;: &#123;</span><br><span class="line">      &quot;title&quot;: &#123;&#125;,</span><br><span class="line">      &quot;genre&quot;: &#123;</span><br><span class="line">        &quot;pre_tags&quot;: &quot;&lt;span&gt;&quot;,  # 定义查询出来的标签，默认标签为em</span><br><span class="line">        &quot;post_tags&quot;: &quot;&lt;/span&gt;&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">highlight&amp;highlight_query实现对筛选后的结果再对其他字段再次筛选并进行高亮</span></span><br><span class="line">GET movies/_search</span><br><span class="line">&#123;</span><br><span class="line">  &quot;query&quot;: &#123;</span><br><span class="line">    &quot;bool&quot;: &#123;</span><br><span class="line">      &quot;must&quot;: [</span><br><span class="line">        &#123;&quot;match&quot;: &#123;</span><br><span class="line">          &quot;year&quot;: 2012</span><br><span class="line">        &#125;&#125;,</span><br><span class="line">        &#123;&quot;match&quot;: &#123;</span><br><span class="line">          &quot;title&quot;: &quot;romance&quot;</span><br><span class="line">        &#125;&#125;</span><br><span class="line">      ]</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;,</span><br><span class="line">  &quot;highlight&quot;: &#123;</span><br><span class="line">    &quot;fields&quot;: &#123;</span><br><span class="line">      &quot;title&quot;: &#123;&#125;,  # 这是直接高亮上方筛选中title中包含romance</span><br><span class="line">      &quot;genre&quot;: &#123;</span><br><span class="line">        &quot;pre_tags&quot;: &quot;&lt;span&gt;&quot;,</span><br><span class="line">        &quot;post_tags&quot;: &quot;&lt;/span&gt;&quot;,</span><br><span class="line">        &quot;highlight_query&quot;: &#123;  # 对上层筛选出来的结果按children进行再次筛选并高亮</span><br><span class="line">          &quot;match&quot;: &#123;</span><br><span class="line">            &quot;genre&quot;: &quot;children&quot;</span><br><span class="line">          &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h4 id="IK分词器"><a href="#IK分词器" class="headerlink" title="IK分词器"></a>IK分词器</h4><p>ik分词器安装及使用自己的词库</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">下载安装，下载解压即可，然后复制到其他主机，需要重启es</span></span><br><span class="line">wget https://github.com/medcl/elasticsearch-analysis-ik/releases/download/v7.10.1/elasticsearch-analysis-ik-7.10.1.zip</span><br><span class="line">unzip elasticsearch-analysis-ik-7.10.1.zip -d /usr/local/servers/elasticsearch/plugins/ik</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">添加自己的分词库及停用词库</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">第一步：新建自己的文件夹和分词文件、停用词文件</span></span><br><span class="line">cd /usr/local/servers/elasticsearch/plugins/ik/config/ik</span><br><span class="line">mkdir custom</span><br><span class="line">touch custom/myword.dic custom/mystopword.dic</span><br><span class="line"><span class="meta"># </span><span class="language-bash">第二步：在myword.dic文件中写入自己的词库，在mystopword.dic下写入自己的停用词</span></span><br><span class="line">vi myword.dic</span><br><span class="line">vi mystopword.dic</span><br><span class="line"><span class="meta"># </span><span class="language-bash">第三步：修改ik配置文件指向自己的分词库</span></span><br><span class="line">vi config/IKAnalyzer.cfg.xml</span><br><span class="line"></span><br><span class="line">&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;!DOCTYPE properties SYSTEM &quot;http://java.sun.com/dtd/properties.dtd&quot;&gt;</span><br><span class="line">&lt;properties&gt;</span><br><span class="line">        &lt;comment&gt;IK Analyzer 扩展配置&lt;/comment&gt;</span><br><span class="line">        &lt;!--用户可以在这里配置自己的扩展字典 --&gt;</span><br><span class="line">        &lt;entry key=&quot;ext_dict&quot;&gt;custom/myword.dic&lt;/entry&gt;  # 这里是分词库相对路径</span><br><span class="line">         &lt;!--用户可以在这里配置自己的扩展停止词字典--&gt;</span><br><span class="line">        &lt;entry key=&quot;ext_stopwords&quot;&gt;custom/mystopword.dic&lt;/entry&gt;  # 这里是停用词库相对路径</span><br><span class="line">        &lt;!--用户可以在这里配置远程扩展字典 --&gt;</span><br><span class="line">        &lt;!-- &lt;entry key=&quot;remote_ext_dict&quot;&gt;words_location&lt;/entry&gt; --&gt;</span><br><span class="line">        &lt;!--用户可以在这里配置远程扩展停止词字典--&gt;</span><br><span class="line">        &lt;!-- &lt;entry key=&quot;remote_ext_stopwords&quot;&gt;words_location&lt;/entry&gt; --&gt;</span><br><span class="line">&lt;/properties&gt;</span><br><span class="line"><span class="meta"># </span><span class="language-bash">第四步：重启elasticsearch服务</span></span><br></pre></td></tr></table></figure><p>使用IK分词器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">一般分词（包含ik_smart和ik_max_word两种方式）</span></span><br><span class="line">GET _analyze</span><br><span class="line">&#123;</span><br><span class="line">  &quot;analyzer&quot;: &quot;ik_smart&quot;,  # ik_smart倾向于尽量少的分词，ik_max_word则是尽可能多的列出所有情况</span><br><span class="line">  &quot;text&quot;: &quot;中国教育真是好啊&quot;</span><br><span class="line">&#125;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">实际使用IK分词器</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">首先需要定义mapping，指定字段使用IK分词器，其次就可以正常传入数据并且查询了，特殊词组则建立或新增自己的分词库</span></span><br><span class="line">PUT news</span><br><span class="line">&#123;</span><br><span class="line">  &quot;mappings&quot;: &#123;</span><br><span class="line">    &quot;properties&quot;: &#123;</span><br><span class="line">      &quot;title&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;text&quot;,</span><br><span class="line">        &quot;analyzer&quot;: &quot;ik_max_word&quot;,  # 传入数据的时候使用最详细的分词，这样匹配度会更高</span><br><span class="line">        &quot;search_analyzer&quot;: &quot;ik_smart&quot;  # 查询时的匹配则使用尽量少的分词，使查询结果不至于偏差太大</span><br><span class="line">      &#125;, </span><br><span class="line">      &quot;content&quot;: &#123;</span><br><span class="line">        &quot;type&quot;: &quot;text&quot;, </span><br><span class="line">        &quot;analyzer&quot;: &quot;ik_max_word&quot;, </span><br><span class="line">        &quot;search_analyzer&quot;: &quot;ik_smart&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="Logstash语法"><a href="#Logstash语法" class="headerlink" title="Logstash语法"></a>Logstash语法</h3><p><a href="https://cloud.tencent.com/developer/article/1499881">grok语法</a> | <a href="https://www.elastic.co/guide/en/logstash/current/index.html">官方文档</a> | <a href="http://grokdebug.herokuapp.com/">grok测试</a></p>]]></content>
      
      
      <categories>
          
          <category> Software </category>
          
      </categories>
      
      
        <tags>
            
            <tag> ES </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>R语言常用包</title>
      <link href="/2020/08/22/Language/R%E8%AF%AD%E8%A8%80%E5%B8%B8%E7%94%A8%E5%8C%85/"/>
      <url>/2020/08/22/Language/R%E8%AF%AD%E8%A8%80%E5%B8%B8%E7%94%A8%E5%8C%85/</url>
      
        <content type="html"><![CDATA[<p>R语言基础知识以及常用包操作。</p><span id="more"></span><h2 id="底层基础"><a href="#底层基础" class="headerlink" title="底层基础"></a>底层基础</h2><h3 id="函数的几个环境以及参数寻址"><a href="#函数的几个环境以及参数寻址" class="headerlink" title="函数的几个环境以及参数寻址"></a>函数的几个环境以及参数寻址</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">f <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">,</span> label<span class="operator">=</span>deparse<span class="punctuation">(</span>x<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">&#123;</span></span><br><span class="line">              label</span><br><span class="line">              x <span class="operator">&lt;-</span> x<span class="operator">+</span><span class="number">1</span></span><br><span class="line">              print<span class="punctuation">(</span><span class="string">&#x27;这是f函数：&#x27;</span><span class="punctuation">,</span> <span class="built_in">quote</span> <span class="operator">=</span> <span class="built_in">F</span><span class="punctuation">)</span></span><br><span class="line">              print<span class="punctuation">(</span>label<span class="punctuation">)</span></span><br><span class="line">              print<span class="punctuation">(</span>x<span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">f1 <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">,</span> label<span class="operator">=</span>deparse<span class="punctuation">(</span>x<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">&#123;</span></span><br><span class="line">              x <span class="operator">&lt;-</span> x<span class="operator">+</span><span class="number">1</span></span><br><span class="line">              print<span class="punctuation">(</span><span class="string">&#x27;这是f1函数：&#x27;</span><span class="punctuation">,</span> <span class="built_in">quote</span> <span class="operator">=</span> <span class="built_in">F</span><span class="punctuation">)</span></span><br><span class="line">              print<span class="punctuation">(</span>label<span class="punctuation">)</span></span><br><span class="line">              print<span class="punctuation">(</span>x<span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">f<span class="punctuation">(</span><span class="number">1</span><span class="punctuation">)</span>; f1<span class="punctuation">(</span><span class="number">1</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h4 id="函数的四个环境"><a href="#函数的四个环境" class="headerlink" title="函数的四个环境"></a>函数的四个环境</h4><ul><li>第一个环境是<strong>全局环境</strong>~当前环境；</li><li>第二个环境是函数<strong>被定义的环境</strong>，在这里就是当前环境；</li><li>第三个环境是function的<strong>参数环境</strong>，是一个局部环境参数等号左边是局部变量，等号右边是全局变量、缺省值或者传入的参数；</li><li>第四个环境是函数在被调用的时候函数体内部运行时内存给分配的<strong>临时环境（堆栈）</strong>，函数运行完成后会被销毁。</li></ul><h4 id="参数寻址问题"><a href="#参数寻址问题" class="headerlink" title="参数寻址问题"></a>参数寻址问题</h4><p>关于上方两个函数f和f1，结果不同是因为函数体在运行时参数的寻址环境顺序不同导致的，f函数先运行label，函数体未定义label，它就会去参数环境中寻找，发现它等于deparse(x），然后这个x就是参数环境中的另外一个参数，这个参数在调用f的时候被赋值为1（f(1)），所以label就等于“1”，接下来运行第二步x自增1后等于2，第三步第四步print变量时会优先寻址当前运行的环境，label对应“1”，x对应2；关于f1函数，不同的地方是label变量是在x自增之后才被执行的，deparse(x）会优先寻址当前运行环境，结果就是“2”。</p><h4 id="参数寻址补充"><a href="#参数寻址补充" class="headerlink" title="参数寻址补充"></a>参数寻址补充</h4><p>函数中的参数是<strong>惰性求值</strong>，不执行则不运算，只有在执行或者被调用的时候才会进行运算；函数体执行运算用到变量时若函数体中未定义，参数环境中也未定义，则会向全局环境中寻址；函数在被调用时若有缺省值，则局部参数被调用时会优先使用传入的参数，若未传入参数才会使用缺省值。</p><h3 id="R语言中的复制"><a href="#R语言中的复制" class="headerlink" title="R语言中的复制"></a>R语言中的复制</h3><p>分两个部分来讲，<strong>变量的复制和环境的复制</strong>。</p><p>首先是<strong>变量的复制</strong>，定义一个变量a指向数据c(1,2,3)，再定义一个变量b指向变量a（实际上是指向变量a指向的数据），这个时候两个变量指向内存中的数据地址是一样的（可以用浅复制来理解，但不建议），接着修改变量a指向的数据，此时再看b并没有发生变化，此时再比较两者会发现指向的数据地址已经不一样了，使用base包中的tracemem函数可以追踪两个变量的地址变化，其实是因为修改a变量时会在内存中把a指向的数据复制一份，随后再对复制的这份数据进行修改（可以理解为先浅复制指向同一内存，赋值时会发生深复制指向不同的内存地址，但不建议）。</p><p>其次是<strong>环境的复制</strong>，新建变量a指向一个新环境，随后新建变量b指向a，此时a和b就是环境的内存地址，这里和变量不一样，接着在a环境中定义变量aa，查看b环境时会发现也有aa变量（可以理解为浅复制，不建议）。</p><p>这就是R语言中的复制，<strong>不存在深复制和浅复制</strong>这一说，可以用深复制和浅复制的概念来辅助理解，但不要死搬硬套，不然在变量复制的理解上就会产生矛盾。</p><h3 id="RFC4180-csv文件存储协议"><a href="#RFC4180-csv文件存储协议" class="headerlink" title="RFC4180 csv文件存储协议"></a>RFC4180 csv文件存储协议</h3><p><strong>CSV用逗号分隔字段</strong>的基本思想是清楚的，但是当字段数据也可能<strong>包含逗号或者甚至嵌入换行符</strong>时，该想法变得复杂。 CSV实现可能无法处理这些字段数据，或者可能会使用引号来包围字段。<strong>引用并不能解决所有问题</strong>：有些字段可能需要嵌入引号，因此CSV实现可能<strong>包含转义字符或转义序列</strong>。</p><p><a href="https://www.rfc-editor.org/rfc/inline-errata/rfc4180.html"><strong>RFC 4180</strong></a>提出了MIME类型（”text&#x2F;csv”）对于CSV格式的标准，可以作为一般使用的常用定义，满足大多数实现似乎遵循的格式。</p><ol><li>每一行记录位于一个单独的行上，用回车换行符CRLF(也就是\r\n)分割。</li></ol><p>Each record is located on a separate line, delimited by a line break (CRLF). For example:<br>aaa,bbb,ccc CRLF<br>zzz,yyy,xxx CRLF</p><ol start="2"><li>文件中的最后一行记录可以有结尾回车换行符，也可以没有。</li></ol><p>The last record in the file may or may not have an ending line break. For example:<br>aaa,bbb,ccc CRLF<br>zzz,yyy,xxx</p><ol start="3"><li>第一行可以存在一个可选的标题头，格式和普通记录行的格式一样。标题头要包含文件记录字段对应的名称，应该有和记录字段一样的数量。（在MIME类型中，标题头行的存在与否可以通过MIME type中的可选”header”参数指明）</li></ol><p>There maybe an optional header line appearing as the first line of the file with the same format as normal record lines. This header will contain names corresponding to the fields in the file and should contain the same number of fields as the records in the rest of the file (the presence or absence of the header line should be indicated via the optional “header” parameter of this MIME type). For example:<br>field_name,field_name,field_name CRLF<br>aaa,bbb,ccc CRLF<br>zzz,yyy,xxx CRLF</p><ol start="4"><li>在标题头行和普通行每行记录中，会存在一个或多个由半角逗号(,)分隔的字段。整个文件中每行应包含相同数量的字段，空格也是字段的一部分，不应被忽略。每一行记录最后一个字段后不能跟逗号。（通常用逗号分隔，也有其他字符分隔的CSV，需事先约定）</li></ol><p>Within the header and each record, there may be one or more fields, separated by commas. Each line should contain the same number of fields throughout the file. Spaces are considered part of a field and should not be ignored. The last field in the record must not be followed by a comma. For example:<br>aaa,bbb,ccc</p><ol start="5"><li>每个字段可用也可不用半角双引号(“)括起来（不过有些程序，如Microsoft的Excel就根本不用双引号）。如果字段没有用引号括起来，那么该字段内部不能出现双引号字符。</li></ol><p>Each field may or may not be enclosed in double quotes (however some programs, such as Microsoft Excel, do not use double quotes at all). If fields are not enclosed with double quotes, then double quotes may not appear inside the fields. For example:<br>“aaa”,”bbb”,”ccc” CRLF<br>zzz,yyy,xxx</p><ol start="6"><li>字段中若包含回车换行符、双引号或者逗号，该字段需要用双引号括起来。</li></ol><p>Fields containing line breaks (CRLF), double quotes, and commas should be enclosed in double-quotes. For example:<br>“aaa”,”b CRLF<br>bb”,”ccc” CRLF<br>zzz,yyy,xxx</p><ol start="7"><li>如果用双引号括字段，那么出现在字段内的双引号前必须加一个双引号进行转义。</li></ol><p>If double-quotes are used to enclose fields, then a double-quote appearing inside a field must be escaped by preceding it with another double quote. For example:<br>“aaa”,”b””bb”,”ccc”</p><p><strong>补充：R语言</strong>在这里设置的比较好，默认write.csv的时候会有quote参数设置是否需要将元素用双引号括起来，元素中出现的双引号会默认添加”转义，对于换行符这些则需要添加反斜杠进行转义，read.csv读取的时候也会按这个协议进行读取。</p><h2 id="技术问题"><a href="#技术问题" class="headerlink" title="技术问题"></a>技术问题</h2><h3 id="导入包"><a href="#导入包" class="headerlink" title="导入包"></a>导入包</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">knitr<span class="operator">::</span>opts_chunk<span class="operator">$</span>set<span class="punctuation">(</span>echo <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">)</span></span><br><span class="line">library<span class="punctuation">(</span>dplyr<span class="punctuation">,</span> warn.conflicts <span class="operator">=</span> <span class="built_in">F</span><span class="punctuation">)</span></span><br><span class="line">library<span class="punctuation">(</span>tidyr<span class="punctuation">,</span> warn.conflicts <span class="operator">=</span> <span class="built_in">F</span><span class="punctuation">)</span></span><br><span class="line">library<span class="punctuation">(</span>pryr<span class="punctuation">,</span> warn.conflicts <span class="operator">=</span> <span class="built_in">F</span><span class="punctuation">)</span></span><br><span class="line">library<span class="punctuation">(</span>purrr<span class="punctuation">,</span> warn.conflicts <span class="operator">=</span> <span class="built_in">F</span><span class="punctuation">)</span></span><br><span class="line">library<span class="punctuation">(</span>elastic<span class="punctuation">,</span> warn.conflicts <span class="operator">=</span> <span class="built_in">F</span><span class="punctuation">)</span></span><br><span class="line">library<span class="punctuation">(</span>esHelp<span class="punctuation">)</span></span><br><span class="line">exprs <span class="operator">&lt;-</span> rlang<span class="operator">::</span>exprs</span><br></pre></td></tr></table></figure><h3 id="grep和grepl的区别"><a href="#grep和grepl的区别" class="headerlink" title="grep和grepl的区别"></a>grep和grepl的区别</h3><p>grep查询结果为<strong>整个向量</strong>、列表等数据中是否含有并返回一个逻辑值，grepl查询返回结果为<strong>每个元素对应的</strong>T或者F:</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grep<span class="punctuation">(</span><span class="string">&#x27;youzan&#x27;</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;youzanw&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;fsew&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span>  <span class="comment"># 查询youzan在向量中的结果返回单个T</span></span><br><span class="line">grepl<span class="punctuation">(</span><span class="string">&#x27;youzan&#x27;</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;youzanw&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;fsew&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span>  <span class="comment"># 查询结果则返回每个元素对应的T或者F</span></span><br></pre></td></tr></table></figure><h3 id="gsub、replace替换操作"><a href="#gsub、replace替换操作" class="headerlink" title="gsub、replace替换操作"></a>gsub、replace替换操作</h3><p>替换每个元素中的字符或数字，替换结果为字符串<strong>使用gsub</strong>，当然，也可以使用正则表达式</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 替换字符串中的字符</span></span><br><span class="line">a <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;i am hungry&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;you are xxx&#x27;</span><span class="punctuation">)</span></span><br><span class="line">gsub<span class="punctuation">(</span><span class="string">&#x27; &#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;-&#x27;</span><span class="punctuation">,</span> a<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 替换数字，返回结果为字符串</span></span><br><span class="line">b <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">3</span><span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">,</span> <span class="number">7</span><span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">)</span></span><br><span class="line">gsub<span class="punctuation">(</span><span class="number">2</span><span class="punctuation">,</span> <span class="number">222</span><span class="punctuation">,</span> b<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>替换指定位置的元素或者指定条件的元素<strong>使用replace</strong></p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 替换指定位置元素</span></span><br><span class="line">a <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;i am hungry&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;you are xxx&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;abcde&#x27;</span><span class="punctuation">)</span></span><br><span class="line">replace<span class="punctuation">(</span>a<span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="number">222</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 替换指定条件元素</span></span><br><span class="line">b <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">3</span><span class="punctuation">,</span> <span class="operator">-</span><span class="number">5</span><span class="punctuation">,</span> <span class="number">7</span><span class="punctuation">,</span> <span class="operator">-</span><span class="number">9</span><span class="punctuation">)</span></span><br><span class="line">replace<span class="punctuation">(</span>b<span class="punctuation">,</span> b<span class="operator">&lt;</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">222</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 也可以使用ifelse实现</span></span><br><span class="line">ifelse<span class="punctuation">(</span>b<span class="operator">&lt;</span><span class="number">0</span><span class="punctuation">,</span> <span class="number">222</span><span class="punctuation">,</span> b<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h3 id="行转列spread和列转行gather"><a href="#行转列spread和列转行gather" class="headerlink" title="行转列spread和列转行gather"></a>行转列spread和列转行gather</h3><ul><li>行转列将某一字段中的值提到col中，并将对应的value值reshape，列转行将col转到单个字段中并将原col对应的值一一映射。</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">stocks <span class="operator">&lt;-</span> tibble<span class="punctuation">(</span>time <span class="operator">=</span> as.Date<span class="punctuation">(</span><span class="string">&#x27;2009-01-01&#x27;</span><span class="punctuation">)</span> <span class="operator">+</span> <span class="number">0</span><span class="operator">:</span><span class="number">9</span><span class="punctuation">,</span></span><br><span class="line">                X <span class="operator">=</span> rnorm<span class="punctuation">(</span><span class="number">10</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">                Y <span class="operator">=</span> rnorm<span class="punctuation">(</span><span class="number">10</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">                Z <span class="operator">=</span> rnorm<span class="punctuation">(</span><span class="number">10</span><span class="punctuation">,</span> <span class="number">0</span><span class="punctuation">,</span> <span class="number">4</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">)</span></span><br><span class="line">stocks  <span class="comment"># 原始数据</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># gather列转行，指定需转为行的列名，再指定key对应colname，value对应value的colname</span></span><br><span class="line">a <span class="operator">&lt;-</span> stocks <span class="operator">%&gt;%</span> gather<span class="punctuation">(</span><span class="string">&#x27;X&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;Y&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;Z&#x27;</span><span class="punctuation">,</span> key<span class="operator">=</span><span class="string">&#x27;stock&#x27;</span><span class="punctuation">,</span> value<span class="operator">=</span><span class="string">&#x27;price&#x27;</span><span class="punctuation">)</span>; a</span><br><span class="line"></span><br><span class="line"><span class="comment"># spread行转列，指定需要转的key对应col，把它转为原始状态</span></span><br><span class="line">a <span class="operator">%&gt;%</span> spread<span class="punctuation">(</span>key<span class="operator">=</span><span class="string">&#x27;stock&#x27;</span><span class="punctuation">,</span> value<span class="operator">=</span><span class="string">&#x27;price&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h3 id="环境空间操作"><a href="#环境空间操作" class="headerlink" title="环境空间操作"></a>环境空间操作</h3><p>环境空间的<strong>基本操作</strong>以及空间内变量的基本操作</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建一个新环境</span></span><br><span class="line">env1 <span class="operator">&lt;-</span> new.env<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断是否是一个环境变量</span></span><br><span class="line"><span class="built_in">is.environment</span><span class="punctuation">(</span>env1<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看当前环境变量</span></span><br><span class="line">environment<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看函数中的环境空间</span></span><br><span class="line">environment<span class="punctuation">(</span>ls<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看环境空间的名字</span></span><br><span class="line">environmentName<span class="punctuation">(</span><span class="built_in">baseenv</span><span class="punctuation">(</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">environmentName<span class="punctuation">(</span>environment<span class="punctuation">(</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置环境变量的名字，默认创建出来是没有名字的</span></span><br><span class="line"><span class="built_in">attr</span><span class="punctuation">(</span>env1<span class="punctuation">,</span> <span class="string">&#x27;name&#x27;</span><span class="punctuation">)</span> <span class="operator">&lt;-</span> <span class="string">&#x27;env1&#x27;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看环境变量的属性值</span></span><br><span class="line">env.profile<span class="punctuation">(</span>env1<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h4 id="环境空间中的变量"><a href="#环境空间中的变量" class="headerlink" title="环境空间中的变量"></a>环境空间中的变量</h4><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 清空当前环境空间定义的所有对象</span></span><br><span class="line">rm<span class="punctuation">(</span><span class="built_in">list</span><span class="operator">=</span>ls<span class="punctuation">(</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 定义环境空间中的变量</span></span><br><span class="line">x <span class="operator">&lt;-</span> 1.5; y <span class="operator">&lt;-</span> 2<span class="operator">:</span><span class="number">10</span></span><br><span class="line">env1 <span class="operator">&lt;-</span> new.env<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line">env1<span class="operator">$</span>x <span class="operator">&lt;-</span> <span class="operator">-</span><span class="number">10</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看env1环境变量中的变量</span></span><br><span class="line">ls<span class="punctuation">(</span>env1<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 环境空间变量取值</span></span><br><span class="line">get<span class="punctuation">(</span><span class="string">&#x27;x&#x27;</span><span class="punctuation">,</span> envir<span class="operator">=</span>env1<span class="punctuation">)</span>  <span class="comment"># 取env1环境变量中的x值</span></span><br><span class="line">get<span class="punctuation">(</span><span class="string">&#x27;y&#x27;</span><span class="punctuation">,</span> envir<span class="operator">=</span>env1<span class="punctuation">)</span>  <span class="comment"># 从env1环境空间中取从当前环境中继承的y值</span></span><br><span class="line"><span class="comment"># get(&#x27;y&#x27;, envir=env1, inherits=FALSE)  # 禁止环境空间的继承，会报错</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重新赋值</span></span><br><span class="line">assign<span class="punctuation">(</span><span class="string">&#x27;x&#x27;</span><span class="punctuation">,</span> <span class="number">77</span><span class="punctuation">)</span>; x  <span class="comment"># 重新赋值当前环境变量中的x值</span></span><br><span class="line">assign<span class="punctuation">(</span><span class="string">&#x27;x&#x27;</span><span class="punctuation">,</span> <span class="number">99</span><span class="punctuation">,</span> envir<span class="operator">=</span>env1<span class="punctuation">)</span>; env1<span class="operator">$</span>x  <span class="comment"># 重新赋值环境空间中的x值</span></span><br><span class="line">assign<span class="punctuation">(</span><span class="string">&#x27;y&#x27;</span><span class="punctuation">,</span> <span class="number">333</span><span class="punctuation">,</span> envir<span class="operator">=</span>env1<span class="punctuation">,</span> inherits<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">)</span>  <span class="comment"># 在没有继承的情况下给环境空间中的y赋值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除env1中的变量</span></span><br><span class="line">rm<span class="punctuation">(</span>x<span class="punctuation">,</span> envir<span class="operator">=</span>env1<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 变量在环境变量中是否存在</span></span><br><span class="line">exists<span class="punctuation">(</span><span class="string">&#x27;x&#x27;</span><span class="punctuation">)</span></span><br><span class="line">exists<span class="punctuation">(</span><span class="string">&#x27;x&#x27;</span><span class="punctuation">,</span> envir<span class="operator">=</span>env1<span class="punctuation">)</span></span><br><span class="line"><span class="comment"># exists(&#x27;y&#x27;, envir=env1, isherits=FALSE)  # 在没有继承的情况下y是否存在，会报错</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看函数的环境空间，来自pryr包</span></span><br><span class="line">where<span class="punctuation">(</span><span class="string">&#x27;mean&#x27;</span><span class="punctuation">)</span></span><br><span class="line">where<span class="punctuation">(</span><span class="string">&#x27;where&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h4 id="管理工作空间"><a href="#管理工作空间" class="headerlink" title="管理工作空间"></a>管理工作空间</h4><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># setwd(&#x27;/Users/mac/python/R&#x27;)  # 修改工作目录为</span></span><br><span class="line">getwd<span class="punctuation">(</span><span class="punctuation">)</span>  <span class="comment"># 显示当前工作目录</span></span><br><span class="line">ls<span class="punctuation">(</span><span class="punctuation">)</span>  <span class="comment"># 列出当前工作空间中的对象</span></span><br><span class="line">search<span class="punctuation">(</span><span class="punctuation">)</span>  <span class="comment"># 查看已载入的包</span></span><br><span class="line">q<span class="punctuation">(</span><span class="punctuation">)</span>  <span class="comment"># 退出R</span></span><br><span class="line">library<span class="punctuation">(</span><span class="punctuation">)</span>  <span class="comment"># 查看已安装的包</span></span><br><span class="line">options<span class="punctuation">(</span><span class="punctuation">)</span>  <span class="comment"># 显示或设置当前选项</span></span><br><span class="line">history<span class="punctuation">(</span><span class="punctuation">)</span>  <span class="comment"># 显示最近使用过的指令数量，默认25个</span></span><br><span class="line">savehistory<span class="punctuation">(</span><span class="string">&#x27;myfile&#x27;</span><span class="punctuation">)</span>  <span class="comment"># 保存命令历史到myfile文件中</span></span><br><span class="line">loadhistory<span class="punctuation">(</span><span class="string">&#x27;myfile&#x27;</span><span class="punctuation">)</span>  <span class="comment"># 载入一个命令历史文件</span></span><br><span class="line">save.image<span class="punctuation">(</span><span class="string">&#x27;myfile&#x27;</span><span class="punctuation">)</span>  <span class="comment"># 保存工作空间到myfile中</span></span><br><span class="line">save<span class="punctuation">(</span>objectlist<span class="punctuation">,</span> file<span class="operator">=</span><span class="string">&#x27;myfile&#x27;</span><span class="punctuation">)</span>  <span class="comment"># 保存指定对象到myfile中</span></span><br><span class="line">load<span class="punctuation">(</span><span class="string">&#x27;myfile&#x27;</span><span class="punctuation">)</span>  <span class="comment"># 从myfile中读取一个工作空间</span></span><br></pre></td></tr></table></figure><h3 id="将多个df写入excel的不同位置"><a href="#将多个df写入excel的不同位置" class="headerlink" title="将多个df写入excel的不同位置"></a>将多个df写入excel的不同位置</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># path为不带文件名的文件路径</span></span><br><span class="line"><span class="comment"># file_name为带后缀的文件名</span></span><br><span class="line"><span class="comment"># souce_aw,all_aw,source_dell,all_dell均为df</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 判断是否存在文件，若存在则删除</span></span><br><span class="line"><span class="keyword">if</span><span class="punctuation">(</span>file.exists<span class="punctuation">(</span>paste<span class="punctuation">(</span>path<span class="punctuation">,</span> file_name<span class="punctuation">,</span> sep<span class="operator">=</span><span class="string">&quot;&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">&#123;</span></span><br><span class="line">  file.remove<span class="punctuation">(</span>paste<span class="punctuation">(</span>path<span class="punctuation">,</span> file_name<span class="punctuation">,</span> sep<span class="operator">=</span><span class="string">&quot;&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 建立空工作簿</span></span><br><span class="line">dt_wb <span class="operator">&lt;-</span> createWorkbook<span class="punctuation">(</span><span class="punctuation">)</span> </span><br><span class="line"></span><br><span class="line"><span class="comment"># write_data(sheet_name, df, start_row, start_col)防止搞混这两个参数使用参数名赋值</span></span><br><span class="line">write_data<span class="punctuation">(</span><span class="string">&#x27;aw&#x27;</span><span class="punctuation">,</span> souce_aw<span class="punctuation">,</span> start_col<span class="operator">=</span><span class="number">1</span><span class="punctuation">,</span> start_row<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span></span><br><span class="line">write_data<span class="punctuation">(</span><span class="string">&#x27;aw&#x27;</span><span class="punctuation">,</span> all_aw<span class="punctuation">,</span> start_col<span class="operator">=</span><span class="number">2</span><span class="punctuation">,</span> start_row<span class="operator">=</span><span class="punctuation">(</span>nrow<span class="punctuation">(</span>souce_aw<span class="punctuation">)</span><span class="operator">+</span><span class="number">3</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">write_data<span class="punctuation">(</span><span class="string">&#x27;dell&#x27;</span><span class="punctuation">,</span> souce_dell<span class="punctuation">,</span> start_col<span class="operator">=</span><span class="number">1</span><span class="punctuation">,</span>start_row<span class="operator">=</span><span class="number">1</span><span class="punctuation">)</span></span><br><span class="line">write_data<span class="punctuation">(</span><span class="string">&#x27;dell&#x27;</span><span class="punctuation">,</span> all_dell<span class="punctuation">,</span> start_col<span class="operator">=</span><span class="number">2</span><span class="punctuation">,</span> start_row<span class="operator">=</span><span class="punctuation">(</span>nrow<span class="punctuation">(</span>souce_dell<span class="punctuation">)</span><span class="operator">+</span><span class="number">3</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 保存数据到工作簿中</span></span><br><span class="line">saveWorkbook<span class="punctuation">(</span>dt_wb<span class="punctuation">,</span> file<span class="operator">=</span>paste<span class="punctuation">(</span>path<span class="punctuation">,</span> file_name<span class="punctuation">,</span> sep<span class="operator">=</span><span class="string">&quot;&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span> </span><br></pre></td></tr></table></figure><h3 id="apply家族"><a href="#apply家族" class="headerlink" title="apply家族"></a>apply家族</h3><p>apply函数族是R语言中数据处理的一组核心函数，通过使用apply函数，我们可以实现对数据的<strong>循环、分组、过滤、类型控制</strong>等操作。</p><ul><li><strong>分组计算</strong>：apply（按col&#x2F;row计算）、tapply（按字段groupby计算）</li><li><strong>循环迭代</strong>：lapply、sapply（lapply简化版）、vapply（sapply可设置rownames）、rapply（lapply的递归版）、mapply（sapply的多参数版）</li><li><strong>环境空间遍历</strong>：eapply</li></ul><h4 id="apply函数"><a href="#apply函数" class="headerlink" title="apply函数"></a>apply函数</h4><p>apply函数是最常用的代替for循环的函数。apply函数可以对矩阵、数据框、数组(二维、多维)，按行或列进行循环计算，<strong>对子元素进行迭代，把子元素以参数传递的形式给自定义的FUN函数中，并返回计算结果</strong>。</p><ul><li>apply(X, MARGIN, FUN, …)</li><li>X:数组、矩阵、数据框</li><li>MARGIN: 按行计算或按按列计算，1表示按行，2表示按列</li><li>FUN: 自定义的调用函数</li><li>…: 更多参数，可选</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最简单的实现，按行求和返回向量</span></span><br><span class="line">x <span class="operator">&lt;-</span> matrix<span class="punctuation">(</span><span class="number">1</span><span class="operator">:</span><span class="number">12</span><span class="punctuation">,</span> ncol<span class="operator">=</span><span class="number">3</span><span class="punctuation">)</span></span><br><span class="line">apply<span class="punctuation">(</span>x<span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">,</span> <span class="built_in">sum</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 自定义函数实现新增计算字段</span></span><br><span class="line">myfun <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">,</span> c1<span class="punctuation">,</span> c2<span class="punctuation">)</span><span class="punctuation">&#123;</span></span><br><span class="line">  <span class="built_in">c</span><span class="punctuation">(</span><span class="built_in">sum</span><span class="punctuation">(</span>x<span class="punctuation">[</span>c1<span class="punctuation">]</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">)</span><span class="punctuation">,</span> mean<span class="punctuation">(</span>x<span class="punctuation">[</span>c2<span class="punctuation">]</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">x <span class="operator">&lt;-</span> cbind<span class="punctuation">(</span>x1 <span class="operator">=</span> <span class="number">3</span><span class="punctuation">,</span> x2 <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">4</span><span class="operator">:</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">2</span><span class="operator">:</span><span class="number">5</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">apply<span class="punctuation">(</span>x<span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">,</span> myfun<span class="punctuation">,</span> c1<span class="operator">=</span><span class="string">&#x27;x1&#x27;</span><span class="punctuation">,</span> c2<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;x1&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;x2&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h4 id="lapply函数"><a href="#lapply函数" class="headerlink" title="lapply函数"></a>lapply函数</h4><p>lapply函数是一个最基础循环操作函数之一，用来<strong>对list、data.frame数据集进行循环，并返回和X长度同样的list结构</strong>作为结果集，通过lapply的开头的第一个字母’l’就可以判断返回结果集的类型为list。</p><ul><li>lapply(X, FUN, …)</li><li>X:list、data.frame数据</li><li>FUN: 自定义的调用函数</li><li>…: 更多参数，可选</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回列表中每个key对应value的分位数</span></span><br><span class="line">x <span class="operator">&lt;-</span> <span class="built_in">list</span><span class="punctuation">(</span>a<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="operator">:</span><span class="number">10</span><span class="punctuation">)</span><span class="punctuation">,</span> b<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">1.2</span><span class="punctuation">,</span><span class="number">2.3</span><span class="punctuation">,</span><span class="number">3.4</span><span class="punctuation">,</span><span class="number">4.5</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="built_in">c</span><span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">5</span><span class="operator">:</span><span class="operator">-</span><span class="number">3</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">lapply<span class="punctuation">(</span>x<span class="punctuation">,</span> fivenum<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当x为df时会自动分组计算并返回list</span></span><br><span class="line">x <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>x1 <span class="operator">=</span> <span class="number">3</span><span class="punctuation">,</span> x2 <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">4</span><span class="operator">:</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">2</span><span class="operator">:</span><span class="number">5</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">lapply<span class="punctuation">(</span>x<span class="punctuation">,</span> <span class="built_in">sum</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h4 id="sapply函数"><a href="#sapply函数" class="headerlink" title="sapply函数"></a>sapply函数</h4><p>sapply函数是一个<strong>简化版的lapply</strong>，sapply增加了2个参数simplify和USE.NAMES，主要就是让输出看起来更友好，<strong>返回值为向量</strong>，而不是list对象，它调用lapply函数，然后在它的基础上进行转换。</p><ul><li>sapply(X, FUN, …, simplify&#x3D;TRUE, USE.NAMES &#x3D; TRUE)</li><li>X:数组、矩阵、数据框</li><li>FUN: 自定义的调用函数</li><li>…: 更多参数，可选</li><li>simplify: 是否数组化，当值array时，输出结果按数组进行分组</li><li>USE.NAMES: 如果X为字符串，TRUE设置字符串为数据名，FALSE不设置</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对矩阵进行计算，过程和lapply相同</span></span><br><span class="line">x <span class="operator">&lt;-</span> cbind<span class="punctuation">(</span>x1<span class="operator">=</span><span class="number">3</span><span class="punctuation">,</span> x2<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">2</span><span class="operator">:</span><span class="number">1</span><span class="punctuation">,</span><span class="number">4</span><span class="operator">:</span><span class="number">5</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">sapply<span class="punctuation">(</span>x<span class="punctuation">,</span> <span class="built_in">sum</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对df进行计算</span></span><br><span class="line">sapply<span class="punctuation">(</span>data.frame<span class="punctuation">(</span>x<span class="punctuation">)</span><span class="punctuation">,</span> <span class="built_in">sum</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查结果类型，sapply返回类型为向量，而lapply的返回类型为list</span></span><br><span class="line"><span class="built_in">class</span><span class="punctuation">(</span>lapply<span class="punctuation">(</span>x<span class="punctuation">,</span> <span class="built_in">sum</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="built_in">class</span><span class="punctuation">(</span>sapply<span class="punctuation">(</span>x<span class="punctuation">,</span> <span class="built_in">sum</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当返回结果为多个元素时会转为array</span></span><br><span class="line">sapply<span class="punctuation">(</span>x<span class="punctuation">,</span> fivenum<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对字符型向量可自动生成数据名，设置USE.NAMES为FALSE即可</span></span><br><span class="line">sapply<span class="punctuation">(</span>sample<span class="punctuation">(</span><span class="built_in">letters</span><span class="punctuation">,</span><span class="number">5</span><span class="punctuation">)</span><span class="punctuation">,</span> paste<span class="punctuation">,</span> USE.NAMES<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h4 id="vapply函数"><a href="#vapply函数" class="headerlink" title="vapply函数"></a>vapply函数</h4><p>vapply<strong>类似于sapply</strong>，提供了FUN.VALUE参数，用来<strong>控制返回值的行名</strong>，这样可以让程序更健壮。</p><ul><li>vapply(X, FUN, FUN.VALUE, …, USE.NAMES &#x3D; TRUE)</li><li>X:数组、矩阵、数据框</li><li>FUN: 自定义的调用函数</li><li>FUN.VALUE: 定义返回值的行名row.names</li><li>…: 更多参数，可选</li><li>USE.NAMES: 如果X为字符串，TRUE设置字符串为数据名，FALSE不设置</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对数据框的数据进行累计求和，并对每一行设置行名row.names</span></span><br><span class="line">x <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>cbind<span class="punctuation">(</span>x1<span class="operator">=</span><span class="number">3</span><span class="punctuation">,</span> x2<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">2</span><span class="operator">:</span><span class="number">1</span><span class="punctuation">,</span><span class="number">4</span><span class="operator">:</span><span class="number">5</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">vapply<span class="punctuation">(</span>x<span class="punctuation">,</span><span class="built_in">cumsum</span><span class="punctuation">,</span>FUN.VALUE<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;a&#x27;</span><span class="operator">=</span><span class="number">0</span><span class="punctuation">,</span><span class="string">&#x27;b&#x27;</span><span class="operator">=</span><span class="number">0</span><span class="punctuation">,</span><span class="string">&#x27;c&#x27;</span><span class="operator">=</span><span class="number">0</span><span class="punctuation">,</span><span class="string">&#x27;d&#x27;</span><span class="operator">=</span><span class="number">0</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h4 id="mapply函数"><a href="#mapply函数" class="headerlink" title="mapply函数"></a>mapply函数</h4><p>mapply也是sapply的变形函数，类似<strong>多变量的sapply</strong>，但是参数定义有些变化。第一参数为自定义的FUN函数，第二个参数’…’可以接收多个数据，作为FUN函数的参数调用。</p><ul><li>mapply(FUN, …, MoreArgs &#x3D; NULL, SIMPLIFY &#x3D; TRUE,USE.NAMES &#x3D; TRUE)</li><li>FUN: 自定义的调用函数</li><li>…: 接收多个数据</li><li>MoreArgs: 参数列表</li><li>SIMPLIFY: 是否数组化，当值array时，输出结果按数组进行分组</li><li>USE.NAMES: 如果X为字符串，TRUE设置字符串为数据名，FALSE不设置</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 按列对多个参数进行求和</span></span><br><span class="line">x <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>x1<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">)</span><span class="punctuation">,</span> x2<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">3</span><span class="punctuation">,</span><span class="number">4</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">y <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>y1<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">5</span><span class="punctuation">,</span><span class="number">6</span><span class="punctuation">)</span><span class="punctuation">,</span> y2<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">7</span><span class="punctuation">,</span><span class="number">8</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">mapply<span class="punctuation">(</span><span class="built_in">sum</span><span class="punctuation">,</span> x<span class="punctuation">,</span> y<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h4 id="tapply函数"><a href="#tapply函数" class="headerlink" title="tapply函数"></a>tapply函数</h4><p>tapply用于分组的循环计算，<strong>通过INDEX参数可以把数据集X进行分组，相当于group by的操作</strong>。</p><ul><li>tapply(X, INDEX, FUN &#x3D; NULL, …, simplify &#x3D; TRUE)</li><li>X: 向量</li><li>INDEX: 用于分组的索引</li><li>FUN: 自定义的调用函数</li><li>…: 接收多个数据</li><li>simplify : 是否数组化，当值array时，输出结果按数组进行分组</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对df进行groupby计算，按x1对x2进行求和</span></span><br><span class="line">x <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>x1<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;A&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;B&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;A&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;B&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span> x2<span class="operator">=</span><span class="number">1</span><span class="operator">:</span><span class="number">4</span><span class="punctuation">)</span></span><br><span class="line">with<span class="punctuation">(</span>x<span class="punctuation">,</span> <span class="punctuation">&#123;</span></span><br><span class="line">  tapply<span class="punctuation">(</span>x2<span class="punctuation">,</span> x1<span class="punctuation">,</span> <span class="built_in">sum</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">&#125;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 特殊需求，对数字范围进行分组</span></span><br><span class="line">tapply<span class="punctuation">(</span><span class="number">1</span><span class="operator">:</span><span class="number">10</span><span class="punctuation">,</span> <span class="built_in">ceiling</span><span class="punctuation">(</span><span class="number">1</span><span class="operator">:</span><span class="number">10</span><span class="operator">/</span><span class="number">3</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> x<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h4 id="rapply函数"><a href="#rapply函数" class="headerlink" title="rapply函数"></a>rapply函数</h4><p>rapply是一个<strong>递归版本的lapply，它只处理list类型数据</strong>，对list的每个元素进行递归遍历，如果list包括子元素则继续遍历。</p><ul><li>rapply(object, f, classes &#x3D; “ANY”, deflt &#x3D; NULL, how &#x3D; c(“unlist”, “replace”, “list”), …)</li><li>object:list数据</li><li>f: 自定义的调用函数</li><li>classes : 匹配类型, ANY为所有类型</li><li>deflt: 非匹配类型的默认值</li><li>how: 3种操作方式，当为replace时，则用调用f后的结果替换原list中原来的元素；当为list时，新建一个list，类型匹配调用f函数，不匹配赋值为deflt；当为unlist时，会执行一次unlist(recursive &#x3D; TRUE)的操作</li><li>…: 更多参数，可选</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 对一个list进行guolv，返回每一个元素的最小值，注意how参数的选取对应不同的返回结果</span></span><br><span class="line">x <span class="operator">&lt;-</span> <span class="built_in">list</span><span class="punctuation">(</span>x1<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">,</span><span class="number">3</span><span class="punctuation">)</span><span class="punctuation">,</span> x2<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">5</span><span class="punctuation">,</span><span class="operator">-</span><span class="number">5</span><span class="punctuation">,</span><span class="number">10</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">y <span class="operator">&lt;-</span> <span class="built_in">pi</span></span><br><span class="line">z <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>z1<span class="operator">=</span><span class="number">100</span><span class="punctuation">,</span> z2<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">11</span><span class="punctuation">,</span><span class="number">22</span><span class="punctuation">,</span><span class="number">33</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">lst <span class="operator">&lt;-</span> <span class="built_in">list</span><span class="punctuation">(</span>x<span class="operator">=</span>x<span class="punctuation">,</span> y<span class="operator">=</span>y<span class="punctuation">,</span> z<span class="operator">=</span>z<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line">rapply<span class="punctuation">(</span>lst<span class="punctuation">,</span> <span class="built_in">min</span><span class="punctuation">,</span> how<span class="operator">=</span><span class="string">&#x27;list&#x27;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># classes参数用来匹配类型，与类型相匹配的元素执行函数，其余返回deflt参数值</span></span><br><span class="line">z <span class="operator">&lt;-</span> sample<span class="punctuation">(</span><span class="built_in">letters</span><span class="punctuation">,</span> <span class="number">5</span><span class="punctuation">)</span></span><br><span class="line">lst <span class="operator">&lt;-</span> <span class="built_in">list</span><span class="punctuation">(</span>x<span class="punctuation">,</span> y<span class="punctuation">,</span> z<span class="punctuation">)</span></span><br><span class="line">rapply<span class="punctuation">(</span>lst<span class="punctuation">,</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> paste<span class="punctuation">(</span>x<span class="punctuation">,</span> <span class="string">&#x27;---&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span> classes<span class="operator">=</span><span class="string">&#x27;character&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h4 id="eapply函数"><a href="#eapply函数" class="headerlink" title="eapply函数"></a>eapply函数</h4><p>对一个<strong>环境空间中的所有变量</strong>进行遍历。</p><ul><li>eapply(env, FUN, …, all.names &#x3D; FALSE, USE.NAMES &#x3D; TRUE)</li><li>env: 环境空间</li><li>FUN: 自定义的调用函数</li><li>…: 更多参数，可选</li><li>all.names: 匹配类型, ANY为所有类型</li><li>USE.NAMES: 如果X为字符串，TRUE设置字符串为数据名，FALSE不设置</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新建环境并定义三个变量</span></span><br><span class="line">env1 <span class="operator">&lt;-</span> new.env<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line">env1<span class="operator">$</span>x <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="operator">:</span><span class="number">10</span><span class="punctuation">)</span></span><br><span class="line">env1<span class="operator">$</span>y <span class="operator">&lt;-</span> <span class="built_in">pi</span></span><br><span class="line">env1<span class="operator">$</span>z <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>z1<span class="operator">=</span><span class="number">100</span><span class="punctuation">,</span> z2<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">11</span><span class="punctuation">,</span><span class="number">22</span><span class="punctuation">,</span><span class="number">33</span><span class="punctuation">)</span><span class="punctuation">)</span>  <span class="comment"># 当然它直接求不了均值</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算环境下所有变量的均值</span></span><br><span class="line">eapply<span class="punctuation">(</span>env1<span class="punctuation">,</span> mean<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算环境下所有变量的占用内存大小</span></span><br><span class="line">eapply<span class="punctuation">(</span>env1<span class="punctuation">,</span> object.size<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h3 id="split和unsplit"><a href="#split和unsplit" class="headerlink" title="split和unsplit"></a>split和unsplit</h3><p>split<strong>将向量x中的数据按照f定义的分组</strong>进行划分，替换该划分所对应的值。unsplit反转split的效果。</p><ul><li>split(x, f, drop &#x3D; FALSE, …)</li><li>x:vector or data frame containing values to be divided into groups.</li><li>f:a ‘factor’ in the sense that as.factor(f) defines the grouping, or a list of such factors in which case their interaction is used for the grouping.</li><li>drop:logical indicating if levels that do not occur should be dropped (if f is a factor or a list).</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 按a1对df进行分组，将结果对应返回在一个列表中</span></span><br><span class="line">x <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>x1<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;A&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;A&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;B&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span> x2<span class="operator">=</span><span class="number">1</span><span class="operator">:</span><span class="number">3</span><span class="punctuation">)</span></span><br><span class="line">split<span class="punctuation">(</span>x<span class="punctuation">,</span> x<span class="operator">$</span>x1<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># unsplit是将split输出的结果按x1进行反向操作将它们合起来</span></span><br><span class="line">unsplit<span class="punctuation">(</span>split<span class="punctuation">(</span>x<span class="punctuation">,</span> x<span class="operator">$</span>x1<span class="punctuation">)</span><span class="punctuation">,</span> x<span class="operator">$</span>x1<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h3 id="R中的表达式"><a href="#R中的表达式" class="headerlink" title="R中的表达式"></a>R中的表达式</h3><p>虽然我们在R终端键入的任何有效语句都是表达式，但这些表达式在输入后即被求值（evaluate）了，获得未经求值的纯粹“表达式”就要使用函数。下面我们从函数参数和返回值两方面了解expression、quote、bquote和substitute这几个常用函数。</p><h4 id="expression和quote函数"><a href="#expression和quote函数" class="headerlink" title="expression和quote函数"></a>expression和quote函数</h4><p>expression函数可以有一个或多个参数，它把全部参数当成一个列表，每个参数都被转成一个表达式向量，所以它的返回值是表达式列表，每个元素都是表达式类型对象，返回值的长度等于参数的个数：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># expression形成两个表达式并通过eval索引不同表达式赋值返回结果</span></span><br><span class="line">a <span class="operator">&lt;-</span> <span class="built_in">expression</span><span class="punctuation">(</span>x <span class="operator">*</span> y<span class="punctuation">,</span> x <span class="operator">+</span> y<span class="punctuation">)</span></span><br><span class="line">b <span class="operator">&lt;-</span> <span class="built_in">list</span><span class="punctuation">(</span>x<span class="operator">=</span><span class="number">10</span><span class="punctuation">,</span> y<span class="operator">=</span><span class="number">10</span><span class="punctuation">)</span></span><br><span class="line">eval<span class="punctuation">(</span>a<span class="punctuation">[</span><span class="number">1</span><span class="punctuation">]</span><span class="punctuation">,</span> b<span class="punctuation">)</span>; eval<span class="punctuation">(</span>a<span class="punctuation">[</span><span class="number">2</span><span class="punctuation">]</span><span class="punctuation">,</span> b<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>quote函数只能有一个参数。quote函数的返回值一般情况下是call类型，表达式参数是单个变量的话返回值就是name类型，如果是常量那么返回值的存储模式就和相应常量的模式相同：</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># quote构建表达式并使用eval传参计算</span></span><br><span class="line">a <span class="operator">&lt;-</span> <span class="built_in">quote</span><span class="punctuation">(</span>x <span class="operator">*</span> y<span class="punctuation">)</span></span><br><span class="line">b <span class="operator">&lt;-</span> <span class="built_in">list</span><span class="punctuation">(</span>x<span class="operator">=</span><span class="number">2</span><span class="punctuation">,</span> y<span class="operator">=</span><span class="number">10</span><span class="punctuation">)</span></span><br><span class="line">eval<span class="punctuation">(</span>a<span class="punctuation">,</span> b<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h4 id="bquote和substitute函数"><a href="#bquote和substitute函数" class="headerlink" title="bquote和substitute函数"></a>bquote和substitute函数</h4><p>如果不使用环境变量或环境变量参数，bquote 和 substitute 函数得到的结果与quote函数相同。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 比较三个表达式函数</span></span><br><span class="line">bquote<span class="punctuation">(</span><span class="number">1</span> <span class="operator">+</span> <span class="built_in">sqrt</span><span class="punctuation">(</span>a<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">==</span> <span class="built_in">quote</span><span class="punctuation">(</span><span class="number">1</span> <span class="operator">+</span> <span class="built_in">sqrt</span><span class="punctuation">(</span>a<span class="punctuation">)</span><span class="punctuation">)</span>; <span class="built_in">substitute</span><span class="punctuation">(</span><span class="number">1</span> <span class="operator">+</span> <span class="built_in">sqrt</span><span class="punctuation">(</span>a<span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">==</span> <span class="built_in">quote</span><span class="punctuation">(</span><span class="number">1</span> <span class="operator">+</span> <span class="built_in">sqrt</span><span class="punctuation">(</span>a<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 两者的不同之处在于赋值方式的不同，一般使用substitute</span></span><br><span class="line">a <span class="operator">&lt;-</span> 3 ; b <span class="operator">&lt;-</span> 2 </span><br><span class="line">bquote<span class="punctuation">(</span>y <span class="operator">==</span> <span class="built_in">sqrt</span><span class="punctuation">(</span>.<span class="punctuation">(</span>a<span class="punctuation">)</span><span class="punctuation">,</span> .<span class="punctuation">(</span>b<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span>; <span class="built_in">substitute</span><span class="punctuation">(</span>y <span class="operator">==</span> <span class="built_in">sqrt</span><span class="punctuation">(</span>a<span class="punctuation">,</span> b<span class="punctuation">)</span><span class="punctuation">,</span> <span class="built_in">list</span><span class="punctuation">(</span>a <span class="operator">=</span> <span class="number">3</span><span class="punctuation">,</span> b <span class="operator">=</span> <span class="number">2</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h3 id="df删除重复行"><a href="#df删除重复行" class="headerlink" title="df删除重复行"></a>df删除重复行</h3><p>第一种方法利用<strong>group_by结合row_number</strong>函数对行进行排序，然后选取row_number为1的行则可达到删除其他重复值的效果。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">x <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>a<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;A&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;B&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;A&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;A&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;C&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span> b<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">,</span><span class="number">3</span><span class="punctuation">,</span><span class="number">4</span><span class="punctuation">,</span><span class="number">5</span><span class="punctuation">)</span><span class="punctuation">)</span>; x</span><br><span class="line">x <span class="operator">%&gt;%</span> group_by<span class="punctuation">(</span>a<span class="punctuation">)</span> <span class="operator">%&gt;%</span> filter<span class="punctuation">(</span>row_number<span class="punctuation">(</span>a<span class="punctuation">)</span><span class="operator">==</span><span class="number">1</span><span class="punctuation">)</span> <span class="operator">%&gt;%</span> ungroup</span><br></pre></td></tr></table></figure><p>第二种方法是利用自带的<strong>duplicated和distinct</strong>函数</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>a<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;A&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;B&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;A&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;A&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;C&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span> b<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">,</span><span class="number">3</span><span class="punctuation">,</span><span class="number">4</span><span class="punctuation">,</span><span class="number">5</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># duplicated返回逻辑值，需用到索引中</span></span><br><span class="line">x<span class="punctuation">[</span>duplicated<span class="punctuation">(</span>a<span class="punctuation">)</span><span class="punctuation">,</span> <span class="punctuation">]</span></span><br><span class="line"><span class="comment"># distinct函数需指定.keep_all参数为TRUE才会返回其他字段</span></span><br><span class="line">x <span class="operator">%&gt;%</span> distinct<span class="punctuation">(</span>a<span class="punctuation">,</span> .keep_all<span class="operator">=</span><span class="built_in">T</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 还有一个unique函数返回某一列的唯一值，这和python中一样</span></span><br><span class="line">unique<span class="punctuation">(</span>x<span class="operator">$</span>a<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h3 id="partial传参和compose组合函数"><a href="#partial传参和compose组合函数" class="headerlink" title="partial传参和compose组合函数"></a>partial传参和compose组合函数</h3><p>partial用来<strong>对已有函数传入一定数量的参数</strong>形成一个类似function的partialised对象，你可以调用这个对象再次传参，就像使用函数一样使用它，这对于一些需要指定不同参数的场景特别适用。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 定义一个函数然后然后给它先传入两个参数，再次调用的时候再传入一个参数就可以了</span></span><br><span class="line">a <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">,</span> y<span class="punctuation">,</span> z<span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line">  <span class="built_in">return</span><span class="punctuation">(</span>x<span class="operator">+</span>y<span class="operator">+</span>z<span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">b <span class="operator">&lt;-</span> partial<span class="punctuation">(</span>a<span class="punctuation">,</span> x<span class="operator">=</span><span class="number">1</span><span class="punctuation">,</span> y<span class="operator">=</span><span class="number">2</span><span class="punctuation">)</span>; b</span><br><span class="line">b<span class="punctuation">(</span>z<span class="operator">=</span><span class="number">3</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>compose的作用是组合多个函数，默认从后往前的顺序执行函数。注意需传入函数function，而不是公式formula（~）</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a <span class="operator">&lt;-</span> compose<span class="punctuation">(</span><span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> paste<span class="punctuation">(</span>x<span class="punctuation">,</span><span class="string">&#x27;foo&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> paste<span class="punctuation">(</span>x<span class="punctuation">,</span><span class="string">&#x27;bar&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">a<span class="punctuation">(</span>x<span class="operator">=</span><span class="string">&#x27;input&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h3 id="、-和-公式"><a href="#、-和-公式" class="headerlink" title="!!、!!!和~公式"></a>!!、!!!和~公式</h3><p>!!作用是<strong>使用外部环境的参数或者变量</strong>，在%&gt;%的过程中使用外部变量前指定!!，还有就是在函数中使用rename的时候在参数前加!!，这和R语言在特定情况下字符串不用加双引号有关。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用函数更名，在x重复的时候加!!指定左边的x使用参数，而不是字符串x</span></span><br><span class="line">re_func <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>data<span class="punctuation">,</span>x<span class="punctuation">,</span>y<span class="punctuation">)</span> rename<span class="punctuation">(</span>data<span class="punctuation">,</span> <span class="operator">!</span><span class="operator">!</span>x<span class="operator">:=</span>x<span class="punctuation">,</span> <span class="operator">!</span><span class="operator">!</span>y<span class="operator">:=</span>y<span class="punctuation">)</span></span><br><span class="line">a <span class="operator">&lt;-</span> data.frame<span class="punctuation">(</span>x<span class="operator">=</span><span class="built_in">rep</span><span class="punctuation">(</span><span class="number">1</span><span class="operator">:</span><span class="number">2</span><span class="punctuation">,</span> each<span class="operator">=</span><span class="number">3</span><span class="punctuation">)</span><span class="punctuation">,</span> y<span class="operator">=</span><span class="number">1</span><span class="operator">:</span><span class="number">6</span><span class="punctuation">,</span> z<span class="operator">=</span><span class="number">6</span><span class="operator">:</span><span class="number">1</span><span class="punctuation">)</span></span><br><span class="line">re_func<span class="punctuation">(</span>data<span class="operator">=</span>a<span class="punctuation">,</span> x<span class="operator">=</span><span class="string">&#x27;xx&#x27;</span><span class="punctuation">,</span> y<span class="operator">=</span><span class="string">&#x27;yy&#x27;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p><strong>!!!作用是解包</strong>，一般针对于list类型的数据或准则exprs函数形成的list数据类型，在变量名称前加!!!，在有些情况下这是必须的。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a <span class="operator">&lt;-</span> <span class="built_in">list</span><span class="punctuation">(</span></span><br><span class="line">  exprs<span class="punctuation">(</span>x<span class="operator">=</span><span class="number">1</span><span class="punctuation">,</span> y<span class="operator">=</span><span class="number">2</span><span class="punctuation">,</span> z<span class="operator">=</span><span class="number">3</span><span class="punctuation">)</span><span class="punctuation">,</span> </span><br><span class="line">  exprs<span class="punctuation">(</span>x<span class="operator">=</span><span class="number">9</span><span class="punctuation">,</span> y<span class="operator">=</span><span class="number">8</span><span class="punctuation">,</span> z<span class="operator">=</span><span class="number">7</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">)</span></span><br><span class="line">b <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">,</span>y<span class="punctuation">,</span>z<span class="punctuation">)</span> x</span><br><span class="line">map<span class="punctuation">(</span>a<span class="punctuation">,</span> <span class="operator">~</span> paste<span class="punctuation">(</span>.x<span class="punctuation">)</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p><del>在R语言中是<strong>formula公式类型</strong>，在map等其他函数中中可以当做匿名函数来使用，调用的参数位置使用.x和.y来指定。但当函数内部指定f必须为function时，则不能使用</del>公式替代function。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">a <span class="operator">&lt;-</span> <span class="built_in">list</span><span class="punctuation">(</span></span><br><span class="line">  <span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">,</span><span class="number">3</span><span class="punctuation">)</span><span class="punctuation">,</span> </span><br><span class="line">  <span class="built_in">c</span><span class="punctuation">(</span><span class="number">4</span><span class="punctuation">,</span><span class="number">5</span><span class="punctuation">,</span><span class="number">6</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">)</span></span><br><span class="line">b <span class="operator">&lt;-</span> <span class="built_in">list</span><span class="punctuation">(</span></span><br><span class="line">  <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;a&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;b&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;c&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span> </span><br><span class="line">  <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;d&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;e&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;f&#x27;</span><span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">)</span></span><br><span class="line">map<span class="punctuation">(</span>a<span class="punctuation">,</span> <span class="operator">~</span> <span class="built_in">sum</span><span class="punctuation">(</span>.x<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">map2<span class="punctuation">(</span>a<span class="punctuation">,</span> b<span class="punctuation">,</span> <span class="operator">~</span> paste<span class="punctuation">(</span>.x<span class="punctuation">,</span>.y<span class="punctuation">)</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h3 id="expr系列、sym系列和quo系列"><a href="#expr系列、sym系列和quo系列" class="headerlink" title="expr系列、sym系列和quo系列"></a>expr系列、sym系列和quo系列</h3><p>这三个系列都属于表达式的范畴，原理基本相同，只是在应用场景上有所不同。需要注意的是exprs、quo、get_env、get_expr和eval_tidy函数都属于rlang包，其余的则属于dplyr包。<br>先来看<strong>expr系列</strong>，这也是最常用的，适用于接受参数的场景，包括expr、exprs、enexpr、enexprs。expr和enexpr针对单个表达式或者参数，加s针对多个并返回列表；expr和exprs捕获你输入给它的表达式，enexpr和enexprs用于捕获外部输入的表达式。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 先来看expr和exprs，捕获输入的参数，输入什么返回什么</span></span><br><span class="line">expr<span class="punctuation">(</span>hello<span class="punctuation">)</span>; exprs<span class="punctuation">(</span>hello<span class="punctuation">,</span> <span class="string">&#x27;world&#x27;</span><span class="punctuation">,</span> say<span class="punctuation">(</span>hello<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来是enexpr和enexprs，捕获外部传入的参数，用expr和enexpr举个例子</span></span><br><span class="line">a <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>arg<span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line">  a <span class="operator">&lt;-</span> expr<span class="punctuation">(</span>arg<span class="punctuation">)</span></span><br><span class="line">  b <span class="operator">&lt;-</span> enexpr<span class="punctuation">(</span>arg<span class="punctuation">)</span></span><br><span class="line">  paste<span class="punctuation">(</span>a<span class="punctuation">,</span> b<span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">a<span class="punctuation">(</span>hello<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来是使用!!传入参数所对应的数据，当然如果传入hello是字符串就只会返回字符串了</span></span><br><span class="line">hello <span class="operator">&lt;-</span> 333</span><br><span class="line">b <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>arg<span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line">  b <span class="operator">&lt;-</span> expr<span class="punctuation">(</span><span class="operator">!</span><span class="operator">!</span>arg<span class="punctuation">)</span></span><br><span class="line">  b</span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">b<span class="punctuation">(</span>hello<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p><strong>sym系列</strong>的作用和expr差不多，但是<strong>只接受传入字符串类型</strong>，传入符号会报错，syms的传参方式也略有不同</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sym和base包中的quote差不多，syms需使用list传入字符串</span></span><br><span class="line">sym<span class="punctuation">(</span><span class="string">&#x27;a&#x27;</span><span class="punctuation">)</span>; syms<span class="punctuation">(</span><span class="built_in">list</span><span class="punctuation">(</span><span class="string">&#x27;a&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;b&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ensyms也是接收传入的参数，只接受字符串类型</span></span><br><span class="line">hello <span class="operator">&lt;-</span> <span class="string">&#x27;world&#x27;</span></span><br><span class="line">a <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>...<span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line">  a <span class="operator">&lt;-</span> ensyms<span class="punctuation">(</span>...<span class="punctuation">)</span></span><br><span class="line">  a</span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">a<span class="punctuation">(</span>he<span class="punctuation">,</span> <span class="string">&#x27;llo&#x27;</span><span class="punctuation">,</span> <span class="operator">!</span><span class="operator">!</span>hello<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>最后是<strong>quo系列，它是一个包含环境的表达式</strong>，而且要用到自带的eval_tidy函数进行求值。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># quo定义了一个包含表达式和当前环境的对象，使用get_expr和get_env可以取出表达式和所在的环境</span></span><br><span class="line">quo_eg <span class="operator">&lt;-</span> rlang<span class="operator">::</span>quo<span class="punctuation">(</span>sample<span class="punctuation">(</span><span class="built_in">letters</span><span class="punctuation">,</span> <span class="number">5</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">quo_eg; rlang<span class="operator">::</span>get_expr<span class="punctuation">(</span>quo_eg<span class="punctuation">)</span>; rlang<span class="operator">::</span>get_env<span class="punctuation">(</span>quo_eg<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 接下来使用enquo在函数环境中接受参数，和函数空间中的变量运算后返回值</span></span><br><span class="line">a <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>arg<span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line">  a <span class="operator">&lt;-</span> enquo<span class="punctuation">(</span>arg<span class="punctuation">)</span></span><br><span class="line">  b <span class="operator">&lt;-</span> 10</span><br><span class="line">  rlang<span class="operator">::</span>quo<span class="punctuation">(</span><span class="operator">!</span><span class="operator">!</span>a <span class="operator">*</span> b<span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line"><span class="comment"># 计算它的时候它调用的是函数内部的a和b，env指的也是函数的内部环境</span></span><br><span class="line">a<span class="punctuation">(</span><span class="number">2</span><span class="operator">+</span><span class="number">3</span><span class="punctuation">)</span>; a<span class="punctuation">(</span><span class="number">2</span><span class="operator">+</span><span class="number">3</span><span class="punctuation">)</span> <span class="operator">%&gt;%</span> rlang<span class="operator">::</span>eval_tidy<span class="punctuation">(</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h3 id="map、reduce和map-df"><a href="#map、reduce和map-df" class="headerlink" title="map、reduce和map_df"></a>map、reduce和map_df</h3><p>map函数是将x中的每一个元素进行函数计算，map(1:3,f)等价于list(f(1),f(2),f(3))，相当于任务的分解，分发，而reduce是重复进行函数操作，相当于组合。<br>将map、reduce和map_df放在一起说是因为在特定情况下两个函数可以实现的结果是相同的，其他的map函数需要的时候再去查。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用map函数实现分而治之</span></span><br><span class="line">a <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">,</span><span class="number">3</span><span class="punctuation">)</span></span><br><span class="line">map<span class="punctuation">(</span>a<span class="punctuation">,</span> <span class="operator">~</span>paste<span class="punctuation">(</span>.x<span class="punctuation">,</span> <span class="string">&#x27;ww&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># map结合reduce实现先分发再组合</span></span><br><span class="line">map<span class="punctuation">(</span>a<span class="punctuation">,</span> <span class="operator">~</span>paste<span class="punctuation">(</span>.x<span class="punctuation">,</span> <span class="string">&#x27;ww&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">%&gt;%</span> reduce<span class="punctuation">(</span>.<span class="punctuation">,</span> paste<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># map_df会把计算后的结果拼接起来，省略了reeduce的操作，但只限于数据框</span></span><br><span class="line">a <span class="operator">&lt;-</span> tibble<span class="punctuation">(</span>x<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;d&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;a&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;a&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;b&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;b&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;c&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;c&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;c&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span> </span><br><span class="line">            y<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">,</span><span class="number">1</span><span class="punctuation">,</span><span class="number">3</span><span class="punctuation">,</span><span class="number">3</span><span class="punctuation">)</span><span class="punctuation">,</span> </span><br><span class="line">            z<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">,</span><span class="number">3</span><span class="punctuation">,</span><span class="number">4</span><span class="punctuation">,</span><span class="number">5</span><span class="punctuation">,</span><span class="number">6</span><span class="punctuation">,</span><span class="number">7</span><span class="punctuation">,</span><span class="number">8</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 将a按y分成多个list元素，然后再分别对每个元素进行group_by计算，最后map_df会自动给我们把计算结果合并起来</span></span><br><span class="line">a <span class="operator">%&gt;%</span> split<span class="punctuation">(</span>.<span class="operator">$</span>y<span class="punctuation">)</span> <span class="operator">%&gt;%</span> map_df<span class="punctuation">(</span>.<span class="punctuation">,</span> <span class="operator">~</span>group_by<span class="punctuation">(</span>.<span class="punctuation">,</span>.<span class="operator">$</span>x<span class="punctuation">)</span> <span class="operator">%&gt;%</span> summarise<span class="punctuation">(</span><span class="built_in">sum</span><span class="operator">=</span><span class="built_in">sum</span><span class="punctuation">(</span>z<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"><span class="comment"># 再来看看map、reduce的计算结果，是一样的</span></span><br><span class="line">a <span class="operator">%&gt;%</span> split<span class="punctuation">(</span>.<span class="operator">$</span>y<span class="punctuation">)</span> <span class="operator">%&gt;%</span> map<span class="punctuation">(</span>.<span class="punctuation">,</span> <span class="operator">~</span>group_by<span class="punctuation">(</span>.<span class="punctuation">,</span>.<span class="operator">$</span>x<span class="punctuation">)</span> <span class="operator">%&gt;%</span> summarise<span class="punctuation">(</span><span class="built_in">sum</span><span class="operator">=</span><span class="built_in">sum</span><span class="punctuation">(</span>z<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">%&gt;%</span> reduce<span class="punctuation">(</span>rbind<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>map_df这种情况适用于分批次查询或者计算MySQL数据库的时候，会减少数据库的查询压力，当然得结果split(data, ceiling(1:len(data)&#x2F;n))来使用，它会将数据将数据转换成list中的元素，每个元素长度为n，然后依次排列，这样就达到了分批次（每次执行数量为n）进行查询或者计算操作了。</p><h3 id="ES查询相关"><a href="#ES查询相关" class="headerlink" title="ES查询相关"></a>ES查询相关</h3><p>ES的查询需要借助到elastic库和esHelp库，elastic库主要是建立连接并查询返回，esHelp库则着重于查询条件body的辅助编写，使用它可以快速形成json格式的查询条件。</p><h4 id="建立连接和数据转换"><a href="#建立连接和数据转换" class="headerlink" title="建立连接和数据转换"></a>建立连接和数据转换</h4><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用elastic库带的connect方法建立连接</span></span><br><span class="line">con_51 <span class="operator">&lt;-</span> connect<span class="punctuation">(</span>host <span class="operator">=</span> <span class="string">&quot;47.92.156.202&quot;</span><span class="punctuation">,</span> user <span class="operator">=</span> <span class="string">&#x27;readonly&#x27;</span><span class="punctuation">,</span> pwd <span class="operator">=</span> <span class="string">&#x27;wczyyqc&#x27;</span><span class="punctuation">,</span> port <span class="operator">=</span> <span class="string">&#x27;9201&#x27;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 查询后的数据整理，基本可以用着一个函数转换</span></span><br><span class="line">modifydata <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> map<span class="punctuation">(</span>x<span class="punctuation">,</span> <span class="string">&quot;_source&quot;</span><span class="punctuation">)</span> <span class="operator">%&gt;%</span> transpose <span class="operator">%&gt;%</span>  map<span class="punctuation">(</span>unlist<span class="punctuation">)</span> <span class="operator">%&gt;%</span> as_tibble<span class="punctuation">(</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h4 id="查询语句body的编写"><a href="#查询语句body的编写" class="headerlink" title="查询语句body的编写"></a>查询语句body的编写</h4><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 查询语句有两种写法，层层嵌套和快速生成</span></span><br><span class="line">stime <span class="operator">&lt;-</span> <span class="string">&#x27;2020-11-01 00:00:00&#x27;</span></span><br><span class="line">stime <span class="operator">&lt;-</span> <span class="string">&#x27;2020-11-01 23:59:59&#x27;</span></span><br><span class="line">es_a <span class="operator">&lt;-</span> elastic_q<span class="punctuation">(</span>query<span class="punctuation">(</span>bool<span class="punctuation">(</span>filter<span class="punctuation">(</span>between<span class="punctuation">(</span>local_time<span class="punctuation">,</span> stime<span class="punctuation">,</span> etime<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">                                            event <span class="operator">%in%</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;subscribe&#x27;</span><span class="punctuation">,</span> <span class="string">&quot;SCAN&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">                                            route<span class="operator">==</span><span class="string">&quot;pages/index/index&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                            <span class="operator">?</span>options.fromopenid<span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="operator">%+%</span></span><br><span class="line">                            elastic_s<span class="punctuation">(</span><span class="built_in">list</span><span class="punctuation">(</span><span class="string">&quot;openid&quot;</span><span class="punctuation">,</span><span class="string">&quot;local_time&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span><span class="punctuation">)</span> </span><br><span class="line">es_b <span class="operator">&lt;-</span> bool_query<span class="punctuation">(</span>between<span class="punctuation">(</span>local_time<span class="punctuation">,</span> stime<span class="punctuation">,</span> etime<span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">                                            event <span class="operator">%in%</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;subscribe&#x27;</span><span class="punctuation">,</span> <span class="string">&quot;SCAN&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span></span><br><span class="line">                                            route<span class="operator">==</span><span class="string">&quot;pages/index/index&quot;</span><span class="punctuation">,</span></span><br><span class="line">                                            <span class="operator">?</span>options.fromopenid<span class="punctuation">)</span> <span class="operator">%+%</span> </span><br><span class="line">        <span class="built_in">list</span><span class="punctuation">(</span>`_source`<span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;openid&quot;</span><span class="punctuation">,</span><span class="string">&quot;local_time&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 上方两种写法的结果是一样的(1.1版本)，看看转成json后的结果</span></span><br><span class="line">es_a <span class="operator">=</span> es_b; es_b <span class="operator">%&gt;%</span> jsonlite<span class="operator">::</span>toJSON<span class="punctuation">(</span>pretty <span class="operator">=</span> <span class="built_in">T</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><p>因为esHelp版本问题，结果应该如下文所示，意思是筛选local_time从stime到etime，event字段包含”subscribe”,”SCAN”，是否存在”options.fromopenid”字段并最终取出”openid”,”local_time”字段</p><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line">    <span class="attr">&quot;query&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span></span><br><span class="line">        <span class="attr">&quot;bool&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span></span><br><span class="line">            <span class="attr">&quot;filter&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;range&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;local_time&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span></span><br><span class="line">                            <span class="attr">&quot;gte&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="string">&quot;2020-11-01 00:00:00&quot;</span><span class="punctuation">]</span><span class="punctuation">,</span></span><br><span class="line">                            <span class="attr">&quot;lte&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="string">&quot;2020-11-01 23:59:59&quot;</span><span class="punctuation">]</span></span><br><span class="line">                        <span class="punctuation">&#125;</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;terms&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;event&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="string">&quot;subscribe&quot;</span><span class="punctuation">,</span><span class="string">&quot;SCAN&quot;</span><span class="punctuation">]</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;term&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;route&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="string">&quot;pages/index/index&quot;</span><span class="punctuation">]</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">                <span class="punctuation">&#123;</span></span><br><span class="line">                    <span class="attr">&quot;exists&quot;</span><span class="punctuation">:</span><span class="punctuation">&#123;</span></span><br><span class="line">                        <span class="attr">&quot;field&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="string">&quot;options.fromopenid&quot;</span><span class="punctuation">]</span></span><br><span class="line">                    <span class="punctuation">&#125;</span></span><br><span class="line">                <span class="punctuation">&#125;</span></span><br><span class="line">            <span class="punctuation">]</span></span><br><span class="line">        <span class="punctuation">&#125;</span></span><br><span class="line">    <span class="punctuation">&#125;</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">&quot;_source&quot;</span><span class="punctuation">:</span><span class="punctuation">[</span><span class="string">&quot;openid&quot;</span><span class="punctuation">,</span><span class="string">&quot;local_time&quot;</span><span class="punctuation">]</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><h4 id="查询语句的传入"><a href="#查询语句的传入" class="headerlink" title="查询语句的传入"></a>查询语句的传入</h4><p>使用elastic自带的函数传入相关参数后再转换即可</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 传入上方的连接con，index（db）， type（table）和body参数，再把data传入modifydata中即可得到二维数据。</span></span><br><span class="line">es_tmp_exp  <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>con <span class="operator">=</span> <span class="literal">NULL</span><span class="punctuation">,</span>  index<span class="operator">=</span> <span class="literal">NULL</span><span class="punctuation">,</span> body<span class="operator">=</span> <span class="literal">NULL</span><span class="punctuation">,</span> time_scroll<span class="operator">=</span> <span class="string">&#x27;10s&#x27;</span><span class="punctuation">,</span>... <span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">&#123;</span></span><br><span class="line"></span><br><span class="line">  res <span class="operator">&lt;-</span> Search<span class="punctuation">(</span>conn <span class="operator">=</span> con<span class="punctuation">,</span> index<span class="operator">=</span> index<span class="punctuation">,</span> body<span class="operator">=</span> body<span class="punctuation">,</span> size <span class="operator">=</span> <span class="number">10000</span><span class="punctuation">,</span>  time_scroll<span class="operator">=</span> time_scroll<span class="punctuation">,</span>...<span class="punctuation">)</span></span><br><span class="line">  <span class="built_in">on.exit</span><span class="punctuation">(</span>expr <span class="operator">=</span> scroll_clear<span class="punctuation">(</span>conn <span class="operator">=</span> con<span class="punctuation">,</span> x <span class="operator">=</span> res<span class="operator">$</span>`_scroll_id`<span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line">  data <span class="operator">&lt;-</span> <span class="built_in">list</span><span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> <span class="punctuation">(</span><span class="built_in">T</span><span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line"></span><br><span class="line">    data <span class="operator">&lt;-</span>  <span class="built_in">c</span><span class="punctuation">(</span>data<span class="punctuation">,</span> res<span class="operator">$</span>hits<span class="operator">$</span>hits<span class="punctuation">)</span>  </span><br><span class="line"></span><br><span class="line">    res <span class="operator">&lt;-</span> scroll<span class="punctuation">(</span>conn <span class="operator">=</span> con<span class="punctuation">,</span> x<span class="operator">=</span> res<span class="operator">$</span>`_scroll_id` <span class="punctuation">,</span> time_scroll<span class="operator">=</span> time_scroll<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> <span class="punctuation">(</span><span class="built_in">length</span><span class="punctuation">(</span>res<span class="operator">$</span>hits<span class="operator">$</span>hits<span class="punctuation">)</span> <span class="operator">==</span><span class="number">0</span> <span class="punctuation">)</span><span class="punctuation">&#123;</span></span><br><span class="line">      <span class="keyword">break</span> </span><br><span class="line">    <span class="punctuation">&#125;</span></span><br><span class="line"></span><br><span class="line">  <span class="punctuation">&#125;</span></span><br><span class="line"></span><br><span class="line">  <span class="built_in">return</span><span class="punctuation">(</span>data<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="punctuation">&#125;</span></span><br></pre></td></tr></table></figure><h3 id="发送邮件"><a href="#发送邮件" class="headerlink" title="发送邮件"></a>发送邮件</h3><p>通过mailR库发送邮件，只需要一个主体即可把所有参数全部传完。</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">library<span class="punctuation">(</span>mailR<span class="punctuation">)</span></span><br><span class="line">  mailR<span class="operator">::</span>send.mail<span class="punctuation">(</span>from <span class="operator">=</span> <span class="string">&quot;eub_bi@eub-inc.com&quot;</span><span class="punctuation">,</span>  <span class="comment"># 发件人</span></span><br><span class="line">                   to <span class="operator">=</span> <span class="string">&quot;di.cui@eub-inc.com&quot;</span><span class="punctuation">,</span>  <span class="comment"># 收件人</span></span><br><span class="line">                   cc <span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;frank@eub-inc.com&quot;</span><span class="punctuation">,</span> <span class="string">&quot;xukun.wang@eub-inc.com&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span>  <span class="comment"># 抄送人</span></span><br><span class="line">                   subject <span class="operator">=</span> <span class="string">&quot;dell&amp;aw仍在关粉丝人群渠道分布&quot;</span><span class="punctuation">,</span>  <span class="comment"># 邮件主题</span></span><br><span class="line">                   body <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;Dear all，</span></span><br><span class="line"><span class="string">                     dell&amp;aw仍在关粉丝人群渠道分布数据如下附件，请注意查收。&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span>  <span class="comment"># 邮件内容</span></span><br><span class="line">                   smtp <span class="operator">=</span> <span class="built_in">list</span><span class="punctuation">(</span>host.name <span class="operator">=</span> <span class="string">&quot;smtp.exmail.qq.com&quot;</span><span class="punctuation">,</span> port <span class="operator">=</span><span class="number">25</span><span class="punctuation">,</span>user.name<span class="operator">=</span><span class="string">&quot;eub_bi@eub-inc.com&quot;</span><span class="punctuation">,</span></span><br><span class="line">                               passwd <span class="operator">=</span> <span class="string">&quot;Eubbi0719&quot;</span><span class="punctuation">,</span> ssl <span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">,</span>tls<span class="operator">=</span><span class="literal">FALSE</span><span class="punctuation">)</span><span class="punctuation">,</span>  <span class="comment"># 邮箱的host name 和passwd配置</span></span><br><span class="line">                   authenticate <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">,</span></span><br><span class="line">                   send <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">,</span>  <span class="comment"># 是否立即发送</span></span><br><span class="line">                   attach.files <span class="operator">=</span> path<span class="punctuation">,</span>  <span class="comment"># 附件地址</span></span><br><span class="line">                   encoding <span class="operator">=</span> <span class="string">&quot;utf-8&quot;</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h3 id="R脚本运行自检测"><a href="#R脚本运行自检测" class="headerlink" title="R脚本运行自检测"></a>R脚本运行自检测</h3><p>检测R脚本是否可以在初始环境中运行<br>首先在Rstudio中按Common&#x2F;Ctrl+Shift+F10键重启环境，这时只有初始的环境，使用search命令查看基础包<br>然后在终端中键入Rscript 脚本名称来测试脚本是否可以运行，若不可以运行应该是缺少相关依赖包，导入即可</p><h3 id="打包数据"><a href="#打包数据" class="headerlink" title="打包数据"></a>打包数据</h3><p>使用base库自带的list.files和zip即可完成打包</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># path为需要打包的所有文件的上级目录</span></span><br><span class="line">path <span class="operator">&lt;-</span> <span class="string">&#x27;/home/dingtao/wangxukun/data/dell/dell_scan_subcribe_log/&#x27;</span></span><br><span class="line">path_name <span class="operator">&lt;-</span> list.files<span class="punctuation">(</span>path<span class="punctuation">)</span></span><br><span class="line">zip<span class="punctuation">(</span><span class="string">&#x27;filename.zip&#x27;</span><span class="punctuation">,</span> path_name<span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h3 id="解析json字符串"><a href="#解析json字符串" class="headerlink" title="解析json字符串"></a>解析json字符串</h3><p>要解决的是解析tibble中某一列存在json字符串的情况</p><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用map的时候需要把空字符串筛掉 不然会报错 在map中加上mutate会找到解析后对应的字符串</span></span><br><span class="line">address <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span></span><br><span class="line"><span class="string">&#x27;&#123;&quot;nationalCode&quot;:&quot;440402&quot;,&quot;telNumber&quot;:&quot;13539585518&quot;,&quot;errMsg&quot;:&quot;chooseAddress:ok&quot;,&quot;userName&quot;:&quot;侯艳玲&quot;,&quot;postalCode&quot;:&quot;519000&quot;,&quot;provinceName&quot;:&quot;广东省&quot;,&quot;cityName&quot;:&quot;珠海市&quot;,&quot;countyName&quot;:&quot;香洲区&quot;,&quot;detailInfo&quot;:&quot;泰来花园商铺19号 黄拯邦中医诊所&quot;&#125;&#x27;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&#x27;&#123;&quot;errMsg&quot;:&quot;chooseAddress:ok&quot;,&quot;userName&quot;:&quot;韩嘉乐&quot;,&quot;telNumber&quot;:&quot;19826080561&quot;,&quot;nationalCode&quot;:&quot;320322&quot;,&quot;postalCode&quot;:&quot;221600&quot;,&quot;provinceName&quot;:&quot;江苏省&quot;,&quot;cityName&quot;:&quot;徐州市&quot;,&quot;countyName&quot;:&quot;沛县&quot;,&quot;detailInfo&quot;:&quot;龙固镇&quot;&#125;&#x27;</span><span class="punctuation">,</span></span><br><span class="line"><span class="string">&#x27;&#123;&quot;errMsg&quot;:&quot;chooseAddress:ok&quot;,&quot;userName&quot;:&quot;宋俊纬&quot;,&quot;telNumber&quot;:&quot;13355332907&quot;,&quot;nationalCode&quot;:&quot;370305&quot;,&quot;postalCode&quot;:&quot;255400&quot;,&quot;provinceName&quot;:&quot;山东省&quot;,&quot;cityName&quot;:&quot;淄博市&quot;,&quot;countyName&quot;:&quot;临淄区&quot;,&quot;detailInfo&quot;:&quot;绿茵花园&quot;&#125;&#x27;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line">a <span class="operator">&lt;-</span> tibble<span class="punctuation">(</span>address<span class="operator">=</span>address<span class="punctuation">)</span></span><br><span class="line">a<span class="operator">$</span>address <span class="operator">%&gt;%</span> map_df<span class="punctuation">(</span>.<span class="punctuation">,</span> <span class="operator">~</span>jsonlite<span class="operator">::</span>fromJSON<span class="punctuation">(</span>..1<span class="punctuation">)</span> <span class="operator">%&gt;%</span> as_tibble<span class="punctuation">(</span><span class="punctuation">)</span> <span class="operator">%&gt;%</span> mutate<span class="punctuation">(</span>address<span class="operator">=</span>..1<span class="punctuation">)</span><span class="punctuation">)</span></span><br></pre></td></tr></table></figure><h3 id="S3类-amp-泛型函数"><a href="#S3类-amp-泛型函数" class="headerlink" title="S3类&amp;泛型函数"></a>S3类&amp;泛型函数</h3><ul><li>S3类内部是一个列表，附加一个列表类名称，可以成为该类。list里面的内容就是我们所说的属性；</li><li>S3类可以继承，在原来类的基础上再append一个新的类名即为新的类；</li><li>类中除了含有属性外，肯定还得含有方法。使用某方法.某类来创建某类的方法。比如print.gg就是对gg类的print的方法。但是在创建这种方法之前我们首先得用这个方法的名字创建一个函数，这样运行函数时首先进入这个函数，然后在函数里面使用useMethod函数，在环境中寻找该类的该方法；</li><li>default函数，表示默认的方法，如果该类找不到该类匹配的方法，就会使用默认方法；</li><li>调用方法的时候会按照从左到右的顺序，再这个例子中，默认先调用a的方法，如果想要调用f类的方法，首先写一个f的qwe方法，然后在a类中调用下一类的方法，使用NextMethod。</li></ul><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建S3类</span></span><br><span class="line">dd <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>sep<span class="operator">=</span><span class="string">&#x27;abc&#x27;</span><span class="punctuation">,</span>arg<span class="operator">=</span><span class="number">10</span><span class="punctuation">)</span> <span class="punctuation">&#123;</span>  <span class="comment"># 创建一个S3类</span></span><br><span class="line">  ls <span class="operator">&lt;-</span> <span class="built_in">list</span><span class="punctuation">(</span></span><br><span class="line">      sep <span class="operator">=</span> sep<span class="punctuation">,</span></span><br><span class="line">    arg <span class="operator">=</span> arg</span><br><span class="line">  <span class="punctuation">)</span></span><br><span class="line">  <span class="built_in">class</span><span class="punctuation">(</span>ls<span class="punctuation">)</span> <span class="operator">&lt;-</span> append<span class="punctuation">(</span><span class="built_in">class</span><span class="punctuation">(</span>ls<span class="punctuation">)</span><span class="punctuation">,</span> <span class="string">&#x27;dd&#x27;</span><span class="punctuation">)</span></span><br><span class="line">  <span class="built_in">return</span><span class="punctuation">(</span>ls<span class="punctuation">)</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">dd<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line">dd<span class="punctuation">(</span><span class="string">&#x27;qaz&#x27;</span><span class="punctuation">,</span><span class="number">100</span><span class="punctuation">)</span>  <span class="comment"># 类的实例化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 类继承</span></span><br><span class="line">ddd <span class="operator">&lt;-</span> dd<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line"><span class="built_in">class</span><span class="punctuation">(</span>ddd<span class="punctuation">)</span> <span class="operator">&lt;-</span> append<span class="punctuation">(</span><span class="built_in">class</span><span class="punctuation">(</span>ddd<span class="punctuation">)</span><span class="punctuation">,</span> <span class="string">&#x27;cc&#x27;</span><span class="punctuation">)</span>  <span class="comment"># S3类可以继承，在原来类的基础上再append一个新的类名即为新的类</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 在父类中使用子类的方法</span></span><br><span class="line">a <span class="operator">&lt;-</span> <span class="built_in">list</span><span class="punctuation">(</span>d<span class="operator">=</span><span class="string">&#x27;求和&#x27;</span><span class="punctuation">)</span></span><br><span class="line">b <span class="operator">&lt;-</span> <span class="built_in">list</span><span class="punctuation">(</span>d<span class="operator">=</span><span class="string">&#x27;平均值&#x27;</span><span class="punctuation">)</span></span><br><span class="line">d <span class="operator">&lt;-</span> <span class="built_in">list</span><span class="punctuation">(</span>d<span class="operator">=</span><span class="string">&#x27;中位值&#x27;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line">aa <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span><span class="punctuation">)</span> structure<span class="punctuation">(</span>a<span class="punctuation">,</span> <span class="built_in">class</span><span class="operator">=</span><span class="string">&#x27;a&#x27;</span><span class="punctuation">)</span>  <span class="comment"># 给属性列表自定义class属性值a</span></span><br><span class="line">bb <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span><span class="punctuation">)</span> structure<span class="punctuation">(</span>b<span class="punctuation">,</span> <span class="built_in">class</span><span class="operator">=</span><span class="string">&#x27;b&#x27;</span><span class="punctuation">)</span></span><br><span class="line">dd <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span><span class="punctuation">)</span> structure<span class="punctuation">(</span>b<span class="punctuation">,</span> <span class="built_in">class</span><span class="operator">=</span><span class="string">&#x27;d&#x27;</span><span class="punctuation">)</span></span><br><span class="line">ff <span class="operator">&lt;-</span> aa<span class="punctuation">(</span><span class="punctuation">)</span></span><br><span class="line"><span class="built_in">class</span><span class="punctuation">(</span>ff<span class="punctuation">)</span> <span class="operator">&lt;-</span> append<span class="punctuation">(</span><span class="built_in">class</span><span class="punctuation">(</span>ff<span class="punctuation">)</span><span class="punctuation">,</span> <span class="string">&#x27;f&#x27;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line">qwe <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> <span class="built_in">UseMethod</span><span class="punctuation">(</span><span class="string">&#x27;qwe&#x27;</span><span class="punctuation">)</span></span><br><span class="line">qwe.a <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> <span class="punctuation">&#123;</span></span><br><span class="line">  print<span class="punctuation">(</span><span class="string">&#x27;我是a类型&#x27;</span><span class="punctuation">)</span></span><br><span class="line">  NextMethod<span class="punctuation">(</span><span class="string">&#x27;qwe&#x27;</span><span class="punctuation">)</span>  <span class="comment"># 使用NextMothod函数调用a类型的子类，</span></span><br><span class="line">  x<span class="operator">$</span>d  <span class="comment"># 只有上方代码是调用子类，这里仍然是调用a类型</span></span><br><span class="line"><span class="punctuation">&#125;</span></span><br><span class="line">qwe.f <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> print<span class="punctuation">(</span><span class="string">&#x27;我是f类型&#x27;</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line">qwe<span class="punctuation">(</span>ff<span class="punctuation">)</span>  <span class="comment"># （我是a类型 我是f类型 求和）使用NextMethod方法后调用子类方法</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 构建S3类的泛型函数</span></span><br><span class="line"></span><br><span class="line">ab <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> <span class="built_in">UseMethod</span><span class="punctuation">(</span><span class="string">&#x27;ab&#x27;</span><span class="punctuation">)</span>  <span class="comment"># 创建泛型函数</span></span><br><span class="line">ab.default <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> <span class="string">&#x27;default&#x27;</span>  <span class="comment"># 默认函数</span></span><br><span class="line">ab.a <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> x<span class="operator">$</span>d  <span class="comment"># 根据不同的class属性值创建不同的泛型函数</span></span><br><span class="line">ab.b <span class="operator">&lt;-</span> <span class="keyword">function</span><span class="punctuation">(</span>x<span class="punctuation">)</span> x<span class="operator">$</span>d</span><br><span class="line"></span><br><span class="line">ab<span class="punctuation">(</span>aa<span class="punctuation">(</span><span class="punctuation">)</span><span class="punctuation">)</span>  <span class="comment"># （求和）这样在给泛型函数传入不同属性的列表参数就会运行对应属性的泛型函数</span></span><br><span class="line">ab<span class="punctuation">(</span>bb<span class="punctuation">(</span><span class="punctuation">)</span><span class="punctuation">)</span>  <span class="comment"># （平均值）</span></span><br><span class="line">ab<span class="punctuation">(</span>dd<span class="punctuation">(</span><span class="punctuation">)</span><span class="punctuation">)</span>  <span class="comment"># （default）没有对应d的方法，调用默认函数</span></span><br></pre></td></tr></table></figure><h3 id="stringr包"><a href="#stringr包" class="headerlink" title="stringr包"></a>stringr包</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># boundary用修饰符函数控制匹配行为</span></span><br><span class="line">boundary<span class="punctuation">(</span>type <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;character&quot;</span><span class="punctuation">,</span> <span class="string">&quot;line_break&quot;</span><span class="punctuation">,</span> <span class="string">&quot;sentence&quot;</span><span class="punctuation">,</span> <span class="string">&quot;word&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span> skip_word_none <span class="operator">=</span> <span class="literal">NA</span><span class="punctuation">,</span> ...<span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># str_detect判断子字符串是否存在于目标字符串中，返回TURE或FALSE</span></span><br><span class="line">str_detect<span class="punctuation">(</span><span class="string">&#x27;abcde&#x27;</span><span class="punctuation">,</span> <span class="string">&#x27;dcd&#x27;</span><span class="punctuation">)</span>  <span class="comment"># TRUE</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># str_count判断字符/单词长度</span></span><br><span class="line">str_count<span class="punctuation">(</span><span class="string">&#x27;abc _de&#x27;</span><span class="punctuation">)</span>  <span class="comment"># 7，默认计算字符长度</span></span><br><span class="line">str_count<span class="punctuation">(</span><span class="string">&#x27;abc _de&#x27;</span><span class="punctuation">,</span> boundary<span class="punctuation">(</span><span class="string">&#x27;character&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span>  <span class="comment"># 7</span></span><br><span class="line">str_count<span class="punctuation">(</span><span class="string">&#x27;abc _de&#x27;</span><span class="punctuation">,</span> boundary<span class="punctuation">(</span><span class="string">&#x27;character&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span>  <span class="comment"># 2，返回单词长度，数字、字符和下划线被认为是一个单词</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># str_split拆分字符串</span></span><br><span class="line">str_split<span class="punctuation">(</span><span class="string">&#x27;as b-+_1&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;-&#x27;</span><span class="punctuation">)</span> <span class="operator">%&gt;%</span> unlist<span class="punctuation">(</span><span class="punctuation">)</span>  <span class="comment"># &quot;as b&quot; &quot;+_1&quot; ，按指定字符串拆分</span></span><br><span class="line">str_split<span class="punctuation">(</span><span class="string">&#x27;as b-+_1&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;&#x27;</span><span class="punctuation">)</span> <span class="operator">%&gt;%</span> unlist<span class="punctuation">(</span><span class="punctuation">)</span>  <span class="comment"># &quot;a&quot; &quot;s&quot; &quot; &quot; &quot;b&quot; &quot;-&quot; &quot;+&quot; &quot;_&quot; &quot;1&quot;，拆分为单个字符</span></span><br><span class="line">str_split<span class="punctuation">(</span><span class="string">&#x27;qbc 1ff+2eqw_we&#x27;</span><span class="punctuation">,</span> boundary<span class="punctuation">(</span><span class="string">&#x27;word&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span> <span class="operator">%&gt;%</span> unlist<span class="punctuation">(</span><span class="punctuation">)</span>  <span class="comment"># &quot;qbc&quot; &quot;1ff&quot; &quot;2eqw_we&quot;，按单词分隔</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># str_extract_all按正则表达式提取所有符合条件的子字符串，str_extract只提取第一个</span></span><br><span class="line">str_extract_all<span class="punctuation">(</span><span class="string">&quot;The Cat in the Hat&quot;</span><span class="punctuation">,</span> <span class="string">&quot;[a-z]+&quot;</span><span class="punctuation">)</span> <span class="operator">%&gt;%</span> unlist<span class="punctuation">(</span><span class="punctuation">)</span>  <span class="comment"># &quot;he&quot;  &quot;at&quot;  &quot;in&quot;  &quot;the&quot; &quot;at&quot; </span></span><br><span class="line">str_extract_all<span class="punctuation">(</span><span class="string">&quot;The Cat in the Hat&quot;</span><span class="punctuation">,</span> <span class="string">&quot;at&quot;</span><span class="punctuation">)</span> <span class="operator">%&gt;%</span> unlist<span class="punctuation">(</span><span class="punctuation">)</span>  <span class="comment"># &quot;at&quot; &quot;at&quot;</span></span><br><span class="line">str_extract<span class="punctuation">(</span><span class="string">&quot;The Cat in the Hat&quot;</span><span class="punctuation">,</span> <span class="string">&quot;[a-z]+&quot;</span><span class="punctuation">)</span> <span class="operator">%&gt;%</span> unlist<span class="punctuation">(</span><span class="punctuation">)</span>  <span class="comment"># &quot;he&quot;</span></span><br><span class="line">str_extract<span class="punctuation">(</span><span class="string">&quot;The Cat in the Hat&quot;</span><span class="punctuation">,</span> <span class="string">&quot;at&quot;</span><span class="punctuation">)</span> <span class="operator">%&gt;%</span> unlist<span class="punctuation">(</span><span class="punctuation">)</span>  <span class="comment"># &quot;at&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># str_c将多个字符串连接成一个字符串</span></span><br><span class="line">str_c<span class="punctuation">(</span><span class="string">&#x27;+&#x27;</span><span class="punctuation">,</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;a&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;b&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;c&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span><span class="string">&#x27;:&#x27;</span><span class="punctuation">)</span>  <span class="comment"># &quot;+a:&quot; &quot;+b:&quot; &quot;+c:&quot;，将多个字符串连接成一个字符串</span></span><br><span class="line">str_c<span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;a&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;b&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;c&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">,</span><span class="string">&#x27;:&#x27;</span><span class="punctuation">,</span>sep<span class="operator">=</span><span class="string">&#x27;-&#x27;</span><span class="punctuation">)</span>  <span class="comment"># &quot;a-1-:&quot; &quot;b-1-:&quot; &quot;c-1-:&quot;，使用sep连接</span></span><br><span class="line">str_c<span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;a&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;b&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;c&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span><span class="string">&#x27;:&#x27;</span><span class="punctuation">,</span>sep<span class="operator">=</span><span class="string">&#x27;-&#x27;</span><span class="punctuation">,</span>collapse <span class="operator">=</span> <span class="string">&#x27;&#x27;</span><span class="punctuation">)</span>  <span class="comment"># &quot;a-:b-:c-:&quot;，拼接完成后将所有字符串连接在一起</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># str_glue格式化并使用glue插入一个字符串</span></span><br><span class="line">name <span class="operator">&lt;-</span> <span class="string">&quot;Fred&quot;</span></span><br><span class="line">anniversary <span class="operator">&lt;-</span> as.Date<span class="punctuation">(</span><span class="string">&quot;1991-10-12&quot;</span><span class="punctuation">)</span></span><br><span class="line">str_glue<span class="punctuation">(</span></span><br><span class="line">  <span class="string">&quot;My name is &#123;name&#125;, &quot;</span><span class="punctuation">,</span></span><br><span class="line">  <span class="string">&quot;and my anniversary is &#123;format(anniversary, &#x27;%A, %B %d, %Y&#x27;)&#125;.&quot;</span></span><br><span class="line"><span class="punctuation">)</span>  <span class="comment"># My name is Fred, and my anniversary is Saturday, October 12, 1991.</span></span><br><span class="line">mtcars <span class="operator">%&gt;%</span> str_glue_data<span class="punctuation">(</span><span class="string">&quot;&#123;rownames(.)&#125; has &#123;hp&#125; hp&quot;</span><span class="punctuation">)</span>  <span class="comment"># 针对dataframe的字符串格式化，按行输出</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># str_locate定位子串在字符串中的位置，返回一个列表</span></span><br><span class="line">fruit <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;apple&quot;</span><span class="punctuation">,</span> <span class="string">&quot;banana&quot;</span><span class="punctuation">,</span> <span class="string">&quot;pear&quot;</span><span class="punctuation">,</span> <span class="string">&quot;pineapple&quot;</span><span class="punctuation">)</span></span><br><span class="line">str_locate<span class="punctuation">(</span>fruit<span class="punctuation">,</span> <span class="string">&quot;$&quot;</span><span class="punctuation">)</span></span><br><span class="line">str_locate<span class="punctuation">(</span>fruit<span class="punctuation">,</span> <span class="string">&quot;a&quot;</span><span class="punctuation">)</span></span><br><span class="line">str_locate<span class="punctuation">(</span>fruit<span class="punctuation">,</span> <span class="string">&quot;e&quot;</span><span class="punctuation">)</span></span><br><span class="line">str_locate<span class="punctuation">(</span>fruit<span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;a&quot;</span><span class="punctuation">,</span> <span class="string">&quot;b&quot;</span><span class="punctuation">,</span> <span class="string">&quot;p&quot;</span><span class="punctuation">,</span> <span class="string">&quot;p&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line">str_locate_all<span class="punctuation">(</span>fruit<span class="punctuation">,</span> <span class="string">&quot;a&quot;</span><span class="punctuation">)</span>  <span class="comment"># 返回子串所在的所有位置</span></span><br><span class="line">str_locate_all<span class="punctuation">(</span>fruit<span class="punctuation">,</span> <span class="string">&quot;e&quot;</span><span class="punctuation">)</span></span><br><span class="line">str_locate_all<span class="punctuation">(</span>fruit<span class="punctuation">,</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;a&quot;</span><span class="punctuation">,</span> <span class="string">&quot;b&quot;</span><span class="punctuation">,</span> <span class="string">&quot;p&quot;</span><span class="punctuation">,</span> <span class="string">&quot;p&quot;</span><span class="punctuation">)</span><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># str_remove删掉匹配到的子串，支持正则表达式</span></span><br><span class="line">fruits <span class="operator">&lt;-</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;one apple&quot;</span><span class="punctuation">,</span> <span class="string">&quot;two pears&quot;</span><span class="punctuation">,</span> <span class="string">&quot;three bananas&quot;</span><span class="punctuation">)</span></span><br><span class="line">str_remove<span class="punctuation">(</span>fruits<span class="punctuation">,</span> <span class="string">&quot;[aeiou]&quot;</span><span class="punctuation">)</span>  <span class="comment"># &quot;ne apple&quot;     &quot;tw pears&quot;     &quot;thre bananas&quot;</span></span><br><span class="line">str_remove_all<span class="punctuation">(</span>fruits<span class="punctuation">,</span> <span class="string">&quot;[aeiou]&quot;</span><span class="punctuation">)</span>  <span class="comment"># &quot;n ppl&quot;    &quot;tw prs&quot;   &quot;thr bnns&quot;</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># str_trunc截断一个字符串</span></span><br><span class="line">x <span class="operator">&lt;-</span> <span class="string">&quot;This string is moderately long&quot;</span></span><br><span class="line">rbind<span class="punctuation">(</span></span><br><span class="line">  str_trunc<span class="punctuation">(</span>x<span class="punctuation">,</span> <span class="number">20</span><span class="punctuation">,</span> <span class="string">&quot;right&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span>  <span class="comment"># &quot;This string is mo...&quot;</span></span><br><span class="line">  str_trunc<span class="punctuation">(</span>x<span class="punctuation">,</span> <span class="number">20</span><span class="punctuation">,</span> <span class="string">&quot;left&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span>  <span class="comment"># &quot;...s moderately long&quot;</span></span><br><span class="line">  str_trunc<span class="punctuation">(</span>x<span class="punctuation">,</span> <span class="number">20</span><span class="punctuation">,</span> <span class="string">&quot;center&quot;</span><span class="punctuation">)</span>  <span class="comment"># &quot;This stri...ely long&quot;</span></span><br><span class="line"><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line">str_to_upper<span class="punctuation">(</span><span class="string">&#x27;asd asd qafdwe&#x27;</span><span class="punctuation">)</span>  <span class="comment"># &quot;ASD ASD QAFDWE&quot;，转换为大写</span></span><br><span class="line">str_to_title<span class="punctuation">(</span><span class="string">&#x27;qweq wer wr&#x27;</span><span class="punctuation">)</span>  <span class="comment"># &quot;Qweq Wer Wr&quot;，首字母转换为大写</span></span><br><span class="line">regex<span class="punctuation">(</span><span class="string">&#x27;[0-9]&#x27;</span><span class="punctuation">)</span>  <span class="comment"># 生成正则表达式，有些函数支持输入正则表达式，就不用这个了</span></span><br><span class="line">str_dup<span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;ab&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;bc&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;cd&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span><span class="number">2</span><span class="punctuation">)</span>  <span class="comment"># &quot;abab&quot; &quot;bcbc&quot; &quot;cdcd&quot;，重复每个元素指定次数</span></span><br><span class="line">str_ends<span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;abc&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;abe&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;cde&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="string">&#x27;e&#x27;</span><span class="punctuation">)</span>  <span class="comment"># FALSE  TRUE  TRUE，检查字符串结尾是否为某个子字符串</span></span><br><span class="line">str_starts<span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;abc&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;abe&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;cde&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span> <span class="string">&#x27;a&#x27;</span><span class="punctuation">)</span>  <span class="comment"># TRUE  TRUE  FALSE，检查字符串开头是否为某个子字符串</span></span><br><span class="line">str_flatten<span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;a&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;b&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;c&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span>  <span class="comment"># &quot;abc&quot;，将字符串从头到尾全部连接起来，可以指定collapse相当于sep</span></span><br><span class="line">str_length<span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;i&quot;</span><span class="punctuation">,</span> <span class="string">&quot;like&quot;</span><span class="punctuation">,</span> <span class="literal">NA</span><span class="punctuation">)</span><span class="punctuation">)</span>  <span class="comment"># 1  4 NA，返回字符串的长度</span></span><br><span class="line">str_order<span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;b&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;c&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;a&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span>  <span class="comment"># 3 1 2，按字符索引进行排序</span></span><br><span class="line">str_sort<span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;b&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;c&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;a&#x27;</span><span class="punctuation">)</span><span class="punctuation">)</span>  <span class="comment"># &quot;a&quot; &quot;b&quot; &quot;c&quot;，对字符进行排序</span></span><br><span class="line">str_replace<span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;abc&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;bca&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;ccc&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;bbb&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span><span class="string">&#x27;b&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;-&#x27;</span><span class="punctuation">)</span>  <span class="comment"># &quot;a-c&quot; &quot;-ca&quot; &quot;ccc&quot; &quot;-bb&quot;，替换第一个字符</span></span><br><span class="line">str_replace_all<span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="string">&#x27;abc&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;bca&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;ccc&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;bbb&#x27;</span><span class="punctuation">)</span><span class="punctuation">,</span><span class="string">&#x27;b&#x27;</span><span class="punctuation">,</span><span class="string">&#x27;-&#x27;</span><span class="punctuation">)</span>  <span class="comment"># &quot;a-c&quot; &quot;-ca&quot; &quot;ccc&quot; &quot;---&quot;，替换所有字符</span></span><br><span class="line">str_replace_na<span class="punctuation">(</span><span class="built_in">c</span><span class="punctuation">(</span><span class="literal">NA</span><span class="punctuation">,</span> <span class="string">&quot;abc&quot;</span><span class="punctuation">,</span> <span class="string">&quot;def&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span><span class="string">&#x27;ccc&#x27;</span><span class="punctuation">)</span>  <span class="comment"># &quot;ccc&quot; &quot;abc&quot; &quot;def&quot;，替换空值</span></span><br><span class="line">str_trim<span class="punctuation">(</span><span class="string">&#x27; qwe &#x27;</span><span class="punctuation">)</span>  <span class="comment"># &quot;qwe&quot;，默认删掉字符串两边空格，可以指定side为left或者right</span></span><br><span class="line">str_pad<span class="punctuation">(</span>string<span class="operator">=</span><span class="string">&#x27;abc&#x27;</span><span class="punctuation">,</span> side<span class="operator">=</span><span class="string">&#x27;both&#x27;</span><span class="punctuation">,</span> width<span class="operator">=</span><span class="string">&#x27;10&#x27;</span><span class="punctuation">,</span> pad<span class="operator">=</span><span class="string">&#x27;-&#x27;</span><span class="punctuation">)</span>  <span class="comment"># &quot;---abc----&quot;，字符串填充</span></span><br><span class="line">str_squish<span class="punctuation">(</span><span class="string">&quot;\t \n\nString \n space\n\n &quot;</span><span class="punctuation">)</span>  <span class="comment"># &quot;String space&quot;，删掉字符串中的无用字符（空格、换行等）</span></span><br><span class="line">str_sub<span class="punctuation">(</span><span class="string">&#x27;string&#x27;</span><span class="punctuation">,</span> start<span class="operator">=</span><span class="number">1L</span><span class="punctuation">,</span> end<span class="operator">=</span><span class="number">3</span><span class="punctuation">)</span>  <span class="comment"># &quot;str&quot;，根据位置截取字符串，end为-1截取所有字符串</span></span><br></pre></td></tr></table></figure><h3 id="openxlsx包"><a href="#openxlsx包" class="headerlink" title="openxlsx包"></a>openxlsx包</h3><figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># writeData写入数据到工作不对象的sheet中，以及常用的参数，write.xlsx直接写入文件</span></span><br><span class="line">writeData<span class="punctuation">(</span>wb<span class="punctuation">,</span><span class="string">&#x27;ss&#x27;</span><span class="punctuation">,</span>dd<span class="punctuation">)</span>  <span class="comment"># 普通的写入</span></span><br><span class="line">writeData<span class="punctuation">(</span></span><br><span class="line">  wb<span class="punctuation">,</span>  <span class="comment"># 工作簿对象</span></span><br><span class="line">  sheet<span class="punctuation">,</span>  <span class="comment"># sheet名字</span></span><br><span class="line">  x<span class="punctuation">,</span>  <span class="comment"># 数据</span></span><br><span class="line">  startCol <span class="operator">=</span> <span class="number">1</span><span class="punctuation">,</span>  <span class="comment"># 数据写入的开始列</span></span><br><span class="line">  startRow <span class="operator">=</span> <span class="number">1</span><span class="punctuation">,</span>  <span class="comment"># 开始行</span></span><br><span class="line">  colNames <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">,</span>  <span class="comment"># 是否包含列名</span></span><br><span class="line">  rowNames <span class="operator">=</span> <span class="literal">FALSE</span><span class="punctuation">,</span>  <span class="comment"># 是否包含行名</span></span><br><span class="line">  headerStyle <span class="operator">=</span> <span class="literal">NULL</span><span class="punctuation">,</span>  <span class="comment"># columns的自定义样式，插入一个addStyle样式</span></span><br><span class="line">  borders <span class="operator">=</span> <span class="built_in">c</span><span class="punctuation">(</span><span class="string">&quot;none&quot;</span><span class="punctuation">,</span> <span class="string">&quot;surrounding&quot;</span><span class="punctuation">,</span> <span class="string">&quot;rows&quot;</span><span class="punctuation">,</span> <span class="string">&quot;columns&quot;</span><span class="punctuation">,</span> <span class="string">&quot;all&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span>  <span class="comment"># 四周边框</span></span><br><span class="line">  borderColour <span class="operator">=</span> getOption<span class="punctuation">(</span><span class="string">&quot;openxlsx.borderColour&quot;</span><span class="punctuation">,</span> <span class="string">&quot;black&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span>  <span class="comment"># 边框颜色</span></span><br><span class="line">  borderStyle <span class="operator">=</span> getOption<span class="punctuation">(</span><span class="string">&quot;openxlsx.borderStyle&quot;</span><span class="punctuation">,</span> <span class="string">&quot;thin&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span>  <span class="comment"># 边框样式</span></span><br><span class="line">  na.string <span class="operator">=</span> <span class="literal">NULL</span><span class="punctuation">,</span>  <span class="comment"># 若有NA值，则全部填充为这个值</span></span><br><span class="line"><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># createStyle新建样式，在插入数据的时候使用</span></span><br><span class="line">createStyle<span class="punctuation">(</span></span><br><span class="line">  fontName <span class="operator">=</span> <span class="literal">NULL</span><span class="punctuation">,</span>  <span class="comment"># 字体名称</span></span><br><span class="line">  fontSize <span class="operator">=</span> <span class="literal">NULL</span><span class="punctuation">,</span>  <span class="comment"># 字体大小</span></span><br><span class="line">  fontColour <span class="operator">=</span> <span class="literal">NULL</span><span class="punctuation">,</span>  <span class="comment"># 字体颜色</span></span><br><span class="line">  numFmt <span class="operator">=</span> <span class="string">&quot;GENERAL&quot;</span><span class="punctuation">,</span>  <span class="comment"># 格式化数据</span></span><br><span class="line">  border <span class="operator">=</span> <span class="literal">NULL</span><span class="punctuation">,</span>  <span class="comment"># 四周边框</span></span><br><span class="line">  borderColour <span class="operator">=</span> getOption<span class="punctuation">(</span><span class="string">&quot;openxlsx.borderColour&quot;</span><span class="punctuation">,</span> <span class="string">&quot;black&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span>  <span class="comment"># 边框颜色</span></span><br><span class="line">  borderStyle <span class="operator">=</span> getOption<span class="punctuation">(</span><span class="string">&quot;openxlsx.borderStyle&quot;</span><span class="punctuation">,</span> <span class="string">&quot;thin&quot;</span><span class="punctuation">)</span><span class="punctuation">,</span>  <span class="comment"># 边框样式</span></span><br><span class="line">  bgFill <span class="operator">=</span> <span class="literal">NULL</span><span class="punctuation">,</span>  <span class="comment"># 单元格背景颜色</span></span><br><span class="line">  halign <span class="operator">=</span> <span class="literal">NULL</span><span class="punctuation">,</span>  <span class="comment"># 单元格内容的水平对齐（left、right、center）</span></span><br><span class="line">  valign <span class="operator">=</span> <span class="literal">NULL</span><span class="punctuation">,</span>  <span class="comment"># 单元格内容的垂直对齐（top、center、bottom）</span></span><br><span class="line">  wrapText <span class="operator">=</span> <span class="literal">FALSE</span><span class="punctuation">,</span>  <span class="comment"># 若为TRUE则单元格内自动换行</span></span><br><span class="line">  indent <span class="operator">=</span> <span class="literal">NULL</span><span class="punctuation">,</span>  <span class="comment"># 缩进</span></span><br><span class="line">  locked <span class="operator">=</span> <span class="literal">NULL</span><span class="punctuation">,</span>  <span class="comment"># 锁定单元格</span></span><br><span class="line">  hidden <span class="operator">=</span> <span class="literal">NULL</span>  <span class="comment"># 隐藏单元格内容的公式</span></span><br><span class="line"><span class="punctuation">)</span></span><br><span class="line">headerStyle <span class="operator">&lt;-</span> createStyle<span class="punctuation">(</span></span><br><span class="line">  fontSize <span class="operator">=</span> <span class="number">14</span><span class="punctuation">,</span> fontColour <span class="operator">=</span> <span class="string">&quot;#FFFFFF&quot;</span><span class="punctuation">,</span> numFmt <span class="operator">=</span> <span class="string">&quot;0.00%&quot;</span><span class="punctuation">,</span> halign <span class="operator">=</span> <span class="string">&quot;center&quot;</span><span class="punctuation">,</span></span><br><span class="line">  fgFill <span class="operator">=</span> <span class="string">&quot;#4F81BD&quot;</span><span class="punctuation">,</span> border <span class="operator">=</span> <span class="string">&quot;TopBottom&quot;</span><span class="punctuation">,</span> borderColour <span class="operator">=</span> <span class="string">&quot;#4F81BD&quot;</span></span><br><span class="line"><span class="punctuation">)</span></span><br><span class="line">addStyle<span class="punctuation">(</span>wb<span class="punctuation">,</span> sheet<span class="operator">=</span><span class="number">1</span><span class="punctuation">,</span> headerStyle<span class="punctuation">,</span> rows<span class="operator">=</span><span class="number">1</span><span class="punctuation">,</span> cols<span class="operator">=</span><span class="number">1</span><span class="operator">:</span><span class="number">6</span><span class="punctuation">,</span> gridExpand<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">)</span>  <span class="comment"># 使用样式</span></span><br><span class="line">writeData<span class="punctuation">(</span>wb<span class="punctuation">,</span><span class="string">&#x27;ss&#x27;</span><span class="punctuation">,</span>dd<span class="punctuation">,</span>headerStyle<span class="operator">=</span>headerStyle<span class="punctuation">)</span>  <span class="comment"># 使用样式</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># read.xlsx从本地读取excel数据</span></span><br><span class="line">read.xlsx<span class="punctuation">(</span></span><br><span class="line">  xlsxFile<span class="punctuation">,</span>  <span class="comment"># 数据路径</span></span><br><span class="line">  sheet <span class="operator">=</span> <span class="number">1</span><span class="punctuation">,</span>  <span class="comment"># sheet名称</span></span><br><span class="line">  startRow <span class="operator">=</span> <span class="number">1</span><span class="punctuation">,</span>  <span class="comment"># 开始行</span></span><br><span class="line">  colNames <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">,</span>  <span class="comment"># 是否包含列名</span></span><br><span class="line">  rowNames <span class="operator">=</span> <span class="literal">FALSE</span><span class="punctuation">,</span>  <span class="comment"># 是否包含行名</span></span><br><span class="line">  detectDates <span class="operator">=</span> <span class="literal">FALSE</span><span class="punctuation">,</span>  <span class="comment"># 若为真，则会将日期转换为日期格式</span></span><br><span class="line">  skipEmptyRows <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">,</span>  <span class="comment"># 跳过空行</span></span><br><span class="line">  skipEmptyCols <span class="operator">=</span> <span class="literal">TRUE</span><span class="punctuation">,</span>  <span class="comment"># 跳过空列</span></span><br><span class="line">  rows <span class="operator">=</span> <span class="literal">NULL</span><span class="punctuation">,</span>  <span class="comment"># 默认读取所有的，可以指定读取指定的行</span></span><br><span class="line">  cols <span class="operator">=</span> <span class="literal">NULL</span><span class="punctuation">,</span>  <span class="comment"># 默认读取所有的，可以指定读取指定的列</span></span><br><span class="line">  na.strings <span class="operator">=</span> <span class="string">&quot;NA&quot;</span><span class="punctuation">,</span>  <span class="comment"># 空白单元格的返回值，默认返回NA</span></span><br><span class="line">  fillMergedCells <span class="operator">=</span> <span class="literal">FALSE</span>  <span class="comment"># 如果为真，拆分合并的单元格并分到各单元格</span></span><br><span class="line"><span class="punctuation">)</span></span><br><span class="line"></span><br><span class="line">createWorkbook<span class="punctuation">(</span><span class="punctuation">)</span>  <span class="comment"># 新建一个工作簿对象，可在对象内插入sheet，工具</span></span><br><span class="line">loadWorkbook<span class="punctuation">(</span>filepath<span class="punctuation">)</span>  <span class="comment"># 加载一个现有的.xlsx文件，生成工作簿对象</span></span><br><span class="line">saveWorkbook<span class="punctuation">(</span>wb<span class="punctuation">,</span> file<span class="punctuation">,</span> overwrite <span class="operator">=</span> <span class="literal">FALSE</span><span class="punctuation">)</span>  <span class="comment"># 保存工作簿对象到文件，若存在不会覆盖会报错</span></span><br><span class="line">addWorksheet<span class="punctuation">(</span>wb<span class="punctuation">,</span><span class="string">&#x27;sheet&#x27;</span><span class="punctuation">)</span>  <span class="comment"># 向工作簿对象添加一个空白sheet，更多参数查看help</span></span><br><span class="line">addStyle<span class="punctuation">(</span>wb<span class="punctuation">,</span> sheet<span class="operator">=</span><span class="number">1</span><span class="punctuation">,</span> headerStyle<span class="punctuation">,</span> rows<span class="operator">=</span><span class="number">1</span><span class="punctuation">,</span> cols<span class="operator">=</span><span class="number">1</span><span class="operator">:</span><span class="number">6</span><span class="punctuation">,</span> gridExpand<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">)</span>  <span class="comment"># 直接向工作簿对象添加样式</span></span><br><span class="line">createComment<span class="punctuation">(</span>comment <span class="operator">=</span> <span class="string">&quot;this is comment&quot;</span><span class="punctuation">)</span>  <span class="comment"># 新建一个注释，也有其他参数可以设置</span></span><br><span class="line">writeComment<span class="punctuation">(</span>wb<span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">,</span> col<span class="operator">=</span><span class="string">&quot;B&quot;</span><span class="punctuation">,</span> row<span class="operator">=</span><span class="number">10</span><span class="punctuation">,</span> comment<span class="operator">=</span>c1<span class="punctuation">)</span>  <span class="comment"># 给指定单元格添加注释</span></span><br><span class="line">deleteData<span class="punctuation">(</span>wb<span class="punctuation">,</span>sheet<span class="operator">=</span><span class="number">1</span><span class="punctuation">,</span>cols<span class="operator">=</span><span class="number">3</span><span class="operator">:</span><span class="number">5</span><span class="punctuation">,</span>rows<span class="operator">=</span><span class="number">5</span><span class="operator">:</span><span class="number">7</span><span class="punctuation">,</span>gridExpand<span class="operator">=</span><span class="literal">TRUE</span><span class="punctuation">)</span> <span class="comment"># 删除数据，gridExpand为真删除范围内所有数据</span></span><br><span class="line">freezePane<span class="punctuation">(</span>wb<span class="punctuation">,</span> <span class="string">&quot;Sheet 1&quot;</span><span class="punctuation">,</span> firstActiveRow <span class="operator">=</span> <span class="number">5</span><span class="punctuation">,</span> firstActiveCol <span class="operator">=</span> <span class="number">3</span><span class="punctuation">)</span>  <span class="comment"># 冻结单元格</span></span><br><span class="line">getSheetNames<span class="punctuation">(</span><span class="string">&#x27;/home/dingtao/wangxukun/data/tmp/alcon.xlsx&#x27;</span><span class="punctuation">)</span>  <span class="comment"># 返回本地excel文件的所有sheet名字</span></span><br><span class="line">mergeCells<span class="punctuation">(</span>wb<span class="punctuation">,</span> <span class="string">&quot;Sheet 1&quot;</span><span class="punctuation">,</span> cols <span class="operator">=</span> <span class="number">2</span><span class="punctuation">,</span> rows <span class="operator">=</span> <span class="number">3</span><span class="operator">:</span><span class="number">6</span><span class="punctuation">)</span>  <span class="comment"># 合并工作表中的单元格</span></span><br><span class="line">removeCellMerge<span class="punctuation">(</span>wb<span class="punctuation">,</span> <span class="string">&quot;Sheet 1&quot;</span><span class="punctuation">,</span> cols <span class="operator">=</span> <span class="number">2</span><span class="punctuation">,</span> rows <span class="operator">=</span> <span class="number">3</span><span class="operator">:</span><span class="number">6</span><span class="punctuation">)</span>  <span class="comment"># 拆分已合并的单元格</span></span><br><span class="line">sheets<span class="punctuation">(</span>wb<span class="punctuation">)</span>  <span class="comment"># 返回所有sheet名称</span></span><br><span class="line"><span class="built_in">names</span><span class="punctuation">(</span>wb<span class="punctuation">)</span>  <span class="comment"># 查看工作簿的所有sheet名字，可以修改sheet名字</span></span><br><span class="line">renameWorksheet<span class="punctuation">(</span>wb<span class="punctuation">,</span> <span class="string">&quot;Sheet 1&quot;</span><span class="punctuation">,</span> <span class="string">&quot;ss&quot;</span><span class="punctuation">)</span>  <span class="comment"># 修改sheet名称</span></span><br><span class="line">removeWorksheet<span class="punctuation">(</span>wb<span class="punctuation">,</span> <span class="string">&quot;Sheet 1&quot;</span><span class="punctuation">)</span>  <span class="comment"># 删除指定的sheet</span></span><br><span class="line">replaceStyle<span class="punctuation">(</span>wb<span class="punctuation">,</span> <span class="string">&quot;Sheet 1&quot;</span><span class="punctuation">,</span> newStyle <span class="operator">=</span> newStyle<span class="punctuation">)</span>  <span class="comment"># 替换已经存在的style</span></span><br><span class="line">setColWidths<span class="punctuation">(</span>wb<span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">,</span> cols<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">4</span><span class="punctuation">,</span> <span class="number">6</span><span class="punctuation">,</span> <span class="number">7</span><span class="punctuation">,</span><span class="punctuation">)</span><span class="punctuation">,</span> widths<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">16</span><span class="punctuation">,</span> <span class="number">15</span><span class="punctuation">,</span> <span class="number">12</span><span class="punctuation">,</span> <span class="number">18</span><span class="punctuation">)</span><span class="punctuation">)</span>  <span class="comment"># 设置工作表列宽度</span></span><br><span class="line">setRowHeights<span class="punctuation">(</span>wb<span class="punctuation">,</span> <span class="number">1</span><span class="punctuation">,</span> rows<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">1</span><span class="punctuation">,</span> <span class="number">4</span><span class="punctuation">,</span> <span class="number">22</span><span class="punctuation">,</span> <span class="number">2</span><span class="punctuation">)</span><span class="punctuation">,</span> heights<span class="operator">=</span><span class="built_in">c</span><span class="punctuation">(</span><span class="number">24</span><span class="punctuation">,</span> <span class="number">28</span><span class="punctuation">,</span> <span class="number">32</span><span class="punctuation">,</span> <span class="number">42</span><span class="punctuation">)</span><span class="punctuation">)</span>  <span class="comment"># 设置工作表行高</span></span><br><span class="line">write.xlsx<span class="punctuation">(</span>dd<span class="punctuation">,</span> filepath<span class="punctuation">)</span>  <span class="comment"># 向excel文件写入datatable数据，或者是list，每个元素一个sheet</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Language </category>
          
      </categories>
      
      
        <tags>
            
            <tag> R </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>论加班的意义</title>
      <link href="/2020/06/15/Mind/%E8%AE%BA%E5%8A%A0%E7%8F%AD%E7%9A%84%E6%84%8F%E4%B9%89/"/>
      <url>/2020/06/15/Mind/%E8%AE%BA%E5%8A%A0%E7%8F%AD%E7%9A%84%E6%84%8F%E4%B9%89/</url>
      
        <content type="html"><![CDATA[<p>上周我基本每天都会加班到晚上八点以后，包括周六也没能幸免，精神状态实在不怎么好，也就有了下面的文字。加班现在已成为了一种常态，不管是哪个年龄段或者哪个行业，当然，每个行业的加班状况都不相同。我说不出哪个行业加班最严重，自己对这方面了解甚少，只是就自己的情况聊一聊对加班的看法。</p><span id="more"></span><p>我所在的部门是药企市场部，每天的工作繁琐而杂乱，因为没有系统性的思考或者是被引导自己负责的这一部分工作在整个工作环节中的位置以及所扮演的角色，只被告知要如何做，却不知道做了后是为了什么，所以就导致做一步迷茫一步，往往做到最后一步才明白自己所做的这些是为了什么，这是目标不明确所引起的效率低下导致的加班；还有一种原因是你的团队在协同做一件事情，然后各自分配了相应的任务，大家就去各做各的，做完之后一碰头才发现结果对不上或者理解有偏差，那自然就需要返工重新做，这是团队沟通不到位，理解有偏差引发的重复返工修改导致的加班。</p><p>那么问题来了，为什么会加班，是因为团队负责人没有给每个人明确目标吗？是因为大家沟通太少导致重复返工吗？乍一看好像可以把责任都推给外界，但若不从自身反思，不从自身改变，总是把所有的责任推给领导或同事，是永远不可能解决问题的。团队负责人没有给你明确目标你可以做一个初稿或者有一点思路马上拿去跟他讨论，这样可以确保你的方向始终不会偏离太远，每次沟通就是一次方向的矫正；还有团队的沟通问题，最好的方法就是提醒团队负责人大家多在一起交流讨论，明确方向，检验自己的成果，若有偏差及时纠正，既然是团队的一份子就有义务为团队着想，为团队的目标努力，再不济你也可以拿着自己做好的找他检查提建议，而不是自己苦苦思索结果的正确性，等着大家坐在一起的时候被一顿批。</p><p>其实我不反对加班，每天五点半下班不管有事没事都会待到六点多七点才走，可以思考思考今天的工作状态，有哪些地方做的不好有待提升，然后做第二天的计划，定第二天的日程。我反对低质量或者是没必要的加班，亦或是工作时间没好好利用，将工作都堆在下班后，这样不管所做的事情是否有意义心里都会不痛快，我自己在工作时间的工作安排就很不合理，分明是很少的一部分工作却需要做大半天，浪费了太多的时间去纠结细节，而不是工作的方向和把控全局，这是我目前急需优化的。总而言之，加班是不可避免的，重要的是做到高效加班，减少无意义的加班，让自己开心的加班，当然还需要注意自己的身体健康，这些都是最终的目的。</p>]]></content>
      
      
      <categories>
          
          <category> Mind </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mind </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>勤奋而不可得</title>
      <link href="/2020/06/08/Mind/%E5%8B%A4%E5%A5%8B%E8%80%8C%E4%B8%8D%E5%8F%AF%E5%BE%97/"/>
      <url>/2020/06/08/Mind/%E5%8B%A4%E5%A5%8B%E8%80%8C%E4%B8%8D%E5%8F%AF%E5%BE%97/</url>
      
        <content type="html"><![CDATA[<p>今天来说说勤奋的问题，努力的问题，关于勤奋努力熟知的名句有很多：勤能补拙。吃得苦中苦，方为人上人。只要有付出就会有回报等。单看这些句子好像会给我一种错觉，只要我努力了所谓的成功就会离我越来越近了，而且就算是失败了也可以给自己打一针我已经努力过了的安慰剂，这更像是在逃避未达成目的带给自己的失落感。</p><span id="more"></span><p>我下班后在地铁上经常可以看到靠着座椅睡着的人，当然我也很累，我就在想大家都这么努力，都这么劳累，那我有什么理由坚持不下去呢，但是一周，一个月，到最后可能好几年下去，到时候我的目标还是那么遥不可及，亦或者是离自己想要的生活还是那么遥远，那我会思考是我不够勤奋吗？还是我不够聪明，或者是我运气不好，不管是出于什么原因，我现在将这个问题摆在面前都比我几年后一事无成的时候再去想这个问题要明智得多，所以为什么我分明付出了但却没有得到合理的回报？</p><p>首先勤奋并不是达成目标的充分条件，我要做成一件事，首先得有个目标，然后是判定目标达成的标准，接下来就是时间计划和行动计划，计划制定完成后才是根据计划一步步落实去行动，若是在行动完成之后去反思行动中的不足并加以完善就更好了。我所理解的勤奋和努力是在行动这一步内需要付出的东西。目标未达成有一部分原因是做事之前未考虑其可行性，未曾将事情想明白就匆匆动手，做到一半才发现事情不可行或者落下了什么重要的前提条件。</p><p>其次是状态，状态不好，一味地勤奋到头来还不如其他付出得当的人收获多，再加上给自己强加的各种欲望和压力，找不到合适的状态，所谓的目标只会是个目标。你的心态会从根本上影响你的状态，而你的状态则会影响你勤奋的形态和最终的结果。心态则会受外因和内因的影响，内因指的是你的情绪、主观判断、情感接受度、时间观念、行动力等等，外因指的是外界的干扰，例如天气、温度、别人的态度、时间因素、所要遵守的规则等等。而外因与内因所形成的结果，就是你的基本心态，你对一件事保有积极的心态和消极的心态将会导致两种截然不同的状态。</p><p>最后是专注力，现如今三分钟热度已经成为了社会常态，外界繁杂的娱乐节目和各种软文段子无时无刻不再撕扯着我们的精力，在做一件事情的时候往往没多大一会就想着休息一会去看看朋友圈看看抖音。长此以往，我们已经不能专注的去做一件事情或者哪怕是专注的去休息，我在休息的时候也要去看看公众号都发了什么我感兴趣的内容。每天也不知道自己做了些什么，只知道什么都没做成，而且自己相比于昨天前天甚至上个月都没有明显的变化。说到底讨论专注力就是讨论精力分配的问题，每个人每天的精力除过你必须耗费掉的如睡觉，走路，吃饭等必须精力剩下的本就没多少，这一小部分精力再不好好利用一事无成将是必然结果。</p><p>当我选择了这条路，就意味着我必须尽快从舒适圈中脱离出来，不只是体力上的适应，而是思维，心态必须得改变，适应这个快节奏的环境。我每反思一次，脑子上的锈就会松动一分，我若是对反思后的结果能够及时纠正和完善，那我就会逐渐迈入正轨，跟我的灵魂和想要的生活所契合的正轨。</p>]]></content>
      
      
      <categories>
          
          <category> Mind </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mind </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>停下来思考片刻</title>
      <link href="/2020/05/25/Mind/%E5%81%9C%E4%B8%8B%E6%9D%A5%E6%80%9D%E8%80%83%E7%89%87%E5%88%BB/"/>
      <url>/2020/05/25/Mind/%E5%81%9C%E4%B8%8B%E6%9D%A5%E6%80%9D%E8%80%83%E7%89%87%E5%88%BB/</url>
      
        <content type="html"><![CDATA[<p>前段时间反思了自己的心理状态，负面情绪蔓延到了身体的每一处角落，堵塞了好几个月的情绪始终无法找到宣泄口，直到最近才开始慢慢寻找存在的问题，总结了下大概有两方面的原因。</p><span id="more"></span><p>第一是工作上的，我从去年年底辞职后一直在家待业，说不焦虑那是骗人的，而且自己的底子也不好，再加上属于转行，自然就更焦虑了些。终于在来到北京一个月后找着了一份工作，当时还庆幸留在最后的果然就是最好的，一个月前我入职公司，怀着大干一番的志向，一头扎在工作中，结果最近我才确定我扎进的那是泥潭，不是工作，我的脑子也灌进些许泥浆，搅的自己一阵糊涂，说白了就是干了一个月，不知道自己都干了些啥，不知道自己将要干啥。每天紧张的工作氛围和自己急于求成却闷头乱窜的性格，让我这本刚稳下来的心再次被搅得七上八下。终于我慢下来了，我开始思考未来的职业规划，思考目前最需要做什么工作，我明白这是新的开始，也是一次转折，如果不能想清楚就随大流走下去，我相信那样只会往脑子里灌更多的泥。</p><p>第二是生活上的，我的家境并不富裕，这也就导致了你不能在适当的时候做一些适当的事，这个适当指的是事情就到这了，就是现在，必须得干，尽管不是你所愿的。有一句话叫比上不足比下有余，我的金钱观就是比下有余，虽然偶尔也会感概命运不公，但所幸并不会使自己心理扭曲，我只要赚够我当下需要或者被外界被迫需要的钱并能有些许富余，就是心满意足的了。我会坚持这个理念继续下去并且持续下去，这就是我想要的了。</p><p>对于这些负面情绪，有的时候我有特殊的处理方法 ，可能是被动的，也可能是主动的，被动指的是有人一针见血的指出来：你怎么能这样呢，自己是什么情况心里没点数吗？你这样下去会是怎样一副德行吧啦吧啦之类的。主动是每当有负面情绪产生的时候，我会去挑一些路人，趁等红灯的时间想象他们上班是什么状况，下班回家是怎样一种状态，面对同事和朋友又是怎样一副面孔，我就会觉得太奇妙了，我所经历的这些破事算个啥啊，当然，路人也可以换成困在沙漠中的淘金者，或者劳作的农民，或者天空翱翔的鹰，你所能想到的一切可以比对的事物，目的就是使自己变的渺小，使自己所烦心的事变的更渺小。当然，并不是每次都能主动想到去做这些对比。</p><p>对于自己的生活，一时半会是不会有很大的转变，重要的是自己的心态要摆正。对于自己的工作，我认为还有很大的上升空间，因为自己知道有哪些空白需要去填充，那就可以从这里出发去改善自己的现状，工作方向的转变在未来一段时间将作为自身的一个重点关注对象。关于思想和精神状态，我又双叒叕淘到了一本《作为意志和表象的世界》，作者叔本华告诫读者：我们读书是别人替我们思考，我们不过是在重复作者的精神过程而已。但愿自己在失去思考能力之前能够追寻到思考的本质和真谛。对于自己的兴趣爱好，生活的小习惯小坚持，我有已成习惯的跑步，上下班的骑车，还有偶尔写点思考和心得，这方面我并不需要担心。所以还是要从根本解决问题，重拾对工作和对生活的兴趣，用读书或者其它爱好来丰富自己的精神世界，引导自己回到正常的状态。</p>]]></content>
      
      
      <categories>
          
          <category> Mind </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mind </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>当下的力量</title>
      <link href="/2020/05/11/Mind/%E5%BD%93%E4%B8%8B%E7%9A%84%E5%8A%9B%E9%87%8F/"/>
      <url>/2020/05/11/Mind/%E5%BD%93%E4%B8%8B%E7%9A%84%E5%8A%9B%E9%87%8F/</url>
      
        <content type="html"><![CDATA[<p>你生存在这个世界上就是要使宇宙的神圣目标得以实现，你看，你是多么重要！</p><p>——艾克哈利·托利</p><span id="more"></span><p>以上来自书籍《当下的力量》的前言，当然，文章名也取自书名。此书大概是讲述通过对自身大脑和思维的约束，做到不会陷入对以往美好的留恋，抛弃对艰险未来的恐惧，立足于当下实现自己的目标。为何会用大概，因为我前阵子刚读完前言，就让它躺在书架上吃灰去了，也由此产生了对自身的一些思考。</p><p>我是理性的，可以这么说，在做事方法和处事态度上也是以解决问题为首要目的，对自身有一定的认识程度，但还总是会被周围的事情左右情绪，或是焦虑，紧张，恐惧，或是任由思维情绪陷入恶性循环之中，自己在这个时候就像是个旁观者，不，或许连旁观者都算不上，任由情绪在自己体内宣泄，却始终找不到可以释放的缺口，只有当事情发生过后或是自己被动地发现并制止，这种情绪才会慢慢消散，只剩下自己后知后觉。因为大脑或者思维捣乱所产生的后果大概分以下几种：</p><p>其一是重要事情发生前的紧张、恐惧和焦虑不安。我会急于在特定时间内完成一件事情的时候产生焦虑的情绪，就拿早上起床做早饭来说，我会急于在六点四十分之前做好早饭，若是发现时间不足以在这个点完成，就会有些许焦躁，尽管我五十分做好早饭也来得及。再拿恐惧来说，我们领导脾气不是很好，每次汇报工作的时候都会担心自己哪里做的不好会让他借此发脾气，从产生这个想法到走向领导办公室的这段时间内焦虑也可能会跟着凑热闹，自己的大脑和思维也会因为这些情绪而变得滞后。</p><p>其二是隐忍所带来的后果。因为自己的理性，我如果事先知道做一件事情会引发的后果，就会尽可能的避免让它发生，你可能会说做事保持清醒这是好事啊，但我将它也带入了自己的生活当中。有时在与女友发生矛盾以后，我很难为逞一时之快说一些气愤违心的话，我知道那样并不能解决问题，偏偏自己表达能力又不好，往往是以沉默不语当做回应，但我的思维可没有闲着，它在思考着这场博弈中谁才是受害者，为什么会因为这么小个事情发这么大的火，分明自己做的没有错等等。它时刻想着把我撑爆，混杂着呼不出来的那口气，就这样陷入另一个战场。</p><p>其三是神游。在我的印象中，有很久没有持续认真思考过问题了，更多的是凭本能去做去想，在思考批判这方面，我的大脑和思维反而懈怠了，我会不时地陷入思想神游，去想一些不切实际的事情，清醒过来又是黄粱一梦，春梦了无痕。我并不知道产生神游的根本原因是什么，是工作或者生活中的压力还是对未来的恐惧和迷茫，我将它暂归为思维的麻木和懈怠。</p><p>在遇到上述这些情况的时候，我很少去主动思考或是去有效的控制思维，在当下的力量这本书中提到了ABC理论，A是已经发生的事情，C是你所理解的或者作出的反应，而B则是经过你的思维这货过滤的东西，它根据自己的想法强加各种情绪，而你要做的就是引导B去做正确的判断或者理性的思考。</p><p>罪魁祸首已经揪出来了，至于怎样去引导，去纠正，还需要深入思考，需要用实践去检验，就留着下次再写吧。</p>]]></content>
      
      
      <categories>
          
          <category> Mind </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mind </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>关于我跑步的前世今生</title>
      <link href="/2020/04/27/Mind/%E5%85%B3%E4%BA%8E%E6%88%91%E8%B7%91%E6%AD%A5%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/"/>
      <url>/2020/04/27/Mind/%E5%85%B3%E4%BA%8E%E6%88%91%E8%B7%91%E6%AD%A5%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/</url>
      
        <content type="html"><![CDATA[<h3 id="跑步的原因是什么来着"><a href="#跑步的原因是什么来着" class="headerlink" title="跑步的原因是什么来着"></a><strong>跑步的原因是什么来着</strong></h3><p>犹记我第一次跑步应该是在2019年三月初，当时在重庆市奉节县的一个小镇做项目（没错，当时出差就是常态），平日里工作并不是很多，闲暇时间脑海里经常会蹦出来一些有的没的的想法，下决心去跑步就是其中之一，我还记得决定好了之后就立马去搜了马拉松视频，想着学习专业人员的跑步姿势（直到现在也没学会）。</p><span id="more"></span><p>第二天起了个大早，沿着街道跑了两圈，一圈一公里多，一共跑了三公里，当时的配速是九分多，以至于我现在都很诧异为何跑的如此之慢，要问我第一次跑步感受如何，跑过的人都知道——我再也不想跑步了，当时跑下来就跟大学时候体能测试跑一千米感觉差不多。但不知为何我第二天，第三天就这么坚持下来了，我的跑步之路，就这样开始了。</p><p>我一直想努力回忆我是因为什么而开始跑步的，强身健体？还是培养习惯？还是想让身体持续在路上？总觉的感觉都不太对，但是我已经开始跑步了不是吗，那么究竟是什么原因促使我开始跑步还重要吗？</p><h3 id="开始坚持跑步"><a href="#开始坚持跑步" class="headerlink" title="开始坚持跑步"></a><strong>开始坚持跑步</strong></h3><p>我所理解的坚持跑步是每天总得匀出点时间去做这件事情，比如早上或者晚上，因为不可控因素打乱了自己的计划或者突然不想跑步就会有很深的罪恶感，这些我都是有所经历的。</p><p>这段时期还是我在四川省出差的时候经历的，大概是距开始跑步的四个月至九个月之间，这段时期我大概平均两天一次，每周至少跑三次，记得有一周每天都有去跑步，我所说的那种罪恶感会不时在这期间产生。虽说跑的距离越来越远，从十公里到十五公里，再到半程马拉松；配速也从六分钟到五分半再到五分十分左右，但总觉得跑的不自在，好像是在给自己完成任务一样。这种状态在今年年初，也就是新冠肺炎肆虐的时候被打破了。</p><h3 id="放下对跑步的执念"><a href="#放下对跑步的执念" class="headerlink" title="放下对跑步的执念"></a><strong>放下对跑步的执念</strong></h3><p>今年年初随着新冠肺炎的爆发，所有人都没能幸免，多少都有点影响，在此不谈疫情的危害和后果，对我跑步的影响就是我不能一大早随时穿好装备去外面挥洒汗水了，刚开始的时候又是那种熟悉的罪恶感，甚至会有些焦虑，疫情刚开始一个月只能在家拉拉伸，做做力量训练，生怕自己长时间不跑步四肢就会退化了。</p><p>这种状态持续到三月中旬的时候，那时候疫情已经有所控制，我也可以戴着口罩去人少的地方跑跑步。度过两个月的跑步空档期，我慢慢想明白了，跑步并不是我的任务，也不是我的工作，更不应该成为我的执念，它应该成为我的一种习惯，成为我的爱好，成为我一旦有空闲时间就会去做的让我充满活力的事情，仅此而已，它给你带来的焦虑、罪恶感和必须去完成的那种感受是你强加在跑步这件事情上的，应该把这些外部感受统统丢掉，就剩下单纯的跑步。</p><p>或许是因为我现在的工作除了周末便没时间跑步而给自己找的借口，但不管怎样，我依然喜爱跑步，享受跑步带来的感觉，有这些结果已经足够了，过程心态什么的，就让它随风去吧。</p>]]></content>
      
      
      <categories>
          
          <category> Mind </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mind </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>以苦为乐 苦中作乐</title>
      <link href="/2020/03/22/Mind/%E4%BB%A5%E8%8B%A6%E4%B8%BA%E4%B9%90%20%E8%8B%A6%E4%B8%AD%E4%BD%9C%E4%B9%90/"/>
      <url>/2020/03/22/Mind/%E4%BB%A5%E8%8B%A6%E4%B8%BA%E4%B9%90%20%E8%8B%A6%E4%B8%AD%E4%BD%9C%E4%B9%90/</url>
      
        <content type="html"><![CDATA[<p>人不管走到哪一步，总得找点乐子，想一点办法，老是愁眉苦脸的，干吗呢！</p><p>——汪曾祺</p><span id="more"></span><p>在戴了“右派分子”的帽子后，汪曾祺被发配到西山种树，在石多土少的山上刨坑，这是一个非常重的活。一大早就上山，带两个干馒头，一块腌萝卜。已经是秋天了，山上的酸枣熟了，他们就摘酸枣吃，草里有蝈蝈，也抓起来烤着吃，烤蝈蝈就馒头，配着饭后甜点酸枣，就是汪曾祺的乐子。</p><p>上面的文字选自《生活还是很美好的》，我当时读到这里的时候全书已过三分之一，这才明白过来汪老想表达的并不是悠然自在，花前月下，而是苦中作乐，是一种于苦难中的豁达，这也是我所追求的境界。</p><p>那么何以为苦，何以为乐。我所面对的苦是想要买房但是首付还没凑够，是我的工作目前还没有定位到一个准确的位置，是困惑将来的自己会以怎样一副面孔去融入周围的大环境等等，这些事情在你没有将它们解决掉或者完全想明白之前就会源源不断的给你制造麻烦情绪，这些情绪可能会经过时间的发酵爆发出来，也可能随着时间的推移又被新的痛苦挤下去。何以为乐呢，在和女朋友周末躺在床上看电视的时候，是我坐在阳台的椅子上吹着风看书的时候，是我思路无比清晰认真做着手头事的时候，在这些事情发生的时候都会反馈给我一种无比舒服的感觉，那么这就是我的乐，并不局限于发生了使我快乐的事情。</p><p>不从宏观的角度去讲，从我自身来说大多数时候都是苦的，这是相对于乐来说，从总的时间来考虑占最多的时间应该是平淡和无聊。苦是因为欲望，是因为期盼，我最近想买房，我想要在短时间内找到自己的工作定位，我想找到我在这个城市的容身之处，你产生了这些欲望自然就会在特定的时间段内因为没有能力实现的原因而痛苦，当然没有实现才叫痛苦，实现了那就是乐了。因为我做不到无欲无求，这也不是我追求的境界，自然就少不了欲望，更少不了苦，但是我还是想在这苦中找到那么一点点的乐，这就是我的动力所在。</p><p>苦与乐存在转换的过程，乐极生悲的事时有发生，想想你带着老婆，吃着火锅唱着歌，突然就让麻匪劫了，还怎么乐的起来。当然除了乐极生悲，还有这样一种可能，你想做一件事，然后做成了，这样你就很开心，但过了一段时间你发现你做的这个事它相比另外一件事根本不算个啥，你有了新的期盼，于是新的欲望产生了，新的轮回开始了。</p><p>对于我来说苦的来源就这么几种，我的目的是将苦化为乐，或者彻底消除苦，那么我需要调整的就是发现或者制造乐的心态，发现乐则需要通过苦之外的喜好或者兴趣，能给你带来正反馈的东西，培养一些兴趣，多出去走走，比待在房子里玩手机收获的会更多。制造乐则是你正处于痛苦之中，采取一些办法让你暂时脱离苦的范畴，比如听听窗外的鸟叫和风声，看看天上的白云，那么在你做完这些事情之后你的心态肯定会有些许变化，虽然事情还是这个事情，但总感觉拥有的苦少了一些。</p><p>其实写完发现整篇文章有点乱，对一些观点只是浅尝辄止，举了几个自己的例子，也并没有说实质性的东西，但也不打算改了，这也只是我在读到汪老这句话的时候所引发的感想而已。</p>]]></content>
      
      
      <categories>
          
          <category> Mind </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Mind </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Oracle常用操作命令</title>
      <link href="/2019/12/11/Language/Oracle%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/"/>
      <url>/2019/12/11/Language/Oracle%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<p>Oracle数据库常用操作命令。</p><span id="more"></span><h3 id="创建测试表"><a href="#创建测试表" class="headerlink" title="创建测试表"></a>创建测试表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> employees (</span><br><span class="line">    id NUMBER ( <span class="number">4</span> ) <span class="keyword">primary</span> key,</span><br><span class="line">    name VARCHAR2 ( <span class="number">100</span> ),</span><br><span class="line">    email VARCHAR2 ( <span class="number">100</span> ),</span><br><span class="line">    mobile VARCHAR2 ( <span class="number">100</span> ),</span><br><span class="line">    sal NUMBER ( <span class="number">10</span>, <span class="number">2</span> ),</span><br><span class="line">drly_sal NUMBER ( <span class="number">10</span>, <span class="number">2</span> ) generated always <span class="keyword">AS</span> ( sal <span class="operator">/</span> <span class="number">22</span> ),</span><br><span class="line">year_sal NUMBER ( <span class="number">20</span>, <span class="number">2</span> ) generated always <span class="keyword">AS</span> ( sal <span class="operator">*</span> <span class="number">12</span> )) tablespace users;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees(id, name, email, mobile, sal) <span class="keyword">values</span>(<span class="number">1</span>, <span class="string">&#x27;wxk&#x27;</span>, <span class="string">&#x27;w749@qq.com&#x27;</span>, <span class="string">&#x27;19991259321&#x27;</span>, <span class="number">12000</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees(id, name, email, mobile, sal) <span class="keyword">values</span>(<span class="number">2</span>, <span class="string">&#x27;wx&#x27;</span>, <span class="string">&#x27;wangxu@qq.com&#x27;</span>, <span class="string">&#x27;19991259321&#x27;</span>, <span class="number">12000</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees(id, name, email, mobile, sal) <span class="keyword">values</span>(<span class="number">3</span>, <span class="string">&#x27;zs&#x27;</span>, <span class="string">&#x27;wangxu@qq.com&#x27;</span>, <span class="string">&#x27;110&#x27;</span>, <span class="number">1000</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees(id, name, email, mobile, sal) <span class="keyword">values</span>(<span class="number">5</span>, <span class="string">&#x27;rfv&#x27;</span>, <span class="string">&#x27;qwe&#x27;</span>, <span class="string">&#x27;19991259321&#x27;</span>, <span class="number">10000</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees(id, name, email, mobile, sal) <span class="keyword">values</span>(<span class="number">6</span>, <span class="string">&#x27;asd&#x27;</span>, <span class="string">&#x27;wangxu@qq.com&#x27;</span>, <span class="string">&#x27;19991259321&#x27;</span>, <span class="number">12000</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees(id, name, email, mobile, sal) <span class="keyword">values</span>(<span class="number">7</span>, <span class="string">&#x27;axc&#x27;</span>, <span class="string">&#x27;wangxu@qq.com&#x27;</span>, <span class="string">&#x27;110&#x27;</span>, <span class="number">1000</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees(id, name, email, mobile, sal) <span class="keyword">values</span>(<span class="number">8</span>, <span class="string">&#x27;eee&#x27;</span>, <span class="string">&#x27;eee&#x27;</span>, <span class="string">&#x27;110&#x27;</span>, <span class="number">1000</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> employees(id, name, email, mobile, sal) <span class="keyword">values</span>(<span class="number">9</span>, <span class="string">&#x27;yyy&#x27;</span>, <span class="string">&#x27;yyy&#x27;</span>, <span class="string">&#x27;110&#x27;</span>, <span class="number">1000</span>);</span><br></pre></td></tr></table></figure><h3 id="常用操作"><a href="#常用操作" class="headerlink" title="常用操作"></a>常用操作</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> test1 tablespace users nologging <span class="keyword">as</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> employees <span class="keyword">where</span> name <span class="operator">=</span> <span class="string">&#x27;wxk&#x27;</span>;  <span class="comment">-- 子查询</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> test1;  <span class="comment">-- 删除表</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> employees <span class="keyword">add</span>(dt <span class="type">date</span> <span class="keyword">default</span> sysdate <span class="keyword">not</span> <span class="keyword">null</span>);  <span class="comment">-- 添加字段</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> employees <span class="keyword">drop</span> unused columns;  <span class="comment">-- 删除字段</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> employees rename <span class="keyword">column</span> dt <span class="keyword">to</span> ddt;  <span class="comment">-- 修改字段名</span></span><br><span class="line">comment <span class="keyword">on</span> <span class="keyword">table</span> employees <span class="keyword">is</span> <span class="string">&#x27;工资薪酬表&#x27;</span>;  <span class="comment">-- 给表加注释</span></span><br><span class="line">comment <span class="keyword">on</span> <span class="keyword">column</span> employees.name <span class="keyword">is</span> <span class="string">&#x27;员工姓名&#x27;</span>;  <span class="comment">-- 给字段加注释</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> table_name, tablespace_name, logging, cache, status, blocks <span class="keyword">from</span> user_tables <span class="keyword">where</span> tablespace_name <span class="operator">=</span> <span class="string">&#x27;USERS&#x27;</span>;  <span class="comment">-- 查看users表空间下的表信息</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> employees <span class="keyword">add</span> <span class="keyword">constraint</span> em_name <span class="keyword">unique</span>(name);  <span class="comment">-- 给表增加唯一约束</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> employees enable <span class="keyword">constraint</span> em_name;  <span class="comment">-- 打开约束</span></span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> employees <span class="keyword">where</span> id <span class="operator">=</span> <span class="number">4</span>; <span class="comment">-- 删除表内容</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> constraint_name, column_name <span class="keyword">from</span> user_cons_columns <span class="keyword">where</span> table_name<span class="operator">=</span><span class="string">&#x27;EMPLOYEES&#x27;</span>;  <span class="comment">-- 查看employees表下的约束信息</span></span><br><span class="line"><span class="keyword">create</span> index name_index <span class="keyword">on</span> employees(ddt);  <span class="comment">-- 新建约束</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> employeess tablespace USERS <span class="keyword">as</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> employees;  <span class="comment">-- 使用as select新建表</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> employeess;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> MATERIALIZED <span class="keyword">view</span> demand_view <span class="keyword">as</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> employees;  <span class="comment">-- 新建实体化视图</span></span><br><span class="line"><span class="keyword">create</span> MATERIALIZED <span class="keyword">view</span> commit_view  refresh force <span class="keyword">on</span> <span class="keyword">commit</span> <span class="keyword">as</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> employees;  <span class="comment">-- 创建实体化视图并在提交时刷新</span></span><br><span class="line"><span class="keyword">exec</span> DBMS_MVIEW.REFRESH(<span class="string">&#x27;DEMAND_VIEW&#x27;</span>);  <span class="comment">-- 刷新实体化视图</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> commit_view;</span><br><span class="line"><span class="keyword">drop</span> MATERIALIZED <span class="keyword">view</span> demand_view;  <span class="comment">-- 删除实体化视图</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> MATERIALIZED <span class="keyword">view</span> log <span class="keyword">on</span> employees <span class="keyword">with</span> <span class="keyword">primary</span> key,rowid,sequence;</span><br><span class="line"><span class="keyword">create</span> MATERIALIZED <span class="keyword">view</span> fast_commit_view  refresh fast <span class="keyword">on</span> <span class="keyword">commit</span> <span class="keyword">as</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> employees;</span><br><span class="line"><span class="keyword">desc</span> MLOG$_EMPLOYEES;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> all_clusters;</span><br><span class="line"><span class="keyword">select</span> DIRECTORY;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> sequence test_seq <span class="keyword">start</span> <span class="keyword">with</span> <span class="number">10</span> increment <span class="keyword">by</span> <span class="number">5</span> maxvalue <span class="number">30</span> nocache;  <span class="comment">-- 创建序列</span></span><br><span class="line"><span class="keyword">select</span> test_seq.currval <span class="keyword">from</span> dual;</span><br><span class="line"><span class="keyword">drop</span> sequence test_seq;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> public synonym tmp_emp <span class="keyword">for</span> system.employees;  <span class="comment">-- 创建公共同义词，给表名绑定个名字</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> all_synonyms <span class="keyword">where</span> synonym_name<span class="operator">=</span><span class="string">&#x27;TMP_EMP&#x27;</span>;  <span class="comment">-- 查看同义词信息</span></span><br><span class="line"><span class="keyword">update</span> tmp_emp <span class="keyword">set</span> email<span class="operator">=</span><span class="string">&#x27;qwe@gmail.com&#x27;</span> <span class="keyword">where</span> id<span class="operator">=</span><span class="number">9</span>;  <span class="comment">-- 利用同义词对表进行操作</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tmp_emp;</span><br><span class="line"><span class="keyword">drop</span> public synonym tmp_emp;  <span class="comment">-- 删除同义词</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> concat(name, concat(<span class="string">&#x27;-&#x27;</span>, email)) www <span class="keyword">from</span> employees;  <span class="comment">-- concat字符串拼接</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- groupby</span></span><br><span class="line"><span class="keyword">select</span> email,mobile,<span class="built_in">sum</span>(sal) sum_sal <span class="keyword">from</span> employees <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">rollup</span>(email,mobile) <span class="keyword">order</span> <span class="keyword">by</span> email,mobile;  <span class="comment">-- rollup</span></span><br><span class="line"><span class="keyword">select</span> email,mobile,<span class="built_in">sum</span>(sal) sum_sal <span class="keyword">from</span> employees <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">cube</span>(email,mobile) <span class="keyword">order</span> <span class="keyword">by</span> email,mobile;  <span class="comment">-- cube</span></span><br><span class="line"><span class="keyword">select</span> email,mobile,<span class="built_in">sum</span>(sal) sum_sal <span class="keyword">from</span> employees <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">grouping</span> sets(email,mobile);   <span class="comment">-- grouping sets</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 累加统计over</span></span><br><span class="line"><span class="keyword">select</span> id,name,sal,<span class="built_in">sum</span>(sal) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> id) sum_sal,<span class="built_in">count</span>(<span class="operator">*</span>) <span class="keyword">over</span>(<span class="keyword">order</span> <span class="keyword">by</span> id) cot <span class="keyword">from</span> employees;   <span class="comment">-- 利用按列排序实现指定字段累加</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 子查询</span></span><br><span class="line"><span class="keyword">select</span> id,name,sal <span class="keyword">from</span> employees <span class="keyword">where</span> (name,sal) <span class="keyword">in</span> (<span class="keyword">select</span> name,sal <span class="keyword">from</span> employees <span class="keyword">where</span> sal <span class="operator">&gt;</span> <span class="number">10000</span>);  <span class="comment">-- 多行多列无关子查询</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> employees e <span class="keyword">where</span> <span class="keyword">exists</span>(<span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> employees m <span class="keyword">where</span> e.id<span class="operator">=</span>m.id <span class="keyword">and</span> length(m.email) <span class="operator">=</span> length(e.mobile));   <span class="comment">-- 和外部查询相关的子查询</span></span><br><span class="line"><span class="keyword">select</span> e.id,e.name,m.email,m.sal <span class="keyword">from</span> employees e,(<span class="keyword">select</span> id,email,sal <span class="keyword">from</span> employees <span class="keyword">where</span> sal <span class="operator">&gt;=</span> <span class="number">10000</span>) m <span class="keyword">where</span> e.id <span class="operator">=</span> m.id;   <span class="comment">-- 在from中使用子查询</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 集合操作</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> employees <span class="keyword">where</span> id <span class="operator">&lt;</span> <span class="number">7</span> <span class="keyword">union</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> employees <span class="keyword">where</span> id <span class="operator">&gt;</span><span class="number">5</span>;   <span class="comment">-- union返回两个集合的并集，重复的记录只保留一条，默认按第一列进行排序</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> employees <span class="keyword">where</span> id <span class="operator">&lt;</span> <span class="number">7</span> <span class="keyword">union</span> <span class="keyword">all</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> employees <span class="keyword">where</span> id <span class="operator">&gt;</span><span class="number">5</span>;   <span class="comment">-- union all 返回两个集合的并集，保留重复的记录，不排序</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> employees <span class="keyword">where</span> id <span class="operator">&lt;</span> <span class="number">7</span> <span class="keyword">intersect</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> employees <span class="keyword">where</span> id <span class="operator">&gt;</span><span class="number">5</span>;   <span class="comment">-- intersect 返回两个集合的交集，默认按第一列排序</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> employees <span class="keyword">where</span> id <span class="operator">&lt;</span> <span class="number">7</span> minus <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> employees <span class="keyword">where</span> id <span class="operator">&gt;</span><span class="number">5</span>;  <span class="comment">-- minus 返回第一个集合存在第二个集合不存在的集合，默认按第一列排序</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- insert</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> em <span class="keyword">as</span> <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> employees;</span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> em;</span><br><span class="line"><span class="keyword">insert</span> <span class="comment">/*+APPEND*/</span> <span class="keyword">into</span> em <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> employees <span class="keyword">where</span> id <span class="operator">&lt;</span> <span class="number">5</span>;  <span class="comment">-- 使用子查询可插入多行记录，/*+APPEND*/利用子查询装载的方式插入，操作日志不写入日志文件，效率提升</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> em;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> em;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- merge</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> em <span class="keyword">as</span> <span class="keyword">select</span> id,name <span class="keyword">from</span> employees;</span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> em;</span><br><span class="line"><span class="keyword">insert</span> <span class="comment">/*+APPEND*/</span> <span class="keyword">into</span> em <span class="keyword">select</span> id,name <span class="keyword">from</span> employees <span class="keyword">where</span> id <span class="operator">&lt;</span> <span class="number">5</span>;</span><br><span class="line"><span class="keyword">update</span> em <span class="keyword">set</span> name<span class="operator">=</span><span class="string">&#x27;wwxxkk&#x27;</span> <span class="keyword">where</span> name<span class="operator">=</span><span class="string">&#x27;wxk&#x27;</span>;</span><br><span class="line"><span class="keyword">update</span> em <span class="keyword">set</span> name<span class="operator">=</span><span class="string">&#x27;wwxx&#x27;</span> <span class="keyword">where</span> name<span class="operator">=</span><span class="string">&#x27;wx&#x27;</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> em;</span><br><span class="line"><span class="comment">-- 按id匹配的更新(id=1)不匹配的插入(id&gt;6)</span></span><br><span class="line"><span class="keyword">merge</span> <span class="keyword">into</span> em e <span class="keyword">using</span> employees m <span class="keyword">on</span> (e.id<span class="operator">=</span>m.id) <span class="keyword">when</span> matched <span class="keyword">then</span> <span class="keyword">update</span> <span class="keyword">set</span> e.name<span class="operator">=</span>m.name <span class="keyword">where</span> e.id<span class="operator">=</span><span class="number">1</span> <span class="keyword">when</span> <span class="keyword">not</span> matched <span class="keyword">then</span> <span class="keyword">insert</span> <span class="keyword">values</span>(m.id,m.name) <span class="keyword">where</span> m.id<span class="operator">&gt;</span><span class="number">6</span>;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> em;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> em;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- width_bucket</span></span><br><span class="line"><span class="keyword">select</span> sal,<span class="built_in">width_bucket</span>(sal,<span class="number">1000</span>,<span class="number">12000</span>,<span class="number">3</span>) wb <span class="keyword">from</span> employees;</span><br><span class="line"><span class="comment">-- rpad</span></span><br><span class="line"><span class="keyword">select</span> name,mobile,rpad(name,<span class="number">10</span>,mobile) lp <span class="keyword">from</span> employees;</span><br><span class="line"><span class="comment">-- 日期函数</span></span><br><span class="line"><span class="keyword">select</span> sysdate，<span class="built_in">current_date</span>,systimestamp,<span class="built_in">localtimestamp</span>,add_months(sysdate,<span class="number">2</span>),next_day(sysdate,<span class="number">2</span>),last_day(sysdate),round(sysdate),<span class="built_in">extract</span>(<span class="keyword">year</span> <span class="keyword">from</span> sysdate) <span class="keyword">year</span>,<span class="built_in">extract</span>(<span class="keyword">day</span> <span class="keyword">from</span> sysdate) <span class="keyword">day</span> <span class="keyword">from</span> dual;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- PL/SQL语句</span></span><br><span class="line"><span class="comment">-- 匿名块</span></span><br><span class="line"><span class="keyword">declare</span></span><br><span class="line">    out_name varchar2(<span class="number">20</span>);</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">    <span class="keyword">select</span> name <span class="keyword">into</span> out_name <span class="keyword">from</span> employees <span class="keyword">where</span> id<span class="operator">=</span><span class="number">123</span>;</span><br><span class="line">    dbms_output.put_line(out_name);</span><br><span class="line">    exception</span><br><span class="line">        <span class="keyword">when</span> no_data_found <span class="keyword">then</span> </span><br><span class="line">            dbms_output.put_line(<span class="string">&#x27;no found&#x27;</span>);</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure><h3 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- transaction</span></span><br><span class="line"><span class="keyword">DECLARE</span></span><br><span class="line">  l_id NUMBER;</span><br><span class="line">  l_name  VARCHAR2(<span class="number">100</span>);</span><br><span class="line">  l_mobile  VARCHAR2(<span class="number">50</span>);</span><br><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">  <span class="comment">-- 为保证set transaction是事务的第一条语句，先使用commit或rollback来结束掉前面可能存在的事务</span></span><br><span class="line">  <span class="keyword">COMMIT</span>;</span><br><span class="line">  <span class="comment">-- 使用name给事务命名，定义为只读事务防止DML</span></span><br><span class="line">  <span class="keyword">SET</span> TRANSACTION READ <span class="keyword">ONLY</span> NAME <span class="string">&#x27;查询报表&#x27;</span>;</span><br><span class="line">  <span class="keyword">SELECT</span> id</span><br><span class="line">    <span class="keyword">INTO</span> l_id</span><br><span class="line">    <span class="keyword">FROM</span> employees</span><br><span class="line">   <span class="keyword">WHERE</span> id <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">SELECT</span> name</span><br><span class="line">    <span class="keyword">INTO</span> l_name</span><br><span class="line">    <span class="keyword">FROM</span> employees</span><br><span class="line">   <span class="keyword">WHERE</span> id <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">SELECT</span> mobile</span><br><span class="line">    <span class="keyword">INTO</span> l_mobile</span><br><span class="line">    <span class="keyword">FROM</span> employees</span><br><span class="line">   <span class="keyword">WHERE</span> id <span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line">  <span class="comment">-- 终止只读事务</span></span><br><span class="line">  <span class="keyword">COMMIT</span>;</span><br><span class="line">  dbms_output.put_line(<span class="string">&#x27;输出：&#x27;</span> <span class="operator">||</span> l_id <span class="operator">||</span> <span class="string">&#x27;,&#x27;</span> <span class="operator">||</span> l_name <span class="operator">||</span> <span class="string">&#x27;,&#x27;</span> <span class="operator">||</span> l_mobile);</span><br><span class="line"><span class="keyword">END</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">    <span class="keyword">update</span> employees <span class="keyword">set</span> name<span class="operator">=</span><span class="string">&#x27;rfv&#x27;</span> <span class="keyword">where</span> id<span class="operator">=</span><span class="number">5</span>;</span><br><span class="line">    <span class="keyword">savepoint</span> A;  <span class="comment">-- 设置保存点</span></span><br><span class="line">    <span class="keyword">update</span> employees <span class="keyword">set</span> name<span class="operator">=</span><span class="string">&#x27;plm&#x27;</span> <span class="keyword">where</span> id<span class="operator">=</span><span class="number">5</span>;</span><br><span class="line">    <span class="keyword">rollback</span> <span class="keyword">to</span> A;  <span class="comment">-- 回滚到指定保存点</span></span><br><span class="line">    <span class="keyword">commit</span>;</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"><span class="keyword">select</span> name <span class="keyword">from</span> employees <span class="keyword">where</span> id<span class="operator">=</span><span class="number">5</span>;</span><br></pre></td></tr></table></figure><h3 id="变量与常量"><a href="#变量与常量" class="headerlink" title="变量与常量"></a>变量与常量</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">number</span></span><br><span class="line"><span class="comment">binary_integer</span></span><br><span class="line"><span class="comment">变量与常量</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="keyword">declare</span></span><br><span class="line">    num number;</span><br><span class="line">    bin_int BINARY_INTEGER;</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">    num :<span class="operator">=</span> <span class="number">100</span>;</span><br><span class="line">    bin_int :<span class="operator">=</span> <span class="number">200</span>;</span><br><span class="line">    dbms_output.put_line(<span class="string">&#x27;NUMBER &#x27;</span> <span class="operator">||</span> num <span class="operator">||</span> <span class="string">&#x27;, BINART_INTEGER &#x27;</span> <span class="operator">||</span> bin_int);</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">declare</span></span><br><span class="line">    v1 number(<span class="number">2</span>);</span><br><span class="line">    v2 number(<span class="number">4</span>) :<span class="operator">=</span> <span class="number">100</span>;</span><br><span class="line">    v3 constant number(<span class="number">4</span>) <span class="keyword">default</span> <span class="number">111</span>;</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">    if v1 <span class="keyword">is</span> <span class="keyword">null</span> <span class="keyword">then</span></span><br><span class="line">        dbms_output.put_line(<span class="string">&#x27;v1 is null&#x27;</span>);</span><br><span class="line">    <span class="keyword">end</span> if;</span><br><span class="line">    dbms_output.put_line(<span class="string">&#x27;v2: &#x27;</span> <span class="operator">||</span> v2 <span class="operator">||</span> <span class="string">&#x27;  v3: &#x27;</span> <span class="operator">||</span>v3);</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 自定义类型/使用原有数据表类型接收查询结果并输出</span></span><br><span class="line"><span class="keyword">declare</span></span><br><span class="line">    iid system.employees.id<span class="operator">%</span>type;</span><br><span class="line">    nname system.employees.name<span class="operator">%</span>type;</span><br><span class="line">    mmobile system.employees.mobile<span class="operator">%</span>type;</span><br><span class="line"><span class="keyword">begin</span> </span><br><span class="line">    <span class="keyword">select</span> id,name,mobile <span class="keyword">into</span> iid,nname,mmobile <span class="keyword">from</span> employees <span class="keyword">where</span> id<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line">    dbms_output.put_line(iid <span class="operator">||</span> <span class="string">&#x27;,&#x27;</span> <span class="operator">||</span> nname <span class="operator">||</span> <span class="string">&#x27;,&#x27;</span> <span class="operator">||</span> mmobile);</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">declare</span></span><br><span class="line">    type t_tmp <span class="keyword">is</span> record(id number(<span class="number">2</span>),name varchar2(<span class="number">20</span>),mobile varchar2(<span class="number">20</span>));</span><br><span class="line">    v_tmp t_tmp;</span><br><span class="line"><span class="keyword">begin</span> </span><br><span class="line">    <span class="keyword">select</span> id,name,mobile <span class="keyword">into</span> v_tmp <span class="keyword">from</span> employees <span class="keyword">where</span> id<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line">    dbms_output.put_line(v_tmp.id <span class="operator">||</span> <span class="string">&#x27;,&#x27;</span> <span class="operator">||</span> v_tmp.name <span class="operator">||</span> <span class="string">&#x27;,&#x27;</span> <span class="operator">||</span> v_tmp.mobile);</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">declare</span> </span><br><span class="line"> v_tmp system.employees<span class="operator">%</span>rowtype;</span><br><span class="line"><span class="keyword">begin</span> </span><br><span class="line">    <span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">into</span> v_tmp <span class="keyword">from</span> employees <span class="keyword">where</span> id<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line">    dbms_output.put_line(v_tmp.id <span class="operator">||</span> <span class="string">&#x27;,&#x27;</span> <span class="operator">||</span> v_tmp.name <span class="operator">||</span> <span class="string">&#x27;,&#x27;</span> <span class="operator">||</span> v_tmp.mobile <span class="operator">||</span> <span class="string">&#x27;,&#x27;</span> <span class="operator">||</span> v_tmp.sal <span class="operator">||</span> <span class="string">&#x27;,&#x27;</span> <span class="operator">||</span>v_tmp.drly_sal <span class="operator">||</span> <span class="string">&#x27;,&#x27;</span> <span class="operator">||</span>v_tmp.year_sal <span class="operator">||</span> <span class="string">&#x27;,&#x27;</span> <span class="operator">||</span>v_tmp.ddt);</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure><h3 id="条件控制语句及循环"><a href="#条件控制语句及循环" class="headerlink" title="条件控制语句及循环"></a>条件控制语句及循环</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 条件控制语句</span></span><br><span class="line"><span class="keyword">declare</span></span><br><span class="line">    digits number(<span class="number">5</span>);</span><br><span class="line">    arg varchar2(<span class="number">20</span>);</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">    digits :<span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line">    if digits <span class="operator">=</span> <span class="number">1</span> <span class="keyword">then</span> arg :<span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>;</span><br><span class="line">    elsif digits <span class="operator">=</span> <span class="number">2</span> <span class="keyword">then</span> arg :<span class="operator">=</span> <span class="string">&#x27;2&#x27;</span>;</span><br><span class="line">    <span class="keyword">else</span> arg :<span class="operator">=</span> <span class="string">&#x27;3&#x27;</span>;</span><br><span class="line">    <span class="keyword">end</span> if;</span><br><span class="line">    dbms_output.put_line(arg);</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">declare</span></span><br><span class="line">    digits number(<span class="number">5</span>);</span><br><span class="line">    arg varchar2(<span class="number">20</span>);</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">    digits :<span class="operator">=</span> <span class="number">10</span>;</span><br><span class="line">    <span class="keyword">case</span> digits</span><br><span class="line">        <span class="keyword">when</span> <span class="number">10</span> <span class="keyword">then</span> arg :<span class="operator">=</span> <span class="string">&#x27;1&#x27;</span>;</span><br><span class="line">        <span class="keyword">when</span> <span class="number">20</span> <span class="keyword">then</span> arg :<span class="operator">=</span> <span class="string">&#x27;2&#x27;</span>;</span><br><span class="line">        <span class="keyword">else</span> arg :<span class="operator">=</span> <span class="string">&#x27;3&#x27;</span>;</span><br><span class="line">    <span class="keyword">end</span> <span class="keyword">case</span>;</span><br><span class="line">    dbms_output.put_line(arg);</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">declare</span></span><br><span class="line">    digits number(<span class="number">5</span>);</span><br><span class="line">    arg varchar2(<span class="number">20</span>);</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">    digits :<span class="operator">=</span> <span class="number">20</span>;</span><br><span class="line">    <span class="keyword">case</span> </span><br><span class="line">        <span class="keyword">when</span> digits<span class="operator">&lt;</span><span class="number">20</span> <span class="keyword">then</span> arg :<span class="operator">=</span> <span class="string">&#x27;2&#x27;</span>;</span><br><span class="line">        <span class="keyword">when</span> digits<span class="operator">&lt;</span><span class="number">30</span> <span class="keyword">then</span> arg :<span class="operator">=</span> <span class="string">&#x27;3&#x27;</span>;</span><br><span class="line">        <span class="keyword">else</span> arg :<span class="operator">=</span> <span class="string">&#x27;4&#x27;</span>;</span><br><span class="line">    <span class="keyword">end</span> <span class="keyword">case</span>;</span><br><span class="line">    dbms_output.put_line(arg);</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 循环loop、while、for</span></span><br><span class="line"><span class="keyword">declare</span></span><br><span class="line">    arg number(<span class="number">2</span>) :<span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">    loop </span><br><span class="line">        dbms_output.put_line(arg);</span><br><span class="line">        arg :<span class="operator">=</span> arg<span class="operator">+</span><span class="number">1</span>;</span><br><span class="line">        exit <span class="keyword">when</span> arg <span class="operator">&gt;</span> <span class="number">5</span>;</span><br><span class="line">    <span class="keyword">end</span> loop;</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">declare</span></span><br><span class="line">    arg number :<span class="operator">=</span> <span class="number">1</span>;</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">    while arg <span class="operator">&lt;</span> <span class="number">5</span> loop </span><br><span class="line">        dbms_output.put_line(arg);</span><br><span class="line">        arg :<span class="operator">=</span> arg<span class="operator">+</span><span class="number">1</span>;</span><br><span class="line">    <span class="keyword">end</span> loop;</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">    <span class="keyword">for</span> arg <span class="keyword">in</span> <span class="number">1.</span><span class="number">.5</span> loop</span><br><span class="line">        dbms_output.put_line(arg);</span><br><span class="line">    <span class="keyword">end</span> loop;</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure><h3 id="游标"><a href="#游标" class="headerlink" title="游标"></a>游标</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">declare</span></span><br><span class="line">    <span class="keyword">cursor</span> v_cur <span class="keyword">is</span> <span class="keyword">select</span> id,name,mobile <span class="keyword">from</span> employees;</span><br><span class="line">    t_cur v_cur<span class="operator">%</span>ROWTYPE;</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">    <span class="keyword">open</span> v_cur; </span><br><span class="line">    loop</span><br><span class="line">        <span class="keyword">fetch</span> v_cur <span class="keyword">into</span> t_cur;</span><br><span class="line">        exit <span class="keyword">when</span> v_cur<span class="operator">%</span>NOTFOUND;</span><br><span class="line">        dbms_output.put_line(<span class="string">&#x27;id:&#x27;</span> <span class="operator">||</span> t_cur.id <span class="operator">||</span> <span class="string">&#x27;,name:&#x27;</span> <span class="operator">||</span> t_cur.name <span class="operator">||</span> <span class="string">&#x27;,mobile:&#x27;</span> <span class="operator">||</span> t_cur.mobile);</span><br><span class="line">    <span class="keyword">end</span> loop;</span><br><span class="line">    <span class="keyword">close</span> v_cur;</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"><span class="comment">-- 使用for循环则不用考虑打开和关闭游标，以及判断数据是否取完</span></span><br><span class="line"><span class="keyword">declare</span></span><br><span class="line">    <span class="keyword">cursor</span> v_cur <span class="keyword">is</span> <span class="keyword">select</span> id,name,mobile <span class="keyword">from</span> employees;</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">    <span class="keyword">for</span> t_cur <span class="keyword">in</span> v_cur loop</span><br><span class="line">        dbms_output.put_line(<span class="string">&#x27;id:&#x27;</span> <span class="operator">||</span> t_cur.id <span class="operator">||</span> <span class="string">&#x27;,name:&#x27;</span> <span class="operator">||</span> t_cur.name <span class="operator">||</span> <span class="string">&#x27;,mobile:&#x27;</span> <span class="operator">||</span> t_cur.mobile);</span><br><span class="line">    <span class="keyword">end</span> loop;</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure><h3 id="存储过程"><a href="#存储过程" class="headerlink" title="存储过程"></a>存储过程</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">or</span> replace <span class="keyword">PROCEDURE</span> pro ( tmp system.employees.id <span class="operator">%</span> TYPE ) <span class="keyword">as</span></span><br><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line"><span class="keyword">update</span> employees <span class="keyword">set</span> name<span class="operator">=</span><span class="string">&#x27;wangxukun&#x27;</span> <span class="keyword">where</span> id<span class="operator">=</span>tmp;</span><br><span class="line">    EXCEPTION </span><br><span class="line">        <span class="keyword">WHEN</span> NO_DATA_FOUND <span class="keyword">THEN</span></span><br><span class="line">        dbms_output.put_line ( <span class="number">123</span> );<span class="keyword">END</span> pro;</span><br><span class="line"><span class="keyword">call</span> pro (<span class="number">1</span>);</span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">PROCEDURE</span> pro;</span><br></pre></td></tr></table></figure><h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建函数</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">or</span> replace <span class="keyword">FUNCTION</span> func ( num system.employees.id <span class="operator">%</span> TYPE ) </span><br><span class="line"><span class="keyword">return</span> system.employees.name <span class="operator">%</span> TYPE </span><br><span class="line"><span class="keyword">AS</span> </span><br><span class="line">strr system.employees.name <span class="operator">%</span> TYPE;</span><br><span class="line"><span class="keyword">BEGIN</span></span><br><span class="line">    <span class="keyword">SELECT</span></span><br><span class="line">        name <span class="keyword">INTO</span> strr </span><br><span class="line">    <span class="keyword">FROM</span></span><br><span class="line">        employees </span><br><span class="line">    <span class="keyword">WHERE</span></span><br><span class="line">        id <span class="operator">=</span> num;</span><br><span class="line">    <span class="keyword">return</span> strr;<span class="keyword">END</span> func;</span><br><span class="line"><span class="keyword">SELECT</span></span><br><span class="line">    func ( id ) nn</span><br><span class="line"><span class="keyword">FROM</span></span><br><span class="line">    employees;</span><br><span class="line"><span class="keyword">DROP</span> <span class="keyword">FUNCTION</span> func;</span><br></pre></td></tr></table></figure><h3 id="包操作"><a href="#包操作" class="headerlink" title="包操作"></a>包操作</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建包规范（必须先定义后使用）</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">or</span> replace package pck</span><br><span class="line"><span class="keyword">as</span></span><br><span class="line">    str system.employees.name <span class="operator">%</span> TYPE;</span><br><span class="line">    num number;</span><br><span class="line">  <span class="keyword">procedure</span> proce ( tmp1 system.employees.id <span class="operator">%</span> TYPE,tmp2 system.employees.name <span class="operator">%</span> TYPE );</span><br><span class="line">    <span class="keyword">procedure</span> proce ( tmp2 system.employees.name <span class="operator">%</span> TYPE,tmp3 system.employees.email <span class="operator">%</span> TYPE );  <span class="comment">-- 包的重载</span></span><br><span class="line"><span class="keyword">end</span> pck;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建包体（包体中的必须是规范中定义的，否则就是局部的，外部不可调用）</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">or</span> replace package body pck</span><br><span class="line"><span class="keyword">as</span> </span><br><span class="line">    <span class="keyword">PROCEDURE</span> proce ( tmp1 system.employees.id <span class="operator">%</span> TYPE,tmp2 system.employees.name <span class="operator">%</span> TYPE )  <span class="comment">-- 根据id修改name</span></span><br><span class="line">    <span class="keyword">as</span></span><br><span class="line">    <span class="keyword">BEGIN</span></span><br><span class="line">    <span class="keyword">update</span> employees <span class="keyword">set</span> name<span class="operator">=</span>tmp2 <span class="keyword">where</span> id<span class="operator">=</span>tmp1;</span><br><span class="line">    EXCEPTION </span><br><span class="line">        <span class="keyword">WHEN</span> NO_DATA_FOUND <span class="keyword">THEN</span></span><br><span class="line">        dbms_output.put_line ( <span class="number">123</span> );<span class="keyword">END</span> proce;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">PROCEDURE</span> proce ( tmp2 system.employees.name <span class="operator">%</span> TYPE,tmp3 system.employees.email <span class="operator">%</span> TYPE )  <span class="comment">-- 根据name修改email</span></span><br><span class="line">    <span class="keyword">as</span></span><br><span class="line">    <span class="keyword">BEGIN</span></span><br><span class="line">    <span class="keyword">update</span> employees <span class="keyword">set</span> email<span class="operator">=</span>tmp3 <span class="keyword">where</span> name<span class="operator">=</span>tmp2;</span><br><span class="line">    EXCEPTION </span><br><span class="line">        <span class="keyword">WHEN</span> NO_DATA_FOUND <span class="keyword">THEN</span></span><br><span class="line">        dbms_output.put_line ( <span class="number">123</span> );<span class="keyword">END</span> proce;</span><br><span class="line"><span class="keyword">end</span> pck;</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 包的调用</span></span><br><span class="line"><span class="keyword">declare</span></span><br><span class="line">    tmp1 system.employees.id <span class="operator">%</span> type :<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line">    tmp2 system.employees.name <span class="operator">%</span> type :<span class="operator">=</span><span class="string">&#x27;wxk&#x27;</span>;</span><br><span class="line">    tmp3 system.employees.email <span class="operator">%</span> type :<span class="operator">=</span><span class="string">&#x27;w749@qq.com&#x27;</span>;</span><br><span class="line"></span><br><span class="line">    id1 system.employees.id <span class="operator">%</span> type;</span><br><span class="line">    name1 system.employees.name <span class="operator">%</span> type;</span><br><span class="line">    email1 system.employees.email <span class="operator">%</span> type;</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">    pck.proce(tmp1,tmp2);</span><br><span class="line">    pck.proce(tmp2,tmp3);</span><br><span class="line">    <span class="keyword">select</span> id,name,email <span class="keyword">into</span> id1,name1,email1 <span class="keyword">from</span> employees <span class="keyword">where</span> id<span class="operator">=</span>tmp1;</span><br><span class="line">    dbms_output.put_line(id1 <span class="operator">||</span> <span class="string">&#x27; , &#x27;</span> <span class="operator">||</span> name1 <span class="operator">||</span> <span class="string">&#x27; , &#x27;</span> <span class="operator">||</span> email1);</span><br><span class="line"><span class="keyword">end</span>;</span><br></pre></td></tr></table></figure><h3 id="触发器"><a href="#触发器" class="headerlink" title="触发器"></a>触发器</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">or</span> replace <span class="keyword">trigger</span> trig  <span class="comment">-- 限定只能在工作日内进行DML操作</span></span><br><span class="line">    before <span class="keyword">update</span> <span class="keyword">or</span> <span class="keyword">insert</span> <span class="keyword">or</span> <span class="keyword">delete</span> </span><br><span class="line">    <span class="keyword">on</span> system.employees </span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">    if to_char(sysdate,<span class="string">&#x27;HH24:MI&#x27;</span>) <span class="keyword">not</span> <span class="keyword">between</span> <span class="string">&#x27;12:00&#x27;</span> <span class="keyword">and</span> <span class="string">&#x27;18:00&#x27;</span></span><br><span class="line">    <span class="keyword">or</span> to_char(sysdate,<span class="string">&#x27;DY&#x27;</span>,<span class="string">&#x27;NLS_DATE_LANGUAGE=AMERICAN&#x27;</span>)</span><br><span class="line">    <span class="keyword">in</span> (<span class="string">&#x27;SAT&#x27;</span>,<span class="string">&#x27;SUN&#x27;</span>) <span class="keyword">then</span></span><br><span class="line">    raise_application_error(<span class="number">-20005</span>,<span class="string">&#x27;只能在工作日内的指定时间进行DML操作。&#x27;</span>);</span><br><span class="line">    <span class="keyword">end</span> if;</span><br><span class="line"><span class="keyword">end</span> trig;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">or</span> replace <span class="keyword">trigger</span> trigg  <span class="comment">-- 限定修改后的薪资必须高于修改前的薪资</span></span><br><span class="line">    before <span class="keyword">update</span> <span class="keyword">of</span> sal </span><br><span class="line">    <span class="keyword">on</span> system.employees</span><br><span class="line">    <span class="keyword">for</span> <span class="keyword">each</span> <span class="type">row</span></span><br><span class="line"><span class="keyword">when</span>(old.sal <span class="operator">&gt;=</span> new.sal)</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line">    raise_application_error(<span class="number">-20001</span>,<span class="string">&#x27;修改后的薪资必须高于修改前的薪资。&#x27;</span>);</span><br><span class="line"><span class="keyword">end</span> trigg;</span><br><span class="line"><span class="keyword">update</span> system.employees <span class="keyword">set</span> name<span class="operator">=</span><span class="string">&#x27;wwxxkk&#x27;</span> <span class="keyword">where</span> id<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line"><span class="keyword">update</span> system.employees <span class="keyword">set</span> sal<span class="operator">=</span><span class="number">12000</span> <span class="keyword">where</span> id<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">trigger</span> trig enable;  <span class="comment">-- 启用/禁用触发器</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">trigger</span> trig;  <span class="comment">-- 触发器必须是启用状态才可以删除</span></span><br></pre></td></tr></table></figure><h3 id="表空间"><a href="#表空间" class="headerlink" title="表空间"></a>表空间</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> Dba_Data_Files ddf <span class="keyword">WHERE</span> ddf.tablespace_name <span class="operator">=</span> <span class="string">&#x27;TBS1&#x27;</span>;  <span class="comment">-- 查看表空间信息</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span>space tbs1 datafile <span class="string">&#x27;/home/oracle/app/oracle/oradata/helowin/tbs1.dbf&#x27;</span> size <span class="number">50</span>m;  <span class="comment">-- 创建表空间</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span>space tbs2 datafile <span class="string">&#x27;/home/oracle/app/oracle/oradata/helowin/tbs2.dbf&#x27;</span> size <span class="number">50</span>m;  <span class="comment">-- 创建表空间</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span>space tbs1 <span class="keyword">add</span> datafile <span class="string">&#x27;/home/oracle/app/oracle/oradata/helowin/tbs3.dbf&#x27;</span> size <span class="number">5</span>m;  <span class="comment">-- 改变表空间大小，为表空间增加数据文件</span></span><br><span class="line"><span class="keyword">SELECT</span> <span class="operator">*</span> <span class="keyword">FROM</span> Dba_Data_Files ddf <span class="keyword">WHERE</span> ddf.tablespace_name <span class="keyword">like</span> <span class="string">&#x27;TBS%&#x27;</span>;  <span class="comment">-- 添加的数据文件和原始表空间数据文件是分开的，但属于同一个表空间</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span>space tbs1 read <span class="keyword">only</span>;  <span class="comment">-- 将表空间权限修改为只读，这样表空间中的表都是只读改为read write恢复读写</span></span><br><span class="line"><span class="keyword">alter</span> database <span class="keyword">default</span> tablespace tbs1;  <span class="comment">-- 设置数据库默认表空间</span></span><br><span class="line"><span class="keyword">drop</span> tablespace tbs1 including contents <span class="keyword">and</span> datafiles;  <span class="comment">-- 删除表空间以及其中所有内容以及数据文件</span></span><br></pre></td></tr></table></figure><h3 id="分区表"><a href="#分区表" class="headerlink" title="分区表"></a>分区表</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> part(</span><br><span class="line">    id number <span class="keyword">primary</span> key,</span><br><span class="line">    type varchar2(<span class="number">10</span>),</span><br><span class="line">    extra varchar2(<span class="number">10</span>)</span><br><span class="line">    )</span><br><span class="line">tablespace users </span><br><span class="line"><span class="keyword">partition</span> <span class="keyword">by</span> list(type) </span><br><span class="line">(</span><br><span class="line">    <span class="keyword">partition</span> aaa <span class="keyword">values</span>(<span class="string">&#x27;A&#x27;</span>,<span class="string">&#x27;a&#x27;</span>) tablespace tbs1,</span><br><span class="line">    <span class="keyword">partition</span> bbb <span class="keyword">values</span>(<span class="string">&#x27;B&#x27;</span>,<span class="string">&#x27;b&#x27;</span>) tablespace tbs2,</span><br><span class="line">    <span class="keyword">partition</span> ddd <span class="keyword">values</span>(<span class="keyword">NULL</span>),</span><br><span class="line">    <span class="keyword">partition</span> eee <span class="keyword">values</span>(<span class="keyword">DEFAULT</span>)</span><br><span class="line">);</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> dba_tab_partitions <span class="keyword">where</span> table_name<span class="operator">=</span><span class="string">&#x27;PART&#x27;</span>;  <span class="comment">-- 查看分区表情况</span></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> part <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">&#x27;A&#x27;</span>,<span class="string">&#x27;aa&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> part <span class="keyword">values</span>(<span class="number">2</span>,<span class="string">&#x27;B&#x27;</span>,<span class="string">&#x27;aa&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> part <span class="keyword">values</span>(<span class="number">3</span>,<span class="string">&#x27;B&#x27;</span>,<span class="string">&#x27;aa&#x27;</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> part <span class="keyword">values</span>(<span class="number">4</span>,<span class="string">&#x27;C&#x27;</span>,<span class="string">&#x27;aa&#x27;</span>);</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> part;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Language </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL常用操作命令</title>
      <link href="/2019/11/05/Language/MySQL%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/"/>
      <url>/2019/11/05/Language/MySQL%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<p>MySQL数据库常用操作命令。</p><span id="more"></span><h3 id="数据库和表操作"><a href="#数据库和表操作" class="headerlink" title="数据库和表操作"></a>数据库和表操作</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> databases;  <span class="comment">-- 查看所有数据库</span></span><br><span class="line"><span class="keyword">create</span> database test;  <span class="comment">-- 新建数据库</span></span><br><span class="line"><span class="keyword">drop</span> database test;  <span class="comment">-- 删除数据库</span></span><br><span class="line">use test;  <span class="comment">-- 使用数据库</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">show</span> tables  <span class="comment">-- 查看所有表</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tb(a <span class="type">int</span>(<span class="number">10</span>) <span class="keyword">primary</span> key, b <span class="type">varchar</span>(<span class="number">20</span>), c <span class="type">varchar</span>(<span class="number">25</span>) <span class="keyword">default</span> &quot;33&quot;);  <span class="comment">-- 新建表</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb modify a <span class="type">int</span>(<span class="number">15</span>);  <span class="comment">-- 修改字段数据类型</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb change a aa <span class="type">int</span>(<span class="number">20</span>);  <span class="comment">-- 修改字段名和数据类型</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb <span class="keyword">add</span> d <span class="type">varchar</span>(<span class="number">50</span>) <span class="keyword">first</span>;  <span class="comment">-- 默认在最后一列</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb <span class="keyword">add</span> d <span class="type">varchar</span>(<span class="number">50</span>) after a;  <span class="comment">-- 在a字段后添加新字段</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb <span class="keyword">drop</span> d;  <span class="comment">-- 删除字段</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb engien<span class="operator">=</span>MyISAM;  <span class="comment">-- 修改存储引擎</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb <span class="keyword">drop</span> <span class="keyword">foreign</span> key key_name;  <span class="comment">-- 删除外键</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb <span class="keyword">add</span> index idx(aa(<span class="number">10</span>))  <span class="comment">-- 添加索引</span></span><br><span class="line"><span class="keyword">drop</span> index idx <span class="keyword">on</span> tb;  <span class="comment">-- 删除索引</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> if <span class="keyword">exists</span> tb;  <span class="comment">-- 删除表，若表不存在不会报错，若表中和其他表有关联则删除失败（表中的主键是其他表的外键）</span></span><br></pre></td></tr></table></figure><h3 id="存储过程"><a href="#存储过程" class="headerlink" title="存储过程"></a>存储过程</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 新建存储过程</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">procedure</span> pr()</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">procedure</span> if <span class="keyword">exists</span> pr;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> if <span class="keyword">exists</span> tb;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tb(id <span class="type">int</span>(<span class="number">10</span>) <span class="keyword">primary</span> key auto_increment, fn <span class="type">varchar</span>(<span class="number">20</span>), ln <span class="type">varchar</span>(<span class="number">20</span>));</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tb(fn, ln) <span class="keyword">values</span>(&quot;wang&quot;,&quot;xk&quot;),(&quot;wang&quot;,&quot;xu&quot;);</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"><span class="keyword">call</span> pr();  <span class="comment">-- 运行存储过程</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建变量及变量赋值</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">procedure</span> if <span class="keyword">exists</span> pr;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">procedure</span> pr()</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line"><span class="keyword">declare</span> var1, var2 <span class="type">varchar</span>(<span class="number">20</span>);</span><br><span class="line"><span class="keyword">select</span> fn, ln <span class="keyword">into</span> var1, var2 <span class="keyword">from</span> tb <span class="keyword">where</span> id<span class="operator">=</span><span class="number">1</span>;</span><br><span class="line"><span class="keyword">select</span> var1, var2;</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">call</span> pr();</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 定义条件</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">procedure</span> if <span class="keyword">exists</span> pr;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">procedure</span> pr()</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line"><span class="keyword">declare</span> continue handler <span class="keyword">for</span> <span class="keyword">sqlstate</span> <span class="string">&#x27;23000&#x27;</span> <span class="keyword">set</span> <span class="variable">@x1</span><span class="operator">=</span><span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tb_tmp(id <span class="type">int</span>(<span class="number">10</span>) <span class="keyword">primary</span> key);</span><br><span class="line"><span class="keyword">set</span> <span class="variable">@x</span><span class="operator">=</span><span class="number">1</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tb_tmp(id) <span class="keyword">values</span>(<span class="number">1</span>);</span><br><span class="line"><span class="keyword">set</span> <span class="variable">@x</span><span class="operator">=</span><span class="number">2</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tb_tmp(id) <span class="keyword">values</span>(<span class="number">1</span>);</span><br><span class="line"><span class="keyword">set</span> <span class="variable">@x</span><span class="operator">=</span><span class="number">3</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> <span class="variable">@x1</span>,<span class="variable">@x</span>;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> if <span class="keyword">exists</span> tb_tmp;</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">call</span> pr();</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 光标</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">procedure</span> if <span class="keyword">exists</span> pr;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">procedure</span> pr()</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line"><span class="keyword">declare</span> var1, var2 <span class="type">varchar</span>(<span class="number">20</span>);</span><br><span class="line"><span class="keyword">declare</span> cur <span class="keyword">cursor</span> <span class="keyword">for</span> <span class="keyword">select</span> fn, ln <span class="keyword">from</span> tb;</span><br><span class="line"></span><br><span class="line"><span class="keyword">open</span> cur;</span><br><span class="line"><span class="keyword">fetch</span> cur <span class="keyword">into</span> var1, var2;</span><br><span class="line"><span class="keyword">close</span> cur;</span><br><span class="line"></span><br><span class="line"><span class="keyword">select</span> var1,var2;</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">call</span> pr();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- if语句</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">procedure</span> if <span class="keyword">exists</span> pr;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">procedure</span> pr()</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">declare</span> var <span class="type">varchar</span>(<span class="number">20</span>);</span><br><span class="line"><span class="keyword">set</span> <span class="variable">@x</span><span class="operator">=</span><span class="number">3</span>;</span><br><span class="line">if <span class="variable">@x</span><span class="operator">=</span><span class="number">1</span></span><br><span class="line">    <span class="keyword">then</span> <span class="keyword">select</span> <span class="string">&#x27;x=1&#x27;</span>;</span><br><span class="line">    elseif <span class="variable">@x</span><span class="operator">=</span><span class="number">2</span></span><br><span class="line">    <span class="keyword">then</span> <span class="keyword">select</span> <span class="string">&#x27;x=2&#x27;</span>;</span><br><span class="line">    <span class="keyword">else</span></span><br><span class="line">    <span class="keyword">select</span> <span class="string">&#x27;x=3&#x27;</span>;</span><br><span class="line"><span class="keyword">end</span> if;</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">call</span> pr();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- case语句</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">procedure</span> if <span class="keyword">exists</span> pr;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">procedure</span> pr()</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line"><span class="keyword">set</span> <span class="variable">@x</span><span class="operator">=</span><span class="number">1</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">@x</span></span><br><span class="line">    <span class="keyword">when</span> <span class="number">1</span> <span class="keyword">then</span> <span class="keyword">select</span> <span class="string">&#x27;x=1&#x27;</span>;</span><br><span class="line">    <span class="keyword">when</span> <span class="number">2</span> <span class="keyword">then</span> <span class="keyword">select</span> <span class="string">&#x27;x=2&#x27;</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">select</span> <span class="string">&#x27;x!=1&amp;x!=2&#x27;</span>;</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">case</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span></span><br><span class="line">    <span class="keyword">when</span> <span class="variable">@x</span><span class="operator">&gt;</span><span class="number">0</span> <span class="keyword">then</span> <span class="keyword">select</span> <span class="string">&#x27;x&gt;0&#x27;</span>;</span><br><span class="line">    <span class="keyword">when</span> <span class="variable">@x</span><span class="operator">&lt;</span><span class="number">0</span> <span class="keyword">then</span> <span class="keyword">select</span> <span class="string">&#x27;x&lt;0&#x27;</span>;</span><br><span class="line">    <span class="keyword">else</span> <span class="keyword">select</span> <span class="string">&#x27;x=0&#x27;</span>;</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">case</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">call</span> pr();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- loop循环</span></span><br><span class="line"><span class="comment">-- LEAVE语句用来退出任何被标注的流程控制构造</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">procedure</span> if <span class="keyword">exists</span> pr;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">procedure</span> pr()</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">declare</span> x <span class="type">int</span>(<span class="number">10</span>) <span class="keyword">default</span> <span class="number">0</span>;</span><br><span class="line">add_loop:loop</span><br><span class="line"><span class="keyword">set</span> x<span class="operator">=</span>x<span class="operator">+</span><span class="number">1</span>;</span><br><span class="line">if x<span class="operator">&gt;=</span><span class="number">5</span> <span class="keyword">then</span> leave add_loop;</span><br><span class="line"><span class="keyword">end</span> if;</span><br><span class="line"><span class="keyword">select</span> x;</span><br><span class="line"><span class="keyword">end</span> loop add_loop;</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">call</span> pr();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- iterate循环迭代</span></span><br><span class="line"><span class="comment">-- ITERATE语句将执行顺序转到语句段开头处,只可以出现在LOOP、REPEAT和WHILE语句内。</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">procedure</span> if <span class="keyword">exists</span> pr;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">procedure</span> pr()</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">declare</span> x <span class="type">int</span>(<span class="number">10</span>) <span class="keyword">default</span> <span class="number">0</span>;</span><br><span class="line">loop1:loop</span><br><span class="line"><span class="keyword">set</span> x<span class="operator">=</span>x<span class="operator">+</span><span class="number">1</span>;</span><br><span class="line">if x<span class="operator">&lt;</span><span class="number">10</span> <span class="keyword">then</span> iterate loop1;</span><br><span class="line">elseif x<span class="operator">&gt;</span><span class="number">15</span> <span class="keyword">then</span> leave loop1;</span><br><span class="line"><span class="keyword">end</span> if;</span><br><span class="line"><span class="keyword">end</span> loop loop1;</span><br><span class="line"><span class="keyword">select</span> x;</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">call</span> pr();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- repeat语句</span></span><br><span class="line"><span class="comment">-- REPEAT语句创建一个带条件判断的循环过程，每次语句执行完毕之后会对条件表达式进行判断，</span></span><br><span class="line"><span class="comment">-- 如果表达式为真，则循环结束；否则重复执行循环中的语句。</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">procedure</span> if <span class="keyword">exists</span> pr;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">procedure</span> pr()</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">declare</span> x <span class="type">int</span>(<span class="number">10</span>) <span class="keyword">default</span> <span class="number">0</span>;</span><br><span class="line">repeat</span><br><span class="line"><span class="keyword">set</span> x<span class="operator">=</span>x<span class="operator">+</span><span class="number">1</span>;</span><br><span class="line"><span class="keyword">select</span> x;</span><br><span class="line">until x<span class="operator">&gt;</span><span class="number">3</span> </span><br><span class="line"><span class="keyword">end</span> repeat;</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">call</span> pr();</span><br><span class="line"></span><br><span class="line"><span class="comment">-- while语句</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">procedure</span> if <span class="keyword">exists</span> pr;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">procedure</span> pr()</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">declare</span> x <span class="type">int</span>(<span class="number">10</span>) <span class="keyword">default</span> <span class="number">0</span>;</span><br><span class="line">while x<span class="operator">&lt;</span><span class="number">3</span> do</span><br><span class="line"><span class="keyword">set</span> x<span class="operator">=</span>x<span class="operator">+</span><span class="number">1</span>;</span><br><span class="line"><span class="keyword">end</span> while;</span><br><span class="line"><span class="keyword">select</span> x;</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">call</span> pr();</span><br></pre></td></tr></table></figure><h3 id="用户及权限"><a href="#用户及权限" class="headerlink" title="用户及权限"></a>用户及权限</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 新增删除用户（CREATE USER语句会在MySQL.user表中添加一条新记录，但是新创建的账户没有任何权限）</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">user</span> <span class="string">&#x27;user01&#x27;</span>@<span class="string">&#x27;%&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span>;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">user</span> <span class="string">&#x27;user01&#x27;</span>@<span class="string">&#x27;%&#x27;</span>;</span><br><span class="line"><span class="keyword">delete</span> <span class="keyword">from</span> mysql.user <span class="keyword">where</span> host<span class="operator">=</span><span class="string">&#x27;%&#x27;</span> <span class="keyword">and</span> <span class="keyword">user</span><span class="operator">=</span><span class="string">&#x27;user01&#x27;</span>;</span><br><span class="line"><span class="comment">-- 更新用户密码</span></span><br><span class="line"><span class="keyword">update</span> mysql.user <span class="keyword">set</span> authentication_string<span class="operator">=</span>MD5(<span class="string">&#x27;root_user&#x27;</span>) <span class="keyword">where</span> host<span class="operator">=</span><span class="string">&#x27;%&#x27;</span> <span class="keyword">and</span> <span class="keyword">user</span><span class="operator">=</span><span class="string">&#x27;root_user&#x27;</span>;  <span class="comment">-- 修改root用户密码</span></span><br><span class="line">flush privileges;  <span class="comment">-- 重新加载权限</span></span><br><span class="line"><span class="keyword">set</span> password <span class="keyword">for</span> <span class="string">&#x27;user01&#x27;</span>@<span class="string">&#x27;%&#x27;</span><span class="operator">=</span><span class="string">&#x27;654321&#x27;</span>;  <span class="comment">-- 修改普通用户的密码，也可以使用update</span></span><br><span class="line"><span class="comment">-- 新增用户并赋予权限（赋予新用户更新、查询权限，并授予grant权限）</span></span><br><span class="line"><span class="keyword">grant</span> <span class="keyword">select</span>, <span class="keyword">update</span> <span class="keyword">on</span>  <span class="operator">*</span>.<span class="operator">*</span> <span class="keyword">to</span> <span class="string">&#x27;user01&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span> identified <span class="keyword">by</span> <span class="string">&#x27;123456&#x27;</span> whth <span class="keyword">grant</span> option;</span><br><span class="line"><span class="comment">-- 收回权限</span></span><br><span class="line"><span class="keyword">revoke</span> <span class="keyword">all</span> privileges, <span class="keyword">grant</span> option <span class="keyword">from</span> <span class="string">&#x27;user02&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span>;  <span class="comment">-- 收回所有权限</span></span><br><span class="line"><span class="keyword">revoke</span> <span class="keyword">update</span> <span class="keyword">on</span>  <span class="operator">*</span>.<span class="operator">*</span> <span class="keyword">from</span> <span class="string">&#x27;user02&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span>;  <span class="comment">-- 收回指定权限</span></span><br><span class="line"><span class="keyword">show</span> grants <span class="keyword">for</span> <span class="string">&#x27;user02&#x27;</span>@<span class="string">&#x27;localhost&#x27;</span>;<span class="comment">-- 查看权限</span></span><br></pre></td></tr></table></figure><h3 id="触发器"><a href="#触发器" class="headerlink" title="触发器"></a>触发器</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 调用存储过程</span></span><br><span class="line"><span class="comment">-- 传入参数x输出参数y，调用时需要指定接收参数然后使用select查看</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">procedure</span> if <span class="keyword">exists</span> pr;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">procedure</span> pr(<span class="keyword">in</span> x <span class="type">int</span>, <span class="keyword">out</span> y <span class="type">varchar</span>(<span class="number">20</span>))</span><br><span class="line"><span class="keyword">begin</span></span><br><span class="line"><span class="keyword">select</span> ln <span class="keyword">into</span> y <span class="keyword">from</span> tb <span class="keyword">where</span> id<span class="operator">=</span>x;</span><br><span class="line"><span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">call</span> pr(<span class="number">1</span>, <span class="variable">@y</span>);</span><br><span class="line"><span class="keyword">select</span> <span class="variable">@y</span>;</span><br><span class="line"><span class="comment">-- 查看procedure的状态</span></span><br><span class="line"><span class="keyword">show</span> <span class="keyword">procedure</span> status;</span><br><span class="line"><span class="comment">-- 查看指定存储过程的详细信息</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> information_schema.ROUTINES <span class="keyword">where</span> ROUTINE_NAME<span class="operator">=</span><span class="string">&#x27;pr&#x27;</span>;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 创建触发器（不能在存储过程中创建触发器）</span></span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">trigger</span> if <span class="keyword">exists</span> tri;</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">trigger</span> tri before <span class="keyword">insert</span> <span class="keyword">on</span> tb <span class="keyword">for</span> <span class="keyword">each</span> <span class="type">row</span> </span><br><span class="line">    <span class="keyword">begin</span> </span><br><span class="line">    <span class="keyword">set</span> NEW.times<span class="operator">=</span>now();  <span class="comment">-- 操作当前表使用NEW代替</span></span><br><span class="line">    <span class="keyword">end</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tb(fn,ln) <span class="keyword">values</span>(<span class="string">&#x27;zhang&#x27;</span>, <span class="string">&#x27;san&#x27;</span>);</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tb;</span><br><span class="line"><span class="comment">-- 查看触发器</span></span><br><span class="line"><span class="keyword">show</span> triggers;</span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> information_schema.`TRIGGERS`;</span><br></pre></td></tr></table></figure><h3 id="备份恢复及日志"><a href="#备份恢复及日志" class="headerlink" title="备份恢复及日志"></a>备份恢复及日志</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 数据库的备份与恢复</span></span><br><span class="line">mysqldump <span class="operator">-</span>u root <span class="operator">-</span>p 数据库 <span class="operator">&gt;</span> file.sql  <span class="comment">-- 需要在终端运行</span></span><br><span class="line">mysql <span class="operator">-</span>u root <span class="operator">-</span>p 数据库 <span class="operator">&lt;</span> file.sql  <span class="comment">-- 确保已有目标数据库，在终端运行</span></span><br><span class="line">use 数据库; source <span class="operator">/</span>路径<span class="operator">/</span>file.sql  <span class="comment">-- 在进入MySQL后运行</span></span><br><span class="line"><span class="comment">-- 数据库迁移（仅限相同版本数据库）</span></span><br><span class="line">mysqldump <span class="operator">-</span>h rm<span class="number">-2</span>zeae42nlw26e1b003o.mysql.rds.aliyuncs.com <span class="operator">-</span>u root_user <span class="operator">-</span>p test <span class="operator">|</span> mysql <span class="operator">-</span>h rm<span class="number">-2</span>zem56ps0i81072ssgo.mysql.rds.aliyuncs.com <span class="operator">-</span>u root_user <span class="operator">-</span>p</span><br><span class="line"></span><br><span class="line"><span class="comment">-- 表的导入和导出</span></span><br><span class="line"><span class="keyword">select</span> <span class="operator">*</span> <span class="keyword">from</span> tb <span class="keyword">into</span> outfile <span class="string">&#x27;/Users/mac/Downloads/tb.txt&#x27;</span>;</span><br><span class="line">mysqldump <span class="operator">-</span>T <span class="string">&#x27;/Users/mac/Downloads/tb.txt&#x27;</span> test tb <span class="operator">-</span>h rm<span class="number">-2</span>zeae42nlw26e1b003o.mysql.rds.aliyuncs.com <span class="operator">-</span>u root_user <span class="operator">-</span>p（在终端执行）</span><br><span class="line">load data infile <span class="string">&#x27;/Users/mac/Downloads/tb.txt&#x27;</span> info <span class="keyword">table</span> test.tb;  <span class="comment">-- 导入文件</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 二进制日志</span></span><br><span class="line"><span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;log_%&#x27;</span>;  <span class="comment">-- 查询日志设置</span></span><br><span class="line"><span class="keyword">show</span> <span class="type">binary</span> logs;  <span class="comment">-- 查看现有二进制日志文件</span></span><br><span class="line">reset master;  <span class="comment">--C 删除所有二进制日志</span></span><br><span class="line">purge master logs <span class="keyword">to</span> <span class="string">&#x27;mysql-bin.000035&#x27;</span>;  <span class="comment">-- 删除创建时间比指定二进制日志文件早的文件</span></span><br><span class="line">purge master logs before <span class="string">&#x27;20200101&#x27;</span>;  <span class="comment">-- 删除早于20200101以前的二进制日志</span></span><br><span class="line"><span class="comment">-- 使用指定日志恢复到特定时间点之前的状态（终端执行）</span></span><br><span class="line">mysqlbinlog <span class="comment">--stop-datetime=&quot;2020-11-16 18:12:00&quot; /usr/local/mysql-8.0.15-macos10.14-x86_64/data/binlog.000029 | mysql -uroot -p</span></span><br><span class="line"><span class="keyword">set</span> sql_log_bin<span class="operator">=</span><span class="number">0</span>;  <span class="comment">-- 暂停记录二进制日志</span></span><br><span class="line"><span class="keyword">set</span> sql_log_bin<span class="operator">=</span><span class="number">1</span>;  <span class="comment">-- 恢复记录二进制日志</span></span><br><span class="line"></span><br><span class="line"><span class="comment">-- 其他日志</span></span><br><span class="line"><span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;log_error&#x27;</span>;  <span class="comment">-- 查看错误日志</span></span><br><span class="line"><span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;%general%&#x27;</span>;  <span class="comment">-- 查看通用查询日志状态</span></span><br><span class="line"><span class="keyword">set</span> @<span class="variable">@global</span>.general_log<span class="operator">=</span><span class="number">1</span>  <span class="comment">-- 打开通用查询日志状态</span></span><br><span class="line"><span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;%slow_query%&#x27;</span>;  <span class="comment">-- 慢查询的状态及地址</span></span><br><span class="line"><span class="keyword">show</span> variables <span class="keyword">like</span> <span class="string">&#x27;%long_query%&#x27;</span>;  <span class="comment">-- 慢查询的时间阈值</span></span><br></pre></td></tr></table></figure><h3 id="加密解密"><a href="#加密解密" class="headerlink" title="加密解密"></a>加密解密</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 加密解密，使用密匙对数据库内容进行加密</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> tmp(a <span class="type">varbinary</span>(<span class="number">16</span>), b <span class="type">binary</span>(<span class="number">16</span>), c <span class="type">blob</span>);</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> tmp(a, b, c) <span class="keyword">values</span>(aes_encrypt(<span class="string">&#x27;张三&#x27;</span>,<span class="string">&#x27;key001&#x27;</span>), aes_encrypt(<span class="string">&#x27;i am a teather!&#x27;</span>, <span class="string">&#x27;key001&#x27;</span>), aes_encrypt(<span class="string">&#x27;李四&#x27;</span>, <span class="string">&#x27;key001&#x27;</span>));</span><br><span class="line"><span class="keyword">select</span> aes_decrypt(a, <span class="string">&#x27;key001&#x27;</span>), aes_decrypt(b, <span class="string">&#x27;key001&#x27;</span>), aes_decrypt(c, <span class="string">&#x27;key001&#x27;</span>) <span class="keyword">from</span> tmp;</span><br><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> if <span class="keyword">exists</span> tmp;</span><br></pre></td></tr></table></figure><h3 id="性能优化"><a href="#性能优化" class="headerlink" title="性能优化"></a>性能优化</h3><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">show</span> status;  <span class="comment">-- 查看状态（连接次数，错误次数，查询，更新次数等）</span></span><br><span class="line">explain <span class="keyword">select</span> ln, fn <span class="keyword">from</span> tb <span class="keyword">where</span> fn<span class="operator">=</span><span class="string">&#x27;wang&#x27;</span>;  <span class="comment">-- 分析查询语句</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb disable keys;  <span class="comment">-- 禁用索引</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb enable keys;  <span class="comment">-- 重新使用索引</span></span><br><span class="line"><span class="keyword">set</span> unique_checks<span class="operator">=</span><span class="number">0</span>;  <span class="comment">-- 禁用唯一性检查</span></span><br><span class="line"><span class="keyword">set</span> unique_checks<span class="operator">=</span><span class="number">1</span>;  <span class="comment">-- 启用唯一性检查</span></span><br><span class="line"><span class="keyword">set</span> foreign_key_checks<span class="operator">=</span><span class="number">0</span>;  <span class="comment">-- 禁用外键检查</span></span><br><span class="line"><span class="keyword">set</span> foreign_key_checks<span class="operator">=</span><span class="number">1</span>;  <span class="comment">-- 启用外键检查</span></span><br><span class="line"><span class="keyword">set</span> autocommit<span class="operator">=</span><span class="number">0</span>;  <span class="comment">-- 禁用自动提交</span></span><br><span class="line"><span class="keyword">set</span> autocommit<span class="operator">=</span><span class="number">1</span>;  <span class="comment">-- 恢复自动提交</span></span><br><span class="line">analyze <span class="keyword">table</span> tb;  <span class="comment">-- 分析表</span></span><br><span class="line"><span class="keyword">check</span> <span class="keyword">table</span> tb;  <span class="comment">-- 检查表</span></span><br><span class="line">optimize <span class="keyword">table</span> tb;  <span class="comment">-- 优化表</span></span><br><span class="line"><span class="keyword">set</span> <span class="keyword">global</span> max_execution_time<span class="operator">=</span><span class="number">2000</span>;  <span class="comment">-- 设置服务器语句超时时间，单位为毫秒</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb <span class="keyword">alter</span> index idx invisible;  <span class="comment">-- 隐藏索引</span></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> tb <span class="keyword">alter</span> index idx visible;  <span class="comment">-- 使索引可见</span></span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Language </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL基础理论知识</title>
      <link href="/2019/10/28/Language/MySQL%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86/"/>
      <url>/2019/10/28/Language/MySQL%E5%9F%BA%E7%A1%80%E7%90%86%E8%AE%BA%E7%9F%A5%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<p>MySQL数据库基础理论知识。</p><span id="more"></span><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><h4 id="整型"><a href="#整型" class="headerlink" title="整型"></a><strong>整型</strong></h4><p>数据类型**INT(11)**，注意后面的数字11，它表示的是该数据类型指定的显示宽度，即能够显示的数值中数字的个数。显示宽度和数据类型的取值范围是无关的。显示宽度只是指明MySQL最大可能显示的数字个数，数值的位数小于指定的宽度时会由空格填充；如果插入了大于显示宽度的值，只要该值不超过该类型整数的取值范围，数值依然可以插入，而且能够显示出来。显示宽度只用于显示，并不能限制取值范围和占用空间。例如：INT(3)会占用4字节的存储空间，并且允许的最大值不会是999，而是INT整型所允许的最大值。TINYINT、SMALLINT、MEDIUMINT、INT（INTEGER）、BIGINT类型分别占用1、2、3、4、8个字节，对应存储最大值的范围也就不一样。</p><h4 id="浮点型"><a href="#浮点型" class="headerlink" title="浮点型"></a><strong>浮点型</strong></h4><p>MySQL中使用浮点数和定点数来表示小数。浮点数类型有两种：<strong>单精度浮点类型（FLOAT）和双精度浮点类型（DOUBLE）</strong>。定点数类型只有一种：<strong>DECIMAL</strong>。浮点数类型和定点数类型都可以用（M，N）来表示。其中，M称为精度，表示总共的位数；N称为标度，表示小数的位数。分别占用的字节为4、8、M+2个字节。如果用户指定的精度超出精度范围，则会四舍五入。</p><h4 id="日期"><a href="#日期" class="headerlink" title="日期"></a><strong>日期</strong></h4><p>MySQL中有多种表示日期的数据类型，主要有<strong>YEAR、TIME、DATE、DATETIME和TIMESTAMP</strong>。分别占用的字节为1、3、3、8、4个字节。<br>TIMESTAMP与DATETIME除了存储字节和支持的范围不同外，还有一个最大的区别就是：DATETIME在存储日期数据时，按实际输入的格式存储，即输入什么就存储什么，与时区无关；而TIMESTAMP值的存储是以UTC（世界标准时间）格式保存的，存储时对当前时区进行转换，检索时再转换回当前时区。</p><h4 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a><strong>字符串</strong></h4><p>MySQL支持两类字符型数据：文本字符串和二进制字符串。在MySQL中，文本字符串类型是指<strong>CHAR、VARCHAR、TEXT、ENUM和SET</strong>。<br>**CHAR(M)**为固定长度字符串，在定义时指定字符串列长。VARCHAR(M)是长度可变的字符串，M表示最大列长度。M的范围是0~65535。CHAR是固定长度，所以它的处理速度比VARCHAR的速度要快，但是它的缺点是浪费存储空间，所以对存储不大但在速度上有要求的可以使用CHAR类型，反之可以使用VARCHAR类型来实现。对于MyISAM存储引擎：最好使用固定长度的数据列代替可变长度的数据列。这样可以使整个表静态化，从而使数据检索更快，用空间换时间。对于InnoDB存储引擎：使用可变长度的数据列，因为InnoDB数据表的存储格式不分固定长度和可变长度，因此使用CHAR不一定比使用VARCHAR更好，但由于VARCHAR是按照实际的长度存储的，比较节省空间，所以对磁盘I&#x2F;O和数据存储总量比较好。<br><strong>VARCHAR</strong>的最大实际长度由最长的行的大小和使用的字符集确定，而其实际占用的空间为字符串的实际长度加1。TEXT列保存非二进制字符串，如文章内容、评论等。当保存或查询TEXT列的值时，不删除尾部空格。Text类型分为4种：TINYTEXT、TEXT、MEDIUMTEXT和LONGTEXT。不同的TEXT类型的存储空间和数据长度不同。<br><strong>ENUM</strong>是一个字符串对象，其值为表创建时在列规定中枚举的一列值。<br><strong>SET</strong>是一个字符串对象，可以有零或多个值。与ENUM类型不同的是，ENUM类型的字段只能从定义的列值中选择一个值插入，而SET类型的列可从定义的列值中选择多个字符的联合。如果插入SET字段中列值有重复，则MySQL自动删除重复的值；如果插入了不正确的值，默认情况下，MySQL将忽视这些值，并给出警告。</p><h4 id="二进制数据类型"><a href="#二进制数据类型" class="headerlink" title="二进制数据类型"></a><strong>二进制数据类型</strong></h4><p>MySQL中的二进制数据类型有<strong>BIT、BINARY、VARBINARY、TINYBLOB、BLOB、MEDIUMBLOB和LONGBLOB</strong>。<br><strong>BIT</strong>类型是位字段类型。M表示每个值的位数，范围为1~64。<br><strong>BINARY和VARBINARY</strong>类型类似于CHAR和VARCHAR，不同的是它们包含二进制字节字符串。<br><strong>BLOB</strong>是一个二进制大对象，用来存储可变数量的数据。BLOB列存储的是二进制字符串（字节字符串），TEXT列存储的是非二进制字符串（字符字符串）。BLOB主要存储图片、音频信息等，而TEXT只能存储纯文本文件。</p><p>**CAST(x, AS type)和CONVERT(x, type)**函数将一个类型的值转换为另一个类型的值，可转换的type有BINARY、CHAR(n)、DATE、TIME、DATETIME、DECIMAL、SIGNED、UNSIGNED。</p><h3 id="函数"><a href="#函数" class="headerlink" title="函数"></a>函数</h3><h4 id="数学函数"><a href="#数学函数" class="headerlink" title="数学函数"></a><strong>数学函数</strong></h4><p>**ROUND(x,y)**返回最接近于参数x的数，其值保留到小数点后面y位，若y为负值，则将保留x值到小数点左边y位。y值为负数时，保留的小数点左边的相应位数直接保存为0，不进行四舍五入。<br>**TRUNCATE(x,y)**返回被舍去至小数点后y位的数字x。若y的值为0，则结果不带有小数点或不带有小数部分。若y设为负数，则截去（归零）x小数点左起第y位开始后面所有低位的值。ROUND(x,y)函数在截取值的时候会四舍五入，而TRUNCATE (x,y)直接截取值，并不进行四舍五入。<br>**SIGN(x)**返回参数的符号，x的值为负、零或正时返回结果依次为-1、0或1。</p><h4 id="字符串函数"><a href="#字符串函数" class="headerlink" title="字符串函数"></a><strong>字符串函数</strong></h4><p>**CONCAT(s1,s2,…)<strong>返回结果为连接参数产生的字符串，或许有一个或多个参数。<br>在</strong>CONCAT_WS(x,s1,s2,…)**中，CONCAT_WS代表CONCAT With Separator，是CONCAT()的特殊形式。第一个参数x是其他参数的分隔符，分隔符的位置放在要连接的两个字符串之间。<br>**INSERT(s1,x,len,s2)**返回字符串s1，其子字符串起始于x位置和被字符串s2取代的len字符。如果x超过字符串长度，则返回值为原始字符串。假如len的长度大于其他字符串的长度，则从位置x开始替换。若任何一个参数为NULL，则返回值为NULL。<br>**LEFT(s,n)**返回字符串s开始的最左边n个字符。<br>**RIGHT(s,n)**返回字符串str最右边的n个字符。<br>**LPAD(s1,len,s2)**返回字符串s1，其左边由字符串s2填补到len字符长度。假如s1的长度大于len，则返回值被缩短至len字符。<br>**RPAD(s1,len,s2)**返回字符串sl，其右边被字符串s2填补至len字符长度。假如字符串s1的长度大于len，则返回值被缩短到len字符长度。<br>**LTRIM(s)**返回字符串s，字符串左侧空格字符被删除。<br>**RTRIM(s)**返回字符串s，字符串右侧空格字符被删除。<br>**TRIM(s1 FROM s)**删除字符串s中两端所有的子字符串s1。s1为可选项，在未指定情况下，删除空格。<br>**REPEAT(s,n)**返回一个由重复的字符串s组成的字符串，字符串s的数目等于n。若n&lt;&#x3D;0，则返回一个空字符串。若s或n为NULL，则返回NULL。<br>**REPLACE(s,s1,s2)**使用字符串s2替代字符串s中所有的字符串s1。<br>**STRCMP(s1,s2)**：若所有的字符串均相同，则返回0；若根据当前分类次序，第一个参数小于第二个，则返回-1；其他情况返回1。<br>**SUBSTRING(s,n,len)**带有len参数的格式，从字符串s返回一个长度与len字符相同的子字符串，起始于位置n。也可能对n使用一个负值。假若这样，则子字符串的位置起始于字符串结尾的n字符，即倒数第n个字符，而不是字符串的开头位置。<br>**MID(s,n,len)**与SUBSTRING(s,n,len)的作用相同。<br>**LOCATE(str1,str)、POSITION(str1 IN str)和INSTR(str, str1)**3个函数的作用相同，返回子字符串str1在字符串str中的开始位置。<br>**REVERSE(s)**将字符串s反转，返回的字符串的顺序和s字符串顺序相反。<br>**ELT(N，字符串1，字符串2，字符串3，…，字符串N)**：若N &#x3D; 1，则返回值为字符串1；若N&#x3D;2，则返回值为字符串2；以此类推；若N小于1或大于参数的数目，则返回值为NULL。<br>**FIELD(s,s1,s2,…,sn)**返回字符串s在列表s1,s2,…,sn中第一次出现的位置，在找不到s的情况下，返回值为0。如果s为NULL，则返回值为0，原因是NULL不能同任何值进行同等比较。<br>**FIND_IN_SET(s1,s2)**返回字符串s1在字符串列表s2中出现的位置，字符串列表是一个由多个逗号‘,’分开的字符串组成的列表。如果s1不在s2或s2为空字符串，则返回值为0。如果任意一个参数为NULL，则返回值为NULL。这个函数在第一个参数包含一个逗号‘,’时将无法正常运行。</p><h4 id="日期函数"><a href="#日期函数" class="headerlink" title="日期函数"></a><strong>日期函数</strong></h4><p>**CURDATE()和CURRENT_DATE()**函数的作用相同，将当前日期按照‘YYYY-MM-DD’或YYYYMMDD格式的值返回。<br>**CURTIME()和CURRENT_TIME()**函数的作用相同，将当前时间以‘HH:MM:SS’或HHMMSS的格式返回。<br>**CURRENT_TIMESTAMP()、LOCALTIME()、NOW()和SYSDATE() **4个函数的作用相同，均返回当前日期和时间值，格式为‘YYYY-MM-DD HH:MM:SS’或YYYYMMDDHHMMSS，具体格式根据函数在字符串或数字语境中而定。<br>**UNIX_TIMESTAMP(date)**若无参数调用，则返回一个UNIX时间戳（‘1970-01-01 00:00:00’GMT之后的秒数）作为无符号整数。其中，GMT（Green wich mean time）为格林尼治标准时间。若用date来调用UNIX_TIMESTAMP()，它会将参数值以‘1970-01-01 00:00:00’GMT后的秒数的形式返回。date可以是一个DATE字符串、DATETIME字符串、TIMESTAMP或一个当地时间的YYMMDD或YYYYMMDD格式的数字。<br>**FROM_UNIXTIME(date)**函数把UNIX时间戳转换为普通格式的时间，与UNIX_TIMESTAMP (date)函数互为反函数。<br>**UTC_DATE()**函数返回当前UTC（世界标准时间）日期值，其格式为‘YYYY-MM-DD’或YYYYMMDD，具体格式取决于函数是否用在字符串或数字语境中。<br>**UTC_TIME()**返回当前UTC时间，格式为‘HH:MM:SS’或HHMMSS，具体格式取决于函数是否用在字符串或数字语境中。<br>**MONTHNAME(date)**函数返回日期date对应月份的英文全名。<br>**DAYNAME(d)**函数返回d对应的工作日的英文名称，例如Sunday、Monday等。<br>**DAYOFWEEK(d)**函数返回d对应的一周中的索引（位置，1表示周日，2表示周一，…，7表示周六）。<br>**WEEKDAY(d)**返回d对应的工作日索引：0表示周一，1表示周二，…，6表示周日。<br>**WEEKOFYEAR(d)**计算某天位于一年中的第几周，范围是1<del>53，相当于WEEK(d,3)。<br>**DAYOFYEAR(d)**函数返回d是一年中的第几天，范围是1</del>366。<br>**DAYOFMONTH(d)**函数返回d是一个月中的第几天，范围是1<del>31。<br>**QUARTER(date)**返回date对应的一年中的季度值，范围是1</del>4。</p><h4 id="系统函数"><a href="#系统函数" class="headerlink" title="系统函数"></a>系统函数</h4><p>**VERSION()**返回指示MySQL服务器版本的字符串。这个字符串使用utf8字符集。<br>**CONNECTION_ID()**返回MySQL服务器当前连接的次数，每个连接都有各自唯一的ID。<br><strong>SHOW PROCESSLIST</strong>命令输出当前用户的连接信息。processlist命令的输出结果显示了有哪些线程在运行，不仅可以查看当前所有的连接数，还可以查看当前的连接状态、帮助识别出有问题的查询语句等。如果是root账号，能看到所有用户的当前连接。如果是其他普通账号，则只能看到自己占用的连接。show processlist只列出前100条，如果想全部列出可使用show full processlist命令。<br>**USER()、CURRENT_USER、CURRENT_USER()、SYSTEM_USER()和SESSION_USER()**这几个函数返回当前被MySQL服务器验证的用户名和主机名组合。这个值符合确定当前登录用户存取权限的MySQL账户。一般情况下，这几个函数的返回值是相同的。<br>**CHARSET(str)**返回字符串str自变量的字符集。<br>**COLLATION(str)**返回字符串str的字符排列方式。</p><h4 id="加密函数"><a href="#加密函数" class="headerlink" title="加密函数"></a>加密函数</h4><p>**MD5(str)**为字符串算出一个MD5 128比特校验和。该值以32位十六进制数字的二进制字符串形式返回，若参数为NULL，则会返回NULL。<br>**SHA(str)**从原明文密码str计算并返回加密后的密码字符串，当参数为NULL时，返回NULL。SHA加密算法比MD5更加安全。<br>**SHA2(str, hash_length)**使用hash_length作为长度，加密str。hash_length支持的值为224、256、384、512和0。其中，0等同于256。</p><h3 id="日志"><a href="#日志" class="headerlink" title="日志"></a>日志</h3><p>MySQL日志主要分为4类，使用这些日志文件，可以查看MySQL内部发生的事情。这4类日志分别是：</p><p><strong>错误日志</strong>：记录MySQL服务的启动、运行或停止MySQL服务时出现的问题。<br><strong>查询日志</strong>：记录建立的客户端连接和执行的语句。<br><strong>二进制日志</strong>：记录所有更改数据的语句，可以用于数据复制。<br><strong>慢查询日志</strong>：记录所有执行时间超过long_query_time的所有查询或不使用索引的查询。</p><p>默认情况下，所有日志创建于MySQL数据目录中。通过刷新日志，可以强制MySQL关闭和重新打开日志文件（或者在某些情况下切换到一个新的日志）。当执行一个FLUSH LOGS语句或执行MySQLadmin flush-logs或MySQLadmin refresh时，将刷新日志。如果正使用MySQL复制功能，在复制服务器上可以维护更多日志文件，这种日志称为接替日志。启动日志功能会降低MySQL数据库的性能。例如，在查询非常频繁的MySQL数据库系统中，如果开启了通用查询日志和慢查询日志，MySQL数据库会花费很多时间记录日志。同时，日志会占用大量的磁盘空间。</p><h4 id="二进制日志"><a href="#二进制日志" class="headerlink" title="二进制日志"></a>二进制日志</h4><p>二进制日志主要记录MySQL数据库的变化。二进制日志以一种有效的格式并且是事务安全的方式包含更新日志中可用的所有信息。二进制日志包含了所有更新了数据或者已经潜在更新了数据（例如，没有匹配任何行的一个DELETE）的语句。语句以“事件”的形式保存，描述数据更改。</p><p>二进制日志还包含关于每个更新数据库的语句的执行时间信息。它不包含没有修改任何数据的语句。如果想要记录所有语句（例如，为了识别有问题的查询），需要使用一般查询日志。使用二进制日志的主要目的是最大可能地恢复数据库，因为二进制日志包含备份后进行的所有更新。</p><p>可通过show binary logs;  # 查看现有二进制日志文件，一般保存在安装目录下&#x2F;data目录下，binlog.0000编号（路径名称可以在my.ini&#x2F;cnf配置）</p><h4 id="错误日志"><a href="#错误日志" class="headerlink" title="错误日志"></a>错误日志</h4><p>错误日志文件包含了当MySQLd启动和停止时，以及服务器在运行过程中发生任何严重错误时的相关信息。在MySQL中，错误日志也是非常有用的，MySQL会将启动和停止数据库信息以及一些错误信息记录到错误日志文件中。</p><p>可通过show variables like ‘%log_error%’;  # 查看错误日志状态和位置，一般保存在安装目录下&#x2F;data目录下，服务器主机名.err（路径名称可以在my.ini&#x2F;cnf配置）</p><h4 id="通用日志"><a href="#通用日志" class="headerlink" title="通用日志"></a>通用日志</h4><p>通用查询日志记录MySQL的所有用户操作，包括启动和关闭服务、执行查询和更新语句等。本节将为读者介绍通用查询日志的启动、查看、删除等内容。通用查询日志中记录了用户的所有操作。通过查看通用查询日志，可以了解用户对MySQL进行的操作。通用查询日志记录用户的所有操作，因此在用户查询、更新频繁的情况下，通用查询日志会增长得很快。数据库管理员可以定期删除比较早的通用日志，以节省磁盘空间。</p><p>可通过show variables like ‘%general%’;  # 查看通用查询日志状态和位置，一般保存在安装目录下&#x2F;data目录下，服务器主机名.log</p><h4 id="慢查询日志"><a href="#慢查询日志" class="headerlink" title="慢查询日志"></a>慢查询日志</h4><p>慢查询日志是记录查询时长超过指定时间的日志。慢查询日志主要用来记录执行时间较长的查询语句。通过慢查询日志，可以找出执行时间较长、执行效率较低的语句，然后进行优化。</p><h3 id="优化"><a href="#优化" class="headerlink" title="优化"></a>优化</h3><p>通过对查询语句的分析，可以了解查询语句执行情况，找出查询语句执行的瓶颈，从而优化查询语句。<br>语法是explain 查询语句。主要去看type和rows（扫描的记录行数），possible_keys：指出MySQL能使用哪个索引在该表中找到行。如果该列是NULL，则没有相关的索引；key：表示查询实际使用到的索引，如果没有选择索引，该列的值是NULL；rows：显示MySQL在表中进行查询时必须检查的行数。用来对比各种查询语句是否最优。</p><h4 id="使用索引查询"><a href="#使用索引查询" class="headerlink" title="使用索引查询"></a>使用索引查询</h4><p>在使用LIKE关键字进行查询的查询语句中，如果匹配字符串的第一个字符为“%”，索引不会起作用。只有“%”不在第一个位置，索引才会起作用。</p><p>对于多列索引，只有查询条件中使用了这些字段中的第1个字段时索引才会被使用。（最左匹配原则）</p><p>查询语句的查询条件中只有OR关键字，且OR前后的两个条件中的列都是索引时，查询中才使用索引；否则，查询将不使用索引。</p><h4 id="优化子查询"><a href="#优化子查询" class="headerlink" title="优化子查询"></a>优化子查询</h4><p>执行子查询时，MySQL需要为内层查询语句的查询结果建立一个临时表。然后外层查询语句从临时表中查询记录。查询完毕后，再撤销这些临时表。</p><p>在MySQL中，可以使用连接（JOIN）查询来替代子查询。连接查询不需要建立临时表，其速度比子查询要快，如果查询中使用索引，性能会更好。连接之所以更有效率，是因为MySQL不需要在内存中创建临时表来完成查询工作。</p><h4 id="优化数据库结构"><a href="#优化数据库结构" class="headerlink" title="优化数据库结构"></a>优化数据库结构</h4><p><strong>将字段很多的表分解成多个表</strong>：对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来，形成新表。通过这种分解，可以提高表的查询效率。对于字段很多且有些字段使用不频繁的表，可以通过这种分解的方式来优化数据库的性能。</p><p><strong>增加中间表</strong>：对于需要经常联合查询的表，可以建立中间表，以提高查询效率。通过建立中间表，把需要经常联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询，以此来提高查询效率。</p><p><strong>增加冗余字段</strong>：合理地加入冗余字段可以提高查询速度。冗余字段会导致一些问题。比如，冗余字段的值在一个表中被修改了，就要想办法在其他表中更新该字段，否则就会使原本一致的数据变得不一致。从数据库性能来看，为了提高查询速度而增加少量的冗余大部分时候是可以接受的。是否通过增加冗余来提高数据库性能，这要根据实际需求综合分析。</p><p><strong>优化插入记录的速度</strong>：插入记录时，影响插入速度的主要是索引、唯一性校验、一次插入记录条数等。根据这些情况，可以分别进行优化。</p><ol><li><strong>禁用索引</strong><br>对于非空表，插入记录时，MySQL会根据表的索引对插入的记录建立索引。如果插入大量数据，建立索引会降低插入记录的速度。为了解决这种情况，可以在插入记录之前禁用索引，数据插入完毕后再开启索引。对于空表批量导入数据，则不需要进行此操作。</li><li><strong>禁用唯一性检查</strong><br>插入数据时，MySQL会对插入的记录进行唯一性校验。这种唯一性校验也会降低插入记录的速度。为了降低这种情况对查询速度的影响，可以在插入记录之前禁用唯一性检查，等到记录插入完毕后再开启。</li><li><strong>使用批量插入</strong><br>使用批量插入速度比一条一条插入速度快。</li><li><strong>使用LOAD DATA INFILE批量导入</strong><br>当需要批量导入数据时，如果能用LOAD DATA INFILE语句，就尽量使用。因为LOAD DATA INFILE语句导入数据的速度比INSERT语句快。</li><li><strong>禁用外键检查</strong><br>插入数据之前执行禁止对外键的检查，数据插入完成之后再恢复对外键的检查。</li><li><strong>禁止自动提交</strong><br>插入数据之前禁止事务的自动提交，数据导入完成之后，执行恢复自动提交操作。</li></ol><h4 id="优化MySQL服务器"><a href="#优化MySQL服务器" class="headerlink" title="优化MySQL服务器"></a><strong>优化MySQL服务器</strong></h4><p>优化MySQL服务器主要从两方面来优化：一方面是对硬件进行优化；另一方面是对MySQL服务的参数进行优化。这部分的内容需要较全面的知识，一般只有专业的数据库管理员才能进行这一类的优化。对于可以定制参数的操作系统，也可以针对MySQL进行操作系统优化。</p><h4 id="服务器语句超时处理"><a href="#服务器语句超时处理" class="headerlink" title="服务器语句超时处理"></a><strong>服务器语句超时处理</strong></h4><p>可以设置服务器语句超时的限制，单位可以达到毫秒级别。当中断的执行语句超过设置的毫秒数后，服务器将终止查询影响不大的事务或连接，然后将错误报给客户端。</p><h4 id="创建全局通用表空间"><a href="#创建全局通用表空间" class="headerlink" title="创建全局通用表空间"></a><strong>创建全局通用表空间</strong></h4><p>创建全局通用表空间，全局表空间可以被所有数据库的表共享，而且相比于独享表空间，手动创建共享表空间可以节约元数据方面的内存。可以在创建表的时候指定属于哪个表空间，也可以对已有表进行表空间修改。</p><h4 id="支持不可见索引"><a href="#支持不可见索引" class="headerlink" title="支持不可见索引"></a><strong>支持不可见索引</strong></h4><p>不可见索引的特性对于性能调试非常有用。在MySQL 8.0中，索引可以被“隐藏”和“显示”。当一个索引被隐藏时，它不会被查询优化器所使用。也就是说，管理员可以隐藏一个索引，然后观察对数据库的影响。如果数据库性能有所下降，就说明这个索引是有用的，于是将其“恢复显示”即可；如果数据库性能看不出变化，说明这个索引是多余的，可以删掉了。当索引被隐藏时，它的内容仍然是和正常索引一样实时更新的。</p><h4 id="使用查询缓冲区"><a href="#使用查询缓冲区" class="headerlink" title="使用查询缓冲区"></a><strong>使用查询缓冲区</strong></h4><p>查询缓冲区可以提高查询的速度，但是这种方式只适合查询语句比较多、更新语句比较少的情况。默认情况下查询缓冲区的大小为0，也就是不可用。可以修改query_cache_size以调整查询缓冲区大小，修改query_cache_type以调整查询缓冲区的类型。在my.ini中修改query_cache_size和query_cache_type的值。</p><p>query_cache_type&#x3D;1表示开启查询缓冲区。只有在查询语句中包含SQL_NO_CACHE关键字时，才不会使用查询缓冲区。可以使用FLUSH QUERY CACHE语句来刷新缓冲区，清理查询缓冲区中的碎片。</p><h3 id="主从复制"><a href="#主从复制" class="headerlink" title="主从复制"></a>主从复制</h3><p>在MySQL中，复制操作是异步进行的，slaves服务器不需要持续地保持连接接收master服务器的数据。</p><p>MySQL支持一台主服务器同时向多台从服务器进行复制操作，从服务器同时可以作为其他从服务器的主服务器，如果MySQL主服务器访问量比较大，可以通过复制数据，然后在从服务器上进行查询操作，从而降低主服务器的访问压力，同时从服务器作为主服务器的备份，可以避免主服务器因为故障数据丢失的问题。</p><p>MySQL数据库复制操作大致可以分成3个步骤：<br>步骤01　主服务器将数据的改变记录到二进制日志（binary log）中。<br>步骤02　从服务器将主服务器的binary log events复制到它的中继日志（relay log）中。<br>步骤03　从服务器重做中继日志中的事件，将数据的改变与从服务器保持同步。</p><p>首先，主服务器会记录二进制日志，每个事务更新数据完成之前，主服务器将这些操作的信息记录在二进制日志里面，在事件写入二进制日志完成后，主服务器通知存储引擎提交事务。</p><p>Slave上面的I&#x2F;O进程连接上Master，并发出日志请求，Master接收到来自Slave的IO进程的请求后，根据请求信息添加位置信息后，返回给Slave的IO进程。返回信息中除了日志所包含的信息之外，还包括本次返回的信息已经到Master端的bin-log文件的名称以及bin-log的位置。</p><p>Slave的I&#x2F;O进程接收到信息后，将接收到的日志内容依次添加到Slave端的relay-log文件的最末端，并将读取到Master端的bin-log文件名和位置记录到master-info文件中。</p><p>Slave的Sql进程检测到relay-log中新增加了内容后，会马上解析relay-log的内容成为在Master端真实执行时的那些可执行内容，并在自身执行。</p><h3 id="读写分离"><a href="#读写分离" class="headerlink" title="读写分离"></a>读写分离</h3><p>阿里云可设置<a href="https://help.aliyun.com/document_detail/96073.html?spm=a2c4g.11174283.2.54.19ff5b8393N8Jz">读写分离</a>，使用数据库独享代理，达到写操作自动发送到主实例，读操作则发送到只读实例，若有多个只读实例可分配权重，按时间收费。</p>]]></content>
      
      
      <categories>
          
          <category> Language </category>
          
      </categories>
      
      
        <tags>
            
            <tag> SQL </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Windows常用命令</title>
      <link href="/2019/08/17/Language/Windows%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/2019/08/17/Language/Windows%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<p>Windows常用操作命令。</p><span id="more"></span><h3 id="Windows关机命令"><a href="#Windows关机命令" class="headerlink" title="Windows关机命令"></a>Windows关机命令</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">立即关机</span></span><br><span class="line">shutdown -s</span><br><span class="line"><span class="meta"># </span><span class="language-bash">十五分钟后关机单位是秒，默认为30s</span></span><br><span class="line">shutdown -s -t 900</span><br><span class="line"><span class="meta"># </span><span class="language-bash">立即重启电脑</span></span><br><span class="line">shutdown -r -t 0</span><br><span class="line"><span class="meta"># </span><span class="language-bash">关闭定时关机计划</span></span><br><span class="line">shutdown -a</span><br></pre></td></tr></table></figure><h3 id="VM虚拟机端口映射"><a href="#VM虚拟机端口映射" class="headerlink" title="VM虚拟机端口映射"></a>VM虚拟机端口映射</h3><p>主要解决的是以供另外一台主机连接当前主机内的虚拟机，思路是将当前主机下的所有虚拟机的22端口全部映射到当前主机的不同端口，这样另外一台主机直接连接当前主机的映射端口即可达到远程登录虚拟机的效果了。这里的当前主机指的是Windows及Windows下的vmware虚拟机</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">查看已有的所有端口映射</span></span><br><span class="line">netsh interface portproxy show all</span><br><span class="line"><span class="meta"># </span><span class="language-bash">端口映射，将192.168.150.128虚拟机的22端口映射到主机的9000端口上并监听所有访问地址</span></span><br><span class="line">netsh interface portproxy add v4tov4 listenaddress=0.0.0.0 listenport=8032 connectaddress=node01 connectport=8032</span><br><span class="line"><span class="meta"># </span><span class="language-bash">删除已有的映射关系，只需删除监听的这个端口</span></span><br><span class="line">netsh interface portproxy delete v4tov4 listenaddress=0.0.0.0 listenport=9000</span><br><span class="line"><span class="meta"># </span><span class="language-bash">另外一台主机连接当前主机虚拟机，假设当前主机IP地址为192.168.192.195，root为虚拟机用户名，密码为虚拟机密码</span></span><br><span class="line">ssh root@192.168.192.195</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">添加多个端口映射</span></span><br><span class="line">netsh interface portproxy add v4tov4 listenaddress=0.0.0.0 listenport=9201 connectaddress=192.168.163.100 connectport=9200</span><br><span class="line">netsh interface portproxy add v4tov4 listenaddress=0.0.0.0 listenport=9202 connectaddress=192.168.163.110 connectport=9200</span><br><span class="line">netsh interface portproxy add v4tov4 listenaddress=0.0.0.0 listenport=9203 connectaddress=192.168.163.120 connectport=9200</span><br><span class="line">netsh interface portproxy add v4tov4 listenaddress=0.0.0.0 listenport=5601 connectaddress=192.168.163.100 connectport=5601</span><br><span class="line"><span class="meta"># </span><span class="language-bash">添加防火墙规则</span></span><br><span class="line">netsh advfirewall firewall add rule name=&quot;node01 9200&quot; dir=in action=allow protocol=tcp localport=9201</span><br><span class="line">netsh advfirewall firewall add rule name=&quot;node02 9200&quot; dir=in action=allow protocol=tcp localport=9202</span><br><span class="line">netsh advfirewall firewall add rule name=&quot;node03 9200&quot; dir=in action=allow protocol=tcp localport=9203</span><br><span class="line">netsh advfirewall firewall add rule name=&quot;node01 5601&quot; dir=in action=allow protocol=tcp localport=5601</span><br></pre></td></tr></table></figure><h3 id="Windows-curl"><a href="#Windows-curl" class="headerlink" title="Windows curl"></a>Windows curl</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Invoke-WebRequest http://localhost:5601 -Method GET  -UseBasicParsing</span><br></pre></td></tr></table></figure><h3 id="Windows端口管理"><a href="#Windows端口管理" class="headerlink" title="Windows端口管理"></a>Windows端口管理</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">netstat -ano  # 查看所有端口占用情况</span><br><span class="line">netstat -aon|findstr &quot;5601&quot;  # 查询指定端口的占用情况</span><br><span class="line">tasklist|findstr &quot;3372&quot;  # 查看PID对应的进程</span><br><span class="line">taskkill /f /t /im javaw.exe  # 强制终止该进程及子进程（一定得搞清楚进程作用再终止！！！）</span><br></pre></td></tr></table></figure><h3 id="查看Windows内存"><a href="#查看Windows内存" class="headerlink" title="查看Windows内存"></a>查看Windows内存</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">systeminfo  # 查看Windows基本信息以及内存使用情况</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Language </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Windows </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux常用命令</title>
      <link href="/2019/08/07/Language/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/"/>
      <url>/2019/08/07/Language/Linux%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/</url>
      
        <content type="html"><![CDATA[<p>Linux常用操作命令。</p><span id="more"></span><h2 id="文件操作"><a href="#文件操作" class="headerlink" title="文件操作"></a>文件操作</h2><h3 id="文件夹-x2F-文件操作"><a href="#文件夹-x2F-文件操作" class="headerlink" title="文件夹&#x2F;文件操作"></a>文件夹&#x2F;文件操作</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 新建文件夹</span></span><br><span class="line"><span class="built_in">mkdir</span> 文件名</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除文件夹/文件：</span></span><br><span class="line"><span class="built_in">rm</span> -rf 文件名/文件</span><br><span class="line"></span><br><span class="line"><span class="comment"># 移动文件夹/文件：</span></span><br><span class="line"><span class="built_in">mv</span> 原路径 新路径（也可用于修改文件名）</span><br><span class="line"></span><br><span class="line"><span class="comment"># 复制文件夹/文件：</span></span><br><span class="line"><span class="built_in">cp</span> 原路径 新路径</span><br><span class="line"></span><br><span class="line"><span class="comment"># 显示文件内容（英文全拼：concatenate）命令用于连接文件并打印到标准输出设备上：</span></span><br><span class="line"><span class="built_in">cat</span> 文件名</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分页查看文件，操作命令和vim基本相同</span></span><br><span class="line">less 文件名</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件状态</span></span><br><span class="line"><span class="built_in">stat</span> 文件夹</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置文件所有者：（将当前前目录下的所有文件与子目录的拥有者皆设为 runoob，群体的使用者 runoobgroup）；</span></span><br><span class="line"><span class="built_in">chown</span> -R runoob:runoobgroup</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看文件权限：</span></span><br><span class="line"><span class="built_in">ls</span> -ld 文件名称</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改用户对文件的权限：（用数字来表示权限）三个数字分别表示User、Group、及Other的权限。r=4，w=2，x=1</span></span><br><span class="line"><span class="comment"># 若要 rwx 属性则 4+2+1=7</span></span><br><span class="line"><span class="comment"># 若要 rw- 属性则 4+2=6</span></span><br><span class="line"><span class="comment"># 若要 r-x 属性则 4+1=5</span></span><br><span class="line"><span class="built_in">chmod</span> 777 file</span><br><span class="line"></span><br><span class="line"><span class="comment"># 远程复制：scp 是 linux 系统下基于 ssh 登陆进行安全的远程文件拷贝命令。（同一局域网或者与远程服务器）</span></span><br><span class="line"><span class="comment"># 若是复制到目标主机的同目录下，则目标主机后的地址写$PWD</span></span><br><span class="line">scp local_file remote_username@remote_ip:remote_folder；</span><br><span class="line"></span><br><span class="line"><span class="comment"># 添加环境变量：</span></span><br><span class="line">vim /etc/profile</span><br><span class="line"><span class="comment"># 在结尾添加export PATH=&quot;$PATH:/home/software/cmake-3.19.0-Linux-x86_64/bin&quot;</span></span><br><span class="line"><span class="comment"># esc + :wq保存退出</span></span><br><span class="line"><span class="comment"># 使配置生效</span></span><br><span class="line"><span class="built_in">source</span> /etc/profile</span><br></pre></td></tr></table></figure><h3 id="find文件查找"><a href="#find文件查找" class="headerlink" title="find文件查找"></a>find文件查找</h3><ul><li>-maxdepth level 最大搜索目录深度</li><li>-mindepth level 最小搜索目录深度</li><li>-depth 先处理目录下的文件再处理目录本身</li><li>-name 文件名称 根据文件名查找，支持通配符，使用时要用引号括起来</li><li>-iname 文件名称 不区分大小写</li><li>-regex PATTERN 以PATTERN匹配整个文件路径，而非文件名称</li><li>-user USERNAME 查找属主为指定用户的文件</li><li>-group GROUPNAME 查找属组为指定组的文件</li><li>-uid UserID 查找属主为指定UID的文件</li><li>-gid GroupID 查找属组为指定GID的文件</li><li>-nouser 查找没有属主的文件</li><li>-nogroup 查找没有属组的文件</li><li>-type TYPE 根据文件类型查找（f普通文件、d目录文件、l符号链接文件、s套接字文件、b块设备文件、c字符设备文件、p管道文件）</li><li>-empty 空文件或目录</li><li>-a 与，默认多个条件就是与关系</li><li>-o 或</li><li>-not 非，或者使用!</li><li>-prune 排除目录</li><li>-size [+|-]UNIT 根据文件大小查找，常用大小k、M、G、c；6k表示(5k,6k]，-6k表示[0,5k]，+6k表示(6k,∞)</li><li>-mtime [+|-] DAY 根据修改或创建时间查找文件，单位为天</li><li>-mmin [+|-] MINUTE 根据分钟查找文件</li><li>-perm MODE 根据权限查找</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">find /etc -maxdepth 2 mindepth 2</span><br><span class="line">find -regex <span class="string">&quot;.*\.txt$&quot;</span>  <span class="comment"># 查找.txt结尾</span></span><br><span class="line">find -maxdepth 1 -name <span class="string">&quot;*.md&quot;</span>  <span class="comment"># 查找当前目录下.md结尾</span></span><br><span class="line">find /home -<span class="built_in">type</span> d -<span class="built_in">ls</span>  <span class="comment"># 查看/home下的所有目录</span></span><br><span class="line">find /home -<span class="built_in">type</span> d -empty  <span class="comment"># 查找空文件或目录</span></span><br><span class="line">find /etc -<span class="built_in">type</span> d -o -<span class="built_in">type</span> l | <span class="built_in">wc</span> -l  <span class="comment"># 或者</span></span><br><span class="line">find /etc -path <span class="string">&#x27;/etc/security&#x27;</span> -a -prune -o -name <span class="string">&quot;*.conf&quot;</span>  <span class="comment"># 排除目录</span></span><br><span class="line">find / \( -path <span class="string">&quot;/sys&quot;</span> -o -path <span class="string">&quot;/proc&quot;</span> \) -a -prune -o -<span class="built_in">type</span> f -a -mmin -1  <span class="comment"># 排除/proc和/sys目录</span></span><br><span class="line">find /var/log/ -mtime +3 -<span class="built_in">type</span> f -<span class="built_in">print</span>  <span class="comment"># 查找三天前被改动的文件</span></span><br><span class="line">find /var/log/ -mtime -3 -<span class="built_in">type</span> f -<span class="built_in">print</span>  <span class="comment"># 查找三天内被改动的文件</span></span><br><span class="line">find /var/log/ -mtime 3 -<span class="built_in">type</span> f -<span class="built_in">print</span>  <span class="comment"># 查找第三天被改动的文件</span></span><br><span class="line">find -name <span class="string">&quot;*.md&quot;</span> -newermt <span class="string">&#x27;2022-09-02&#x27;</span> ! -newermt <span class="string">&#x27;2022-09-06&#x27;</span>  <span class="comment"># 查找一段时间内的文件</span></span><br><span class="line">find -mmin -1  <span class="comment"># 查找一分钟内被改动的文件</span></span><br><span class="line">find -maxdepth 1 -perm 644  <span class="comment"># 查找当前目录下权限为644的文件</span></span><br></pre></td></tr></table></figure><h3 id="参数替换xargs"><a href="#参数替换xargs" class="headerlink" title="参数替换xargs"></a>参数替换xargs</h3><p>由于很多命令不支持管道|来传递参数，xargs用于产生某个命令的参数，xargs 可以读入 stdin 的数据，并且以空格符或回车符将 stdin 的数据分隔成为参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">seq</span> 10 | xargs</span><br><span class="line"><span class="built_in">ls</span> <span class="string">&quot;*.log&quot;</span> | xargs <span class="built_in">rm</span> -rf  <span class="comment"># 删除当前目录下的指定文件，谨慎使用</span></span><br><span class="line">find /bin/ -perm /7000 | xargs <span class="built_in">ls</span> -Sl  <span class="comment"># 查找有特殊权限的文件并排序</span></span><br></pre></td></tr></table></figure><h3 id="硬链接软连接"><a href="#硬链接软连接" class="headerlink" title="硬链接软连接"></a>硬链接软连接</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ln创建链接</span></span><br><span class="line"><span class="built_in">ln</span> test1 test2  <span class="comment"># 默认建立硬链接，两个文件同步，删除一个另外一个照样存在</span></span><br><span class="line"><span class="built_in">ln</span> -s test1 test3  <span class="comment"># 加s参数创建软链接，两个文件同步，删除test1后test3也不可访问</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于不允许创建硬链接的文件也可使用cp -l命令也可达到创建硬链接的效果</span></span><br><span class="line"><span class="built_in">cp</span> -rl /var/lib/docker/volumes/wordpress_db/_data/* ./db</span><br></pre></td></tr></table></figure><h3 id="重定向"><a href="#重定向" class="headerlink" title="重定向"></a>重定向</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将命令执行后在控制台打印的结果和错误重定向到黑洞，全扔掉</span></span><br><span class="line"><span class="built_in">ls</span> 1&gt; /dev/null  <span class="comment"># 1表示控制台打印的结果，2表示错误信息</span></span><br><span class="line"><span class="built_in">ls</span> &gt; /dev/null 2&gt;&amp;1  <span class="comment"># 简写，将所有结果都重定向到黑洞</span></span><br><span class="line"><span class="built_in">ls</span> &gt; /dev/null 2&gt;&amp;1 &amp;  <span class="comment"># 让程序后台执行，不会阻塞控制台</span></span><br><span class="line"><span class="built_in">ls</span> &gt;&gt; tmp.log 2&gt;&amp;1 &amp;  <span class="comment"># 将结果打印到日志文件</span></span><br><span class="line"><span class="built_in">ls</span> &gt;&gt; stdout.log 2&gt; stderr.log  <span class="comment"># 分别重定向</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 多行重定向，将多行结果写入文件</span></span><br><span class="line"><span class="built_in">cat</span> &gt; <span class="built_in">test</span> &lt;&lt; <span class="string">EOF</span></span><br><span class="line"><span class="string">        This is line 1 of the message.</span></span><br><span class="line"><span class="string">        This is line 2 of the message.</span></span><br><span class="line"><span class="string">        This is line 3 of the message.</span></span><br><span class="line"><span class="string">        This is line 4 of the message.</span></span><br><span class="line"><span class="string">        This is the last line of the message.</span></span><br><span class="line"><span class="string">       </span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 重定向中的-</span></span><br><span class="line">wget -O - http://www.wangxiaochun.com/testdir/hello.sh  <span class="comment"># 不加-符号会把这个文件下载到当前文件夹，但是加上-会作为stdout输出</span></span><br><span class="line">tar -cvf - /home | tar -xvf -  <span class="comment"># 将/home下的文件打包，但不是打包到文件而是打包到stdout通过管道符传给后面的命令，后面的-则用来接收管道符前的stdout</span></span><br></pre></td></tr></table></figure><h3 id="转换或删除字符tr"><a href="#转换或删除字符tr" class="headerlink" title="转换或删除字符tr"></a>转换或删除字符tr</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">tr</span> <span class="string">&#x27;a-z&#x27;</span> <span class="string">&#x27;A-Z&#x27;</span> &lt; /etc/issue  <span class="comment"># 该命令会把/etc/issue中的小写字符都转换成大写字符</span></span><br><span class="line"><span class="built_in">tr</span> –d abc &lt; /etc/fstab  <span class="comment"># 删除fstab文件中的所有abc中任意字符</span></span><br><span class="line"><span class="built_in">tr</span> -s <span class="string">&#x27;-&#x27;</span> <span class="string">&#x27;,&#x27;</span> &lt; <span class="built_in">test</span>  <span class="comment"># 将连续的-字符替换为一个单独的,字符，类似于去重后替换</span></span><br><span class="line"><span class="built_in">tr</span> -d <span class="string">&#x27;-&#x27;</span> &lt; <span class="built_in">test</span>  <span class="comment"># 删除所有的-字符</span></span><br><span class="line"><span class="built_in">tr</span> -d <span class="string">&#x27;\r&#x27;</span> &lt; windows.txt &gt; windows2.txt  <span class="comment"># 将 Windows 的文本转化 Linux的文本格式</span></span><br></pre></td></tr></table></figure><h3 id="下载wget"><a href="#下载wget" class="headerlink" title="下载wget"></a>下载wget</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">wget http://cn.wordpress.org/wordpress-3.1-zh_CN.zip  <span class="comment"># 下载文件保存到当前目录</span></span><br><span class="line">wget -O wordpress.zip http://www.centos.bz/download.php?<span class="built_in">id</span>=1080  <span class="comment"># 下载资源并保存到wordpress.zip</span></span><br><span class="line">wget -c http://cn.wordpress.org/wordpress-3.1-zh_CN.zip  <span class="comment"># 断点续传</span></span><br><span class="line">wget -b http://cn.wordpress.org/wordpress-3.1-zh_CN.zip  <span class="comment"># 后台下载</span></span><br><span class="line">wget –tries=40 http://cn.wordpress.org/wordpress-3.1-zh_CN.zip  <span class="comment"># 失败最大重试</span></span><br><span class="line">wget -i list.txt  <span class="comment"># 下载多个文件，list.txt保存多个链接</span></span><br><span class="line">wget –ftp-user=USERNAME –ftp-password=PASSWORD url  <span class="comment"># 使用wget用户名和密码认证的ftp下载</span></span><br></pre></td></tr></table></figure><h3 id="压缩和解压缩"><a href="#压缩和解压缩" class="headerlink" title="压缩和解压缩"></a>压缩和解压缩</h3><h4 id="gzip-gunzip"><a href="#gzip-gunzip" class="headerlink" title="gzip gunzip"></a>gzip gunzip</h4><p>gzip [OPTION]… FILE …</p><ul><li>-k keep，保留原文件，Centos8</li><li>-d 解压缩，相当于gunzip</li><li>-c 结果输出至标准输出，保留原文件</li><li>-# 指定压缩比，取值为1-9</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">gunzip file.gz  <span class="comment"># 解压文件</span></span><br><span class="line">zcat file.gz  <span class="comment"># 不显式解压提前查看文件内容</span></span><br><span class="line">gzip -c messages &gt; messages.gz</span><br><span class="line">gzip -c -d messages.gz &gt; messages</span><br><span class="line">zcat messages.gz &gt; messages</span><br><span class="line"><span class="built_in">cat</span> messages | gzip &gt; m.gz</span><br></pre></td></tr></table></figure><h4 id="bzip2-bunzip2"><a href="#bzip2-bunzip2" class="headerlink" title="bzip2 bunzip2"></a>bzip2 bunzip2</h4><p>bzip2 [OPTION]… FILE …</p><ul><li>-k keep，保留原文件</li><li>-d 解压缩</li><li>-c 结果输出至标准输出，保留原文件</li><li>-# 指定压缩比，取值为1-9</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">bunzip2 file.bz2  <span class="comment"># 解压缩</span></span><br><span class="line">bzcat file.bz2  <span class="comment"># 不显式解压提前查看文件内容</span></span><br></pre></td></tr></table></figure><h4 id="zip-unzip"><a href="#zip-unzip" class="headerlink" title="zip unzip"></a>zip unzip</h4><p>zip 可以实现打包目录和多个文件成一个文件并压缩，但可能会丢失文件属性信息，如：所有者和组信息，一般建议使用 tar 代替</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">zip  -r sysconfig.zip /etc/sysconfig/  <span class="comment"># 打包并压缩</span></span><br><span class="line">unzip sysconfig.zip  <span class="comment"># 解压</span></span><br><span class="line">unzip sysconfig.zip -d /tmp/config  <span class="comment"># 解压缩至指定目录,如果指定目录不存在，会在其父目录（必须事先存在）下自动生成</span></span><br></pre></td></tr></table></figure><h4 id="tar"><a href="#tar" class="headerlink" title="tar"></a>tar</h4><p>涉及参数太多，仅列举常用的操作</p><ul><li>-z 相当于gzip压缩工具</li><li>-j 相当于bzip2压缩工具</li><li>-J 相当于xz压缩工具</li><li>–exclude 排除文件<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">tar -cpvf /PATH/FILE.tar FILE...  <span class="comment">#  创建归档，保留权限</span></span><br><span class="line">tar -rf /PATH/FILE.tar FILE...  <span class="comment"># 追加文件至归档： 注：不支持对压缩文件追加</span></span><br><span class="line">tar -t -f /PATH/FILE.tar  <span class="comment"># 查看归档文件中的文件列表</span></span><br><span class="line">tar zcvf /root/a.tgz  --exclude=/app/host1 --exclude=/app/host2 /app</span><br><span class="line">tar zxvf test.tar.gz  <span class="comment"># 解压</span></span><br><span class="line">tar zxvf test.tar.gz -C /tmp  <span class="comment"># 解压到指定目录</span></span><br><span class="line">tar zcvf test.tar.gz <span class="built_in">test</span>/ <span class="comment"># 压缩</span></span><br><span class="line">tar -xvf mysql-8.0.22-1.el8.x86_64.rpm-bundle.tar  <span class="comment"># 解压tar文件</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="切割文件split"><a href="#切割文件split" class="headerlink" title="切割文件split"></a>切割文件split</h3><p>将一个文件分割为数个</p><ul><li>-行数 指定多少行切分为一个小文件</li><li>-b字节 指定多少字节切分为一个小文件</li><li>-C字节 与-b相似，但是在切分时尽量维持每行的完整性</li><li>[输出文件名] 设置切割后的文件名前缀<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">split</span> -6 <span class="built_in">test</span>  <span class="comment"># 按行数切割</span></span><br><span class="line"><span class="built_in">split</span> -C1024 <span class="built_in">test</span> prefix-  <span class="comment"># 指定前缀</span></span><br></pre></td></tr></table></figure></li></ul><h2 id="用户和用户组"><a href="#用户和用户组" class="headerlink" title="用户和用户组"></a>用户和用户组</h2><h3 id="主要配置文件"><a href="#主要配置文件" class="headerlink" title="主要配置文件"></a>主要配置文件</h3><ul><li>&#x2F;etc&#x2F;passwd：用户及其属性信息(名称、UID、主组ID等）</li><li>&#x2F;etc&#x2F;shadow：用户密码及其相关属性</li><li>&#x2F;etc&#x2F;group：组及其属性信息</li><li>&#x2F;etc&#x2F;gshadow：组密码及其相关属性</li></ul><h3 id="管理用户命令"><a href="#管理用户命令" class="headerlink" title="管理用户命令"></a>管理用户命令</h3><h4 id="useradd新增用户"><a href="#useradd新增用户" class="headerlink" title="useradd新增用户"></a>useradd新增用户</h4><ul><li>-u UID </li><li>-o 配合-u 选项，不检查UID的唯一性</li><li>-g GID 指明用户所属基本组，可为组名，也可以GID</li><li>-c “COMMENT“ 用户的注释信息</li><li>-d HOME_DIR 以指定的路径(不存在)为家目录</li><li>-s SHELL 指明用户的默认shell程序，可用列表在&#x2F;etc&#x2F;shells文件中</li><li>-G GROUP1[,GROUP2,…] 为用户指明附加组，组须事先存在</li><li>-N 不创建私用组做主组，使用users组做主组</li><li>-r 创建系统用户 CentOS 6之前: ID&lt;500，CentOS7 以后: ID&lt;1000</li><li>-m 创建家目录，用于系统用户</li><li>-M 不创建家目录，用于非系统用户</li><li>-p 指定加密的密码</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">useradd -r -u 48 -g apache -s /sbin/nologin -d /var/www -c <span class="string">&quot;Apache&quot;</span> apache</span><br></pre></td></tr></table></figure><h4 id="usermod修改用户属性"><a href="#usermod修改用户属性" class="headerlink" title="usermod修改用户属性"></a>usermod修改用户属性</h4><ul><li>-u UID: 新UID</li><li>-g GID: 新主组</li><li>-G GROUP1[,GROUP2,…[,GROUPN]]]：新附加组，原来的附加组将会被覆盖；若保留原有，则要同时使用-a选项</li><li>-s SHELL：新的默认SHELL</li><li>-c ‘COMMENT’：新的注释信息</li><li>-d HOME: 新家目录不会自动创建；若要创建新家目录并移动原家数据，同时使用-m选项</li><li>-l login_name: 新的名字</li><li>-L: lock指定用户,在&#x2F;etc&#x2F;shadow 密码栏的增加 ! </li><li>-U: unlock指定用户,将 &#x2F;etc&#x2F;shadow 密码栏的 ! 拿掉</li><li>-e YYYY-MM-DD: 指明用户账号过期日期</li><li>-f INACTIVE: 设定非活动期限，即宽限期</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">usermod -d /home/hnlinux root  <span class="comment"># 修改用户登录目录</span></span><br></pre></td></tr></table></figure><h4 id="passwd修改用户密码"><a href="#passwd修改用户密码" class="headerlink" title="passwd修改用户密码"></a>passwd修改用户密码</h4><ul><li>-d：删除指定用户密码</li><li>-l：锁定指定用户</li><li>-u：解锁指定用户</li><li>-e：强制用户下次登录修改密码</li><li>-f：强制操作</li><li>-n mindays：指定最短使用期限</li><li>-x maxdays：最大使用期限</li><li>-w warndays：提前多少天开始警告</li><li>-i inactivedays：非活动期限</li><li>–stdin：从标准输入接收用户密码,Ubuntu无此选项<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">passwd  <span class="comment"># 修改当前用户密码</span></span><br><span class="line"><span class="built_in">echo</span> 123456 | passwd <span class="built_in">test</span> --stdin  <span class="comment"># 给指定用户设置密码，注意debian没有stdin参数</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="其他命令"><a href="#其他命令" class="headerlink" title="其他命令"></a>其他命令</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># userdel删除linux用户</span></span><br><span class="line">userdel -rf <span class="built_in">test</span>  <span class="comment"># r表示强制删除，f表示删除家目录和邮箱</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># id可以查看用户的ID信息</span></span><br><span class="line"><span class="built_in">id</span> root</span><br><span class="line"></span><br><span class="line"><span class="comment"># su切换用户</span></span><br><span class="line">su - hdfs  <span class="comment"># 切换到hdfs用户</span></span><br><span class="line">su - hdfs -c <span class="string">&quot;ls ~&quot;</span>  <span class="comment"># 使用指定的用户执行命令，但不切换用户</span></span><br></pre></td></tr></table></figure><h3 id="管理用户组命令"><a href="#管理用户组命令" class="headerlink" title="管理用户组命令"></a>管理用户组命令</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># groupadd创建组</span></span><br><span class="line">groupadd <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># groupmod修改组名称</span></span><br><span class="line">groupmod -n newtest <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># groupdel删除组</span></span><br><span class="line">groupdel -f <span class="built_in">test</span>  <span class="comment"># 强制删除，即使是用户的主组也强制删除组,但会导致无主组的用户不可用无法登录</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># gpasswd，可以修改组密码也可以修改组的成员关系</span></span><br><span class="line"><span class="comment"># -a user 将user添加至指定组中</span></span><br><span class="line"><span class="comment"># -d user 从指定附加组中移除用户user</span></span><br><span class="line"><span class="comment"># -A user1,user2,... 设置有管理权限的用户列表</span></span><br><span class="line">gpasswd -a wang admins  <span class="comment"># 将wang用户加入admins组</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># groupmems 可以管理附加组的成员关系</span></span><br><span class="line"><span class="comment"># -g, --group groupname   #更改为指定组 (只有root)</span></span><br><span class="line"><span class="comment"># -a, --add username     #指定用户加入组</span></span><br><span class="line"><span class="comment"># -d, --delete username #从组中删除用户</span></span><br><span class="line"><span class="comment"># -p, --purge               #从组中清除所有成员</span></span><br><span class="line"><span class="comment"># -l,  --list                 #显示组成员列表</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># groups查看用户组关系</span></span><br><span class="line"><span class="built_in">groups</span> root</span><br></pre></td></tr></table></figure><h3 id="文件权限"><a href="#文件权限" class="headerlink" title="文件权限"></a>文件权限</h3><p>每个文件针对三类访问者有三种不同的权限：</p><p>三类访问者分别是：文件所有用户，用户所有组和其他用户；三种权限分别是读、写和执行，对应的三个字母是r、w、x，对应的数字分别是4、2、1。给文件分配权限可以按照不同权限的组合相加分配给不同的访问者，比如777就是三类访问者都可以读写和执行。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 设置文件的所有者chown</span></span><br><span class="line"><span class="comment"># chown [OPTION]... [OWNER][:[GROUP]] FILE...</span></span><br><span class="line"><span class="comment"># chown [OPTION]... --reference=RFILE FILE...</span></span><br><span class="line"><span class="comment"># OWNER   #只修改所有者</span></span><br><span class="line"><span class="comment"># OWNER:GROUP #同时修改所有者和属组</span></span><br><span class="line"><span class="comment"># :GROUP   #只修改属组，冒号也可用 . 替换</span></span><br><span class="line"><span class="comment"># --reference=RFILE  #参考指定的的属性，来修改   </span></span><br><span class="line"><span class="comment"># -R #递归修改</span></span><br><span class="line"><span class="built_in">chown</span> -R <span class="built_in">test</span>:<span class="built_in">test</span> /root/tmp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置文件的属组信息chgrp</span></span><br><span class="line"><span class="built_in">chgrp</span> <span class="built_in">test</span> /root/tmp  <span class="comment"># chgrp只可以修改文件的所属组</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改文件权限chmod</span></span><br><span class="line"><span class="built_in">chmod</span> -R 755 /root/tmp</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设定文件特殊权限</span></span><br><span class="line">chattr +i file  <span class="comment"># 不能删除、改名、更改</span></span><br><span class="line">chattr +a file  <span class="comment"># 只能追加，不能删除、改名</span></span><br><span class="line">lsattr  <span class="comment"># 显示特殊属性</span></span><br></pre></td></tr></table></figure><h3 id="ACL设置"><a href="#ACL设置" class="headerlink" title="ACL设置"></a>ACL设置</h3><p>setfacl设定ACL权限</p><ul><li>-m：设定 ACL 权限。如果是给予用户 ACL 权限，则使用”u:用户名：权限”格式赋予；如果是给予组 ACL 权限，则使用”g:组名：权限” 格式赋予；</li><li>-x：删除指定的 ACL 权限；</li><li>-b：删除所有的 ACL 权限；</li><li>-d：设定默认 ACL 权限。只对目录生效，指目录中新建立的文件拥有此默认权限；</li><li>-k：删除默认 ACL 权限；</li><li>-R：递归设定 ACL 权限。指设定的 ACL 权限会对目录下的所有子文件生效；<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">setfacl -m u:<span class="built_in">test</span>:rx /root/tmp  <span class="comment"># 给test用户设置tmp目录的ACL权限</span></span><br><span class="line">setfacl -m g:tgroup2:rwx /root/tmp  <span class="comment"># 给用户组分配ACL权限</span></span><br><span class="line">setfacl -m d:u:<span class="built_in">test</span>:rw /root/tmp/  <span class="comment"># 给目录设置默认权限，这样目录内的新建文件也拥有ACL权限</span></span><br><span class="line">setfacl -x u:<span class="built_in">test</span> /root/tmp  <span class="comment"># 删除指定用户的ACL权限</span></span><br><span class="line">setfacl -b /root/tmp  <span class="comment"># 删除所有的ACL权限</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># getfacl获取文件的ACL权限</span></span><br><span class="line">getfacl /root/tmp</span><br></pre></td></tr></table></figure></li></ul><h2 id="文件处理"><a href="#文件处理" class="headerlink" title="文件处理"></a>文件处理</h2><h3 id="VIM"><a href="#VIM" class="headerlink" title="VIM"></a>VIM</h3><h4 id="行号"><a href="#行号" class="headerlink" title="行号"></a>行号</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">显示：<span class="built_in">set</span> number，简写 <span class="built_in">set</span> nu</span><br><span class="line">取消显示：<span class="built_in">set</span> nonumber, 简写 <span class="built_in">set</span> nonu</span><br></pre></td></tr></table></figure><h4 id="忽略字符的大小写"><a href="#忽略字符的大小写" class="headerlink" title="忽略字符的大小写"></a>忽略字符的大小写</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">启用：<span class="built_in">set</span> ignorecase，简写 <span class="built_in">set</span> ic</span><br><span class="line">不忽略：<span class="built_in">set</span> noic</span><br></pre></td></tr></table></figure><h4 id="自动缩进"><a href="#自动缩进" class="headerlink" title="自动缩进"></a>自动缩进</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">启用：<span class="built_in">set</span> autoindent，简写 <span class="built_in">set</span> ai</span><br><span class="line">禁用：<span class="built_in">set</span> noai</span><br></pre></td></tr></table></figure><h4 id="复制保留格式"><a href="#复制保留格式" class="headerlink" title="复制保留格式"></a>复制保留格式</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">启用：<span class="built_in">set</span> <span class="built_in">paste</span></span><br><span class="line">禁用：<span class="built_in">set</span> nopaste</span><br></pre></td></tr></table></figure><h4 id="显示Tab-I和换行符和-显示"><a href="#显示Tab-I和换行符和-显示" class="headerlink" title="显示Tab ^I和换行符和$显示"></a>显示Tab ^I和换行符和$显示</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">启用：<span class="built_in">set</span> list</span><br><span class="line">禁用：<span class="built_in">set</span> nolist</span><br></pre></td></tr></table></figure><h4 id="高亮搜索"><a href="#高亮搜索" class="headerlink" title="高亮搜索"></a>高亮搜索</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">启用：<span class="built_in">set</span> hlsearch</span><br><span class="line">禁用：<span class="built_in">set</span> nohlsearch 简写：nohl</span><br></pre></td></tr></table></figure><h4 id="语法高亮"><a href="#语法高亮" class="headerlink" title="语法高亮"></a>语法高亮</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">启用：syntax on</span><br><span class="line">禁用：syntax off</span><br></pre></td></tr></table></figure><h4 id="文件格式"><a href="#文件格式" class="headerlink" title="文件格式"></a>文件格式</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">启用windows格式：<span class="built_in">set</span> fileformat=dos</span><br><span class="line">启用unix格式：<span class="built_in">set</span> fileformat=unix</span><br><span class="line">简写 <span class="built_in">set</span> ff=dos|unix</span><br></pre></td></tr></table></figure><h4 id="Tab用空格代替"><a href="#Tab用空格代替" class="headerlink" title="Tab用空格代替"></a>Tab用空格代替</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">启用：<span class="built_in">set</span> expandtab   默认为8个空格代替Tab</span><br><span class="line">禁用：<span class="built_in">set</span> noexpandtab</span><br><span class="line">简写：<span class="built_in">set</span> et</span><br></pre></td></tr></table></figure><h4 id="Tab用指定空格的个数代替"><a href="#Tab用指定空格的个数代替" class="headerlink" title="Tab用指定空格的个数代替"></a>Tab用指定空格的个数代替</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">启用：<span class="built_in">set</span> tabstop=<span class="comment"># 指定#个空格代替Tab</span></span><br><span class="line">简写：<span class="built_in">set</span> ts=4</span><br></pre></td></tr></table></figure><h4 id="设置缩进宽度"><a href="#设置缩进宽度" class="headerlink" title="设置缩进宽度"></a>设置缩进宽度</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#向右缩进 命令模式&gt;&gt;</span></span><br><span class="line"><span class="comment">#向左缩进 命令模式&lt;&lt;</span></span><br><span class="line"><span class="comment">#设置缩进为4个字符</span></span><br><span class="line"><span class="built_in">set</span> shiftwidth=4</span><br></pre></td></tr></table></figure><h5 id="设置文本宽度"><a href="#设置文本宽度" class="headerlink" title="设置文本宽度"></a>设置文本宽度</h5><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span> textwidth=65 (vim only) <span class="comment">#从左向右计数</span></span><br><span class="line"><span class="built_in">set</span> wrapmargin=15           <span class="comment">#从右到左计数</span></span><br></pre></td></tr></table></figure><h4 id="设置光标所在行的标识线"><a href="#设置光标所在行的标识线" class="headerlink" title="设置光标所在行的标识线"></a>设置光标所在行的标识线</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">启用：<span class="built_in">set</span> cursorline，简写 <span class="built_in">set</span> cul</span><br><span class="line">禁用：<span class="built_in">set</span> nocursorline</span><br></pre></td></tr></table></figure><h4 id="加密"><a href="#加密" class="headerlink" title="加密"></a>加密</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">启用： <span class="built_in">set</span> key=password</span><br><span class="line">禁用： <span class="built_in">set</span> key=</span><br></pre></td></tr></table></figure><h4 id="单词间跳转"><a href="#单词间跳转" class="headerlink" title="单词间跳转"></a>单词间跳转</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">w：下一个单词的词首</span><br><span class="line">e：当前或下一单词的词尾</span><br><span class="line">b：当前或前一个单词的词首</span><br><span class="line"><span class="string">&#x27;#&#x27;</span>COMMAND：由<span class="comment">#指定一次跳转的单词数</span></span><br></pre></td></tr></table></figure><h4 id="当前页跳转"><a href="#当前页跳转" class="headerlink" title="当前页跳转"></a>当前页跳转</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">H：页首     </span><br><span class="line">M：页中间行     </span><br><span class="line">L：页底</span><br><span class="line">zt：将光标所在当前行移到屏幕顶端</span><br><span class="line">zz：将光标所在当前行移到屏幕中间</span><br><span class="line">zb：将光标所在当前行移到屏幕底端</span><br></pre></td></tr></table></figure><h4 id="行首行尾跳转"><a href="#行首行尾跳转" class="headerlink" title="行首行尾跳转"></a>行首行尾跳转</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">^ 跳转至行首的第一个非空白字符</span><br><span class="line">0 跳转至行首</span><br><span class="line">\$ 跳转至行尾</span><br></pre></td></tr></table></figure><h4 id="行间移动"><a href="#行间移动" class="headerlink" title="行间移动"></a>行间移动</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&#x27;#&#x27;</span>G 或者扩展命令模式下 </span><br><span class="line">:<span class="string">&#x27;#&#x27;</span>   跳转至由第<span class="string">&#x27;#&#x27;</span>行</span><br><span class="line">G 最后一行</span><br><span class="line">1G, gg 第一行</span><br></pre></td></tr></table></figure><h4 id="翻页"><a href="#翻页" class="headerlink" title="翻页"></a>翻页</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Ctrl+f 向文件尾部翻一屏,相当于Pagedown</span><br><span class="line">Ctrl+b 向文件首部翻一屏,相当于Pageup</span><br><span class="line">Ctrl+d 向文件尾部翻半屏</span><br><span class="line">Ctrl+u 向文件首部翻半屏</span><br></pre></td></tr></table></figure><h4 id="字符编辑"><a href="#字符编辑" class="headerlink" title="字符编辑"></a>字符编辑</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">x 剪切光标处的字符</span><br><span class="line"><span class="string">&#x27;#&#x27;</span>x 剪切光标处起始的<span class="comment">#个字符</span></span><br><span class="line">xp 交换光标所在处的字符及其后面字符的位置</span><br><span class="line">~ 转换大小写</span><br><span class="line">J 删除当前行后的换行符</span><br></pre></td></tr></table></figure><h4 id="替换命令"><a href="#替换命令" class="headerlink" title="替换命令"></a>替换命令</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">r 只替换光标所在处的一个字符</span><br><span class="line">R 切换成REPLACE模式（在末行出现-- REPLACE -- 提示）,按ESC回到命令模式</span><br><span class="line">:%s/1/2/g 将所有的1替换为2</span><br></pre></td></tr></table></figure><h4 id="删除命令"><a href="#删除命令" class="headerlink" title="删除命令"></a>删除命令</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">d 删除命令，可结合光标跳转字符，实现范围删除</span><br><span class="line">d$ 删除到行尾</span><br><span class="line">d^ 删除到非空行首</span><br><span class="line">d0 删除到行首</span><br><span class="line">dw</span><br><span class="line">de</span><br><span class="line">db</span><br><span class="line"><span class="string">&#x27;#&#x27;</span>COMMAND</span><br><span class="line"><span class="built_in">dd</span>：   剪切光标所在的行</span><br><span class="line"><span class="string">&#x27;#&#x27;</span><span class="built_in">dd</span> 多行删除</span><br><span class="line">D：从当前光标位置一直删除到行尾，等同于d$</span><br></pre></td></tr></table></figure><h4 id="复制命令"><a href="#复制命令" class="headerlink" title="复制命令"></a>复制命令</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">y 复制，行为相似于d命令</span><br><span class="line">y$</span><br><span class="line">y0</span><br><span class="line">y^</span><br><span class="line">ye</span><br><span class="line">yw</span><br><span class="line">yb</span><br><span class="line"><span class="string">&#x27;#&#x27;</span>COMMAND</span><br><span class="line">yy：复制行</span><br><span class="line"><span class="string">&#x27;#&#x27;</span>yy 复制多行</span><br><span class="line">Y：复制整行</span><br></pre></td></tr></table></figure><h4 id="粘贴命令"><a href="#粘贴命令" class="headerlink" title="粘贴命令"></a>粘贴命令</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">p 缓冲区存的如果为整行，则粘贴当前光标所在行的下方；否则，则粘贴至当前光标所在处的后面</span><br><span class="line">P 缓冲区存的如果为整行，则粘贴当前光标所在行的上方；否则，则粘贴至当前光标所在处的前面</span><br></pre></td></tr></table></figure><h4 id="查找"><a href="#查找" class="headerlink" title="查找"></a>查找</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">/PATTERN：从当前光标所在处向文件尾部查找</span><br><span class="line">?PATTERN：从当前光标所在处向文件首部查找</span><br><span class="line">n：与命令同方向</span><br><span class="line">N：与命令反方向</span><br></pre></td></tr></table></figure><h4 id="撤销更改"><a href="#撤销更改" class="headerlink" title="撤销更改"></a>撤销更改</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">u 撤销最近的更改，相当于windows中ctrl+z</span><br><span class="line"><span class="string">&#x27;#&#x27;</span>u 撤销之前多次更改</span><br><span class="line">U 撤消光标落在这行后所有此行的更改</span><br><span class="line">Ctrl-r 重做最后的“撤消”更改，相当于windows中crtl+y</span><br><span class="line">. 重复前一个操作</span><br><span class="line"><span class="string">&#x27;#&#x27;</span>. 重复前一个操作<span class="string">&#x27;#&#x27;</span>次</span><br></pre></td></tr></table></figure><h4 id="多文件模式"><a href="#多文件模式" class="headerlink" title="多文件模式"></a>多文件模式</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vim FILE1 FILE2 FILE3 ...</span><br><span class="line">:next 下一个</span><br><span class="line">:prev 前一个</span><br><span class="line">:first 第一个</span><br><span class="line">:last 最后一个</span><br><span class="line">:wall 保存所有</span><br><span class="line">:qall 不保存退出所有</span><br><span class="line">:wqall保存退出所有</span><br></pre></td></tr></table></figure><h4 id="多文件分割"><a href="#多文件分割" class="headerlink" title="多文件分割"></a>多文件分割</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">vim -o|-O FILE1 FILE2 ...</span><br><span class="line">-o: 水平或上下分割</span><br><span class="line">-O: 垂直或左右分割（vim only）</span><br><span class="line">在窗口间切换：Ctrl+w, Arrow</span><br></pre></td></tr></table></figure><h4 id="单文件窗口分割"><a href="#单文件窗口分割" class="headerlink" title="单文件窗口分割"></a>单文件窗口分割</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Ctrl+w,s：<span class="built_in">split</span>, 水平分割，上下分屏</span><br><span class="line">Ctrl+w,v：vertical, 垂直分割，左右分屏</span><br><span class="line">ctrl+w,q：取消相邻窗口</span><br><span class="line">ctrl+w,o：取消全部窗口</span><br><span class="line">:wqall 退出</span><br></pre></td></tr></table></figure><div align=center><img src="vim-keymap.png"></div><h3 id="文本查看"><a href="#文本查看" class="headerlink" title="文本查看"></a>文本查看</h3><h4 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h4><ul><li>-E：显示行结束符$</li><li>-A：显示所有控制符</li><li>-n：对显示出的每一行进行编号</li><li>-b：非空行编号</li><li>-s：压缩连续的空行成一行<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> -n <span class="built_in">test</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="head"><a href="#head" class="headerlink" title="head"></a>head</h4><ul><li>-c # 指定获取前#字节</li><li>-n # 指定获取前#行,#如果为负数,表示从文件头取到倒数第#前</li><li>-# 同上<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">head</span> -n -3 <span class="built_in">test</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="tail"><a href="#tail" class="headerlink" title="tail"></a>tail</h4><ul><li>-c # 指定获取后#字节</li><li>-n # 指定获取后#行,如果#是负数,表示从第#行开始到文件结束</li><li>-# 同上</li><li>-f 跟踪显示文件fd新追加的内容,常用日志监控，相当于 –follow&#x3D;descriptor,当文件删除再新建同名文件,将无法继续跟踪文件</li><li>-F 跟踪文件名，相当于–follow&#x3D;name –retry，当文件删除再新建同名文件,将可以继续跟踪文件<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">tail</span> -10 <span class="built_in">test</span></span><br><span class="line"><span class="built_in">tail</span> -f nginx.log</span><br></pre></td></tr></table></figure></li></ul><h4 id="more-less"><a href="#more-less" class="headerlink" title="more less"></a>more less</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># more可以实现分页查看文件</span></span><br><span class="line">more <span class="built_in">test</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># less也可以实现分页查看文件，但是它有搜索功能，和vim类似，比较好用</span></span><br><span class="line"><span class="built_in">cat</span> <span class="built_in">test</span> | less</span><br></pre></td></tr></table></figure><h3 id="文本分割合并"><a href="#文本分割合并" class="headerlink" title="文本分割合并"></a>文本分割合并</h3><h4 id="cut"><a href="#cut" class="headerlink" title="cut"></a>cut</h4><ul><li>-d DELIMITER: 指明分隔符，默认tab</li><li>-f FILEDS:<ul><li>#: 第#个字段,例如:3</li><li>#,#[,#]：离散的多个字段，例如:1,3,6</li><li>#-#：连续的多个字段, 例如:1-6</li><li>混合使用：1-3,7</li></ul></li><li>-c 按字符切割</li><li>–output-delimiter&#x3D;STRING指定输出分隔符<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> a-b-c-d | cud -d- -f1,3</span><br></pre></td></tr></table></figure></li></ul><h4 id="paste"><a href="#paste" class="headerlink" title="paste"></a>paste</h4><ul><li>-d  #分隔符：指定分隔符，默认用TAB</li><li>-s  #所有行合成一行显示<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">paste</span> -d: alpha.log seq.log</span><br><span class="line"><span class="built_in">paste</span> -s -d: title.txt emp.txt</span><br></pre></td></tr></table></figure></li></ul><h3 id="分析文本"><a href="#分析文本" class="headerlink" title="分析文本"></a>分析文本</h3><h4 id="wc"><a href="#wc" class="headerlink" title="wc"></a>wc</h4><ul><li>-l 只计数行数</li><li>-w 只计数单词总数</li><li>-c 只计数字节总数</li><li>-m 只计数字符总数</li><li>-L 显示文件中最长行的长度<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">wc</span> -l <span class="built_in">test</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h4><ul><li>-r 执行反方向（由上至下）整理</li><li>-R 随机排序</li><li>-n 执行按数字大小整理</li><li>-h 人类可读排序,如: 2K 1G </li><li>-f 选项忽略（fold）字符串中的字符大小写</li><li>-u 选项（独特，unique），合并重复项，即去重</li><li>-t , 选项使用,做为字段界定符</li><li>-k# 选项按照使用,字符分隔的 # 列来整理能够使用多次<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sort</span> -t, -k2 -nr tmp </span><br></pre></td></tr></table></figure></li></ul><h4 id="uniq"><a href="#uniq" class="headerlink" title="uniq"></a>uniq</h4><ul><li>-c: 显示每行重复出现的次数</li><li>-d: 仅显示重复过的行</li><li>-u: 仅显示不曾重复的行<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">uniq</span> -c tmp2 | <span class="built_in">sort</span> -t<span class="string">&#x27; &#x27;</span> -k1 -nr | <span class="built_in">tr</span> -s <span class="string">&#x27; &#x27;</span> <span class="string">&#x27;,&#x27;</span> | <span class="built_in">cut</span> -d, -f3  <span class="comment"># 一列字母按出现次数降序</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="diff-patch"><a href="#diff-patch" class="headerlink" title="diff patch"></a>diff patch</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># diff比较两个文件之间的差别</span></span><br><span class="line"><span class="comment"># -u 选项来输出“统一的（unified）”diff格式文件</span></span><br><span class="line">diff -u f1.txt f2.txt</span><br><span class="line"></span><br><span class="line"><span class="comment"># patch复制在其它文件中进行的改变</span></span><br><span class="line"><span class="comment"># -b 选项来自动备份改变了的文件</span></span><br><span class="line">diff -u foo.conf foo2.conf &gt; foo.patch </span><br><span class="line">patch -b foo.conf foo.patch</span><br><span class="line"></span><br><span class="line"><span class="comment"># vimdiff，相当于vim -d，在vim中比较两个文件的差别</span></span><br><span class="line">vimdiff f1.txt f2.txt  </span><br></pre></td></tr></table></figure><h3 id="文本模式匹配grep"><a href="#文本模式匹配grep" class="headerlink" title="文本模式匹配grep"></a>文本模式匹配grep</h3><p>grep [OPTIONS] PATTERN [FILE…]</p><ul><li>-color&#x3D;auto 对匹配到的文本着色显示</li><li>-m  # 匹配#次后停止</li><li>-v 显示不被pattern匹配到的行,即取反</li><li>-i 忽略字符大小写</li><li>-n 显示匹配的行号</li><li>-c 统计匹配的行数</li><li>-o 仅显示匹配到的字符串</li><li>-q 静默模式，不输出任何信息</li><li>-A # after, 后#行</li><li>-B # before, 前#行</li><li>-C # context, 前后各#行</li><li>-e 实现多个选项间的逻辑or关系,如：grep –e ‘cat ‘ -e ‘dog’ file</li><li>-w 匹配整个单词</li><li>-E 使用ERE，相当于egrep</li><li>-F 不支持正则表达式，相当于fgrep</li><li>-f file 根据模式文件处理</li><li>-r   递归目录，但不处理软链接</li><li>-R   递归目录，但处理软链接<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">grep -f /data/f1.txt /data/f2.txt  <span class="comment"># 返回两个文件相同的部分</span></span><br><span class="line">ifconfig eth0 | grep -Eo <span class="string">&#x27;([0-9]&#123;1,3&#125;\.)&#123;3&#125;[0-9]&#123;1,3&#125;&#x27;</span>|<span class="built_in">head</span> -1  <span class="comment"># 返回rth0网卡的IP地址</span></span><br></pre></td></tr></table></figure></li></ul><h3 id="行编辑器sed"><a href="#行编辑器sed" class="headerlink" title="行编辑器sed"></a>行编辑器sed</h3><p>sed 即 Stream EDitor，和 vi 不同，sed是行编辑器。Sed是从文件或管道中读取一行，处理一行，输出一行；直到最后一行。每当处理一行时，把当前处理的行存储在临时缓冲区中，称为模式空间（Pattern Space），接着用sed命令处理缓冲区中的内容，处理完成后，把缓冲区的内容送往屏幕。接着处理下一行，这样不断重复，直到文件末尾。一次处理一行的设计模式使得sed性能很高，相比vim一次性将文件加载到内存效率更高。</p><p><code>sed [option]... &#39;script;script;...&#39; [inputfile...]</code>，sed [可选参数] ‘位置 命令’ 输入文件</p><h4 id="option"><a href="#option" class="headerlink" title="option"></a>option</h4><ul><li>-n 不输出模式空间内容到屏幕，即不自动打印</li><li>-e 多点编辑</li><li>-f FILE 从指定文件中读取编辑脚本</li><li>-r, -E 使用扩展正则表达式</li><li>-i.bak 备份文件并原处编辑</li><li>-s           将多个文件视为独立文件，而不是单个连续的长文件流</li><li>#说明: </li><li>-ir 不支持</li><li>-i -r 支持</li><li>-ri   支持</li><li>-ni   会清空文件</li></ul><h4 id="script"><a href="#script" class="headerlink" title="script"></a>script</h4><p>script是地址，不指定地址则对全文进行处理</p><ul><li>单地址：<ul><li>#：指定的行，$：最后一行</li><li>&#x2F;pattern&#x2F;：被此处模式所能够匹配到的每一行</li></ul></li><li>地址范围：<ul><li>#,#     #从#行到第#行，3，6 从第3行到第6行</li><li>#,+#   #从#行到+#行，3,+4 表示从3行到第7行</li><li>&#x2F;pat1&#x2F;,&#x2F;pat2&#x2F;</li><li>#,&#x2F;pat&#x2F;</li><li>&#x2F;pat&#x2F;,#</li></ul></li><li>步进：~<ul><li>1~2 奇数行</li><li>2~2 偶数行</li></ul></li></ul><h4 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h4><ul><li>p 打印当前模式空间内容，追加到默认输出之后</li><li>Ip 忽略大小写输出</li><li>d 删除模式空间匹配的行，并立即启用下一轮循环</li><li>a [\]text 在指定行后面追加文本，支持使用\n实现多行追加</li><li>i [\]text 在行前面插入文本</li><li>c [\]text 替换行为单行或多行文本</li><li>w file 保存模式匹配的行至指定文件</li><li>r file 读取指定文件的文本至模式空间中匹配到的行后</li><li>&#x3D; 为模式空间中的行打印行号</li><li>! 模式空间中匹配行取反处理</li><li>q 结束或退出sed</li></ul><h4 id="查找替换"><a href="#查找替换" class="headerlink" title="查找替换"></a>查找替换</h4><ul><li>s&#x2F;pattern&#x2F;string&#x2F;修饰符 查找替换,支持使用其它分隔符，可以是其它形式：s@@@，s###</li><li>替换修饰符：</li><li>g 行内全局替换</li><li>p 显示替换成功的行</li><li>w   &#x2F;PATH&#x2F;FILE 将替换成功的行保存至文件中</li><li>I,i   忽略大小写</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> ab-cd | sed -n <span class="string">&#x27;s/-/,/gp&#x27;</span>  <span class="comment"># 替换并打印结果</span></span><br><span class="line">ifconfig eth0 | sed -n <span class="string">&#x27;2p&#x27;</span>  <span class="comment"># 打印网卡信息的第二行</span></span><br><span class="line">sed -n <span class="string">&#x27;$p&#x27;</span> /etc/passwd  <span class="comment"># 打印指定文件最后一行</span></span><br><span class="line">sed -n <span class="string">&quot;<span class="subst">$(echo $[`cat /etc/passwd|wc -l`-1])</span>p&quot;</span> /etc/passwd  <span class="comment"># 打印倒数第二行</span></span><br><span class="line">ifconfig eth0 |sed -n <span class="string">&#x27;/inet*/p&#x27;</span>  <span class="comment"># 打印匹配到的行</span></span><br><span class="line"><span class="built_in">seq</span> 10 | sed -n <span class="string">&#x27;3,+4p&#x27;</span>  <span class="comment"># 第三行到第七行</span></span><br><span class="line"><span class="built_in">seq</span> 10 | sed <span class="string">&#x27;2~2d&#x27;</span>  <span class="comment"># 删除偶数</span></span><br><span class="line">sed  <span class="string">&#x27;/^$/d&#x27;</span> <span class="built_in">test</span>  <span class="comment"># 删除空行</span></span><br><span class="line"><span class="built_in">seq</span> 10 | sed -e <span class="string">&#x27;2d&#x27;</span> -e <span class="string">&#x27;4d&#x27;</span>  <span class="comment"># 多点编辑，这样也可以：sed &#x27;2d;4d&#x27;</span></span><br><span class="line">sed <span class="string">&#x27;/^#/d;/^$/d&#x27;</span> <span class="built_in">test</span>  <span class="comment"># 不显示注释和空行，这样也可以：grep -Ev &#x27;^#|^$&#x27;</span></span><br><span class="line">sed -i.bak <span class="string">&#x27;2d;4d&#x27;</span> <span class="built_in">test</span>  <span class="comment"># 把原文件备份为test.bak并对原文件原地修改</span></span><br><span class="line"><span class="built_in">seq</span> 10 | sed <span class="string">&#x27;2,3a tea&#x27;</span>  <span class="comment"># 在指定的行后添加内容</span></span><br><span class="line"><span class="comment"># 提取IP地址的几种方法</span></span><br><span class="line">ifconfig eth0 | sed -n <span class="string">&#x27;2s/.*inet //g;s/ netmask.*//gp&#x27;</span></span><br><span class="line">ifconfig eth0 | sed -nr <span class="string">&quot;2s/[^0-9]+([0-9.]+).*/\1/p&quot;</span></span><br><span class="line">ifconfig eth0 | sed -rn <span class="string">&#x27;2s/(.*inet )([0-9].*)( netmask.*)/\2/p&#x27;</span></span><br><span class="line"><span class="built_in">echo</span> a.b.c.gz |sed -En <span class="string">&#x27;s/(.*)\.([^.]+)$/\1/p&#x27;</span>  <span class="comment"># 取文件前缀</span></span><br><span class="line"><span class="built_in">echo</span> a.b.c.gz |sed -En <span class="string">&#x27;s/(.*)\.([^.]+)$/\2/p&#x27;</span>  <span class="comment"># 取文件后缀</span></span><br><span class="line"><span class="built_in">echo</span> /etc/sysconfig/ | sed -rn <span class="string">&#x27;s#(.*)/([^/]+)/?#\1#p&#x27;</span>  <span class="comment"># 取目录名</span></span><br><span class="line"><span class="built_in">echo</span> /etc/sysconfig/ | sed -rn <span class="string">&#x27;s#(.*)/([^/]+)/?#\2#p&#x27;</span>  <span class="comment"># 取基名</span></span><br></pre></td></tr></table></figure><h3 id="AWK文本处理"><a href="#AWK文本处理" class="headerlink" title="AWK文本处理"></a>AWK文本处理</h3><h4 id="awk工作过程"><a href="#awk工作过程" class="headerlink" title="awk工作过程"></a>awk工作过程</h4><ul><li>第一步：执行BEGIN{action;… }语句块中的语句</li><li>第二步：从文件或标准输入(stdin)读取一行，然后执行pattern{ action;… }语句块，它逐行扫描文件，从第一行到最后一行重复这个过程，直到文件全部被读取完毕。</li><li>第三步：当读至输入流末尾时，执行END{action;…}语句块BEGIN语句块在awk开始从输入流中读取行之前被执行，这是一个可选的语句块，比如变量初始化、打印输出表格的表头等语句通常可以写在BEGIN语句块中</li></ul><p>END语句块在awk从输入流中读取完所有的行之后即被执行，比如打印所有行的分析结果这类信息汇总都是在END语句块中完成，它也是一个可选语句块</p><p>pattern语句块中的通用命令是最重要的部分，也是可选的。如果没有提供pattern语句块，则默认执行{print}，即打印每一个读取到的行，awk读取的每一行都会执行该语句块</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># awk [options]   &#x27;program&#x27; var=value   file…</span></span><br><span class="line"><span class="comment"># awk [options]   -f programfile    var=value file…</span></span><br></pre></td></tr></table></figure><h4 id="print"><a href="#print" class="headerlink" title="print"></a>print</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">seq</span> 10 | awk <span class="string">&#x27;&#123;print $1,&quot;hello&quot;&#125;&#x27;</span></span><br><span class="line">awk -F: <span class="string">&#x27;&#123;print $1&quot;\t&quot;$3&#125;&#x27;</span> /etc/passwd</span><br><span class="line">ifconfig eth0 | awk <span class="string">&#x27;/netmask/&#123;print $2&#125;&#x27;</span>  <span class="comment"># 查看ip，/pattern/是匹配pattern所在行</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;1 www.baidu.com&quot;</span> | awk -F <span class="string">&quot;[ .]&quot;</span> <span class="string">&#x27;&#123;print $3&#125;&#x27;</span>  <span class="comment"># 正则表达式分割</span></span><br></pre></td></tr></table></figure><h4 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h4><p>内部变量</p><ul><li>FS：输入字段分隔符，默认为空白字符，相当于-F</li><li>OFS：输出字段分隔符，默认为空白字符</li><li>RS：输入记录分隔符，指定输入时的换行符</li><li>ORS：输出记录分隔符，使用指定字符代替输出换行符</li><li>NF：字段数量</li><li>NR：记录编号</li><li>FNR：各文件分别计数，记录的编号</li><li>FILENAME：当前文件名</li><li>ARGC：命令行参数的个数</li><li>ARGV：数组，保存的是命令行给定的各参数，每一个参数ARGV[0]….</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">awk -v FS=<span class="string">&#x27;:&#x27;</span> <span class="string">&#x27;&#123;print $1,FS,$3&#125;&#x27;</span> /etc/passwd</span><br><span class="line">awk -F: -v OFS=<span class="string">&#x27;-&#x27;</span> <span class="string">&#x27;&#123;print $1,$3&#125;&#x27;</span> /etc/passwd</span><br><span class="line">awk -v RS=<span class="string">&#x27; &#x27;</span> <span class="string">&#x27;&#123;print &#125;&#x27;</span> /etc/passwd</span><br><span class="line">awk -v RS=<span class="string">&#x27; &#x27;</span> -v ORS=<span class="string">&#x27;###&#x27;</span> <span class="string">&#x27;&#123;print $0&#125;&#x27;</span> /etc/passwd</span><br><span class="line">awk -F: <span class="string">&#x27;&#123;print NF&#125;&#x27;</span> /etc/passwd</span><br><span class="line">awk -F: <span class="string">&#x27;&#123;print $(NF-1)&#125;&#x27;</span> /etc/passwd</span><br><span class="line">awk <span class="string">&#x27;&#123;print NR,$0&#125;&#x27;</span> /etc/issue /etc/centos-release</span><br><span class="line">awk <span class="string">&#x27;&#123;print FNR,$0&#125;&#x27;</span> /etc/issue /etc/centos-release</span><br><span class="line">awk <span class="string">&#x27;&#123;print ARGC&#125;&#x27;</span> /etc/issue /etc/redhat-release</span><br><span class="line">awk <span class="string">&#x27;&#123;print ARGV[0]&#125;&#x27;</span> /etc/issue /etc/redhat-release</span><br></pre></td></tr></table></figure><p>自定义变量</p><p>可以使用v操作符也可以在代码块中直接定义</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">awk -v test1=test2=<span class="string">&quot;hello awk&quot;</span> <span class="string">&#x27;BEGIN&#123;print test1,test2&#125;&#x27;</span>  <span class="comment"># 注意赋值结果和在代码块中的结果不相同</span></span><br><span class="line">awk <span class="string">&#x27;BEGIN&#123;test1=test2=&quot;hello awk&quot;;print test1,test2&#125;&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="printf"><a href="#printf" class="headerlink" title="printf"></a>printf</h4><p>printf “FORMAT”, item1, item2</p><ul><li>注意必须指定FORMAT；不会自动换行；FORMAT需要分别为后面每个item指定格式符号</li><li>%s：显示字符串</li><li>%d, %i：显示十进制整数</li><li>%f：显示为浮点数</li><li>%e, %E：显示科学计数法数值 </li><li>%c：显示字符的ASCII码</li><li>%g, %G：以科学计数法或浮点形式显示数值</li><li>%u：无符号整数</li><li>%%：显示%自身</li><li>#[.#] 第一个数字控制显示的宽度；第二个#表示小数点后精度，如：%3.1f</li><li><ul><li>左对齐（默认右对齐） 如：%-15s</li></ul></li><li><ul><li>显示数值的正负符号   如：%+d</li></ul></li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">awk -F: <span class="string">&#x27;&#123;printf &quot;%s\n&quot;,$1&#125;&#x27;</span> /etc/passwd</span><br><span class="line">awk -F: <span class="string">&#x27;&#123;printf &quot;%20s\n&quot;,$1&#125;&#x27;</span> /etc/passwd</span><br><span class="line">awk -F: <span class="string">&#x27;&#123;printf &quot;%-20s %10d\n&quot;,$1,$3&#125;&#x27;</span> /etc/passwd</span><br><span class="line">awk -F: <span class="string">&#x27;&#123;printf &quot;USERname: %-20s UID:%-10d\n&quot;,$1,$3&#125;&#x27;</span> /etc/passwd</span><br></pre></td></tr></table></figure><h4 id="操作符"><a href="#操作符" class="headerlink" title="操作符"></a>操作符</h4><p>算数操作符</p><p>x+y, x-y, x*y, x&#x2F;y, x^y, x%y；-x：转换为负数；+x：将字符串转换为数值</p><p>字符串操作</p><p>&#x3D;, +&#x3D;, -&#x3D;, *&#x3D;, &#x2F;&#x3D;, %&#x3D;, ^&#x3D;，++, –</p><p>比较操作符</p><p>&#x3D;&#x3D;, !&#x3D;, &gt;, &gt;&#x3D;, &lt;, &lt;&#x3D;</p><p>逻辑操作符</p><p>&amp;&amp; 与；|| 或；! 非</p><p>条件表达式</p><p>selector?if-true-expression:if-false-expression</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">&#x27;BEGIN&#123;i=0;print i++,i&#125;&#x27;</span></span><br><span class="line"><span class="built_in">seq</span> 10 | awk <span class="string">&#x27;n++&#x27;</span></span><br><span class="line">awk -v n=0 <span class="string">&#x27;!n++&#123;print n&#125;&#x27;</span> /etc/passwd  <span class="comment"># 取n++的非</span></span><br><span class="line">awk <span class="string">&#x27;NR==2&#x27;</span> /etc/issue</span><br><span class="line">awk -F: <span class="string">&#x27;$3&gt;=1000&#x27;</span> /etc/passwd</span><br><span class="line"><span class="built_in">seq</span> 10 | awk <span class="string">&#x27;NR%2==0&#x27;</span>  <span class="comment"># 取奇偶行</span></span><br><span class="line">awk <span class="string">&#x27;BEGIN&#123;print !i&#125;&#x27;</span></span><br><span class="line">awk -F: <span class="string">&#x27;$3&gt;=0 &amp;&amp; $3&lt;=1000 &#123;print $1,$3&#125;&#x27;</span> /etc/passwd</span><br><span class="line">awk -F: <span class="string">&#x27;&#123;$3&gt;=1000?usertype=&quot;Common User&quot;:usertype=&quot;SysUser&quot;;printf &quot;%-20s:%12s\n&quot;,$1,usertype&#125;&#x27;</span> /etc/passwd  <span class="comment"># 三目表达式</span></span><br></pre></td></tr></table></figure><h4 id="模式匹配"><a href="#模式匹配" class="headerlink" title="模式匹配"></a>模式匹配</h4><p>根据pattern条件过滤匹配的行再做处理</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">&#x27;/^UUID/&#123;print $1&#125;&#x27;</span> /etc/fstab</span><br><span class="line"><span class="built_in">df</span> | awk <span class="string">&#x27;/^\/dev\//&#x27;</span>  <span class="comment"># 以/dev/开头的行</span></span><br><span class="line"><span class="built_in">seq</span> 10 | awk <span class="string">&#x27;1&#x27;</span>  <span class="comment"># 结果为非0值或非空字符串为真</span></span><br><span class="line"><span class="built_in">seq</span> 10 | awk <span class="string">&#x27;0&#x27;</span>  <span class="comment"># 结果为空字符串或0值为假</span></span><br><span class="line"><span class="built_in">seq</span> 10 | awk <span class="string">&#x27;NR&gt;=3 &amp;&amp; NR&lt;=6&#x27;</span>  <span class="comment"># 获取行范围</span></span><br><span class="line">awk -F: <span class="string">&#x27;BEGIN&#123;print &quot;USER USERID\n-----&quot;&#125; &#123;print $1&quot;:&quot;$3&#125; END&#123;print &quot;-----\nEND FILE&quot;&#125;&#x27;</span> /etc/passwd  <span class="comment"># BEGIN和END分别只在开始和结尾运行</span></span><br><span class="line">awk -F: <span class="string">&#x27;BEGIN&#123;printf &quot;--------------------------------\n%-20s|%10s|\n--------------------------------\n&quot;,&quot;username&quot;,&quot;uid&quot;&#125;&#123;printf &quot;%-20s|%10d|\n--------------------------------\n&quot;,$1,$3&#125;&#x27;</span> /etc/passwd</span><br></pre></td></tr></table></figure><h4 id="条件和循环"><a href="#条件和循环" class="headerlink" title="条件和循环"></a>条件和循环</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># if-else</span></span><br><span class="line">awk -F: <span class="string">&#x27;&#123;if($3&lt;=100)&#123;print &quot;&lt;=100&quot;,$3&#125;else if ($3&lt;=1000) &#123;print &quot;&lt;=1000&quot;,$3&#125; else&#123;print &quot;&gt;=1000&quot;,$3&#125;&#125;&#x27;</span> /etc/passwd</span><br><span class="line"><span class="comment"># while循环</span></span><br><span class="line">awk -v n=0 -v <span class="built_in">sum</span>=0 <span class="string">&#x27;BEGIN&#123;while(n&lt;=100)&#123;sum+=n;n++&#125;;print sum&#125;&#x27;</span></span><br><span class="line">awk <span class="string">&#x27;BEGIN&#123;sum=0;n=0;while(n&lt;=100)&#123;sum+=n;n++&#125;;print sum&#125;&#x27;</span></span><br><span class="line"><span class="comment"># do-while</span></span><br><span class="line">awk <span class="string">&#x27;BEGIN&#123;sum=0;n=0;do&#123;sum+=n;n++&#125;while(n&lt;=100);print sum&#125;&#x27;</span></span><br><span class="line"><span class="comment"># for循环</span></span><br><span class="line">awk <span class="string">&#x27;BEGIN&#123;sum=0;for(i=1;i&lt;=100;i++)&#123;sum+=i&#125;;print sum&#125;&#x27;</span></span><br><span class="line"><span class="keyword">for</span> (( i=1,<span class="built_in">sum</span>=0;i&lt;=100;i++ ));<span class="keyword">do</span> <span class="built_in">let</span> <span class="built_in">sum</span>+=i;<span class="keyword">done</span>;<span class="built_in">echo</span> <span class="variable">$sum</span></span><br><span class="line"><span class="comment"># continue和break</span></span><br><span class="line">awk <span class="string">&#x27;BEGIN&#123;for(i=1;i&lt;=100;i++)&#123;if(i==50)continue;sum+=i&#125;;print sum&#125;&#x27;</span></span><br><span class="line">awk <span class="string">&#x27;BEGIN&#123;for(i=1;i&lt;=100;i++)&#123;if(i==50)break;sum+=i&#125;;print sum&#125;&#x27;</span></span><br><span class="line"><span class="comment"># next提前结束对本行的处理进入下一行处理</span></span><br><span class="line"><span class="built_in">seq</span> 10 | awk <span class="string">&#x27;&#123;if($1%2==1) next; print $1&#125;&#x27;</span></span><br></pre></td></tr></table></figure><h4 id="数组"><a href="#数组" class="headerlink" title="数组"></a>数组</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 判断数组元素是否存在</span></span><br><span class="line">awk <span class="string">&#x27;BEGIN&#123;array[&quot;i&quot;]=&quot;x&quot;; array[&quot;j&quot;]=&quot;y&quot; ; print &quot;i&quot; in array, &quot;y&quot; in array &#125;&#x27;</span></span><br><span class="line"><span class="comment"># 遍历数组</span></span><br><span class="line">awk <span class="string">&#x27;BEGIN&#123;weekdays[&quot;mon&quot;]=&quot;Monday&quot;;weekdays[&quot;tue&quot;]=&quot;Tuesday&quot;;for(i in weekdays)&#123;print i,weekdays[i]&#125;&#125;&#x27;</span></span><br><span class="line">awk -F: <span class="string">&#x27;&#123;user[$1]=$3&#125;END&#123;for (i in user)&#123;printf &quot;%15s %10d\n&quot; &quot;User: &quot;,i,user[i] /etc/passwd</span></span><br></pre></td></tr></table></figure><h4 id="awk函数"><a href="#awk函数" class="headerlink" title="awk函数"></a>awk函数</h4><p>内置函数</p><ul><li>rand() 返回0和1之间一个随机数</li><li>srand() 配合rand()函数生成随机数的种子</li><li>int() 返回整数</li><li>length([s]) 返回指定字符串的长度</li><li>sub(r,s,[t]) 对t字符串搜索r表示模式匹配的内容，并将第一个匹配内容替换为s</li><li>gsub(r,s,[t]) 对t字符串搜索r表示模式匹配的内容，并将所有的匹配内容替换为s</li><li>split(s,array,[r]) 以r为分隔符切割字符串s，并将结果保存到array数组中</li><li>system(‘cmd’) 调用shell命令</li><li>systime() 10位时间戳</li><li>strftime() 指定时间格式</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">&#x27;BEGIN&#123;srand(); for(i=1;i&lt;=10;i++)&#123;print int(rand()*100)&#125;&#125;&#x27;</span>  <span class="comment"># 10个0-100随机数</span></span><br><span class="line"><span class="built_in">cut</span> -d: -f1 /etc/passwd | awk <span class="string">&#x27;&#123;print length()&#125;&#x27;</span>  <span class="comment"># 返回字符串长度</span></span><br><span class="line">awk -F: <span class="string">&#x27;&#123;print length($1)&#125;&#x27;</span> /etc/passwd</span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;2008:08:08 08:08:08&quot;</span> | awk <span class="string">&#x27;sub(/:/,&quot;-&quot;,$1)&#x27;</span>  <span class="comment"># 替换第一个</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&quot;2008:08:08 08:08:08&quot;</span> | awk <span class="string">&#x27;gsub(/:/,&quot;-&quot;,$0)&#x27;</span>  <span class="comment"># 替换所有</span></span><br><span class="line">awk <span class="string">&#x27;BEGIN&#123;system(&quot;pwd&quot;)&#125;&#x27;</span>  <span class="comment"># 执行命令</span></span><br><span class="line">awk <span class="string">&#x27;BEGIN&#123;print systime()&#125;&#x27;</span>  <span class="comment"># 10位时间戳</span></span><br><span class="line">awk <span class="string">&#x27;BEGIN&#123;print strftime(&quot;%Y-%m-%d %H:%M%S&quot;)&#125;&#x27;</span>  <span class="comment"># 当前时间格式化</span></span><br><span class="line">awk <span class="string">&#x27;BEGIN&#123;print strftime(&quot;%Y-%m-%d %H:%M%S&quot;, systime()-3600)&#125;&#x27;</span>  <span class="comment"># 指定时间戳格式化</span></span><br></pre></td></tr></table></figure><p>自定义函数</p><p>将程序写为脚本，通过-f参数直接使用，通过-v传递参数</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cat</span> &lt;&lt; <span class="string">EOF &gt; func.awk </span></span><br><span class="line"><span class="string">function max(x,y) &#123;</span></span><br><span class="line"><span class="string">    x&gt;y?var=x:var=y</span></span><br><span class="line"><span class="string">    return var</span></span><br><span class="line"><span class="string">&#125;</span></span><br><span class="line"><span class="string">BEGIN&#123;print max(a,b)&#125;</span></span><br><span class="line"><span class="string">EOF</span></span><br><span class="line">awk -v a=30 -v b=20 -f func.awk </span><br></pre></td></tr></table></figure><h2 id="软件包管理"><a href="#软件包管理" class="headerlink" title="软件包管理"></a>软件包管理</h2><p>主流的软件包管理器：</p><ul><li>redhat：rpm文件, rpm 包管理器，rpm：Redhat Package Manager，RPM Package Manager </li><li>debian：deb文件, dpkg 包管理器</li></ul><h3 id="包的依赖"><a href="#包的依赖" class="headerlink" title="包的依赖"></a>包的依赖</h3><p>软件包之间可能存在依赖关系，甚至循环依赖，即：A包依赖B包，B包依赖C包，C包依赖A包安装软件包时，会因为缺少依赖的包，而导致安装包失败。</p><p>解决依赖包管理工具：</p><ul><li>yum：rpm包管理器的前端工具</li><li>dnf：Fedora 18+ rpm包管理器前端管理工具，CentOS 8 版代替 yum</li><li>apt：deb包管理器前端工具</li><li>zypper：suse上的rpm前端管理工具</li></ul><h3 id="获取程序包"><a href="#获取程序包" class="headerlink" title="获取程序包"></a>获取程序包</h3><p>官方网站</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">CentOS镜像：</span><br><span class="line">https://www.centos.org/download/</span><br><span class="line">Ubuntu镜像：</span><br><span class="line">http://cdimage.ubuntu.com/releases/</span><br><span class="line">http://releases.ubuntu.com</span><br></pre></td></tr></table></figure><p>软件项目官方网站点</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">http://yum.mariadb.org/10.4/centos8-amd64/rpms/</span><br><span class="line">http://repo.mysql.com/yum/mysql-8.0-community/el/8/x86_64/</span><br></pre></td></tr></table></figure><p>搜索引擎</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">http://pkgs.org</span><br><span class="line">http://rpmfind.net</span><br><span class="line">http://rpm.pbone.net</span><br><span class="line">https://sourceforge.net/</span><br></pre></td></tr></table></figure><h3 id="RPM包管理"><a href="#RPM包管理" class="headerlink" title="RPM包管理"></a>RPM包管理</h3><h4 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h4><p>rpm {-i|–install} [install-options] PACKAGE_FILE… </p><ul><li>-v –verbose</li><li>-h 以#显示程序包管理执行进度</li><li>–test: 测试安装，但不真正执行安装，即dry run模式</li><li>–nodeps：忽略依赖关系</li><li>–replacepkgs | replacefiles</li><li>–nosignature: 不检查来源合法性</li><li>–nodigest：不检查包完整性</li><li>–noscripts：不执行程序包脚本<ul><li>%pre: 安装前脚本 –nopre</li><li>%post: 安装后脚本 –nopost</li><li>%preun: 卸载前脚本 –nopreun</li><li>%postun: 卸载后脚本 –nopostun<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rpm -ivh PACKAGE_FILE...</span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="升级和降级"><a href="#升级和降级" class="headerlink" title="升级和降级"></a>升级和降级</h4><p>rpm {-U|–upgrade} [install-options] PACKAGE_FILE…</p><p>rpm {-F|–freshen} [install-options] PACKAGE_FILE…</p><p>upgrade：安装有旧版程序包，则”升级”，如果不存在旧版程序包，则”安装”</p><p>freshen：安装有旧版程序包，则”升级”， 如果不存在旧版程序包，则不执行升级操作</p><p>–oldpackage：降级</p><p>–force: 强制安装</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">rpm -Uvh PACKAGE_FILE ...</span><br><span class="line">rpm -Fvh PACKAGE_FILE ...</span><br></pre></td></tr></table></figure><h4 id="包查询"><a href="#包查询" class="headerlink" title="包查询"></a>包查询</h4><p>rpm {-q|–query} [select-options] [query-options]</p><p>[select-options]</p><ul><li>-a：所有包</li><li>-f：查看指定的文件由哪个程序包安装生成</li><li>-p rpmfile：针对尚未安装的程序包文件做查询操作<br>[query-options]</li><li>–changelog：查询rpm包的changelog</li><li>-c：查询程序的配置文件</li><li>-d：查询程序的文档</li><li>-i：information</li><li>-l：查看指定的程序包安装后生成的所有文件</li><li>–scripts：程序包自带的脚本</li><li>–whatprovides CAPABILITY：查询指定的CAPABILITY由哪个包所提供</li><li>–whatrequires CAPABILITY：查询指定的CAPABILITY被哪个包所依赖</li><li>–provides：列出指定程序包所提供的CAPABILITY</li><li>-R：查询指定的程序包所依赖的CAPABILITY</li></ul><h4 id="包卸载"><a href="#包卸载" class="headerlink" title="包卸载"></a>包卸载</h4><p>rpm {-e|–erase} [–allmatches] [–nodeps] [–noscripts] [–notriggers] [–test] PACKAGE_NAME …</p><h3 id="yum和dnf"><a href="#yum和dnf" class="headerlink" title="yum和dnf"></a>yum和dnf</h3><p>yum&#x2F;dnf是基于C&#x2F;S模式，yum服务端存放rpm包和相关包的元数据，yum客户端访问服务端进行安装或查询</p><h4 id="yum配置文件"><a href="#yum配置文件" class="headerlink" title="yum配置文件"></a>yum配置文件</h4><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">/etc/yum.conf <span class="comment">#为所有仓库提供公共配置</span></span><br><span class="line">/etc/yum.repos.d/*.repo： <span class="comment">#为每个仓库的提供配置文件</span></span><br></pre></td></tr></table></figure><p>yum配置文件定义</p><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[repositoryID]</span><br><span class="line">name=Some name for this repository</span><br><span class="line">baseurl=url://path/to/repository/  # file://, http://, https://, ftp://</span><br><span class="line">enabled=&#123;1|0&#125;</span><br><span class="line">gpgcheck=&#123;1|0&#125;  # 安装包前要做包的合法和完整性校验</span><br><span class="line">gpgkey=URL</span><br><span class="line">enablegroups=&#123;1|0&#125;</span><br><span class="line">failovermethod=&#123;roundrobin|priority&#125;</span><br><span class="line"> roundrobin：意为随机挑选，默认值</span><br><span class="line"> priority:按顺序访问</span><br><span class="line">cost= 默认为1000</span><br></pre></td></tr></table></figure><p>yum的repo配置文件中可用的变量：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$releasever</span>  <span class="comment"># 当前OS的发行版的主版本号，如：8，7，6</span></span><br><span class="line"><span class="variable">$arch</span>  <span class="comment"># CPU架构，如：aarch64, i586, i686，x86_64等</span></span><br><span class="line"><span class="variable">$basearch</span>  <span class="comment"># 系统基础平台；i386, x86_64</span></span><br><span class="line"><span class="variable">$contentdir</span>  <span class="comment"># 表示目录，比如：centos-8，centos-7</span></span><br><span class="line"><span class="variable">$YUM0</span>-<span class="variable">$YUM9</span>  <span class="comment"># 自定义变量</span></span><br></pre></td></tr></table></figure><p>阿里云提供了写好的CentOS和ubuntu的仓库文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">http://mirrors.aliyun.com/repo/</span><br></pre></td></tr></table></figure><p>常用国内yum源</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CentOS系统的yum源</span></span><br><span class="line"><span class="comment">#阿里云</span></span><br><span class="line">https://mirrors.aliyun.com/centos/<span class="variable">$releasever</span>/</span><br><span class="line"><span class="comment">#腾讯云</span></span><br><span class="line">https://mirrors.cloud.tencent.com/centos/<span class="variable">$releasever</span>/</span><br><span class="line"><span class="comment">#华为云</span></span><br><span class="line">https://repo.huaweicloud.com/centos/<span class="variable">$releasever</span>/</span><br><span class="line"><span class="comment">#清华大学</span></span><br><span class="line">https://mirrors.tuna.tsinghua.edu.cn/centos/<span class="variable">$releasever</span>/</span><br><span class="line"></span><br><span class="line"><span class="comment"># EPEL的yum源</span></span><br><span class="line"><span class="comment">#阿里云</span></span><br><span class="line">https://mirrors.aliyun.com/epel/<span class="variable">$releasever</span>/x86_64</span><br><span class="line"><span class="comment">#腾讯云</span></span><br><span class="line">https://mirrors.cloud.tencent.com/epel/<span class="variable">$releasever</span>/x86_64</span><br><span class="line"><span class="comment">#华为云</span></span><br><span class="line">https://mirrors.huaweicloud.com/epel/<span class="variable">$releasever</span>/x86_64</span><br><span class="line"><span class="comment">#清华大学</span></span><br><span class="line">https://mirrors.tuna.tsinghua.edu.cn/epel/<span class="variable">$releasever</span>/x86_64</span><br></pre></td></tr></table></figure><h4 id="yum-config-manager"><a href="#yum-config-manager" class="headerlink" title="yum-config-manager"></a>yum-config-manager</h4><p>可以生成yum仓库的配置文件及启用或禁用仓库，来自于yum-utils包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">yum-config-manager --add-repo <span class="string">&quot;URL或file&quot;</span>  <span class="comment"># 增加仓库</span></span><br><span class="line">yum-config-manager --<span class="built_in">disable</span> <span class="string">&quot;仓库名&quot;</span>  <span class="comment"># 禁用仓库</span></span><br><span class="line">yum-config-manager --<span class="built_in">enable</span>  <span class="string">&quot;仓库名&quot;</span>  <span class="comment"># 启用仓库</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 创建仓库配置</span></span><br><span class="line">yum-config-manager --add-repo https://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo</span><br><span class="line">yum-config-manager --add-repo /data/docker-ce.repo</span><br><span class="line"></span><br><span class="line"><span class="comment"># 启用/禁用repo</span></span><br><span class="line">yum-config-manager --<span class="built_in">disable</span> extras</span><br><span class="line">yum-config-manager --<span class="built_in">enable</span> extras</span><br></pre></td></tr></table></figure><h4 id="yum命令"><a href="#yum命令" class="headerlink" title="yum命令"></a>yum命令</h4><p>yum [options] [command] [package …]</p><ul><li>-y 自动回答为”yes”</li><li>-q 静默模式</li><li>–nogpgcheck 禁止进行gpg check</li><li>–enablerepo&#x3D;repoidglob 临时启用此处指定的repo，支持通配符，如：”*”</li><li>–disablerepo&#x3D;repoidglob 临时禁用此处指定的repo,和上面语句同时使用，放在后面的生效</li></ul><p>显示仓库列表</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum repolist</span><br><span class="line">yum repolist --disebaled  <span class="comment"># 查看已停用的仓库</span></span><br><span class="line">yum repolist -v  <span class="comment"># 显示仓库的详细信息</span></span><br></pre></td></tr></table></figure><p>显示程序包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">yum list  <span class="comment"># 查看所有程序包</span></span><br><span class="line">yum list mariadb-server  <span class="comment"># 显示制定的rpm包</span></span><br><span class="line">yum list exim*  <span class="comment"># 支持通配符</span></span><br><span class="line">yum list installed  <span class="comment"># 只显示已安装的包</span></span><br><span class="line">yum list available  <span class="comment"># 显示可安装的包</span></span><br><span class="line">yum list updates  <span class="comment"># 显示可以升级的包</span></span><br></pre></td></tr></table></figure><p>安装程序包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yum install package1 [package2] [...]</span><br><span class="line">yum reinstall package1 [package2] [...]  <span class="comment">#重新安装</span></span><br><span class="line">--downloadonly  <span class="comment">#只下载相关包默认至/var/cache/yum/x86_64/7/目录下,而不执行install/upgrade/erase </span></span><br><span class="line">--downloaddir=&lt;path&gt;, --destdir=&lt;path&gt;  <span class="comment">#--downloaddir选项来指定下载的目录,如果不存在自动创建</span></span><br><span class="line">yum -y install --downloadonly --downloaddir=/data/httpd httpd</span><br></pre></td></tr></table></figure><p>卸载程序包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum remove | erase package1 [package2] [...]</span><br></pre></td></tr></table></figure><p>升级和降级</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum upgrade|update [package1] [package2] [...]</span><br><span class="line">yum upgrade-minimal   <span class="comment">#最小化升级</span></span><br><span class="line">yum downgrade package1 [package2] [...]  <span class="comment"># 降级</span></span><br></pre></td></tr></table></figure><p>查询搜索</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">yum info mariadb-server  <span class="comment"># 查询程序包信息</span></span><br><span class="line">yum deplist mariadb-server  <span class="comment"># 查看依赖关系</span></span><br><span class="line">yum search mysql*  <span class="comment"># 模糊查询程序包</span></span><br></pre></td></tr></table></figure><p>仓库缓存</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">du</span> -sh /var/cache/yum  <span class="comment"># 查看缓存大小，有时候可能是dnf</span></span><br><span class="line">yum makecache  <span class="comment"># 构建缓存</span></span><br><span class="line">yum clean all  <span class="comment"># 清除所有缓存</span></span><br></pre></td></tr></table></figure><p>yum日志</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># CentOS 7以前版本日志</span></span><br><span class="line">/var/log/yum.log</span><br><span class="line"><span class="comment"># CentOS 8 版本日志</span></span><br><span class="line">/var/log/dnf.rpm.log</span><br><span class="line">/var/log/dnf.log</span><br><span class="line"></span><br><span class="line">yum <span class="built_in">history</span>  <span class="comment"># 查看历史命令</span></span><br><span class="line">yum <span class="built_in">history</span> info 22  <span class="comment"># 查看指定id的历史命令详细信息</span></span><br><span class="line">yum <span class="built_in">history</span> undo 22 -y  <span class="comment"># 撤销</span></span><br><span class="line">yum <span class="built_in">history</span> redo 22 -y  <span class="comment"># 重新运行</span></span><br></pre></td></tr></table></figure><h4 id="dnf"><a href="#dnf" class="headerlink" title="dnf"></a>dnf</h4><p>在 RHEL 8.0 版本正式取代了 YUM，DNF包管理器克服了YUM包管理器的一些瓶颈，提升了包括用户体验，内存占用，依赖分析，运行速度等。用法和yum基本相同</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 配置文件</span></span><br><span class="line">/etc/dnf/dnf.conf</span><br><span class="line"><span class="comment"># 仓库文件</span></span><br><span class="line">/etc/yum.repos.d/ *.repo</span><br><span class="line"><span class="comment"># 日志文件</span></span><br><span class="line">/var/log/dnf.rpm.log</span><br><span class="line">/var/log/dnf.log</span><br></pre></td></tr></table></figure><h2 id="程序包编译"><a href="#程序包编译" class="headerlink" title="程序包编译"></a>程序包编译</h2><h3 id="C语言编译过程"><a href="#C语言编译过程" class="headerlink" title="C语言编译过程"></a>C语言编译过程</h3><ul><li>.&#x2F;configure<ul><li>通过选项传递参数，指定安装路径、启用特性等；执行时会参考用户的指定以及Makefile.in文件生成Makefile</li><li>检查依赖到的外部环境，如依赖的软件包</li></ul></li><li>make 根据Makefile文件，会检测依赖的环境，进行构建应用程序</li><li>make install 复制文件到相应路径</li></ul><h3 id="编译安装准备"><a href="#编译安装准备" class="headerlink" title="编译安装准备"></a>编译安装准备</h3><ul><li>开发工具：make, gcc (c&#x2F;c++编译器GNU C Complier)</li><li>开发环境：开发库（glibc：标准库），头文件，可安装开发包组 Development Tools</li><li>软件相关依赖包</li></ul><p>建议安装相关包</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yum install gcc make autoconf gcc-c++ glibc glibc-devel pcre pcre-devel openssl openssl-devel systemd-devel zlib-devel  vim lrzsz tree tmux lsof tcpdump wget net-tools iotop bc bzip2 zip unzip nfs-utils man-pages</span><br></pre></td></tr></table></figure><h3 id="编译安装"><a href="#编译安装" class="headerlink" title="编译安装"></a>编译安装</h3><ol><li>运行 configure 脚本，生成 Makefile 文件。可以指定安装位置，指定启用的特性<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装路径设定</span></span><br><span class="line">--prefix=/PATH  <span class="comment"># 指定默认安装位置,默认为/usr/local/</span></span><br><span class="line">--sysconfdir=/PATH  <span class="comment"># 配置文件安装位置</span></span><br><span class="line"><span class="comment"># 软件特性和相关指定</span></span><br><span class="line">Optional Features: 可选特性</span><br><span class="line"> --disable-FEATURE</span><br><span class="line"> --enable-FEATURE[=ARG]</span><br><span class="line">Optional Packages: 可选包</span><br><span class="line"> --with-PACKAGE[=ARG] 依赖包</span><br><span class="line"> --without-PACKAGE 禁用依赖关系</span><br><span class="line"><span class="comment"># 注意：通常被编译操作依赖的程序包，需要安装此程序包的&quot;开发&quot;组件，其包名一般类似于name-devel-VERSION</span></span><br></pre></td></tr></table></figure></li><li>make</li><li>make install</li></ol><h3 id="编译安装httpd"><a href="#编译安装httpd" class="headerlink" title="编译安装httpd"></a>编译安装httpd</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装相关包</span></span><br><span class="line">dnf install gcc make autoconf apr-devel apr-util-devel pcre-devel openssl-devel redhat-rpm-config</span><br><span class="line"><span class="comment"># 下载并解压包</span></span><br><span class="line">wget https://mirror.bit.edu.cn/apache//httpd/httpd-2.4.46.tar.bz2</span><br><span class="line">tar xvf httpd-2.4.46.tar.bz2 -C /usr/local/src</span><br><span class="line"><span class="comment"># 配置</span></span><br><span class="line"><span class="built_in">cd</span> /usr/local/src/httpd-2.4.43/</span><br><span class="line">./configure --prefix=/apps/httpd --sysconfdir=/etc/httpd --enable-ssl</span><br><span class="line"><span class="comment"># 编译安装</span></span><br><span class="line">make -j 4 &amp;&amp; make install </span><br><span class="line"><span class="comment"># 配置环境，也可以加入PATH或者软链接到/usr/bin</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">&#x27;PATH=/apps/httpd/bin:$PATH&#x27;</span> &gt; /etc/profile.d/httpd.sh</span><br><span class="line">. /etc/profile.d/httpd.sh</span><br><span class="line"><span class="comment"># 运行</span></span><br><span class="line">apachectl start</span><br></pre></td></tr></table></figure><h2 id="Ubuntu软件管理"><a href="#Ubuntu软件管理" class="headerlink" title="Ubuntu软件管理"></a>Ubuntu软件管理</h2><ul><li>dpkg：package manager for Debian，类似于rpm， dpkg是基于Debian的系统的包管理器。可以安装，删除和构建软件包，但无法自动下载和安装软件包或其依赖项</li><li>apt：Advanced Packaging Tool，功能强大的软件管理工具，甚至可升级整个Ubuntu的系统，基于客户&#x2F;服务器架构，类似于yum</li></ul><h3 id="dpkg包管理"><a href="#dpkg包管理" class="headerlink" title="dpkg包管理"></a>dpkg包管理</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装包</span></span><br><span class="line">dpkg -i package.deb </span><br><span class="line"><span class="comment">#删除包，不建议，不自动卸载依赖于它的包</span></span><br><span class="line">dpkg -r package </span><br><span class="line"><span class="comment">#删除包（包括配置文件）</span></span><br><span class="line">dpkg -P package </span><br><span class="line"><span class="comment">#列出当前已安装的包，类似rpm -qa</span></span><br><span class="line">dpkg -l</span><br><span class="line"><span class="comment">#显示该包的简要说明</span></span><br><span class="line">dpkg -l package </span><br><span class="line"><span class="comment">#列出该包的状态，包括详细信息，类似rpm –qi</span></span><br><span class="line">dpkg -s package </span><br><span class="line"><span class="comment">#列出该包中所包含的文件，类似rpm –ql </span></span><br><span class="line">dpkg -L package </span><br><span class="line"><span class="comment">#搜索包含pattern的包，类似rpm –qf </span></span><br><span class="line">dpkg -S &lt;pattern&gt; </span><br><span class="line"><span class="comment">#配置包，-a 使用，配置所有没有配置的软件包</span></span><br><span class="line">dpkg --configure package </span><br><span class="line"><span class="comment">#列出 deb 包的内容，类似rpm –qpl </span></span><br><span class="line">dpkg -c package.deb </span><br><span class="line"><span class="comment">#解开 deb 包的内容</span></span><br><span class="line">dpkg --unpack package.deb </span><br></pre></td></tr></table></figure><p>注意：一般建议不要使用dpkg卸载软件包。因为删除包时，其它依赖它的包不会卸载，并且可能无法再正常运行</p><h3 id="apt"><a href="#apt" class="headerlink" title="apt"></a>apt</h3><p>之前最常用的 Linux 包管理命令都被分散在了 apt-get、apt-cache 和 apt-config 这三条命令中。</p><p>Ubuntu 16.04 引入新特性之一便是 apt 命令，apt 命令解决了命令过于分散的问题，它包括 apt-get 命令出现以来使用最广泛的功能选项，以及 apt-cache 和 apt-config 命令中很少用到的功能。在使用apt 命令时，用户不必再由 apt-get 转到 apt-cache 或 apt-config，提供管理软件包所需的必要选项。</p><p>apt配置文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">/etc/apt/sources.list</span><br><span class="line">/etc/apt/sources.list.d</span><br><span class="line"></span><br><span class="line"><span class="comment"># 修改为清华源</span></span><br><span class="line">sed -i <span class="string">&#x27;s/mirrors.aliyun.com/mirrors.tuna.tsinghua.edu.cn/&#x27;</span> /etc/apt/sources.list</span><br></pre></td></tr></table></figure><p>常用命令</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#安装包：</span></span><br><span class="line">apt install tree zip</span><br><span class="line"><span class="comment">#安装图形桌面</span></span><br><span class="line">apt install ubuntu-desktop</span><br><span class="line"><span class="comment">#删除包：</span></span><br><span class="line">apt remove tree zip</span><br><span class="line"><span class="comment">#说明：apt remove中添加--purge选项会删除包配置文件，谨慎使用</span></span><br><span class="line"><span class="comment">#更新包索引，相当于yum clean all;yum makecache</span></span><br><span class="line">apt update  </span><br><span class="line"><span class="comment">#升级包：要升级系统，请首先更新软件包索引，再升级</span></span><br><span class="line">apt upgrade</span><br><span class="line"><span class="comment">#apt列出仓库软件包，等于yum list</span></span><br><span class="line">apt list</span><br><span class="line"><span class="comment">#搜索安装包</span></span><br><span class="line">apt search nginx</span><br><span class="line"><span class="comment">#查看某个安装包的详细信息</span></span><br><span class="line">apt show apache2 </span><br><span class="line"><span class="comment">#在线安装软件包</span></span><br><span class="line">apt install apache2 </span><br><span class="line"><span class="comment">#卸载单个软件包但是保留配置⽂件</span></span><br><span class="line">apt remove apache2 </span><br><span class="line"><span class="comment">#删除安装包并解决依赖关系</span></span><br><span class="line">apt autoremove apache2 </span><br><span class="line"><span class="comment">#更新本地软件包列表索引，修改了apt仓库后必须执⾏</span></span><br><span class="line">apt update </span><br><span class="line"><span class="comment">#卸载单个软件包删除配置⽂件</span></span><br><span class="line">apt purge apache2 </span><br><span class="line"><span class="comment">#升级所有已安装且可升级到新版本的软件包</span></span><br><span class="line">apt upgrade</span><br><span class="line"><span class="comment">#升级整个系统，必要时可以移除旧软件包。</span></span><br><span class="line">apt full-upgrade </span><br><span class="line"><span class="comment">#编辑source源⽂件</span></span><br><span class="line">apt edit-sources </span><br><span class="line"><span class="comment">#查看仓库中软件包有哪些版本可以安装</span></span><br><span class="line">apt-cache madison nginx </span><br><span class="line"><span class="comment">#安装软件包的时候指定安装具体的版本</span></span><br><span class="line">apt install nginx=1.14.0-0ubuntu1.6 </span><br><span class="line"><span class="comment">#查看文件来自于哪个包,类似redhat中的yum provides &lt;filename&gt;</span></span><br><span class="line">apt-file search <span class="string">&#x27;string&#x27;</span>  <span class="comment">#默认是包含此字符串的文件</span></span><br><span class="line">apt-file search -x  <span class="string">&#x27;正则表达式&#x27;</span></span><br><span class="line">apt-file search -F /path/file</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Language </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Linux常见问题解决</title>
      <link href="/2019/08/07/Language/Linux%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/"/>
      <url>/2019/08/07/Language/Linux%E5%B8%B8%E8%A7%81%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/</url>
      
        <content type="html"><![CDATA[<p>Linux常见问题解决。</p><span id="more"></span><h3 id="Linux下MySQL安装"><a href="#Linux下MySQL安装" class="headerlink" title="Linux下MySQL安装"></a>Linux下MySQL安装</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">判断是否安装MySQL</span></span><br><span class="line">rpm -qa | grep mysql</span><br><span class="line">rpm -e --nodeps 安装包名称  # 卸载指定安装包</span><br><span class="line"><span class="meta"># </span><span class="language-bash">下载server和client RPM包，在https://dev.mysql.com/downloads/mysql/下选择 SUSE Linux Enterprise Server**，然后选择MySQL Server和Client Utilities两个版本下载</span></span><br><span class="line">wget https://dev.mysql.com/get/Downloads/MySQL-8.0/mysql-community-server-8.0.22-1.sl15.x86_64.rpm</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">在线安装MySQL57</span></span><br><span class="line">wget -i -c http://dev.mysql.com/get/mysql57-community-release-el7-10.noarch.rpm</span><br><span class="line">yum -y install mysql57-community-release-el7-10.noarch.rpm</span><br><span class="line">yum -y install mysql-community-server</span><br><span class="line">systemctl start  mysqld.service  # 开启mysqld服务</span><br><span class="line">systemctl status mysqld.service  # 查看mysqld状态</span><br><span class="line">grep &quot;password&quot; /var/log/mysqld.log  # 查看临时密码，并用这个密码登录</span><br><span class="line">mysql -uroot -p</span><br><span class="line"></span><br><span class="line">set global validate_password_policy=0;  # 设置密码策略</span><br><span class="line">set global validate_password_length=1;  # 设置密码长度</span><br><span class="line">ALTER USER &#x27;root&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;123456&#x27;;  # 修改root密码</span><br><span class="line"><span class="meta"># </span><span class="language-bash">设置为外部机器也可以访问</span></span><br><span class="line">GRANT ALL PRIVILEGES ON *.* TO &#x27;root&#x27;@&#x27;%&#x27;IDENTIFIED BY &#x27;123456&#x27; WITH GRANT OPTION;</span><br><span class="line">FLUSH PRIVILEGES;  # 刷新权限</span><br><span class="line">exit;</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">因为安装了Yum Repository，以后每次yum操作都会自动更新，需要把这个卸载掉</span></span><br><span class="line">yum -y remove mysql57-community-release-el7-10.noarch</span><br></pre></td></tr></table></figure><h3 id="Securecrt连接Mac本地终端"><a href="#Securecrt连接Mac本地终端" class="headerlink" title="Securecrt连接Mac本地终端"></a>Securecrt连接Mac本地终端</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">①启动sshd服务：</span></span><br><span class="line">sudo launchctl load -w /System/Library/LaunchDaemons/ssh.plist</span><br><span class="line"><span class="meta"># </span><span class="language-bash">②查看是否启动：（返回-       0       com.openssh.sshd即为成功）</span></span><br><span class="line">sudo launchctl list | grep ssh</span><br></pre></td></tr></table></figure><p>③ 登录后中文乱码的解决方法：<br>secureCRT连接设置编码为UTF-8编码<br>在&#x2F;etc&#x2F;profile最后加入 export LANG&#x3D;zh_CN.UTF-8<br>修改&#x2F;etc&#x2F;profile 提示文件只读。 sudo chmod +w &#x2F;etc&#x2F;profile<br>执行 source &#x2F;etc&#x2F;profile</p><h3 id="Centos8开放防火墙端口"><a href="#Centos8开放防火墙端口" class="headerlink" title="Centos8开放防火墙端口"></a>Centos8开放防火墙端口</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">查看centos版本号</span></span><br><span class="line">cat /etc/redhat-release </span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看防火墙某个端口是否开放</span></span><br><span class="line">firewall-cmd --query-port=3306/tcp</span><br><span class="line"><span class="meta"># </span><span class="language-bash">开放防火墙端口3306</span></span><br><span class="line">firewall-cmd --zone=public --add-port=3306/tcp --permanent</span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看防火墙状态</span></span><br><span class="line">systemctl status firewalld</span><br><span class="line"><span class="meta"># </span><span class="language-bash">关闭防火墙</span></span><br><span class="line">systemctl stop firewalld</span><br><span class="line"><span class="meta"># </span><span class="language-bash">打开防火墙</span></span><br><span class="line">systemctl start firewalld</span><br><span class="line"><span class="meta"># </span><span class="language-bash">开放一段端口，开放了之后需要重启防火墙</span></span><br><span class="line">firewall-cmd --zone=public --add-port=40000-45000/tcp --permanent</span><br><span class="line"><span class="meta"># </span><span class="language-bash">查看开放的端口列表</span></span><br><span class="line">firewall-cmd --zone=public --list-ports</span><br></pre></td></tr></table></figure><h3 id="时钟同步"><a href="#时钟同步" class="headerlink" title="时钟同步"></a>时钟同步</h3><p>同步到阿里云服务器</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">安装crontab</span></span><br><span class="line">yum install -y ntp</span><br><span class="line"><span class="meta"># </span><span class="language-bash">添加定时任务</span></span><br><span class="line">crontab -e</span><br><span class="line"><span class="meta"># </span><span class="language-bash">内容:*/1 * * * * /usr/sbin/ntpdate ntp4.aliyun.com;</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">每分钟进行一次时钟同步，前提是可以联网</span></span><br></pre></td></tr></table></figure><h3 id="OpenVPN"><a href="#OpenVPN" class="headerlink" title="OpenVPN"></a>OpenVPN</h3><p>网上直接搜 openvpn-install.sh 脚本自动安装配置 OpenVPN server 和 client，下面是手动安装步骤。</p><h3 id="管理Python虚拟环境"><a href="#管理Python虚拟环境" class="headerlink" title="管理Python虚拟环境"></a>管理Python虚拟环境</h3><p>使用Virtaulenvwrapper来管理Python虚拟环境，将所有虚拟环境整合在一个目录下 - 管理（新增，删除，复制）虚拟环境 - 快速切换虚拟环境。</p><ol><li><p>安装</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">on Windows</span></span><br><span class="line">pip install virtualenvwrapper-win</span><br><span class="line"><span class="meta"># </span><span class="language-bash">on macOS / Linux</span></span><br><span class="line">pip install --user virtualenvwrapper</span><br><span class="line"><span class="meta"># </span><span class="language-bash"><span class="keyword">then</span> make Bash load virtualenvwrapper automatically</span></span><br><span class="line">vim ~/.bashrc</span><br><span class="line"></span><br><span class="line">export VIRTUALENVWRAPPER_PYTHON=/usr/bin/python3</span><br><span class="line">export WORKON_HOME=~/virtualenvs</span><br><span class="line">source /usr/local/bin/virtualenvwrapper.sh</span><br><span class="line"></span><br><span class="line">source ~/.bashrc</span><br></pre></td></tr></table></figure></li><li><p>创建虚拟环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">on macOS/Linux:</span></span><br><span class="line">mkvirtualenv --python=python3.6 env</span><br><span class="line"><span class="meta"># </span><span class="language-bash">on Windows</span></span><br><span class="line">mkvirtualenv --python=python3 env</span><br></pre></td></tr></table></figure></li><li><p>管理环境</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">workon  # 列出虚拟环境列表</span><br><span class="line">workon env  # 切换环境到env</span><br><span class="line">deactivate  # 退出环境</span><br><span class="line">rmvirtualenv env  # 删除指定环境</span><br></pre></td></tr></table></figure></li></ol><h3 id="nethogs流量监控"><a href="#nethogs流量监控" class="headerlink" title="nethogs流量监控"></a>nethogs流量监控</h3><p>NetHogs 是一个开源的命令行工具(类似于Linux的top命令)，用来按进程或程序实时统计网络带宽使用率。</p><p>最好是放在screen中执行，不然会随着当前终端的关闭而终止。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">yum -y install nethogs  # 安装</span><br><span class="line">nethogs  # 简单使用</span><br><span class="line">nethogs -d 5  # 5s刷新一次，默认3s</span><br><span class="line">nethogs -v 3  # 按M为单位返回数据</span><br><span class="line">nethogs -d 60 -v 3 &gt;&gt; nethogs.log  # 将查询的数据写入文件，使用cat可以查看</span><br></pre></td></tr></table></figure><h3 id="screen多重视窗管理"><a href="#screen多重视窗管理" class="headerlink" title="screen多重视窗管理"></a>screen多重视窗管理</h3><p><strong>Screen</strong>是一款由GNU计划开发的用于命令行终端切换的自由软件。用户可以通过该软件同时连接多个本地或远程的命令行会话，并在其间自由切换。GNU Screen可以看作是窗口管理器的命令行界面版本。它提供了统一的管理多个会话的界面和相应的功能。</p><p><strong>会话恢复</strong></p><p>只要Screen本身没有终止，在其内部运行的会话都可以恢复。这一点对于远程登录的用户特别有用——即使网络连接中断，用户也不会失去对已经打开的命令行会话的控制。只要再次登录到主机上执行screen -r就可以恢复会话的运行。同样在暂时离开的时候，也可以执行分离命令detach，在保证里面的程序正常运行的情况下让Screen挂起（切换到后台）。这一点和图形界面下的VNC很相似。</p><p><strong>多窗口</strong></p><p>在Screen环境下，所有的会话都独立的运行，并拥有各自的编号、输入、输出和窗口缓存。用户可以通过快捷键在不同的窗口下切换，并可以自由的重定向各个窗口的输入和输出。Screen实现了基本的文本操作，如复制粘贴等；还提供了类似滚动条的功能，可以查看窗口状况的历史记录。窗口还可以被分区和命名，还可以监视后台窗口的活动。 会话共享 Screen可以让一个或多个用户从不同终端多次登录一个会话，并共享会话的所有特性（比如可以看到完全相同的输出）。它同时提供了窗口访问权限的机制，可以对窗口进行密码保护。</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">yum -y install screen  # 安装</span><br><span class="line">screen -S yourname  # 新建一个叫yourname的session（常用）</span><br><span class="line">screen -ls  # 列出当前所有的session（常用）</span><br><span class="line">screen -r yourname  # 进入yourname这个session（常用）</span><br><span class="line">screen -d yourname  # 远程detach某个session</span><br><span class="line">screen -d -r yourname  # 结束当前session并回到yourname这个session</span><br><span class="line">screen -wipe  # 检查目前所有的screen作业，并删除已经无法使用的screen作业（常用）</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">在每个screen session 下，所有命令都以 ctrl+a(C-a) 开始，键入以下命令操作（常用d暂时离开当前会话）</span></span><br><span class="line">C-a ?  # 显示所有键绑定信息</span><br><span class="line">C-a c  # 创建一个新的运行shell的窗口并切换到该窗口</span><br><span class="line">C-a n  # Next，切换到下一个 window </span><br><span class="line">C-a p  # Previous，切换到前一个 window </span><br><span class="line">C-a 0..9  # 切换到第 0..9 个 window</span><br><span class="line">C-a [Space]  # 由视窗0循序切换到视窗9</span><br><span class="line">C-a C-a  # 在两个最近使用的 window 间切换 </span><br><span class="line">C-a x  # 锁住当前的 window，需用用户密码解锁</span><br><span class="line">C-a d  # detach，暂时离开当前session，将目前的 screen session (可能含有多个 windows) 丢到后台执行，并会回到还没进 screen 时的状态，此时在 screen session 里，每个 window 内运行的 process (无论是前台/后台)都在继续执行，即使 logout 也不影响。 </span><br><span class="line">C-a z  # 把当前session放到后台执行，用 shell 的 fg 命令则可回去。</span><br><span class="line">C-a w  # 显示所有窗口列表</span><br><span class="line">C-a t  # time，显示当前时间，和系统的 load </span><br><span class="line">C-a k  # kill window，强行关闭当前的 window</span><br><span class="line">C-a [  # 进入 copy mode，在 copy mode 下可以回滚、搜索、复制就像用使用 vi 一样</span><br><span class="line">    C-b Backward，PageUp </span><br><span class="line">    C-f Forward，PageDown </span><br><span class="line">    H(大写) High，将光标移至左上角 </span><br><span class="line">    L Low，将光标移至左下角 </span><br><span class="line">    0 移到行首 </span><br><span class="line">    $ 行末 </span><br><span class="line">    w forward one word，以字为单位往前移 </span><br><span class="line">    b backward one word，以字为单位往后移 </span><br><span class="line">    Space 第一次按为标记区起点，第二次按为终点 </span><br><span class="line">    Esc 结束 copy mode </span><br><span class="line">C-a ]  # paste，把刚刚在 copy mode 选定的内容贴上</span><br></pre></td></tr></table></figure><h3 id="Centos中文乱码"><a href="#Centos中文乱码" class="headerlink" title="Centos中文乱码"></a>Centos中文乱码</h3><p>是在解决docker中的centos8显示中文乱码的时候找到这个解决办法的，安装中文包在设置支持的语言包即可</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">yum search Chinese</span><br><span class="line">yum -y install langpacks-zh_CN.noarch</span><br><span class="line">localectl set-locale LANG=zh_CN.utf8</span><br><span class="line"><span class="comment"># 如果还未生效重启当前会话</span></span><br></pre></td></tr></table></figure><h3 id="wget下载网站某个目录所有文件"><a href="#wget下载网站某个目录所有文件" class="headerlink" title="wget下载网站某个目录所有文件"></a>wget下载网站某个目录所有文件</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">wget -np -nH -r -k -p –span-hosts https://www.docs4dev.com/docs/en/apache-hive/3.1.1</span><br><span class="line">/reference</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">-c 断点续传</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">-r 递归下载，下载指定网页某一目录下（包括子目录）的所有文件</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">-nd 递归下载时不创建一层一层的目录，把所有的文件下载到当前目录</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">-np 递归下载时不搜索上层目录，如wget -c -r www.xianren.org/pub/path/</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">没有加参数-np，就会同时下载path的上一级目录pub下的其它文件</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">-k 将绝对链接转为相对链接，下载整个站点后脱机浏览网页，最好加上这个参数</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">-L 递归时不进入其它主机，如wget -c -r www.xianren.org/</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">如果网站内有一个这样的链接：</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">www.xianren.org，不加参数-L，就会像大火烧山一样，会递归下载www.xianren.org网站</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">-p 下载网页所需的所有文件，如图片等</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">-A 指定要下载的文件样式列表，多个样式用逗号分隔</span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">-i 后面跟一个文件，文件内指明要下载的URL</span></span><br></pre></td></tr></table></figure><h3 id="Centos8安装Openvpn无法上网"><a href="#Centos8安装Openvpn无法上网" class="headerlink" title="Centos8安装Openvpn无法上网"></a>Centos8安装Openvpn无法上网</h3><p>1、开启内核 IP 地址转发<br>首先查看内核是否开启 IP 地址转发功能</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat /proc/sys/net/ipv4/ip_forward</span><br></pre></td></tr></table></figure><p>返回为 1 已开启，返回 0 则需要手动开一下。<br>以 root 用户身份执行</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">echo &#x27;net.ipv4.ip_forward = 1&#x27; &gt;&gt; /etc/sysctl.conf</span><br><span class="line">sysctl -p /etc/sysctl.conf</span><br><span class="line">cat /proc/sys/net/ipv4/ip_forward</span><br></pre></td></tr></table></figure><p>2、防火墙允许 IP 地址转发<br>默认情况下 firewalld 会禁止转发流量，可以执行 firewall-cmd –query-masquerade 查看状态，如果是 no，可执行下面的命令开启转发。<br>开启 IP 地址转发</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">firewall-cmd --add-masquerade --permanent  #开启 IP 地址转发</span><br><span class="line">firewall-cmd --reload  #重载防火墙规则，使之立即生效</span><br></pre></td></tr></table></figure><h3 id="Openvpn不让所有流量都走VPN"><a href="#Openvpn不让所有流量都走VPN" class="headerlink" title="Openvpn不让所有流量都走VPN"></a>Openvpn不让所有流量都走VPN</h3><p>通过自动安装脚本<code>openvpn-install.sh</code>安装的Openvpn默认设置的所有流量都经过VPN，就会造成异地登陆，网络卡顿，耗费服务器带宽的影响，所以一般只允许Openvpn指定网段下的地址走VPN，其余还是走本地，这样设置也可以同时连接多个VPN不冲突。</p><p>在客户端加入<code>route-nopull</code>配置目的是不从服务端拉取路由表，对于WIN10中的客户端配置需要删除<code>block-outside-dns</code>这一行，Linux则可以不删，因为这个设置， OpenVPN 会添加 Windows 防火墙记录。</p><h3 id="Vim相关配置"><a href="#Vim相关配置" class="headerlink" title="Vim相关配置"></a>Vim相关配置</h3><p>修改~&#x2F;.vimrc文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">&quot; 显示中文 &quot;</span></span><br><span class="line">  <span class="built_in">set</span> fileencodings=utf-8,ucs-bom,gb18030,gbk,gb2312,cp936</span><br><span class="line">  <span class="built_in">set</span> termencoding=utf-8</span><br><span class="line">  <span class="built_in">set</span> encoding=utf-8</span><br><span class="line"><span class="string">&quot; 自动语法高亮 &quot;</span> </span><br><span class="line">  syntax on</span><br><span class="line"><span class="string">&quot; 检测文件类型  &quot;</span></span><br><span class="line">  filetype on</span><br><span class="line"><span class="string">&quot; 设定 tab 长度为 4  &quot;</span></span><br><span class="line">  <span class="built_in">set</span> tabstop=4</span><br><span class="line"><span class="string">&quot; 设置按BackSpace的时候可以一次删除掉4个空格 &quot;</span> </span><br><span class="line">  <span class="built_in">set</span> softtabstop=4</span><br><span class="line"><span class="string">&quot; 搜索时忽略大小写，但在有一个或以上大写字母时仍大小写敏感  &quot;</span></span><br><span class="line">  <span class="built_in">set</span> ignorecase</span><br><span class="line">  <span class="built_in">set</span> smartcase</span><br><span class="line"><span class="string">&quot; 实时搜索  &quot;</span></span><br><span class="line">  <span class="built_in">set</span> incsearch</span><br><span class="line"><span class="string">&quot; 搜索时高亮显示被找到的文本  &quot;</span></span><br><span class="line">  <span class="built_in">set</span> hlsearch</span><br><span class="line"><span class="string">&quot; 关闭错误声音  &quot;</span></span><br><span class="line">  <span class="built_in">set</span> noerrorbells</span><br><span class="line">  <span class="built_in">set</span> novisualbell</span><br><span class="line"><span class="string">&quot; 智能自动缩进  &quot;</span></span><br><span class="line">  <span class="built_in">set</span> smartindent</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Language </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Linux </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Vuepress操作说明</title>
      <link href="/2019/01/01/README/"/>
      <url>/2019/01/01/README/</url>
      
        <content type="html"><![CDATA[<p>::: tip<br>Vuepress 基本使用以及 Make down 语法<br>:::</p><span id="more"></span><h3 id="Vuepress安装"><a href="#Vuepress安装" class="headerlink" title="Vuepress安装"></a>Vuepress安装</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta"># </span><span class="language-bash">安装nodejs指定版本</span></span><br><span class="line">yum remove nodejs npm -y</span><br><span class="line">curl -sL https://rpm.nodesource.com/setup_8.x | bash -</span><br><span class="line">yum -y install nodejs</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">安装yarn</span></span><br><span class="line">curl --silent --location https://dl.yarnpkg.com/rpm/yarn.repo | sudo tee /etc/yum.repos.d/yarn.repo</span><br><span class="line">rpm --import https://dl.yarnpkg.com/rpm/pubkey.gpg</span><br><span class="line">yum install -y yarn</span><br><span class="line"></span><br><span class="line">node -v</span><br><span class="line">yarn -v</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">安装vuepress-theme-reco</span></span><br><span class="line">yarn global add @vuepress-reco/theme-cli</span><br><span class="line">theme-cli init blog</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">安装所需插件</span></span><br><span class="line">cd blog</span><br><span class="line">yarn install</span><br><span class="line">yarn dev</span><br><span class="line"><span class="meta"></span></span><br><span class="line"><span class="meta"># </span><span class="language-bash">中文图片路径支持</span></span><br><span class="line">yarn add markdown-it-disable-url-encode</span><br><span class="line"><span class="meta"># </span><span class="language-bash">中文文章名转为拼音</span></span><br><span class="line">yarn add vuepress-plugin-permalink-pinyin</span><br><span class="line"><span class="meta"># </span><span class="language-bash">代码复制</span></span><br><span class="line">yarn add vuepress-plugin-code-copy</span><br><span class="line"></span><br></pre></td></tr></table></figure><p>修改<code>.vuepress/config.js</code>文件</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 中文图片路径支持</span></span><br><span class="line"><span class="variable language_">module</span>.<span class="property">exports</span> = &#123;</span><br><span class="line">  <span class="attr">markdown</span>: &#123;</span><br><span class="line">    <span class="attr">extendMarkdown</span>: <span class="function"><span class="params">md</span> =&gt;</span> &#123;</span><br><span class="line">      md.<span class="title function_">use</span>(<span class="built_in">require</span>(<span class="string">&quot;markdown-it-disable-url-encode&quot;</span>));</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 中文文章名转为拼音</span></span><br><span class="line"><span class="variable language_">module</span>.<span class="property">exports</span> = &#123;</span><br><span class="line">  <span class="attr">plugins</span>: [</span><br><span class="line">      [</span><br><span class="line">        <span class="string">&quot;permalink-pinyin&quot;</span>,</span><br><span class="line">        &#123;</span><br><span class="line">          <span class="string">&quot;lowercase&quot;</span>: <span class="literal">true</span>, <span class="comment">// Converted into lowercase, default: true</span></span><br><span class="line">          <span class="string">&quot;separator&quot;</span>: <span class="string">&quot;-&quot;</span>, <span class="comment">// Separator of the slug, default: &#x27;-&#x27;</span></span><br><span class="line">        &#125;,</span><br><span class="line">      ],</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 代码复制</span></span><br><span class="line"><span class="variable language_">module</span>.<span class="property">exports</span> = &#123;</span><br><span class="line">    <span class="attr">plugins</span>: [</span><br><span class="line">        [<span class="string">&#x27;vuepress-plugin-code-copy&#x27;</span>, <span class="literal">true</span>]</span><br><span class="line">    ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="标题"><a href="#标题" class="headerlink" title="标题"></a>标题</h3><figure class="highlight md"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 文章标题</span><br><span class="line">author: 汪寻</span><br><span class="line">date: 2021-09-15</span><br><span class="line">tags:</span><br><span class="line"><span class="bullet"> -</span> Other</span><br><span class="line">categories:</span><br><span class="line"><span class="section"> - Other</span></span><br><span class="line"><span class="section">---</span></span><br></pre></td></tr></table></figure><h3 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h3><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">::: tip</span><br><span class="line">这是摘要：Vuepress Make down语法</span><br><span class="line">:::</span><br><span class="line"></span><br><span class="line">&lt;!-- more --&gt;</span><br></pre></td></tr></table></figure><p>这是正文</p><h3 id="容器"><a href="#容器" class="headerlink" title="容器"></a>容器</h3><p>::: tip<br>这是一个提示<br>:::</p><p>::: warning<br>这是一个警告<br>:::</p><p>::: danger<br>这是一个危险警告<br>:::</p><p>::: details<br>这是一个详情块，在 IE &#x2F; Edge 中不生效<br>:::</p><p>::: danger STOP<br>危险区域，禁止通行<br>:::</p><p>::: details 点击查看代码</p><figure class="highlight js"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable language_">console</span>.<span class="title function_">log</span>(<span class="string">&#x27;你好，VuePress！&#x27;</span>)</span><br></pre></td></tr></table></figure><p>:::</p><h3 id="表格"><a href="#表格" class="headerlink" title="表格"></a>表格</h3><table><thead><tr><th>Tables</th><th align="center">Are</th><th align="right">Cool</th></tr></thead><tbody><tr><td>col 3 is</td><td align="center">right-aligned</td><td align="right">$1600</td></tr><tr><td>col 2 is</td><td align="center">centered</td><td align="right">$12</td></tr><tr><td>zebra stripes</td><td align="center">are neat</td><td align="right">$1</td></tr></tbody></table><h3 id="图片居中"><a href="#图片居中" class="headerlink" title="图片居中"></a>图片居中</h3><div align=center><img src="resources/Spark-Dependencies、Linage and Stage/血缘关系.png"></div><h3 id="代码高亮"><a href="#代码高亮" class="headerlink" title="代码高亮"></a>代码高亮</h3><figure class="highlight js"><figcaption><span>&#123;4&#125;</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">export</span> <span class="keyword">default</span> &#123;</span><br><span class="line">  data () &#123;</span><br><span class="line">    <span class="keyword">return</span> &#123;</span><br><span class="line">      <span class="attr">msg</span>: <span class="string">&#x27;Highlighted!&#x27;</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> Other </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Other </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
