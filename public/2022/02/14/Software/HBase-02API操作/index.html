<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><meta name="description" content="HBase-02API操作"><meta name="keywords" content="HBase"><meta name="author" content="WangXun"><meta name="copyright" content="WangXun"><title>HBase-02API操作 | Wake</title><link rel="shortcut icon" href="/img/favicon-blank.svg"><link rel="stylesheet" href="/css/index.css?version=1.9.1"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/font-awesome@latest/css/font-awesome.min.css?version=1.9.1"><meta name="format-detection" content="telephone=no"><meta http-equiv="x-dns-prefetch-control" content="on"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net"><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.css"><script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.1.1/dist/instantsearch.min.js" defer></script><link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/gitalk/dist/gitalk.min.css"><script src="https://cdn.jsdelivr.net/npm/gitalk@latest/dist/gitalk.min.js"></script><script src="https://cdn.jsdelivr.net/npm/blueimp-md5@2.10.0/js/md5.min.js"></script><meta http-equiv="Cache-Control" content="no-transform"><meta http-equiv="Cache-Control" content="no-siteapp"><script src="https://v1.hitokoto.cn/?encode=js&amp;charset=utf-8&amp;select=.footer_custom_text" defer></script><script>var GLOBAL_CONFIG = { 
  root: '/',
  algolia: {"appId":"HSM2EINL2X","apiKey":"6f1478b12150efd917d5ecfcddfb8b8b","indexName":"wangxun","hits":{"per_page":10},"languages":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}.","hits_stats":"${hits} results found in ${time} ms"}},
  localSearch: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  hexoVersion: '6.0.0'
} </script><meta name="generator" content="Hexo 6.0.0"></head><body><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar" data-display="true"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#Client-API"><span class="toc-number">1.</span> <span class="toc-text">Client API</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#HBase-Connection"><span class="toc-number">1.1.</span> <span class="toc-text">HBase Connection</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Create"><span class="toc-number">1.2.</span> <span class="toc-text">Create</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Put"><span class="toc-number">1.3.</span> <span class="toc-text">Put</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Get-x2F-Scan"><span class="toc-number">1.4.</span> <span class="toc-text">Get&#x2F;Scan</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Spark-%E8%AF%BB%E5%86%99"><span class="toc-number">2.</span> <span class="toc-text">Spark 读写</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Read"><span class="toc-number">2.1.</span> <span class="toc-text">Read</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Write"><span class="toc-number">2.2.</span> <span class="toc-text">Write</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B4%E5%90%88-Spark-DataSource"><span class="toc-number">2.3.</span> <span class="toc-text">整合 Spark DataSource</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="/img/avatar.jpg"></div><div class="author-info__name text-center">WangXun</div><div class="author-info__description text-center"></div><div class="follow-button"><a target="_blank" rel="noopener" href="https://github.com/w749">Follow Me</a></div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">71</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">20</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">6</span></a></div></div></div><div id="content-outer"><div id="top-container" style="background-image: url(/img/top-img.jpg)"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">Wake</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus">   <a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a></span><span class="pull-right"><a class="site-page social-icon search"><i class="fa fa-search"></i><span> Search</span></a></span></div><div id="post-info"><div id="post-title">HBase-02API操作</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2022-02-14</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"></i><a class="post-meta__categories" href="/categories/Software/">Software</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1.6k</span><span class="post-meta__separator">|</span><span>Reading time: 7 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div class="article-container" id="post-content"><p>HBase 的基础 API 操作，批量读写主要整合 Spark 来做数据的导入导出。</p>
<span id="more"></span>

<h3 id="Client-API"><a href="#Client-API" class="headerlink" title="Client API"></a>Client API</h3><p>代码主要使用 Scala 编写，只列出了基本的操作方法。新的 API 有两个入口，分别是 Admin 和 Table，Admin 管理表，Table 操作表数据，命令行支持的所有方法基本都能在这两个接口下找到。</p>
<h4 id="HBase-Connection"><a href="#HBase-Connection" class="headerlink" title="HBase Connection"></a>HBase Connection</h4><p>需要新建一个 HBase 配置，可以直接 addResource 或者使用 set 方法指定连接所需配置信息，然后再使用 Conenction 工厂方法创建连接，connection 用完记得 close。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="type">HBaseConfiguration</span>.create()</span><br><span class="line">conf.addResource(<span class="string">&quot;hbase-site.xml&quot;</span>)</span><br><span class="line"><span class="keyword">val</span> connection = <span class="type">ConnectionFactory</span>.createConnection(conf)</span><br></pre></td></tr></table></figure>

<h4 id="Create"><a href="#Create" class="headerlink" title="Create"></a>Create</h4><p>调用 Connection 的 getAdmin 方法返回 Admin 对象，检查表不存在后再建表。建表需要传入 ColumnFamilyDescriptor 对象，使用 ColumnFamilyDescriptorBuilder.newBuilder 创建对象并在里面指定建表所需的参数，最后直接调用 Admin 对象的 createTable 方法，参数可以传入单个或多个 ColumnFamilyDescriptor 对象。若要指定 namespace 直接跟表名写在一起：space:table，Admin对象用完需要 close。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createTable</span></span>(tableName: <span class="type">String</span>, columnFamily: <span class="type">List</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> admin = connection.getAdmin</span><br><span class="line">  <span class="keyword">if</span> (admin.tableExists(<span class="type">TableName</span>.valueOf(tableName))) &#123;</span><br><span class="line">    <span class="type">LOG</span>.warn(<span class="string">s&quot;表 <span class="subst">$&#123;tableName&#125;</span> 已存在&quot;</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">val</span> cfs = columnFamily.map(cf =&gt; <span class="type">ColumnFamilyDescriptorBuilder</span>.newBuilder(<span class="type">Bytes</span>.toBytes(cf)).build())</span><br><span class="line">    <span class="keyword">val</span> desc = <span class="type">TableDescriptorBuilder</span></span><br><span class="line">      .newBuilder(<span class="type">TableName</span>.valueOf(tableName))</span><br><span class="line">      .setColumnFamilies(cfs.asJavaCollection)</span><br><span class="line">      .build()</span><br><span class="line">    admin.createTable(desc)</span><br><span class="line">    <span class="type">LOG</span>.info(<span class="string">s&quot;表 <span class="subst">$&#123;tableName&#125;</span> 创建成功&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  admin.close()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">createTable(<span class="string">&quot;test&quot;</span>, <span class="string">&quot;info&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="Put"><a href="#Put" class="headerlink" title="Put"></a>Put</h4><p>put 需要在 Table 对象下操作，使用 getTable 并传入 TableName 对象得到 Table，然后新建一个 Put 对象，添加列簇、列限定符和数据等信息，再将它传给 Table 对象的 put 方法即可，最后再关闭 Table对象。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">putData</span></span>(tableName: <span class="type">String</span>, rowKey: <span class="type">String</span>, cf: <span class="type">String</span>, column: <span class="type">String</span>, value: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> table = connection.getTable(<span class="type">TableName</span>.valueOf(tableName))</span><br><span class="line">  <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>(<span class="type">Bytes</span>.toBytes(rowKey))</span><br><span class="line">  put.addColumn(<span class="type">Bytes</span>.toBytes(cf), <span class="type">Bytes</span>.toBytes(column), <span class="type">Bytes</span>.toBytes(value))</span><br><span class="line">  table.put(put)</span><br><span class="line">  <span class="type">LOG</span>.info(<span class="string">s&quot;table &#x27;<span class="subst">$&#123;tableName&#125;</span>&#x27; rowKey &#x27;<span class="subst">$&#123;rowKey&#125;</span>&#x27; insert complete&quot;</span>)</span><br><span class="line">  table.close()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">putData(<span class="string">&quot;test&quot;</span>, <span class="string">&quot;1&quot;</span>, <span class="string">&quot;info&quot;</span>, <span class="string">&quot;name&quot;</span>, <span class="string">&quot;Bob&quot;</span>)</span><br></pre></td></tr></table></figure>

<h4 id="Get-x2F-Scan"><a href="#Get-x2F-Scan" class="headerlink" title="Get&#x2F;Scan"></a>Get&#x2F;Scan</h4><p>我把 get 和 scan 功能整合在一个方法中，使用 Scan 构造对象的时候可以将 Get 对象传进去转为 Scan 对象。</p>
<p>首先还是先构建 Table 对象，这里先对列簇和列限定符参数做了一些判断。不指定列簇则返回所有列簇内的所有字段，列簇和 cfColumn 不可以同时指定，指定列簇则返回列簇包含的所有字段。指定列限定符则是通过 Map 的方式传进去，然后调用 Scan 的 addColumn 方法逐个新增到对象中。</p>
<p>调用 getScanner 方法后即可返回 Result 对象组成的列表，Result 对象内包含着一条记录的所有内容，它是由多个 Cell 组成的，每个 Cell 包含了<code>rowKey, columnFamily, qualifier, value</code>等数据，新版本中则使用 CellUtil.cloneRow(cell) 等方法取出对应的数据，除过 getTimestamp 方法其他的 Cell 获取属性的成员方法已经弃用，应避免使用。</p>
<p>最后别忘记关闭 scanner 和 table。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getData</span></span>(tableName: <span class="type">String</span>,</span><br><span class="line">            rowKey: <span class="type">String</span> = <span class="string">&quot;&quot;</span>,</span><br><span class="line">            cf: <span class="type">String</span> = <span class="string">&quot;&quot;</span>,</span><br><span class="line">            cfColumn: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>]()): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">var</span> scan = <span class="keyword">new</span> <span class="type">Scan</span>()</span><br><span class="line">  <span class="keyword">val</span> table = connection.getTable(<span class="type">TableName</span>.valueOf(tableName))</span><br><span class="line">  <span class="keyword">if</span> (cf.nonEmpty &amp;&amp; cfColumn.nonEmpty) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">RuntimeException</span>(<span class="string">&quot;限定字段在cfColumn中指定，cf和cfColumn不可同时指定&quot;</span>)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (rowKey.nonEmpty) &#123;</span><br><span class="line">    <span class="keyword">val</span> get = <span class="keyword">new</span> <span class="type">Get</span>(<span class="type">Bytes</span>.toBytes(rowKey))</span><br><span class="line">    scan = <span class="keyword">new</span> <span class="type">Scan</span>(get)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (cf.nonEmpty) &#123;</span><br><span class="line">    scan.addFamily(<span class="type">Bytes</span>.toBytes(cf))</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (cfColumn.nonEmpty) &#123;</span><br><span class="line">    cfColumn.foreach(col =&gt; scan.addColumn(<span class="type">Bytes</span>.toBytes(col._1), <span class="type">Bytes</span>.toBytes(col._2)))</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> scanner = table.getScanner(scan)</span><br><span class="line">  scanner.asScala.toList.foreach &#123;</span><br><span class="line">    result =&gt; result.rawCells().foreach &#123;</span><br><span class="line">      cell =&gt; &#123;</span><br><span class="line">        <span class="keyword">val</span> data = <span class="type">MyCell</span>.builder(<span class="type">CellUtil</span>.cloneRow(cell),</span><br><span class="line">          <span class="type">CellUtil</span>.cloneFamily(cell),</span><br><span class="line">          <span class="type">CellUtil</span>.cloneQualifier(cell),</span><br><span class="line">          <span class="type">CellUtil</span>.cloneValue(cell),</span><br><span class="line">          cell.getTimestamp).toString</span><br><span class="line">        println(data)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  scanner.close()</span><br><span class="line">  table.close()</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">getData(<span class="string">&quot;test&quot;</span>, <span class="string">&quot;1&quot;</span>)  <span class="comment">// get</span></span><br><span class="line">getData(<span class="string">&quot;test&quot;</span>, <span class="string">&quot;1&quot;</span>, cfColumn = <span class="type">Map</span>(<span class="string">&quot;info&quot;</span> -&gt; <span class="string">&quot;name&quot;</span>))  <span class="comment">// get指定列限定符</span></span><br><span class="line">getData(<span class="string">&quot;test&quot;</span>)  <span class="comment">// scan</span></span><br></pre></td></tr></table></figure>

<h3 id="Spark-读写"><a href="#Spark-读写" class="headerlink" title="Spark 读写"></a>Spark 读写</h3><h4 id="Read"><a href="#Read" class="headerlink" title="Read"></a>Read</h4><p>主要用到 SparkContext 的 newAPIHadoopRDD 将数据读为 RDD，再定义好 schema 转为 DataFrame，测试使用的是 Spark 2.3.2，中间会有些问题，但 debug 时数据确实可以取出来，生产中用到了再研究吧。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 使用Spark将HBase数据读取为DataFrame</span></span><br><span class="line"><span class="comment"> * @param connection Connection</span></span><br><span class="line"><span class="comment"> * @param session SparkSession</span></span><br><span class="line"><span class="comment"> * @param table table名字</span></span><br><span class="line"><span class="comment"> * @param cf 列簇</span></span><br><span class="line"><span class="comment"> * @param fields 字段</span></span><br><span class="line"><span class="comment"> * @return</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readAsDf</span></span>(connection: <span class="type">Connection</span>, session: <span class="type">SparkSession</span>, table: <span class="type">String</span>, cf: <span class="type">String</span>, fields: <span class="type">List</span>[<span class="type">String</span>]): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  <span class="comment">// 配置要读取的表和字段</span></span><br><span class="line">  <span class="keyword">val</span> hbaseConf: <span class="type">Configuration</span> = connection.getConfiguration</span><br><span class="line">  hbaseConf.set(<span class="type">TableInputFormat</span>.<span class="type">INPUT_TABLE</span>, table)</span><br><span class="line">  fields.filter(field =&gt; !field.equalsIgnoreCase(<span class="string">&quot;rowkey&quot;</span>))</span><br><span class="line">  hbaseConf.set(<span class="type">TableInputFormat</span>.<span class="type">SCAN_COLUMNS</span>,</span><br><span class="line">    fields</span><br><span class="line">      .filter(field =&gt; !field.equalsIgnoreCase(<span class="string">&quot;rowkey&quot;</span>))</span><br><span class="line">      .map(field =&gt; <span class="string">s&quot;<span class="subst">$&#123;cf&#125;</span>:<span class="subst">$&#123;field&#125;</span>&quot;</span>)</span><br><span class="line">      .mkString(<span class="string">&quot; &quot;</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 将数据读为RDD</span></span><br><span class="line">  <span class="keyword">val</span> hbaseRDD: <span class="type">RDD</span>[(<span class="type">ImmutableBytesWritable</span>, <span class="type">Result</span>)] = session.sparkContext.newAPIHadoopRDD(</span><br><span class="line">    hbaseConf,</span><br><span class="line">    classOf[<span class="type">TableInputFormat</span>],</span><br><span class="line">    classOf[<span class="type">ImmutableBytesWritable</span>],</span><br><span class="line">    classOf[<span class="type">Result</span>])</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 解析读出来的数据为目标RDD</span></span><br><span class="line">  <span class="keyword">val</span> rowRDD = hbaseRDD.map&#123;row =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> values: <span class="type">ListBuffer</span>[<span class="type">String</span>] = <span class="keyword">new</span> <span class="type">ListBuffer</span>[<span class="type">String</span>]</span><br><span class="line">    <span class="keyword">val</span> result: <span class="type">Result</span>             = row._2</span><br><span class="line">    <span class="keyword">val</span> cells = result.rawCells()</span><br><span class="line">    <span class="keyword">for</span> (cell &lt;- cells) &#123;</span><br><span class="line">      println(<span class="type">Bytes</span>.toString(<span class="type">CellUtil</span>.cloneRow(cell)))</span><br><span class="line">      println(<span class="type">Bytes</span>.toString(<span class="type">CellUtil</span>.cloneValue(cell)))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span> (i &lt;- fields.indices) &#123;</span><br><span class="line">      <span class="keyword">if</span> (fields(i).equalsIgnoreCase(<span class="string">&quot;rowkey&quot;</span>)) &#123;</span><br><span class="line">        values += <span class="type">Bytes</span>.toString(result.getRow)</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        values += <span class="type">Bytes</span>.toString(result.getValue(<span class="type">Bytes</span>.toBytes(cf), <span class="type">Bytes</span>.toBytes(fields(i))))</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="type">Row</span>.fromSeq(values.toList)</span><br><span class="line">  &#125;&#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 构建DataFrame Schema并创建</span></span><br><span class="line">  <span class="keyword">val</span> schema = <span class="type">StructType</span>(</span><br><span class="line">    fields.map(field =&gt; <span class="type">DataTypes</span>.createStructField(field, <span class="type">DataTypes</span>.<span class="type">StringType</span>, <span class="literal">true</span>)))</span><br><span class="line">  <span class="keyword">val</span> dataFrame = session.createDataFrame(rowRDD, schema)</span><br><span class="line"></span><br><span class="line">  session.close()</span><br><span class="line">  dataFrame</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="Write"><a href="#Write" class="headerlink" title="Write"></a>Write</h4><p>主要是用到 HBase 的 BulkLoad 功能，原理是直接将数据写入 HFile 再加载到 Region 中，好处是少了写入 MemStore 和 WAL，没了刷写的步骤，效率提升不少，但数据写入期间如果丢失无法恢复。在批量写入大量数据时可以选择这种方法。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 通过Spark写入DataFrame到HBase</span></span><br><span class="line"><span class="comment"> * @param connection Connection</span></span><br><span class="line"><span class="comment"> * @param session SparkSession</span></span><br><span class="line"><span class="comment"> * @param data DataFrame数据</span></span><br><span class="line"><span class="comment"> * @param tableName 表名</span></span><br><span class="line"><span class="comment"> * @param rowKey 以哪一列作为rowKey字段</span></span><br><span class="line"><span class="comment"> * @param cf 写入到哪个列簇</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">writeFromDf</span></span>(connection: <span class="type">Connection</span>, session: <span class="type">SparkSession</span>, data: <span class="type">DataFrame</span>, tableName: <span class="type">String</span>, rowKey: <span class="type">String</span>, cf: <span class="type">String</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> hbaseConf: <span class="type">Configuration</span> = connection.getConfiguration</span><br><span class="line">  <span class="keyword">var</span> fields = data.columns</span><br><span class="line">  <span class="keyword">val</span> table = <span class="type">TableName</span>.valueOf(tableName.getBytes())</span><br><span class="line">  <span class="keyword">val</span> stagingDir = <span class="string">&quot;/tmp/HBaseBulkLoad&quot;</span>  <span class="comment">// 不会覆盖，需要手动删除或每次用完后删除</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">//去掉rowKey字段</span></span><br><span class="line">  fields = fields.dropWhile(_ == rowKey)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> hbaseContext = <span class="keyword">new</span> <span class="type">HBaseContext</span>(session.sparkContext, hbaseConf)</span><br><span class="line"></span><br><span class="line">  <span class="comment">//将DataFrame转换bulkLoad需要的RDD格式</span></span><br><span class="line">  <span class="keyword">val</span> rddTmp: <span class="type">RDD</span>[<span class="type">Array</span>[(<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[(<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[<span class="type">Byte</span>])])]] = data.rdd.map(row =&gt; &#123;</span><br><span class="line">    <span class="keyword">val</span> rk = row.getAs[<span class="type">String</span>](rowKey)</span><br><span class="line"></span><br><span class="line">    fields.map(field =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> value = row.getAs[<span class="type">String</span>](field)</span><br><span class="line">      (<span class="type">Bytes</span>.toBytes(rk), <span class="type">Array</span>((<span class="type">Bytes</span>.toBytes(cf), <span class="type">Bytes</span>.toBytes(field), <span class="type">Bytes</span>.toBytes(value))))</span><br><span class="line">    &#125;)</span><br><span class="line">  &#125;)</span><br><span class="line">  <span class="keyword">val</span> rddNew: <span class="type">RDD</span>[(<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[(<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[<span class="type">Byte</span>])])] = rddTmp.flatMap(array =&gt; array)</span><br><span class="line">  rddNew.hbaseBulkLoad(hbaseContext, table,</span><br><span class="line">    t =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> rowKey = t._1</span><br><span class="line">      <span class="keyword">val</span> family:<span class="type">Array</span>[<span class="type">Byte</span>] = t._2(<span class="number">0</span>)._1</span><br><span class="line">      <span class="keyword">val</span> qualifier = t._2(<span class="number">0</span>)._2</span><br><span class="line">      <span class="keyword">val</span> value = t._2(<span class="number">0</span>)._3</span><br><span class="line">      <span class="keyword">val</span> keyFamilyQualifier= <span class="keyword">new</span> <span class="type">KeyFamilyQualifier</span>(rowKey, family, qualifier)</span><br><span class="line"></span><br><span class="line">      <span class="type">Seq</span>((keyFamilyQualifier, value)).iterator</span><br><span class="line">    &#125;,</span><br><span class="line">    stagingDir)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// bulk load start</span></span><br><span class="line">  <span class="keyword">val</span> loader = <span class="keyword">new</span> <span class="type">LoadIncrementalHFiles</span>(hbaseConf)</span><br><span class="line">  loader.doBulkLoad(<span class="keyword">new</span> <span class="type">Path</span>(stagingDir), connection.getAdmin, connection.getTable(table),</span><br><span class="line">    connection.getRegionLocator(table))</span><br><span class="line"></span><br><span class="line">  session.close()</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h4 id="整合-Spark-DataSource"><a href="#整合-Spark-DataSource" class="headerlink" title="整合 Spark DataSource"></a>整合 Spark DataSource</h4></div></article><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info"><a href="mailto:undefined">WangXun</a></span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info"><a href="https://wangxukun.top/2022/02/14/Software/HBase-02API操作/">https://wangxukun.top/2022/02/14/Software/HBase-02API操作/</a></span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a target="_blank" rel="noopener" href="https://creativecommons.org/licenses/by-nc-sa/4.0/">CC BY-NC-SA 4.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/HBase/">HBase</a></div><nav id="pagination"><div class="prev-post pull-left"><a href="/2022/02/16/Software/HBase-03%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1%E5%92%8C%E8%AF%BB%E5%86%99%E8%BF%87%E7%A8%8B/"><i class="fa fa-chevron-left">  </i><span>HBase-03架构设计和读写过程</span></a></div><div class="next-post pull-right"><a href="/2022/02/09/Software/HBase-01%E5%9F%BA%E7%A1%80%E6%A6%82%E5%BF%B5/"><span>HBase-01基础概念</span><i class="fa fa-chevron-right"></i></a></div></nav><div id="gitalk-container"></div><script>var gitalk = new Gitalk({
  clientID: 'aaf2844e0aeef4917c17',
  clientSecret: '10b96d24dffda7d3b4544778cf620f81990b676d',
  repo: 'blog-issue',
  owner: 'w749',
  admin: 'w749',
  id: md5(decodeURI(location.pathname)),
  language: 'en'
})
gitalk.render('gitalk-container')</script></div></div><footer class="footer-bg" style="background-image: url(/img/top-img.jpg)"><div class="layout" id="footer"><div class="copyright">&copy;2019 - 2022 By WangXun</div><div class="framework-info"><span>Driven - </span><a target="_blank" rel="noopener" href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a target="_blank" rel="noopener" href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="footer_custom_text">hitokoto</div><div class="busuanzi"><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-eye"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="https://cdn.jsdelivr.net/npm/animejs@latest/lib/anime.min.js"></script><script src="https://cdn.jsdelivr.net/npm/jquery@latest/dist/jquery.min.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/fancybox@latest/dist/jquery.fancybox.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-animate@latest/velocity.min.js"></script><script src="https://cdn.jsdelivr.net/npm/velocity-ui-pack@latest/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.9.1"></script><script src="/js/fancybox.js?version=1.9.1"></script><script src="/js/sidebar.js?version=1.9.1"></script><script src="/js/copy.js?version=1.9.1"></script><script src="/js/fireworks.js?version=1.9.1"></script><script src="/js/transition.js?version=1.9.1"></script><script src="/js/scroll.js?version=1.9.1"></script><script src="/js/head.js?version=1.9.1"></script><script src="/js/search/algolia.js"></script><script>if(/Android|webOS|iPhone|iPod|iPad|BlackBerry/i.test(navigator.userAgent)) {
  $('#nav').addClass('is-mobile')
  $('footer').addClass('is-mobile')
  $('#top-container').addClass('is-mobile')
}</script><div class="search-dialog" id="algolia-search"><div class="search-dialog__title" id="algolia-search-title">Algolia</div><div id="algolia-input-panel"><div id="algolia-search-input"></div></div><hr><div id="algolia-search-results"><div id="algolia-hits"></div><div id="algolia-pagination"></div><div id="algolia-stats"></div></div><span class="search-close-button"><i class="fa fa-times"></i></span></div><div class="search-mask"></div></body></html>