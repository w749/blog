---
title: 常用软件命令
author: 汪寻
date: 2020-10-18 16:17:23
updated: 2020-10-18 16:38:47
tags:
 - Other
categories:
 - Language
---

常用软件的安装方法以及命令操作。

<!-- more -->

## MySQL

### MySQL安装

#### 删除老版本

1. 检查是否有MySQL相关包
   
   ```shell
   rpm -qa|grep -i mysql
   ```

2. 删除之前安装的MySQL
   删除命令：rpm -e –nodeps 包名；
   如果提示依赖包错误，则使用以下命令尝试：rpm -ev 包名 --nodeps；
   如果提示错误：error: %preun(xxxxxx) scriptlet failed, exit status 1，则用以下命令尝试：rpm -e --noscripts 包名。

3. 删除以前的MySQL目录、文件
   
   ```shell
   find / -name mysql
   ```
   
   删除对应目录；
   注意：卸载后/etc/my.cnf不会删除，需要进行手工删除：rm -rf /etc/my.cnf。

4. 再次查找是否存在MySQL包
   
   ```shell
   rpm -qa|grep -i mysql
   ```

#### 安装前的准备

1. 下载包
   
   ```shell
   wget https://downloads.mysql.com/archives/get/p/23/file/mysql-8.0.21-linux-glibc2.12-x86_64.tar.xz
   ```

2. 解压包
   解压包到指定位置并修改文件名：
   
   ```shell
   tar -xvf mysql-8.0.21-linux-glibc2.12-x86_64.tar.xz -C /usr/local
   cd /usr/local
   mv mysql-8.0.21-linux-glibc2.12-x86_64 mysql
   mkdir -p /usr/local/mysql/data
   ```

3. 创建用户组分配权限
   
   ```shell
   groupadd mysql
   useradd -r -g mysql mysql
   chown mysql.mysql -R /usr/local/mysql
   ```

4. 配置文件
   vim /etc/my.profile后添加以下内容，路径随安装路径改变
   
   ```shell
   [mysqld]
   bind-address=0.0.0.0
   port=3306
   user=mysql
   basedir=/usr/local/mysql
   datadir=/usr/local/mysql/data
   socket=/tmp/mysql.sock
   log-error=/usr/local/mysql/data/mysql.err
   pid-file=/usr/local/mysql/data/mysql.pid
   #character config
   character_set_server=utf8mb4
   symbolic-links=0
   explicit_defaults_for_timestamp=true
   ```

#### 初始化登录

1. 初始化
   首先安装依赖libaio，随后cd到mysql/bin目录下初始化
   初始化完成后需要去查看临时密码以供登录
   
   ```shell
   yum install -y libaio
   cd /usr/local/mysql/bin
   # 初始化
   ./mysqld --defaults-file=/etc/my.cnf --basedir=/usr/local/mysql/ --datadir=/usr/local/mysql/data/ --user=mysql --initialize
   # 查看临时密码，在最后一行
   cat /usr/local/mysql/data/mysql.err
   # 将mysql.server放置到 /etc/init.d/mysql中
   cp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysql
   # 启动Mysql
   service mysql start
   # 使用临时密码登录MySQL
   ./mysql -uroot -p
   ```

2. 登录修改密码
   
   ```sql
   # 修改密码
   alter user 'root'@'localhost' identified by '123456'；
   alter user 'root'@'localhost' password expire never;
   flush privileges;
   quit;
   ```
   
   这时候需要退出登录，重新使用修改后的密码登录
   
   ```sql
   # 使root可以在任何host访问
   update user set host = '%' where user = 'root';
   flush privileges;
   quit;
   ```

### 远程连接服务器MySQL

以阿里云Centos8云服务器为例，需要注意的是需要使用公网ip，若是个人电脑则需要两台电台在同一个网段下才可以。

1. 服务器端MySQL设置
   
   ```sql
   # 登录mysql
   use mysql;
   # 更新host允许外部访问
   update user set host='%' where user ='root';
   # 刷新权限
   flush privileges;
   ```
   
   以上是root账号和密码，若需新增用户需要分配对应的读写权限。

2. 服务器端开放端口
   
   ```shell
   # 查看防火墙状态
   systemctl status firewalld
   # 查看3306端口是否开放
   firewall-cmd --query-port=3306/tcp
   # 开放防火墙端口3306
   firewall-cmd --zone=public --add-port=3306/tcp --permanent
   # 重启防火墙
   systemctl restart firewalld
   ```
   
   这是Centos8的设置方法，若是其他系统可能方式不一样。

3. 最重要的步骤
   阿里云服务器需要在控制台设置安全组，开放3306端口，否则还是无法远程访问。

### MySQL主从复制

前提条件是从MySQL（slave）可以远程访问主MySQL（master）

#### **配置master机器**

首先需要在master开启binlog日志，其次指定binlog日志目录（如果已有就不要改了）和server-id=1；

```shell
vim /etc/my.cnf

# 添加以下代码到文件最后
log-bin = "/usr/local/mysql/bin-log"
server_id = 1
binlog-do-db = mydb  # 复制哪个数据库

# 配置完成后需要重启MySQL
service mysql restart
```

接下来需要分配一个账号并给replacation slave权限（这里容易出错，要测试账号），查看master状态并记住File和Position对应的值，后面会用到。

```sql
# 查看log_bin状态
show variables like '%log_bin%';
# 新建账号并分配权限

# 查看master状态，注意File和Position
show master status;
```

最后需要把现有数据库的数据迁移到slave数据库，position是偏移量，从现有数据库状态的基础上记录日志的行数位置，这里需要同版本同系统，若不相同就不要使用Mysqldump方法了

#### **配置slave**

首先需要在slave开启binlog日志，这里和master一样，其次指定binlog日志目录（如果已有就不要改了）和server-id=2

```shell
vim /etc/my.cnf
# 添加以下代码到文件最后
log-bin = "/usr/local/mysql/bin-log"
server_id = 2
# 配置完成后需要重启MySQL
service mysql restart
```

接下来登录MySQL进行配置

```sql
# 关闭slave服务后再进行操作
stop slave;
# master_log_file和master_log_pos对应刚才master的file和position
change master to master_host='8.131.90.208',master_user='root',master_password='Wang1024...',master_log_file='mysql-bin.000002',master_log_pos=775;

# 启用slave并检查状态
start slave;
show slave status;
```

主从复制配置到这里就结束了，需要注意的地方有master账号的权限，尽量不要使用root账号；其次是server-id的设置；再然后是复制的前提是从服务器已存在目标数据库；最后是file和position的正确配置。

#### **故障处理**

**问题一**：如果是表结构的问题，可以先停止服务，修改表结构和master相同再start slave

**问题二**：出现“log event entry exceeded max_allowed_pack”错误。

如果在应用中使用大的BLOG列或者长字符串，那么在从服务器上回复时可能会出现“log event entryexceeded max_allowed_pack”错误，这是因为含有大文本的记录无法通过网络进行传输，解决方法是在主从服务器上添加max_allowed_packet参数（默认设置是1MB）

```sql
show variables like'MAX_ALLOWED_PACKET';
set @@global.max_allowed_packet=16777216;  # 16M
```

同时，在my.cnf里设置max_allowed_packet=16MB，数据库重新启动之后该参数将有效。

**问题三**：多主复制时的自增长变量冲突问题。

大多数情况下使用一台主服务器对一台或者多台从服务器，但是在某些情况下可能会将多个服务器配置为复制主服务器，所以使用auto_increment时应采取特殊步骤以防止键值冲突，否则插入行时多个主服务器会试图使用相同的auto_increment值。

服务器变量auto_increment_increment和auto_increment_offset可以协调多主服务器复制和auto_increment列。

在多主服务器复制到从服务器过程中，迟早会发生主键冲突，为了解决这种情况，可以重新设置不同主服务器的这两个参数，比如在A数据库服务器上设置auto_increment_increment=1、auto_increment_offset=1，在B数据库服务器上设置auto_increment_increment=1、auto_increment_offset=0。

**问题四**：利用pos偏移量解决小问题

不建议使用这种方法，可能会造成数据丢失，而且bin-log数据文件可能因为过大或者重启服务已变更为新的文件。

#### **切换主从服务器**

在实际工作环境中，有时候遇到这样的问题：在一个工作环境中，有一个主数据库服务器A，两个从数据库服务器B、C同时指向主数据库服务器，当主数据库服务器A发生故障时，需要将其中的一个从数据库B服务器切换成主数据库，同时修改数据库C服务器的配置，使其指向新的主数据库B。

1. 首先要确保所有的从数据库都已经执行了relay log中的全部更新，查看从数据库的状态是否是Has read all relay log（是否更新都已经执行完成）。

2. 在从数据库B上停止slave服务，然后执行reset master，重置成主数据库。（前提是确保/etc/my.cnf中设置了bin-log地址，并且重启服务器）

3. 在从数据库B上添加具有replication权限的用户rep1，查询主数据库状态。在从数据库C上配置复制的参数。在从数据库C上执行show slave status命令，查看从数据库服务是否成功启动。
   
   ```sql
   # 查看relay log是否全部更新，查看Slave_SQL_Running_Statu字段是否为Slave has read all relay log
   show slave status;
   # B服务器停止slave，配置bin-log地址，重置成主数据库
   stop slave;
   reset master;
   show master status;
   # 需要重新配置数据库C，方法就没什么差别了，停止服务，配置slave参数，启动服务
   change master to master_host='8.131.90.208',master_user='root',master_password='Wang1024...',master_log_file='mysql-bin.000002',master_log_pos=775;
   ```

#### 多源复制

MySQL 8.0添加了多源复制功能，可以实现多主服务器和一从服务器的复制。

1. 如果在主服务器进行了分库分表的操作，可以在从服务器进行数据汇总。为了实现后期的一些数据统计功能，往往需要把数据汇总在一起再统计。
2. 在从服务器时对主服务器的数据进行备份，在MySQL 8.0之前每一个主服务器都需要一个从服务器，很容易造成资源浪费，同时也加大了数据库管理员的维护成本；MySQL 8.0则引入了多源复制，可以把多个主服务器的数据同步到一个从服务器进行备份。

## Redis

### Redis安装

1. 下载解压
   
   ```shell
   wget http://download.redis.io/releases/redis-6.0.9.tar.gz  # 下载
   tar -zxvf redis-6.0.9.tar.gz  # 解压
   make PREFIX=/usr/local/redis install  # 安装到/usr/local/redis
   cd /usr/local/redis/bin
   ```
   
   目录结构对应关系
   
   | 文件              | 功能              |
   |:---------------:|:---------------:|
   | redis-check-aof | AOF文件修复工具       |
   | redis-check-rdb | redis-check-rdb |
   | redis-cli       | redis命令行客户端     |
   | redis.conf      | redis配置文件       |
   | redis-sentinal  | redis集群管理工具     |
   | redis-server    | redis服务进程       |
   | redis-benchmark | redis性能测试工具     |

2. 基本配置
   
   ```shell
   vim /usr/local/redis/bin/redis.conf
   # 找到daemonize=no并改为yes
   ```

3. 启动Redis
   
   ```shell
   # 启动Redis
   cd /usr/local/redis/bin
   ./redis-server ./redis.conf
   # 连接Redis
   ./redis-cli
   # 关闭Redis
   ./redis-cli shutdown
   # 强行终止Redis
   pkill redis-server
   
   # 设置开机自启动
   vim /etc/rc.local
   # 添加
   /usr/local/redis/bin/redis-server /usr/local/redis/bin/redis-conf
   ```

### Redis配置

1. 基础参数配置
   
   ```shell
   # 是否在后台执行，yes：后台运行；no：不是后台运行（老版本默认）
   daemonize yes
   # 是否开启保护模式（默认开启）
   # 要是配置里没有指定bind和密码。开启该参数后，redis只会本地进行访问，拒绝外部访问。要是开启了密码  和bind，可以开启。否  则最好关闭，设置为no。
   protected-mode yes
   # redis的进程文件
   pidfile /var/run/redis/redis-server.pid
   # redis监听的端口号
   port 6379
   # 此参数确定了TCP连接中已完成队列的长度(默认511)
   tcp-backlog 511
   # 指定redis只接收指定IP地址的请求
   # 如需处理所有请求（远程访问） bind 0.0.0.0
   bind 127.0.0.1
   # 配置unix socket来让redis支持监听本地连接。
   unixsocket /var/run/redis/redis.sock
   # 配置unix socket使用文件的权限
   unixsocketperm 700
   # 此参数为设置客户端空闲超过timeout，服务端会断开连接，为0则服务端不会主动断开连接，不能小于0。
   timeout 0
   # tcp keepalive参数。如果设置不为0，就使用配置tcp的SO_KEEPALIVE值，使用keepalive有两个好处:检测挂掉的对端。降低中间设备出问题而导致网络看似连接却已经与对端端口的问题。在Linux内核中，设置了keepalive，redis会定时给对端发送ack。检测到对端关闭需要两倍的设置值。
   tcp-keepalive 0
   # 指定了服务端日志的级别
   # debug（适合开发、测试环境）
   # verbose（较少于debug级别 适合开发、测试环境）
   # notice（适合生产环境）
   # warn（只有非常重要的信息）
   loglevel notice
   # 指定了记录日志的文件。空字符串的话，日志会打印到标准输出设备。后台运行的redis标准输出是/dev/null。
   logfile /var/log/redis/redis-server.log
   # 是否打开记录syslog功能
   # syslog-enabled no
   # syslog的标识符
   # syslog-ident redis
   # 日志的来源、设备
   # syslog-facility local0
   # 数据库的数量（默认16）
   databases 16
   ```

2. 持久化配置
   
   ```shell
   # 注释掉"save"这一行配置项就可以让保存数据库功能失效
   # 设置redis进行数据库镜像的频率。
   # 900秒（15分钟）内至少1个key值改变（则进行数据库保存--持久化）
   # 300秒（5分钟）内至少10个key值改变（则进行数据库保存--持久化）
   # 60秒（1分钟）内至少10000个key值改变（则进行数据库保存--持久化）
   save 900 1
   save 300 10
   save 60 10000
   # 当rdb持久化出现错误后，是否依然进行继续进行工作，
   # yes：不能进行工作，
   # no：可以继续进行工作，
   # 可以通过info中的rdb_last_bgsave_status了解RDB持久化是否有错误
   stop-writes-on-bgsave-error yes
   # 使用压缩rdb文件
   # yes：压缩，但是需要一些cpu的消耗
   # no：不压缩，需要更多的磁盘空间
   rdbcompression yes
   # 是否校验rdb文件
   rdbchecksum yes
   # rdb文件的名称
   dbfilename dump.rdb
   # 数据目录，数据库的写入会在这个目录。rdb、aof文件也会写在这个目录
   dir /var/lib/redis
   # 默认redis使用的是rdb方式持久化，这种方式在许多应用中已经足够用了。但是redis如果中途宕机，会导致可能有几分钟的数据丢失，根据save来策略进行持久化，Append Only File是另一种持久化方式，可以提供更好的持久化特性。Redis会把每次写入的数据在接收后都写入 appendonly.aof 文件，每次启动时Redis都会先把这个文件的数据读入内存里，先忽略RDB文件。
   appendonly no
   # aof文件名
   appendfilename "appendonly.aof"
   # aof持久化策略的配置
   # no表示不执行fsync，由操作系统保证数据同步到磁盘，速度最快。
   # always表示每次写入都执行fsync，以保证数据同步到磁盘。
   # everysec表示每秒执行一次fsync，可能会导致丢失这1s数据。
   appendfsync everysec
   # 在aof重写或者写入rdb文件的时候，会执行大量IO，此时对于everysec和always的aof模式来说，执行fsync会造成阻塞过长时间，no-appendfsync-on-rewrite字段设置为默认设置为no。如果对延迟要求很高的应用，这个字段可以设置为yes，否则还是设置为no，这样对持久化特性来说这是更安全的选择。设置为yes表示rewrite期间对新写操作不fsync,暂时存在内存中,等rewrite完成后再写入，默认为no，建议yes。Linux的默认fsync策略是30秒。可能丢失30秒数据。
   no-appendfsync-on-rewrite no
   # aof自动重写配置。当目前aof文件大小超过上一次重写的aof文件大小的百分之多少进行重写，即当aof文件增长到一定大小的时候Redis能够调用bgrewriteaof对日志文件进行重写。当前AOF文件大小是上次日志重写得到AOF文件大小的二倍（设置为100）时，自动启动新的日志重写过程。
   auto-aof-rewrite-percentage 100
   # 设置允许重写的最小aof文件大小，避免了达到约定百分比但尺寸仍然很小的情况还要重写
   auto-aof-rewrite-min-size 64mb
   # aof文件可能在尾部是不完整的，当redis启动的时候，aof文件的数据被载入内存。重启可能发生在redis所在的主机操作系统宕机后，尤其在ext4文件系统没有加上data=ordered选项（redis宕机或者异常终止不会造成尾部不完整现象。）出现这种现象，可以选择让redis退出，或者导入尽可能多的数据。如果选择的是yes，当截断的aof文件被导入的时候，会自动发布一个log给客户端然后load。如果是no，用户必须手动redis-check-aof修复AOF文件才可以。
   aof-load-truncated yes
   ```

3. 主从配置
   
   ```shell
   # 复制选项，slave复制对应的master
   slaveof <masterip> <masterport>
   # 如果master设置了requirepass，那么slave要连上master，需要有master的密码才行。masterauth就是用来配置master的密码，这样可以在连上master后进行认证
   masterauth <master-password>
   # 当slave同master失去连接或者复制正在进行，slave的运行方式
   # 1.如果slave-serve-stale-data设置为yes(默认设置)，slave会继续响应客户端的请求。
   # 2.如果slave-serve-stale-data设置为no，除去INFO和SLAVOF命令之外的任何请求都会返回一个错误”SYNC with master in progress”。
   slave-serve-stale-data yes
   # slave服务器读写配置
   # 默认情况下是只读的（yes）
   # 修改成no，可读可写（不建议）
   slave-read-only yes
   # 是否使用socket方式复制数据。目前redis复制提供两种方式，disk和socket。如果新的slave连上来或者重连的slave无法部分同步，就会执行全量同步，master会生成rdb文件。有2种方式：disk方式是master创建一个新的进程把rdb文件保存到磁盘，再把磁盘上的rdb文件传递给slave。socket是master创建一个新的进程，直接把rdb文件以socket的方式发给slave。disk方式的时候，当一个rdb保存的过程中，多个slave都能共享这个rdb文件。socket的方式就的一个个slave顺序复制。在磁盘速度缓慢，网速快的情况下推荐用socket方式。
   repl-diskless-sync no
   # diskless复制的延迟时间，防止设置为0。一旦复制开始，节点不会再接收新slave的复制请求直到下一个rdb传输。所以最好等待一段时间，等更多的slave连上来。
   repl-diskless-sync-delay 5
   # slave根据指定的时间间隔向服务器发送ping请求
   # 时间间隔可以通过 repl_ping_slave_period 来设置，默认10秒。
   repl-ping-slave-period 10
   # 复制连接超时时间。master和slave都有超时时间的设置。master检测到slave上次发送的时间超过repl-timeout，即认为slave离线，清除该slave信息。slave检测到上次和master交互的时间超过repl-timeout，则认为master离线。需要注意的是repl-timeout需要设置一个比repl-ping-slave-period更大的值，不然会经常检测到超时。
   repl-timeout 60
   # 是否禁止复制tcp链接的tcp nodelay参数，可传递yes或者no。默认是no，即使用tcp nodelay。如果master设置了yes来禁止tcp nodelay设置，在把数据复制给slave的时候，会减少包的数量和更小的网络带宽。但是这也可能带来数据的延迟。默认我们推荐更小的延迟，但是在数据量传输很大的场景下，建议选择yes。
   repl-disable-tcp-nodelay no
   # 复制缓冲区大小，这是一个环形复制缓冲区，用来保存最新复制的命令。这样在slave离线的时候，不需要完全复制master的数据，如果可以执行部分同步，只需要把缓冲区的部分数据复制给slave，就能恢复正常复制状态。缓冲区的大小越大，slave离线的时间可以更长，复制缓冲区只有在有slave连接的时候才分配内存。没有slave的一段时间，内存会被释放出来，默认1m。
   repl-backlog-size 5mb
   # master没有slave一段时间会释放复制缓冲区的内存，repl-backlog-ttl用来设置该时间长度。单位为秒。
   repl-backlog-ttl 3600
   # 当master不可用，Sentinel会根据slave的优先级选举一个master。最低的优先级的slave，当选master。而配置成0，永远不会被选举。
   slave-priority 100
   # redis提供了可以让master停止写入的方式，如果配置了min-slaves-to-write，健康的slave的个数小于N，mater就禁止写入。master最少得有多少个健康的slave存活才能执行写命令。这个配置虽然不能保证N个slave都一定能接收到master的写操作，但是能避免没有足够健康的slave的时候，master不能写入来避免数据丢失。设置为0是关闭该功能。
   # min-slaves-to-write 3
   # 延迟小于min-slaves-max-lag秒的slave才认为是健康的slave。
   min-slaves-max-lag 10
   # 设置1或另一个设置为0禁用这个特性。
   min-slaves-max-lag is set to 10
   ```

4. 安全配置
   
   ```shell
   # 配置redis连接密码（默认未启用，建议启用）
   requirepass foobared
   # 把危险的命令给修改成其他名称。比如CONFIG命令可以重命名为一个很难被猜到的命令，这样外部连接不能使用，而服务器内部连接工具还能继续使用。
   rename-command CONFIG cmd
   # 设置成一个空的值，可以禁止一个命令
   rename-command CONFIG ""
   ```

5. 连接配置
   
   ```shell
   # 设置能连上redis的最大客户端连接数量。默认是10000个客户端连接。由于redis不区分连接是客户端连接还是内部打开文件或者和slave连接等，所以maxclients最小建议设置到32。如果超过了maxclients，redis会给新的连接发送’max number of clients reached’，并关闭连接。
   maxclients 10000
   # redis配置的最大内存容量。当内存满了，需要配合maxmemory-policy策略进行处理。注意slave的输出缓冲区是不计算在maxmemory内的。所以为了防止主机内存使用完，建议设置的maxmemory需要更小一些。
   maxmemory <bytes>
   # 内存容量超过maxmemory后的处理策略。
   # volatile-lru：利用LRU算法移除设置过过期时间的key。
   # volatile-random：随机移除设置过过期时间的key。
   # volatile-ttl：移除即将过期的key，根据最近过期时间来删除（辅以TTL）
   # allkeys-lru：利用LRU算法移除任何key。
   # allkeys-random：随机移除任何key。
   # noeviction：不移除任何key，只是返回一个写错误。
   # 上面的这些驱逐策略，如果redis没有合适的key驱逐，对于写命令，还是会返回错误。redis将不再接收写请求，只接收get请求。
   maxmemory-policy noeviction
   # lru检测的样本数。使用lru或者ttl淘汰算法，从需要淘汰的列表中随机选择sample个key，选出闲置时间最长的key移除。
   maxmemory-samples 5
   # 如果达到最大时间限制（毫秒），redis会记个log，然后返回error。当一个脚本超过了最大时限。只有SCRIPT KILL和SHUTDOWN NOSAVE可以用。第一个可以杀没有调write命令的东西。要是已经调用了write，只能用第二个命令杀。
   lua-time-limit 5000
   ```

6. 集群配置
   
   ```shell
   # 集群开关，默认是不开启集群模式。
   cluster-enabled yes
   # 集群配置文件的名称，每个节点都有一个集群相关的配置文件，持久化保存集群的信息。这个文件并不需要手动配置，这个配置文件有Redis生成并更新，每个Redis集群节点需要一个单独的配置文件，请确保与实例运行的系统中配置文件名称不冲突
   # cluster-config-file nodes-a.conf
   # 节点互连超时的阀值。集群节点超时毫秒数
   # cluster-node-timeout 15000
   # 在进行故障转移的时候，全部slave都会请求申请为master，但是有些slave可能与master断开连接一段时间了，导致数据过于陈旧，这样的slave不应该被提升为master。该参数就是用来判断slave节点与master断线的时间是否过长。判断方法是：
   # 比较slave断开连接的时间和(node-timeout * slave-validity-factor) +repl-ping-slave-period
   # 如果节点超时时间为三十秒, 并且slave-validity-factor为10,假设默认的repl-ping-slave-period是10秒，即如果超过310秒slave将不会尝试进行故障转移
   cluster-slave-validity-factor 10
   # master的slave数量大于该值，slave才能迁移到其他孤立master上，如这个参数若被设为2，那么只有当一个主节点拥有2 个可工作的从节点时，它的一个从节点会尝试迁移。
   cluster-migration-barrier 1
   # 默认情况下，集群全部的slot有节点负责，集群状态才为ok，才能提供服务。设置为no，可以在slot没有全部分配的时候提供服务。不建议打开该配置，这样会造成分区的时候，小分区的master一直在接受写请求，而造成很长时间数据不一致。
   cluster-require-full-coverage yes
   ```

7. [其他配置](https://www.jianshu.com/p/8d64c6e849d9)

### Redis基础语法

#### shell

```shell
./redis-server ./redis.conf  # 从redis.conf读取配置并启动redis服务
./redis-cli -h 127.0.0.1 -p 6379  # 从默认端口启动redis服务
./redis-cli ping  # 返回PONG则说明客户端与redis服务的连接正常
./redis-cli shutdown  # 关闭redis服务
```

#### string

```shell
set foo 1  # 赋值
get foo  # 字符串回复，当请求一个字符串类型键的键值或一个其他类型键中的某个元素
keys *  # keys命令的作用是获取数据库中符合指定规则的键名，支持glob风格通配符格式（?,*,[],\x）
exists key  # exists判断key键是否存在，返回1和0
del key1 key2 ...  # del删除一个或多个键
type key  # type获得key值的数据类型
incr foo  # 增键值的INCR命令会以整数形式返回递增后的键值
incrby foo 2  # 增加指定的整数
incrbyfloat foo 0.1  # 增加指定的浮点数
decr foo  # 自减1
decrby foo 2  # 减少指定的整数
append key "value"  # 在key键原有的值尾部追加“value”
strlen key  # strlen返回字符串长度
mget key1 key2...  # 同时获取多个键值
mset key1 value1 key2 value2...  # 同时赋值多个键值对
```

#### hash

```shell
hset key field1 value1  # 对键、字段、value赋值，若字段已存在则更新value
hmset key field1 value1 field2 value2...  # 赋值多个字段和value
hsetnx key field1 value1...  # 若字段不存在则新建赋值，已存在则不进行任何操作（nx：if not exists）
hget key field1  # 返回键、字段对应的value
hmget key field1 field2...  # 返回键对应的多个字段value
hgetall key  # 返回键对应的所有字段和value，返回的结果是字段和字段值遍历组成的列表，不是很直观
hkeys key  # 只获取键下的字段名
hvals key  # 只获取键下所有的value
hlen key  # 获取键长度，也就是字段数量
hexists key field1  # 判断键对应的字段是否存在，返回1和0
hincrbr key field1 2  # 对value进行自增指定数字，若不存在则会新建
hdel key field1 field2...  # 删除键下的一个或多个字段
```

#### list

```shell
lpush key value1 value2...  # 向列表左边添加一个或多个元素，会首先添加value1
rpush key value1 value2...  # 向列表右边添加一个或多个元素
lpop key  # 从列表左边弹出一个元素并从列表中删除这个元素
rpop key  # 从列表右边弹出一个元素并从列表中删除这个元素
brpop key... timeout  # brpop和rpop相似，区别是当列表中没有元素时brpop命令会一直阻塞住连接，直到有新元素加入，timeout指定超时时间，单位是秒。当超过了此时间仍然没有获得新元素的话就会返回nil，若为0则不限制等待时间
llen key  # 列表中元素的个数
lrange key start stop  # 获得列表从start到stop的所有的列表元素，返回的元素从上到下即从左到右
lrange key 0 -1  # 获得列表中的所有元素
lrem key count value  # 删除前count个值为value的元素（当count＞0时从列表左边开始删除前count个值为value的元素；当count＜0时从列表右边开始删除前|count|个值为value的元素；当count = 0时会删除所有值为value的元素）
lindex key index  # 获得列表中指定index的value，index从左往右从0开始依次排列，或者从右至左从-1开始
lset key index value  # 设置列表中指定index的值为value，若该index位置已存在值，则替换原有值
linsert key before|after pivot value  # 在指定值前/后插入value。首先在列表中从左到右查找值为pivot的元素，然后根据第二个参数是BEFORE还是AFTER来决定将value插入到该元素的前面还是后面。
rpoplpush key key1  # 将元素从一个列表转移到另外一个列表。先执行rpop命令再执行lpush命令。先从key列表类型键的右边弹出一个元素，然后将其加入到key1列表类型键的左边，并返回这个元素的值。
```

#### set

```shell
sadd key member1 member2...  # 向集合中添加一个或多个元素，返回实际修改元素数量，若已存在则忽略
srem key member1 member2...  # 从集合中删除一个或多个元素，并返回删除成功的个数
smembers key  # 获得集合中的所有元素
sismember key member1  # 判断元素member1是否存在于集合key中，返回1和0
sdiff A B  # 返回A与B的差集，A-B，支持多个集合
sinter A B  # 返回A与B的交集，A∩B，支持多个集合
sunion A B  # 返回A与B的并集，A∪B，支持多个集合
scard key  # 返回集合长度，也就是元素数量
sdiffstore new key1 key2...  # 计算两个或多个集合的差集并存储到new中
sinterstore new key1 key2...  # 计算两个或多个集合的交集并存储到new中
sunion new key1 key2...  # 计算两个或多个集合的并集并存储到new中
spop key  # 从集合中随机弹出一个元素
srandmember key [count]  # 随机获得集合中的元素。count参数来一次随机获得多个元素：当count为正数时，随机从集合里获得count个不重复的元素。如果count的值大于集合中的元素个数，则会返回集合中的全部元素；当count为负数时，会随机从集合里获得|count|个的元素，这些元素有可能相同。
```

sorted set

```shell
zadd key score member [score member...]  # 新建/添加/修改有序索引，score为分数，member为元素
zscore key member  # 获得一个member的score
zrange key start stop [withscores]  # 从start stop指定下标从key取出元素，withscores指定是否输出score
zrevrange key start stop [withscores]  # 与zrange不同之处是会按score从大到小排列输出元素
zrangebyscore key min max [withscores] [limit offset count]  # 获得指定分数范围的元素，如果希望分数范围不包含端点值，可以在分数前加上“(”符号；min和max还支持无穷大，同ZADD命令一样，-inf和+inf分别表示负无穷和正无穷；在本命令中limit offset count 与SQL中的用法基本相同，即在获得的元素列表的基础上向后偏移offset个元素，并且只获取前count个元素。
zincrby key increment member  # 增加某个元素的分数，若指定的元素不存在，会先建立它并将它的分数赋为0再操作。
zcard key  # 获得集合中元素的数量
zcount key min max  # 返回指定分数范围内的元素个数
zremrangebyrank key min max  # 按照排名范围删除元素，按照元素分数从小到大的顺序（即索引0表示最小的值）删除处在指定排名范围内的所有元素，并返回删除的元素数量
zrank key member  # 返回member元素的排名（从小到大排列）
zrevrank key member  # 返回member元素的排名（从大到小排列）
```

#### 其他

```shell
# 事务 multi开始，exec结束
multi
sadd key1 1
sadd key2 2
exec

# watch命令可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行。
watch key

# 生存时间
expire key 900  # 指定key在15分钟（900s）后删除，再次指定可重新设置生存时间，覆盖已有生存时间
expireat key 1351858600  # 同样是生存时间，不过单位是以秒为单位的Unix时间戳
paxpireat key 1351858600000  # 单位是以毫秒为单位的Unix时间戳
ttl key  # 查看键在多少秒后删除
persist key  # 取消键的生存时间设置（即将键恢复成永久的）

# 排序
sort key [desc]  # 按照key值对key进行从小到大排列，若是有序集合则忽略score对value进行排序，desc从大到小
sort key alpha [desc]  # 若value是字符串，则使用alpha指定按第一个字母排序，desc从大到小

# 按key排序
lpush fdf 3 5 6  # 需要排序的列表
set dd:3 10  # 新建三个dd:v 普通类型数据，v值对应列表中的value值
set dd:6 7
set dd:5 5
sort fdf by dd:* desc  # 则sort fdf会按dd:v对应的值对列表中的value值进行排序，结果为3 6 5

# SORT是Redis中最强大最复杂的命令之一，如果使用不好很容易成为性能瓶颈。
```

### Redis基础知识

#### 多数据库

Redis是一个字典结构的存储服务器，而实际上一个Redis实例提供了多个用来存储数据的字典，客户端可以指定将数据存储在哪个字典中。这与我们熟知的在一个关系数据库实例中可以创建多个数据库类似，所以可以将其中的每个字典都理解成一个独立的数据库。

每个数据库对外都是以一个从0开始的递增数字命名，Redis默认支持16个数据库，可以通过配置参数databases来修改这一数字。客户端与Redis建立连接后会自动选择0号数据库，不过可以随时使用SELECT命令更换数据库。

> 然而这些以数字命名的数据库又与我们理解的数据库有所区别。首先Redis不支持自定义数据库的名字，每个数据库都以编号命名，开发者必须自己记录哪些数据库存储了哪些数据。另外Redis也不支持为每个数据库设置不同的访问密码，所以一个客户端要么可以访问全部数据库，要么连一个数据库也没有权限访问。最重要的一点是多个数据库之间并不是完全隔离的，比如FLUSHALL命令可以清空一个Redis实例中所有数据库中的数据。综上所述，这些数据库更像是一种命名空间，而不适宜存储不同应用程序的数据。比如可以使用0号数据库存储某个应用生产环境中的数据，使用1号数据库存储测试环境中的数据，但不适宜使用0号数据库存储A应用的数据而使用1号数据库存储B应用的数据，不同的应用应该使用不同的Redis实例存储数据。由于Redis非常轻量级，一个空Redis实例占用的内存只有1MB左右，所以不用担心多个Redis实例会额外占用很多内存。

#### 数据类型

1. 字符串类型
   字符串类型是Redis中最基本的数据类型，它能存储任何形式的字符串，包括二进制数据。你可以用其存储用户的邮箱、JSON化的对象甚至是一张图片。一个字符串类型键允许存储的数据的最大容量是512 MB

2. 哈希类型
   哈希类型（hash）的键值也是一种字典结构，其存储了字段（field）和字段值的映射，但字段值只能是字符串，不支持其他数据类型，换句话说，哈希类型不能嵌套其他的数据类型。一个哈希类型键可以包含至多232-1个字段。
   
   > 除了哈希类型，Redis的其他数据类型同样不支持数据类型嵌套。比如集合类型的每个元素都只能是字符串，不能是另一个集合或哈希表等。

3. 列表类型
   列表类型（list）可以存储一个有序的字符串列表，常用的操作是向列表两端添加元素，或者获得列表的某一个片段。
   
   列表类型内部是使用双向链表（double linked list）实现的，所以向列表两端添加元素的时间复杂度为O(1)，获取越接近两端的元素速度就越快。这意味着即使是一个有几千万个元素的列表，获取头部或尾部的10条记录也是极快的。

4. 集合类型
   在集合中的每个元素都是不同的，且没有顺序。一个集合类型（set）键可以存储至多232 -1个字符串。
   
   集合类型的常用操作是向集合中加入或删除元素、判断某个元素是否存在等，由于集合类型在Redis内部是使用值为空的哈希表（hash table）实现的，所以这些操作的时间复杂度都是O(1)。最方便的是多个集合类型键之间还可以进行并集、交集和差集运算。

5. 有序集合
   在集合类型的基础上有序集合类型为集合中的每个元素都关联了一个分数，这使得我们不仅可以完成插入、删除和判断元素是否存在等集合类型支持的操作，还能够获得分数最高（或最低）的前N个元素、获得指定分数范围内的元素等与分数有关的操作。
   虽然集合中每个元素都是不同的，但是它们的分数却可以相同。有序集合类型在某些方面和列表类型有些相似。（1）二者都是有序的。
   （2）二者都可以获得某一范围的元素。
   但是二者有着很大的区别，这使得它们的应用场景也是不同的。
   （1）列表类型是通过链表实现的，获取靠近两端的数据速度极快，而当元素增多后，访问中间数据的速度会较慢，所以它更加适合实现如“新鲜事”或“日志”这样很少访问中间元素的应用。
   （2）有序集合类型是使用散列表和跳跃表（Skip list）实现的，所以即使读取位于中间部分的数据速度也很快（时间复杂度是O(log(N))）。
   （3）列表中不能简单地调整某个元素的位置，但是有序集合可以（通过更改这个元素的分数）。
   （4）有序集合要比列表类型更耗费内存。

#### 事务

Redis也支持事务，由multi开始，exec结束，中间是要运行的代码。Redis保证一个事务中的所有命令要么都执行，要么都不执行。如果在发送EXEC命令前客户端断线了，则Redis会清空事务队列，事务中的所有命令都不会执行。

语法错误。语法错误指命令不存在或者命令参数的个数不对。而只要有一个命令有语法错误，执行EXEC命令后Redis就会直接返回错误，连语法正确的命令也不会执行。

运行错误。运行错误指在命令执行时出现的错误，比如使用散列类型的命令操作集合类型的键，这种错误在实际执行之前Redis是无法发现的，所以在事务里这样的命令是会被Redis接受并执行的。如果事务里的一条命令出现了运行错误，事务里其他的命令依然会继续执行（包括出错命令之后的命令）。

Redis的事务没有关系数据库事务提供的回滚（rollback）[插图]功能。为此开发者必须在事务执行出错后自己收拾剩下的摊子（将数据库复原回事务执行前的状态等）。

#### 持久化

Redis的强劲性能很大程度上是由于其将所有数据都存储在了内存中，为了使Redis在重启之后仍能保证数据不丢失，需要将数据从内存中以某种形式同步到硬盘中，这一过程就是持久化。Redis支持两种方式的持久化，一种是RDB方式，一种是AOF方式。可以单独使用其中一种或将二者结合使用。

1. RDB方式的持久化是通过快照（snapshotting）完成的，当符合一定条件时Redis会自动将内存中的所有数据进行快照并存储在硬盘上。进行快照的条件可以由用户在配置文件中自定义，由两个参数构成：时间和改动的键的个数。当在指定的时间内被更改的键的个数大于指定的数值时就会进行快照。RDB是Redis默认采用的持久化方式。
   
   ```shell
   # save参数指定了快照条件，可以存在多个条件，条件之间是“或”的关系。save 900 1的意思是在15分钟（900秒钟）内有至少一个键被更改则进行快照。
   save 900 1
   save 300 10
   sava 6010000
   ```
   
   Redis默认会将快照文件存储在当前目录的dump.rdb文件中，可以通过配置dir和dbfilename两个参数分别指定快照文件的存储路径和文件名。
   理清Redis实现快照的过程对我们了解快照文件的特性有很大的帮助。快照的过程如下。
   （1）Redis使用fork函数复制一份当前进程（父进程）的副本（子进程）；
   （2）父进程继续接收并处理客户端发来的命令，而子进程开始将内存中的数据写入硬盘中的临时文件；
   （3）当子进程写入完所有数据后会用该临时文件替换旧的RDB文件，至此一次快照操作完成。
   
   通过上述过程可以发现Redis在进行快照的过程中不会修改RDB文件，只有快照结束后才会将旧的文件替换成新的，也就是说任何时候RDB文件都是完整的。这使得我们可以通过定时备份RDB文件来实现Redis数据库备份。RDB文件是经过压缩（可以配置rdbcompression参数以禁用压缩节省CPU占用）的二进制格式，所以占用的空间会小于内存中的数据大小，更加利于传输。
   
   除了自动快照，还可以手动发送SAVE或BGSAVE命令让Redis执行快照，两个命令的区别在于，前者是由主进程进行快照操作，会阻塞住其他请求，后者会通过fork子进程进行快照操作。Redis启动后会读取RDB快照文件，将数据从硬盘载入到内存。根据数据量大小与结构和服务器性能不同，这个时间也不同。通常将一个记录一千万个字符串类型键、大小为1GB的快照文件载入到内存中需要花费20～30秒钟。
   
   通过RDB方式实现持久化，一旦Redis异常退出，就会丢失最后一次快照以后更改的所有数据。这就需要开发者根据具体的应用场合，通过组合设置自动快照条件的方式来将可能发生的数据损失控制在能够接受的范围。如果数据很重要以至于无法承受任何损失，则可以考虑使用AOF方式进行持久化。

2. AOF（append only file）方式的持久化
   
   ```shell
   # 默认情况下Redis没有开启AOF（append only file）方式的持久化，可以通过appendonly参数开启
   appendonly yes
   # 开启AOF持久化后每执行一条会更改Redis中的数据的命令，Redis就会将该命令写入硬盘中的AOF文件。文件位置和RDB文件的位置相同，都是通过dir参数设置的，默认的文件名是appendonly.aof，可通过appendfilename name修改
   appendfilename appendonly.aof
   ```
   
   可见AOF文件是纯文本文件，其内容正是Redis客户端向Redis发送的原始通信协议的内容（Redis的通信协议会在7.4节中介绍，为了便于阅读，这里将实际的命令部分以粗体显示），从中可见Redis确实只记录了前3条命令。然而这时有一个问题是前2条命令其实都是冗余的，因为这两条的执行结果会被第三条命令覆盖。随着执行的命令越来越多，AOF文件的大小也会越来越大，即使内存中实际的数据可能并没有多少。很自然地，我们希望Redis可以自动优化AOF文件，就上例而言，就是将前两条无用的记录删除，只保留第三条。实际上Redis也正是这样做的，每当达到一定条件时Redis就会自动重写AOF文件，这个条件可以在配置文件中设置。
   
   Redis允许同时开启AOF和RDB，既保证了数据安全又使得进行备份等操作十分容易。此时重新启动Redis后Redis会使用AOF文件来恢复数据，因为AOF方式的持久化可能丢失的数据更少。

#### 主从复制

在Redis中使用复制功能非常容易，只需要在从数据库的配置文件中加入“slaveof主数据库IP主数据库端口”即可，主数据库无需进行任何配置。

```shell
redis-server --slaveof 8.131.90.208 6379
```

从数据库持久化

另一个相对耗时的操作是持久化，为了提高性能，可以通过复制功能建立一个（或若干个）从数据库，并在从数据库中启用持久化，同时在主数据库禁用持久化。当从数据库崩溃时重启后主数据库会自动将数据同步过来，所以无需担心数据丢失。而当主数据库崩溃时，需要在从数据库中使用SLAVEOF NO ONE命令将从数据库提升成主数据库继续服务，并在原来的主数据库启动后使用SLAVEOF命令将其设置成新的主数据库的从数据库，即可将数据同步回来。

## Hadoop

### 编译Hadoop

```shell
# 下载相关包（一定要用java1.8）
wget https://mirrors.tuna.tsinghua.edu.cn/apache/hadoop/common/hadoop-3.2.1/hadoop-3.2.1-src.tar.gz  # Hadoop源码包
wget https://mirrors.tuna.tsinghua.edu.cn/apache/maven/maven-3/3.6.3/binaries/apache-maven-3.6.3-bin.tar.gz  # Maven包

# 下载ProtocolBuffer2.5.0，MapReduce和HDFS用protocol buffer来压缩和交换数据
# 百度网盘：链接：https://pan.baidu.com/s/1ljoP-tLCPkVbtssEdiGTFQ 提取码：q5fe

# 安装依赖库
yum -y updates
yum -y install kernel-devel  
yum -y install gcc* 
yum -y install cmake3  # 必须是最新的
ln -s /usr/bin/cmake3 /usr/bin/cmake  # 最后将cmake3做一个软链cmake
yum -y install glibc-headers  
yum -y install gcc-c++  
yum -y install zip-devel  
yum -y install openssl-devel  
yum -y install svn  
yum -y install git  
yum -y install ncurses-devel  
yum -y install lzo-devel  
yum -y install autoconf  
yum -y install libtool  
yum -y install automake  
yum -y install patch
yum -y install doxygen
yum -y install protobuf
yum install -y graphviz
yum install -y protobuf-devel
yum -y install build-essential libtool zlib1g-dev pkg-config libssl-dev libsasl2-dev
yum install -y cyrus-sasl* 
yum install -y libgsasl-devel*
```

编译前的准备

```shell
# Maven配置
# 解压Maven包并把路径添加到环境变量中
export PATH="/usr/local/servers/apache-maven-3.6.3/bin:$PATH"
# 将Maven镜像更换为阿里云中央仓库
vim /usr/local/servers/apache-maven-3.6.3/conf/settings.xml
# 在<mirrors>节点中添加
<mirror>
    <id>alimaven</id>
    <name>aliyun maven</name>
    <url>http://maven.aliyun.com/nexus/content/groups/public/</url>
    <mirrorOf>central</mirrorOf>        
</mirror>

# 安装ProtocolBuffer
tar -zxf protobuf-2.5.0.tar.gz
cd protobuf-2.5.0/bin
./configure --prefix=/usr/local/servers/protobuf-2.5.0
make && make install
# 在/etc/profile中添加bin路径到path中
export LD_LIBRARY_PATH="/usr/local/servers/protobuf-2.5.0/lib"
export PATH="/usr/local/servers/protobuf-2.5.0/bin:$PATH"

source /etc/profile
protoc --version  # 安装完成查看版本 

# 设置Hadoop编译时下载为阿里云maven镜像
cd hadoop-3.2.1-src
vim pom.xml
# 在repositories下添加如下内容
<repositories>  
    <repository>  
        <id>alimaven</id>  
        <name>aliyun maven</name>  
        <url>http://maven.aliyun.com/nexus/content/groups/public/</url>  
        <releases>  
            <enabled>true</enabled>  
        </releases>  
        <snapshots>  
            <enabled>false</enabled>  
        </snapshots>  
    </repository>  
</repositories> 
# 注释掉不必要的代码
vim /usr/local/softwares/hadoop-3.2.1-src/hadoop-client-modules/pom.xml

<module>hadoop-client-minicluster</module>
<!-- Checks invariants above -->
<module>hadoop-client-check-invariants</module>
# 修改这里注释掉这一行
<!-- <module>hadoop-client-check-test-invariants</module>-->
<!-- Attempt to use the created libraries -->
<module>hadoop-client-integration-tests</module>
```

开始编译，有三种编译方式

```shell
# cd到Hadoop源码包中
# 仅编译正常源码部分，对于native部分不进行编译，最终结果打包
mvn package -Pdist -DskipTests -Dtar -Dmaven.javadoc.skip=true 
# 编译正常部分源码、native依赖库以及帮助文档，最终结果打包
mvn package -Pdist,native,docs -DskipTests -Dtar 
# 一般使用下面这种方法
mvn -X package -Pdist,native,docs -DskipTests -Dtar -Dmaven.skip.test=true -Dmaven.javadoc.skip=true 

# 查看编译好的包（hadoop-3.2.1和hadoop-3.2.1.tar.gz一样）
ls /usr/local/softwares/hadoop-3.2.1-src/hadoop-dist/target/
```

### 安装

#### 安装Hadoop

```shell
# 将编译好的安装包解压到/usr/local/servers目录下，设置Hadoop_home
vim /etc/profile
export HADOOP_HOME=/usr/local/servers/hadoop-3.2.1
export PATH=$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin
source /etc/profile

# 新建使用的文件夹
cd /usr/local/servers/hadoop-3.2.1
mkdir -p data/dfs/name
mkdir -p data/dfs/name2
mkdir -p data/dfs/data
mkdir -p data/dfs/data2
mkdir -p data/jobhistory/donedatas
mkdir -p data/jobhistory/intermediate
mkdir -p data/nn/edits
mkdir -p data/nodemanager/data
mkdir -p data/nodemanager/logs
mkdir -p data/remote/logs
mkdir -p data/snn/name
mkdir -p data/snn/edits
```

配置文件（一共修改七个）

​    core-site.xml

```xml
<configuration>

<!-- 指定NameNode的地址 -->
    <property>
        <name>fs.defaultFS</name>
        <value>hdfs://node01:9009</value>
    </property>

<!-- 指定hadoop数据的存储目录 -->
    <property>
        <name>hadoop.tmp.dir</name>
        <value>/usr/local/servers/hadoop-3.2.1/tmp</value>
    </property>

<!--  缓冲区大小，实际工作中根据服务器性能动态调整-->
    <property>
        <name>io.file.buffer.size</name>
        <value>8192</value>
    </property>

<!--  开启hdfs的垃圾桶机制，删除掉的数据可以从垃圾桶中回收，默认10080分钟-->
    <property>
        <name>fs.trash.interval</name>
        <value>10080</value>
    </property>
</configuration>
```

hdfs-site.xml

```xml
<configuration>

    <property>
        <name>dfs.namenode.secondary.http-address</name>
        <value>node03:50090</value>
    </property>
    <property>
        <!--  指定namenode的访问端口和端口  -->
        <name>dfs.namenode.http-address</name>
        <value>node01:50070</value>
    </property>
    <property>
        <!--  指定namenode存储元数据的地址  -->
        <name>dfs.namenode.name.dir</name>
        <value>file:///usr/local/servers/hadoop-3.2.1/data/dfs/name,file:///usr/local/servers/hadoop-3.2.1/data/dfs/name2</value>
    </property>
    <property>
        <!--  指定datanode存储数据的地址  -->
        <name>dfs.datanode.name.dir</name>
        <value>file:///usr/local/servers/hadoop-3.2.1/data/dfs/data,file:///usr/local/servers/hadoop-3.2.1/data/dfs/data2</value>
    </property>
    <property>
        <!--  指定存放日志文件的文件地址  -->
        <name>dfs.namenode.edits.dir</name>
        <value>file:///usr/local/servers/hadoop-3.2.1/data/nn/edits</value>
    </property>
    <property>
        <!--  检查点路径  -->
        <name>dfs.namenode.checkpoint.dir</name>
        <value>file:///usr/local/servers/hadoop-3.2.1/data/snn/name</value>
    </property>    
    <property>
        <!--  检查点日志路径  -->
        <name>dfs.namenode.checkpoint.dir</name>
        <value>file:///usr/local/servers/hadoop-3.2.1/data/snn/edits</value>
    </property>
    <property>
        <!--  文件切片的副本个数  -->
        <name>dfs.replication.enabled</name>
        <value>3</value>
    </property>
    <property>
        <!--  设置hdfs的文件权限  -->
        <name>dfs.permissions</name>
        <value>false</value>
    </property>
    <property>
        <!--  设置一个文件切片的大小：128M  -->
        <name>dfs.blocksize</name>
        <value>134217728</value>
    </property>
</configuration>
```

mapred-site.xml

```xml
<configuration>
    <!-- 指定MapReduce程序运行在Yarn上 -->
    <property>
        <name>mapreduce.framework.name</name>
        <value>yarn</value>
    </property>

    <!--  -->
    <property>
        <name>mapreduce.map.memory.mb</name>
        <value>1024</value>
    </property>
    <!--  -->
    <property>
        <name>mapreduce.map.java.opts</name>
        <value>Xmx512M</value>
    </property>
    <!--  -->
    <property>
        <name>mapreduce.reduce.memory.mb</name>
        <value>1024</value>
    </property>
    <!--  -->
    <property>
        <name>mapreduce.reduce.java.opts</name>
        <value>Xmx512M</value>
    </property>
    <!--  -->
    <property>
        <name>mapreduce.task.io.sort.mb</name>
        <value>256</value>
    </property>
    <!--  -->
    <property>
        <name>mapreduce.task.io.sort.factor</name>
        <value>100</value>
    </property>
    <!--  -->
    <property>
        <name>mapreduce.reduce.shuffle.parallelcopies</name>
        <value>25</value>
    </property>
    <!-- 历史服务器端地址 -->
    <property>
        <name>mapreduce.jobhistory.address</name>
        <value>node01:10020</value>
    </property>

    <!-- 历史服务器web端地址 -->
    <property>
        <name>mapreduce.jobhistory.webapp.address</name>
        <value>node01:19888</value>
    </property>
    <!--  -->
    <property>
        <name>mapreduce.jobhistory.intermediate-done-dir</name>
        <value>/usr/local/servers/hadoop-3.2.1/data/jobhistory/intermediate</value>
    </property>
    <!--  -->
    <property>
        <name>mapreduce.jobhistory.done-dir</name>
        <value>/usr/local/servers/hadoop-3.2.1/data/jobhistory/donedatas</value>
    </property>
    <!--  -->
    <property>
        <name>yarn.app.mapreduce.am.env</name>
        <value>HADOOP_MAPRED_HOME=/usr/local/servers/hadoop-3.2.1</value>
    </property>
    <!--  -->
    <property>
        <name>mapreduce.map.env</name>
        <value>HADOOP_MAPRED_HOME=/usr/local/servers/hadoop-3.2.1</value>
    </property>
</configuration>
```

yarn-site.xml

```xml
<configuration>
<!-- 指定MR走shuffle -->
    <property>
        <name>yarn.nodemanager.aux-services</name>
        <value>mapreduce_shuffle</value>
    </property>
<!-- 指定ResourceManager的地址-->
    <property>
        <name>yarn.resourcemanager.hostname</name>
        <value>node01</value>
    </property>
<!-- -->
    <property>
        <name>yarn.resourcemanager.address</name>
        <value>node01:8032</value>
    </property>
<!-- -->
    <property>
        <name>yarn.resourcemanager.scheduler.address</name>
        <value>node01:8030</value>
    </property>
<!-- -->
    <property>
        <name>yarn.resourcemanager.resource-tracker.address</name>
        <value>node01:8031</value>
    </property>
<!-- -->
    <property>
        <name>yarn.resourcemanager.admin.address</name>
        <value>node01:8033</value>
    </property>
<!-- -->
    <property>
        <name>yarn.resourcemanager.webapp.address</name>
        <value>node01:8088</value>
    </property>
<!-- 开启日志聚集功能 -->
    <property>
        <name>yarn.log-aggregation-enable</name>
        <value>true</value>
    </property>
<!-- 设置日志聚集服务器地址 -->
    <property>
        <name>yarn.log.server.url</name>
        <value>http://node01:19888/jobhistory/logs</value>
    </property>
<!-- 设置日志保留时间为7天 -->
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>604800</value>
    </property>
<!-- 环境变量的继承 -->
    <property>
        <name>yarn.nodemanager.env-whitelist</name>
        <value>JAVA_HOME,HADOOP_COMMON_HOME,HADOOP_HDFS_HOME,HADOOP_CONF_DIR,CLASSPATH_PREPEND_DISTCACHE,HADOOP_YARN_HOME,HADOOP_MAPRED_HOME</value>
    </property>
<!-- yarn容器允许分配的最大最小内存 -->
    <property>
        <name>yarn.scheduler.minimum-allocation-mb</name>
        <value>1024</value>
    </property>
    <property>
        <name>yarn.scheduler.maximum-allocation-mb</name>
        <value>2048</value>
    </property>
<!-- yarn容器允许管理的物理内存大小 -->
    <property>
        <name>yarn.nodemanager.resource.memory-mb</name>
        <value>1024</value>
    </property>
<!--  -->
    <property>
        <name>yarn.nodemanager.vmem-pmem-ratio</name>
        <value>2.1</value>
    </property>
<!-- 关闭yarn对物理内存和虚拟内存的限制检查 -->
    <property>
        <name>yarn.nodemanager.pmem-check-enabled</name>
        <value>false</value>
    </property>
    <property>
        <name>yarn.nodemanager.vmem-check-enabled</name>
        <value>false</value>
    </property>
<!--  -->
    <property>
        <name>yarn.nodemanager.resource.detect-hardware-capabilities</name>
        <value>true</value>
    </property>
<!--  -->
    <property>
        <name>yarn.nodemanager.local-dirs</name>
        <value>file:///usr/local/servers/hadoop-3.2.1/dfs/nodemanager/data</value>
    </property>
<!--  -->
    <property>
        <name>yarn.nodemanager.log-dirs</name>
        <value>file:///usr/local/servers/hadoop-3.2.1/dfs/nodemanager/logs</value>
    </property>
<!--  -->
    <property>
        <name>yarn.nodemanager.remote-app-log-dir</name>
        <value>file:///usr/local/servers/hadoop-3.2.1/dfs/remote/logs</value>
    </property>
<!--  -->
    <property>
        <name>yarn.nodemanager.remote-app-log-dir-suffix</name>
        <value>logs</value>
    </property>
<!--  -->
    <property>
        <name>yarn.log-aggregation.retain-seconds</name>
        <value>18144000</value>
    </property>
<!--  -->
    <property>
        <name>yarn.log-aggregation.retain-check-interval-seconds</name>
        <value>86400</value>
    </property>
</configuration>
```

yarn-env.sh

```bash
# 在最后添加语句
export JAVA_HOME=/usr/local/servers/jdk8
```

mapred-env.sh

```shell
# 在最后添加语句
export JAVA_HOME=/usr/local/servers/jdk8
export HADOOP_JOB_HISTORYSERVER_HEAPSIZE=1000
export HADOOP_MAPRED_ROOT_LOGGER=INFO,RFA
```

hadoop-env.sh

```shell
# 在最后添加语句
export JAVA_HOME=/usr/local/servers/jdk8
export HDFS_NAMENODE_OPTS="-XX:+UseParallelGC -Xmx4g"
export NAMENODE_HEAPSIZE="-Xmx204m"
```

#### 安装zookeeper

```shell
# 下载安装包
cd /usr/local/softwares
wget https://mirrors.tuna.tsinghua.edu.cn/apache/zookeeper/zookeeper-3.6.2/apache-zookeeper-3.6.2-bin.tar.gz
tar -zxvf apache-zookeeper-3.6.2-bin.tar.gz -C ../servers
cd ../servers
mv apache-zookeeper-3.6.2-bin zookeeper-3.6.2
cd zookeeper-3.6.2

# 配置文件修改
mkdir ./data
cp ./conf/zoo_sample.cfg ./conf/zoo.cfg
vim ./conf/zoo.cfg
# 在最后添加以下语句
dataDir=/usr/local/servers/zookeeper-3.6.2/data
server.1=node01:2888:3888
server.2=node02:2888:3888
server.3=node03:2888:3888

# 创建myid文件
touch ./data/myid
echo "1" >> ./data/myid  # 在scp到其他机器上后要修改myid为2、3、4...

# 将用户修改为elk，复制到其他主机并启动
chown -R elk. /usr/local/servers/zookeeper-3.6.2
scp -r zookeeper-3.6.2 node02:$PWD  # 记得修改myid
scp -r zookeeper-3.6.2 node03:$PWD  # 记得修改myid
su -elk  # 切换用户并启动
cd /usr/local/servers/zookeeper-3.6.2/bin
./zkServer.start  # 其他主机也是一样
./zkServer.status  # 查看主机是leader或者follower
./zkServer.stop  # 停止zookeeper
```

#### 安装Hive

```shell
# 下载解压
cd /usr/local/softwares
wget http://archive.apache.org/dist/hive/hive-3.1.2/apache-hive-3.1.2-bin.tar.gz
tar -zxvf apache-hive-3.1.2-bin.tar.gz -C /usr/local/servers/
cd /usr/local/servers
mv apache-hive-3.1.2-bin hive-3.1.2
cd hive-3.1.2/conf

# 修改配置
cp ./hive-env.sh.template hive-env.sh
mkdir -p /data/hive/iotmp
vim hive-env.sh  # 修改HADOOP_HOME和HIVE_CONF_DIR（hive的conf目录）
vim hive-site.xml  # 新建配置文件，内容如下
cd ../lib
wget https://dev.mysql.com/get/Downloads/Connector-J/mysql-connector-java-8.0.23.tar.gz  # 下载连接mysql所需的jar包并放入lib文件夹里
tar -zxvf mysql-connector-java-8.0.23.tar.gz 
cd mysql-connector-java-8.0.23/
mv mysql-connector-java-8.0.23.jar ../
cd ..
rm -rf mysql-connector-java-8.0.23.tar.gz  mysql-connector-java-8.0.23
chown -R elk. /usr/local/servers/hive-3.1.2
chown -R elk. /data/hive

# 启动前的准备（检查Hadoop和hive的guava.jar版本是否一致，如不一致删除低版本的）
vim /etc/profile  # 确保hive和hadoop home在环境变量中

export  HADOOP_HOME="/usr/local/servers/hadoop-3.2.1"
export HIVE_HOME="/usr/local/servers/hive-3.1.2"
export PATH="$PATH:$HIVE_HOME/bin
export PATH="$PATH:$HADOOP_HOME/sbin:$HADOOP_HOME/bin"

ls /usr/local/servers/hive-3.1.2/lib/
ls /usr/local/servers/hadoop-3.2.1/share/hadoop/common/lib
cp /usr/local/servers/hadoop-3.2.1/share/hadoop/common/lib/guava-27.0-jre.jar /usr/local/servers/hive-3.1.2/lib/  # 将高版本的jar包复制过去
rm -rf /usr/local/servers/hive-3.1.2/lib/guava-19.0.jar  # 删除低版本的jar包
su - elk
cd /usr/local/servers/hive-3.1.2
./bin/schematool -dbType mysql -initSchema # schematool初始化当前Hive版本的Metastore架构
./bin/hive  # 启动hive
```

hive-site.xml

```xml
<configuration>

    <!--连接MySQL的链接-->
    <property>
        <name>javax.jdo.option.ConnectionURL</name>
        <value>jdbc:mysql://node01:3306/hive?createDatabaseIfNotExist=true</value>
    </property>
    <!--指定驱动类型-->
    <property>
        <name>javax.jdo.option.ConnectionDriverName</name>
        <value>com.mysql.cj.jdbc.Driver</value>
    </property>
    <!--指定MySQL用户名-->
    <property>
        <name>javax.jdo.option.ConnectionUserName</name>
        <value>root</value>
    </property>
    <!--连接MySQL密码-->
    <property>
        <name>javax.jdo.option.ConnectionPassword</name>
        <value>123456</value>
    </property>
    <!--Hive表存放位置-->
    <property>
        <name>hive.metastore.warehouse.dir</name>
        <value>/user/hive/warehouse</value>
    </property>

    <property>
        <name>hive.exec.scratchdir</name>
        <value>/user/hive/tmp</value>     #会在hdfs生成相应路径
    </property>

    <property>
        <name>hive.querylog.location</name>
        <value>/user/hive/log</value>
    </property>

    <property>
        <name>hive.exec.local.scratchdir</name>
        <value>/data/hive/iotmp</value>
        <description>Local scratch space for Hive jobs</description>
    </property>

    <property>
        <name>hive.downloaded.resources.dir</name>
        <value>/data/hive/iotmp</value>
        <description>Temporary local directory for added resources in the remote file system.</description>
    </property>

</configuration>
```

### Zookeeper

```shell
cd /usr/local/servers/zookeeper-3.6.2/bin
./zkServer.sh start  # 启动zkserver，关闭则改为stop
./zkClient.sh  # 进入client交互模式
ls /  # 查看节点、文件（-s返回详细信息，-R返回递归目录）
# 新建节点并写入hello内容（-s顺序节点，-e临时节点；也可在已有节点下创建子节点，不可以在上层节点不存在的情况下创建子节点）
create -s -e /tmp hello
get /tmp  # 获取节点数据（-s获取详细内容，包括创建修改时间、子节点数量等）
get -w /app1  # watch监控（内容修改时会发送消息）
set /tmp world  # 修改节点内容
# 删除节点
delete /tmp  # 若节点下有子节点则删除失败
deleteall /app1  # 递归删除（删除节点及子节点）
```

### HDFS

```shell
start-dfs.sh  # 启动HDFS集群
stop-dfs.sh  # 关闭HDFS集群

hdfs dfs -ls /  # 查看路径下的文件或者文件夹
hdfs dfs -mkdir -p /a/b/c  # 递归创建文件夹
hdfs dfs -moveFronLocal localpath hdfspath  # 从本地上传文件到hdfs中（本地文件会被删除）
hdfs dfs -mv oldpath newpath  # 移动文件
hdfs dfs -put localpath hdfspath  # 从本地上传文件到hdfs中（文件还在本地）
hdfs dfs -appendToFile a.txt b.txt /c.txt  # 追加本地文件a、b到hdfs文件c中
hdfs dfs -cat /c.txt  # 打印文件到shell
hdfs dfs -cp oldpath newpath  # 拷贝文件
hdfs dfs -rm path  # 删除文件夹或者文件（如需递归加参数r）

hdfs dfs -chmod -R 777 filepath  # 更改文件访问权限
hdfs dfs -chown -R elk:elk filepath  # 更改文件用户组和用户
```

### MapReduce

```shell
start-yarn.sh # 启动Map Reduce
stop-yarn.sh # 停止Map Reduce
mr-jobhistory-daemon.sh start historyserver  # 启动Map Reduce历史记录
mr-jobhistory-daemon.sh stop historyserver  # 停止Map Reduce历史记录
```

## Hive

Hive基本操作如下。Hive真正的难点在自定义函数，详见==[官方文档](https://cwiki.apache.org/confluence/display/HIve/HivePlugins)==。

### 初始化启动

```shell
# Hive启动一（会话关闭Hive关闭，log打印到控制台）
hive --service metastore &  # 启动Hive元数据服务
netstat -atunlp | grep 9083
hive --service hiveserver2 &  # 启动Hive的服务端，供外部连接使用
netstat -atunlp | grep 10000
# Hive启动二（会话关闭Hive关闭，log重定向到文件）
hive --service metastore >> /tmp/root/hivemetastore.log 2>&1 &
hive --service hiveserver2 >> /tmp/root/hiveserver2.log 2>&1 &
# Hive启动三（Hive不受会话关闭影响，log重定向到文件）
nohup hive --service metastore >> /tmp/root/hivemetastore.log 2>&1 &
nohup hive --service hiveserver2 >> /tmp/root/hiveserver2.log 2>&1 &

# 初始化MySQL元数据库
schematool -initSchema -dbType mysql -verbose

# Hive关闭
jps  # 找到RunJar对应的端口号，一般有两个
kill -9 端口号

# Hive参数
hive -e "select * from mall.user;"  # 执行sql语句
hive -f ./xxx.sql  # 执行本地sql文件
hive -f hdfs://tmp/xxx.sql  # 执行hdfs sql文件
hive -hiveconf proprepty=value  # 给指定参数传入值，在当前会话有效

# beeline连接Hive
beeline
!connect jdbc:hive2://localhost:10000

# 测试tez
$HADOOP_HOME/bin/yarn jar $TEZ_HOME/tez-examples-0.10.1-SNAPSHOT.jar orderedwordcount /tez/word.txt /tez/output/
```

### 常用语法

```sql
# HQL语法
create database dbname location '/hivedb';  # 指定新建数据库的存放位置，默认在xml中配置
alter database dbname set dbpropreties('createtime'='20210101');  # 修改数据库基本信息
desc database [extended] dbname;  # 查看数据库基本信息，extended参数查看详细内容
desc [formatted] tb;  # 查看表信息，formatted查看详细信息
drop database dbname [cascade];  # 删除数据库，cascade如果其中有表强制删除
insert into tb values(1,2,3);  # 插入数据，会很慢，因为使用mapreduce
create table tb as select * from othertb;  # 复制表结构和表内容
create table tb like othertb;  # 复制表结构
alter table oldname rename to newname;  # 更改表名
show functions;  # 查看内置函数
desc function [extended] upper; # 查看函数的用法，extended返回详细内容

# 创建表（内部表：会将数据移动到数据仓库指向的路径；外部表：仅记录数据所在的路径，不改变数据位置，删除表时，内部表的元数据和数据一起被删除，而外部表只会删除元数据，不会删除数据）
CREATE [EXTERNAL] TABLE [IF NOT EXISTS] table_name  # EXTERNAL表示创建外部表
[(col_name data_type [COMMENT col_comment], ...)]  # 表字段
[COMMENT table_comment]  # 为表与字段增加注释
[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]  # 分区，一个表可以有多个分区，每个分区以文件夹的方式单独存储在表的目录下
[CLUSTERED BY (col_name, col_name, ...)  # 分桶，获得更高的查询效率
[SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]  # 排序
[ROW FORMAT row_format]  # 字段之间的分隔符
[STORED AS file_format]  # 存储类型，一般是textfile纯文本，需要压缩SEQUENCE
[LOCATION hdfs_path]  # 指定存储位置
```

### 示例

```sql
# 创建外部表示例
create external table movies(id int,content string,genre string) 
comment 'This is movies.' 
row format delimited fields terminated by ',' 
stored as textfile;  # 创建外部表示例
# 从本地加载数据到hive，数据上传到movies表目录下，overwrite覆盖原数据（数据不要有标题）
load data local inpath '/tmp/win/movies.csv' [overwrite] into table movies;
# 从hdfs文件系统加载数据到hive，会将文件移动到movies表目录下
load data inpath '/user/elk/tmp/movies.csv' into table movies;

# 创建分区表，实则就是在表下面用一个类型标识这部分数据并分开存储在不同的目录下，一般用时间标识
create table tb(id int, name string) 
partitioned by (year string)  # 可支持多个分区
row format delimited fields terminated by ',';  # 创建分区表
# 加载数据并标识数据为2020，继续加载其他年份的数据改变year值即可，会在表目录下生成不同的文件夹
load data local inpath '/tmp/win/tb.csv' into table movies partition(year='2020');
# 如果不加where条件会查询出所有的内容，如需查询单个分区则需加上where
select * from tb where year == '2020';
# 若需查询多个分区的内容，各查询之间用union连接
select * from tb where year == '2020' union select * from tb where year == '2021';
show partition tb;  # 查看所有分区
alter table tb add partition(year='2019')  # 增加分区
alter table tb drop partition(year='2019')  # 删除分区

# 创建分桶表（按照数据字段将数据划分到多个文件中去）
set hive.enforce.bucketing=true;  # 开启分桶功能
set mapreduce.job.reduces=3;  # 设置reduce个数
create table tb(id int, name string, type string) 
clustered by (type) into 3 buckets  # 可支持多个分区
row format delimited fields terminated by ',';  # 创建分桶表
# 加载数据不用以前的方法，使用insert overwrite
create table tb2(id int, name string, type string) 
row format delimited fields terminated by ',';  # 创建普通表用来辅助
load data inpath '/user/elk/tmp/movies.csv' into table tb2;  # 加载数据到辅助表
insert overwrite table tb select * from tb2 cluster by(type);  # 通过cluster by分桶加载
```

## Docker

### 安装

```shell
# Linux安装起来比较麻烦
yum install -y yum-utils
yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo
yum install -y docker-ce docker-ce-cli containerd.io
systemctl start docker
```

### 镜像命令

```shell
# 查看镜像
docker images
docker images [-aq]  # 加参数aq只显示镜像id
# 搜索docker hub中的镜像
docker search mysql
# 拉取镜像
docker pull mysql
docker pull mysql:5.7  # 拉取指定版本镜像
# 删除镜像
docker rmi -f 镜像ID [镜像ID] [镜像ID]
docker rmi -f $(docker images -aq)  # 删除所有镜像
```

### 容器命令

```shell
# 容器运行
docker fun [可选参数] image
--name="name"  # 容器名字，用来区分容器
-d  # 后台运行
-it  # 使用交互式运行，进入容器查看内容
-p  # 指定容器端口 -p 8080:8080
    -p ip:主机端口：容器端口
    -p 主机端口：容器端口（常用）
    -p 容器端口
    -p  # 不加参数随机指定端口
docker run -it ubuntu /bin/bash  # 启动并进入容器
docker run -d --name mysql01 -p:3344:3306 mysql  # 后台启动一个MySQL镜像命名为mysql01并将容器内的3306端口映射到主机3344端口上，访问主机3344就可以访问到mysql01
docker run -it --rm tomcat  # 用完即删，用作测试，不建议使用

# win10启动容器（必须在docker desktop setting/Resources/FILE SHARING中增加文件共享目录e：workspace/docker/volume）
docker run -d -it --name test -v /e/workspace/docker/volume/test:/root centos 

exit  # 退出容器，容器也会停止（Ctrl + P + Q退出容器不停止容器）
docker ps  # 查看正在运行的容器
docker ps -a  # 查看以前运行过的所有容器
docker rm 容器id  # 删除没有正在运行的容器
docker rm -f 容器id  # 强制删除容器
docker rm -f $(docker ps -aq)  # 删除所有容器

# 启动和停止容器
docker start 容器id  # 启动容器
docker restart 容器id  # 重启容器
docker stop 容器id  # 停止容器
docker kill 容器id  # 杀死容器
# 只要容器在数据就在，不会丢失，重新启动即可（使用docker ps -a查看已关闭的容器）
```

### 其他命令

```shell
# 查看日志、元数据
docker -tfn 20 容器ID  # 查看容器日志，t表示显示时间戳，f跟踪日志输出，n指定输出日志数量
docker top 容器ID  # 查看容器内进程
docker inspect 容器ID  # 查看容器元数据
docker stats [容器ID] # 查看所有/指定容器内存使用情况
docker exec -it 容器ID /bin/bash  # 进入正在运行的容器（进入容器开启一个新的终端，常用）
docker attach 容器ID  # 进入正在运行的容器（进入容器正在执行的终端）

# 拷贝容器内文件到当前主机
docker cp 容器ID:/root/test /home/wxk

# 内存管理
docker stats 容器ID  # 查看容器占用内存情况
docker run -d --name elasticsearch -p 9200:9200 -p 9300:9300 -e ES_JAVA_OPTS="-Xms64m -Xmx512m" elasticsearch  # 通过-e修改配置文件，最大最小内存

# Commit镜像，保存当前容器的所有状态，打包成一个镜像
docker commit -a "作者" -m "提交信息说明" 容器ID 镜像名称:镜像版本
docker commit -a "wxk" -m "add test" 38ff0892001a ubuntu01:1.0
```

### 容器数据卷

容器之间的一个共享技术，Docker容器中产生的数据，可以同步到本地作持久化，将容器内的目录挂载到Linux中，跟容器是否启动无关，这样在容器删了之后不会面对数据也不存在的情况。而且容器之间也是可以数据共享的。

```shell
# 新建数据卷
docker run -d -v 本机目录:容器目录 [-v 本机目录:容器目录] 容器ID  # 指定路径挂载，将两个/多个目录连接起来
docker run -d -v 容器目录 容器ID  # 匿名挂载，只有容器内的路径
docker run -d -v 卷名:容器目录 容器ID  # 具名挂载，给数据卷起个名字（常用）
docker run -d -v 卷名:容器目录:ro/rw 容器ID  # ro表示容器内只读，只能主机写；默认rw，主机和容器都可读可写

# 查看数据卷情况
docker volume ls  # 查看所有数据卷
docker colume inspect 卷名  # 查看指定卷名的情况，包括本地路径
```

### Dockerfile、Docker网络

Dockerfile就是用来构造docker镜像的构造文件，通过脚本可以生成自己的镜像，指路[教程](https://www.bilibili.com/video/BV1og4y1q7M4?p=26)。
Docker网络在容器之间的通信和集群的搭建需要考虑，同样指路上方教程。

### 发布镜像

可以发布镜像到自己的[阿里云容器镜像服务](https://cr.console.aliyun.com/cn-beijing/instances/repositories)，相当于是一个仓库，创建自己的镜像，以供自己和他人随时随地进行拉取。新建仓库push自己的镜像，使用的时候直接使用公网地址pull即可。

### Docker网络

每启动一个Docker容器，就会给容器分配一个IP，使用的是evth-pair桥接模式，在宿主机使用ip-addr查看docker0以及桥接的网络。
evth-pair就是一对的虚拟接口设备，他们都是成对出现的，一端连着协议，一端连着彼此，正因为有这个特性，evth-pair充当一个桥梁，连接各种虚拟网络设备。

Tomcat01和Tomcat02共用同一个路由器docker0，所有容器不指定网络的情况下，都是由docker0路由的，docker会给容器分配一个默认的可用IP，当然每一次启动都会重新分配新的IP。
而Docker使用的是Linux的桥接，宿主机是一个Docker容器的网桥docker0。Docker中所有的网络接口都是虚拟的，转发效率高。
Docker0不支持容器名连接，只能使用link，但是会有局限性，所以一般会搭建自己的网络。

```shell
# 查看容器网络配置信息
docker network ls  # 查看所有容器网络
docker network inspect 网络ID  # 查看容器网络配置的详细信息

# 创建自己的网络
docker run -d -it --net bridge centos  # 默认创建容器时使用bridge桥接
docker network create --driver bridge --subnet 192.168.0.0/16 --gateway 192.168.0.1 mynet  # 创建自己的网络，使用docker network create --help查看使用方法
docker run -d -it --name ubuntu01 --net mynet ubuntu  # 使用自己的网络启动容器
docker run -d -it --name ubuntu02 --net mynet ubuntu 
docker exec -it ubuntu01 ping ubuntu02  # 网络下启动的所有容器彼此可以通信，重要的是可以直接使用容器名（当然你得先在容器中安装iputils-ping）

# 连接一个容器到一个网络（一个容器两个地址，解决不同网络的容器互连问题）
docker run -d -it --name ubuntu-docker0 ubuntu  # 使用默认网络bridge
docker run -d -it --name ubuntu-mynet --net mynet ubuntu  # 使用自己的网络mynet
docker network connect mynet ubuntu-docker0  # 连接一个容器到一个网络中，这样ubuntu-docker0和ubuntu-mynet就可以互相通信了
```

### Docker-Compose

[Docker Compose](https://docs.docker.com/compose/)是一个用来定义和运行复杂应用的Docker工具。使用Compose，你可以在一个文件中定义一个多容器应用，然后使用一条命令来启动并管理你的应用，完成一切准备工作。

```shell
# 安装docker-compose
yum install docker-compose 
docker-compose version

# docker-compose基本命令
build 构建或重建服务
help 命令帮助
kill 杀掉容器
logs 显示容器的输出内容
port 打印绑定的开放端口
ps 显示容器
pull 拉取服务镜像
restart 重启服务
rm 删除停止的容器
run 运行一个一次性命令
scale 设置服务的容器数目
start 开启服务
stop 停止服务
up 创建并启动容器

# 使用docker-compose构建wordpress
mkdir wordpress
cd wordpress
vim docker-compose.yml  # 内容如下
docker-compose -f docker-compose.wordpress.yml up -d  # 后台运行
# 后台访问地址：IP:3344/wp-admin

# 基本操作（需要在docker-compose.yml目录下操作）
docker-compose ps  # 查看容器运行情况
docker-compose logs  # 返回容器log
docker-compose stop  # 停止相关的容器
docker-compose start  # 启动相关的容器
```

docker-compose.yml

```yml
version: '3.3'
services:
  db:
    image: mysql:5.7
    volumes:
      - db:/var/lib/mysql  # 数据卷，使用docker volume ls & docker inspect 卷名查看挂载地址
    restart: always
    environment:
      MYSQL_ROOT_PASSWORD: wordpress
      MYSQL_DATABASE: wordpress
      MYSQL_USER: wordpress
      MYSQL_PASSWORD: wordpress
  wordpress:
    depends_on:
      - db  # 依赖
    image: wordpress:latest
    ports:
      - "3344:80"
    restart: always
    environment:
      WORDPRESS_DB_HOST: db:3306
      WORDPRESS_DB_USER: wordpress
      WORDPRESS_DB_PASSWORD: wordpress
      WORDPRESS_DB_NAME: wordpress
    volumes:
      - wordpress:/var/www/html  # 数据卷
volumes:  # 使数据卷生效
  db:
  wordpress:
```

## MongoDB

#### 命令行

```shell
# 数据库集合相关
show dbs;  # 显示所有数据库
use mydb;  # 进入数据库
db;  # 显示当前数据库名称
db.dropDatabase();  # 删除当前进入的数据库，并且释放磁盘
show collections;  # 查看所有集合，也可以用tables
db.fruit.drop();  # 删除一个集合

# 插入数据
db.fruit.insetOne({name : "apple"});  # 插入一条数据
db.fruit.insetMany([  # 插入多条数据
    {name : "pair"},
    {name : "orange"}
]);
db.fruit.insertOne(  # 插入多层次的数据
    {name:"app",
    from:{
        country:"china",
        province:"beijing"}
    }
)
db.fruit.insert([  # 使用insert也可以插入多条，插入一个数组
    {name:"ppp",color:["black","yeelow"]},
    {name:"www",color:["white","blue"]}]
)

# 查找数据
db.moives.find();  # find查询语句
db.users.findOne();  # 只返回一条记录
db.fruit.find().pretty();  # 结构化显示查询结果
db.moives.find({"year" : 1975, "title" : "agg"});  # 多条件and查询
db.moives.find({$and : [{"year":1975}, {"title":"agg"}]});  # 多条件and
db.moives.find({$or : [{"year":1975}, {"title":"agg"}]});  # 多条件or
db.fruit.find({"from.country":"china"});  # 查找多层次结构数据
db.fruit.find({},{"_id":0,"name":1});  # 第一个大括号内是条件，第二个指定不返回_id字段，只返回name字段

# 删除数据
db.fruit.remove({});  # 删除集合内所有内容，相当于sql中的delete
db.fruit.remove({a: {$lt: 5}})  # 删除a小于5的记录

# 更新数据
db.fruit.updateOne({name:"app"},{$set:{from.province:"xian"}});  # 更新一条记录，第一个大括号是条件，第二个是更新的内容，若更新字段不存在则自动新建，不管匹配多少条，默认更新第一条
db.fruit.updateMany({name:"app"},{$set:{from.province:"xian"}}); # 更新匹配到的多条记录，必须有以下字段：
# $set/$unset
# /$push（增加一个对象到数组底部）/$pushAll（增加多个对象到数组底部）/$pop（从数组底部删除一个对象）
# /$pull（如果匹配指定的值则从数组中删除）/$pullAll（如果匹配任意的值则从数组中删除相应的对象）
# /$addToSet（如果不存在则增加一个值到数组）

# 聚合查询
db.users.aggregate([#select username "姓名",age "年龄" from users where phone="19991259321" limit 1 1;
    {"$match":{"phone":"19991259321"}},
    {"$skip":1},
    {"$limit":1},
    {"$project":{"姓名":"$username","年龄":"$age"}}
])

user_coll.insert(  # 插入一条数组记录
    {"username":"wang","score":[
        {"subject":"语文","score":80},
        {"subject":"数学","score":90},
        {"subject":"英语","score":78}
]})
db.users.aggregate([{"$unwind":"$score"}])  # 将数组拆分展示

db.users.insert([
    {"name":"wxk", "price":90},
    {"name":"wx", "price":56},
    {"name":"qqq", "price":66},
    {"name":"www", "price":56},
    {"name":"eee", "price":96},
    {"name":"rrr", "price":36},
    {"name":"ttt", "price":86},
    {"name":"yyy", "price":76}
])
db.users.aggregate([  # 分桶计数
    {$bucket:{
        groupBy:"$score",
        boundaries:[0,30,60,80,90],
        default:"Other",
        output:{"count":{"$sum":1}}
    }
}])
```

#### SQL和MQL

| SQL             | MQL                                    |
| --------------- | -------------------------------------- |
| a = 1           | {a : 1}                                |
| a <> 1          | {a : {$ne : 1}}                        |
| a > 1           | {a : {$gt : 1}}                        |
| a >= 1          | {a : {$gte : 1}}                       |
| a < 1           | {a : {$lt : 1}}                        |
| a <= 1          | {a : {$lte : 1}}                       |
| a = 1 AND b = 1 | {a = 1, b = 1}或者{$and: [{a=1}, {b=1}]} |
| a = 1 OR b = 1  | {$or: [{a=1}, {b=1}]}                  |
| a IS NULL       | {a: {$exsits: false}}                  |
| a IN (1, 2, 3)  | {a: {$in: [1, 2, 3]}}                  |
| WHERE           | $match                                 |
| AS              | $project                               |
| ORDER BY        | $sort                                  |
| GROUP BY        | $group                                 |
| SKIP/LIMIT      | $sklp/$limit                           |
| LEFT OUTER JOIN | $lookup                                |

#### Python操作

```python
import pymongo  # pymongo包
import dns  # dnspython包
# 建立连接，使用的是Atlas集群
client = pymongo.MongoClient(
    "mongodb+srv://wang:wangxukun1024@wxk.t4vz2.mongodb.net/myFirstDatabase?        retryWrites=true&w=majority")
# client = pymongo.MongoClient("mongodb://localhost:27017")  # 本地连接方式

# 新建数据库eshop和表users
db = client['eshop']
user_coll = db['users']
user_coll.insert_many([  # 插入数据
    {"username":"wxk","age":26,"email":"w749@qq.com"},
    {"username":"wx","age":26,"email":"31923@qq.com"}
])

# 更新数据
user_coll.update_one({"username":"wxk"},{"$set":{"phone":"19991259321"}})

# 聚合
user_coll.aggregate([
    {"$match":{"phone":"19991259321"}},
    {"$skip":1},
    {"$limit":1},
    {"$project":{"姓名":"$username","年龄":"$age"}}
])
```

## Spark

### 安装

1. Local模式，直接下载安装包解压即可，甚至不用配置，直接运行bin目录下的spark-shell就可以启动一个交互式spark环境
2. Standalone模式，区别于Local模式，这是多台机器组成的集群，有Master用于管理任务和Worker用于运行任务，配置时修改conf目录下的spark-env.sh文件，添加JAVA_HOME、SPARK_MASTER_HOST和SPARK_MASTER_PORT三个环境变量即可，用以指定Master机器，随后修改workers文件，添加计算任务的机器IP。最后将安装包分发到其他机器上，在Master机器上启动sbin目录下的start-all.sh即可。

```shell
# spark-env.sh
JAVA_HOME=/usr/lib/java/jdk1.8.0_181
SPARK_MASTER_HOST=node01
SPARK_MASTER_PORT=7077
SPARK_MASTER_WEBUI_PORT=8080

# workers
node01
node02
node03
```

3. Yarn on Spark模式，这种模式是将Spark托管在Hadoop下的Yarn用以资源调度，配合Hadoop使用，只需要在spark-env.sh再添加两个环境变量，在spark-defaults.conf中添加几项配置，然后将jars目录下的所有jar包打包放在hdfs下，这个位置需要在spark-defaults.conf中指定，需要特别注意lzo的配置，目前就卡在这了，要将所需jar包放在jars目录下。

```shell
# spark-env.sh
JAVA_HOME=/usr/lib/java/jdk1.8.0_181
HADOOP_CONF_DIR=/opt/app/hadoop-3.1.3/etc/hadoop
YARN_CONF_DIR=/opt/app/hadoop-3.1.3/etc/hadoop
SPARK_MASTER_HOST=node01
SPARK_MASTER_PORT=7077
SPARK_MASTER_WEBUI_PORT=8080
SPARK_HISTORY_OPTS="-Dspark.history.fs.logDirectory=hdfs://node01:9000/sparklog/ -Dspark.history.fs.cleaner.enabled=true"

# workers
node01
node02
node03

# spark-defaults.conf
spark.eventLog.enabled           true  # 开启日志
spark.eventLog.dir               hdfs://node01:9000/sparklog/  # 日志位置，hdfs提前建好目录
spark.eventLog.compress          true
spark.yarn.historyServer.address node01:18080
spark.yarn.jars                  hdfs://node01:9000/spark/jars/* /opt/app/spark-3.1.2-bin-hadoop3.2/jars/
```

### 运行

Spark任务一般使用spark-submit命令提交，有下面几个参数比较常用

```shell
spark-submit \
--class spark.core.WordCount  # Spark程序中包含主程序的类
--master local[*]             # Spark程序运行的模式（环境）
--executor-memory 1G          # 指定每个executor可用内存为1G（512m）
--total-executor-cores 2      # 指定所有executor使用的cpu核数为两个
--executor-cores 2            # 指定每个executor使用的cpu核数
spark-wordcount-1.0.jar       # 打包好的包含依赖的jar包，本地或者hdfs
```

### 架构

Spark 框架的核心是一个计算引擎，整体来说，它采用了标准 master-slave 的结构。如下图所示，它展示了一个 Spark 执行时的基本结构。图形中的 Driver 表示 master， 负责管理整个集群中的作业任务调度。图形中的 Executor 则是 slave，负责实际执行任务。

<div align=center><img src="image-20210825151104511.png"></div>

**Diver & Executor**

- **Diver**
  Spark驱动节点，用于执行Spark任务中的main方法，负责实际代码的执行工作。
- **Executor**
  Spark Executor 是集群中工作节点(Worker)中的一个 JVM 进程，负责在 Spark 作业中运行具体任务(Task)，任务彼此之间相互独立。Spark应用启动时，Executor节点被同时启动，并且始终伴随着整个Spark应用的生命周期而存在。如果有Executor节点发生了 故障或崩溃，Spark应用也可以继续执行，会将出错节点上的任务调度到其他 Executor节点上继续运行。

**Master & Worker**

Spark集群的独立部署环境中，不需要依赖其他的资源调度框架，自身就实现了资源调 度的功能，所以环境中还有其他两个核心组件:Master和Worker，这里的Master是一个进程，主要负责资源的调度和分配，并进行集群的监控等职责，类似于Yarn环境中的ResourceManager, 而Worker呢，也是进程，一个Worker运行在集群中的一台服务器上，由 Master分配资源对 数据进行并行的处理和计算，类似于 Yarn 环境中NodeManager。

**ApplicationMaster**
Hadoop用户向YARN 集群提交应用程序时,提交程序中应该包含ApplicationMaster，用 于向资源调度器申请执行任务的资源容器Container，运行用户自己的程序任务job，监控整个任务的执行，跟踪整个任务的状态，处理任务失败等异常情况。说的简单点就是，ResourceManager(资源)和Driver(计算)之间的解耦合靠的就是ApplicationMaster。

### 核心概念

**Executor与Core**

Spark Executor 是集群中运行在工作节点(Worker)中的一个 JVM 进程，是整个集群中的专门用于计算的节点。在提交应用中，可以提供参数指定计算节点的个数，以及对应的资 源。这里的资源一般指的是工作节点 Executor 的内存大小和使用的虚拟 CPU 核(Core)数 量。
应用程序相关启动参数如下:`--num-executors`、`--executor-memory`、`--executor-cores`

**并行度(Parallelism)** 

在分布式计算框架中一般都是多个任务同时执行，由于任务分布在不同的计算节点进行
计算，所以能够真正地实现多任务并行执行，记住，这里是并行，而不是并发。这里我们将 整个集群并行执行任务的数量称之为并行度。那么一个作业到底并行度是多少呢?这个取决 于框架的默认配置。应用程序也可以在运行过程中动态修改。

**有向无环图(DAG)**

大数据计算引擎框架我们根据使用方式的不同一般会分为四类，其中第一类就是 Hadoop 所承载的 MapReduce,它将计算分为两个阶段，分别为 Map 阶段 和 Reduce 阶段。 对于上层应用来说，就不得不想方设法去拆分算法，甚至于不得不在上层应用实现多个 Job 的串联，以完成一个完整的算法，例如迭代计算。 由于这样的弊端，催生了支持DAG 框 架的产生。因此，支持 DAG 的框架被划分为第二代计算引擎。如 Tez 以及更上层的 Oozie。这里我们不去细究各种 DAG 实现之间的区别，不过对于当时的 Tez 和 Oozie 来 说，大多还是批处理的任务。接下来就是以 Spark 为代表的第三代的计算引擎。第三代计 算引擎的特点主要是 Job 内部的 DAG 支持(不跨越 Job)，以及实时计算。

**提交流程**

所谓的提交流程，其实就是我们开发人员根据需求写的应用程序通过 Spark 客户端提交 给 Spark 运行环境执行计算的流程。在不同的部署环境中，这个提交过程基本相同，但是又 有细微的区别，我们这里不进行详细的比较，但是因为国内工作中，将 Spark 引用部署到 Yarn 环境中会更多一些，所以本课程中的提交流程是基于 Yarn 环境的。

<div align=center><img src="image-20210825154407863.png"></div>

Spark 应用程序提交到 Yarn 环境中执行的时候，一般会有两种部署执行的方式:Client 和 Cluster。两种模式主要区别在于:Driver 程序的运行节点位置。

1. **Yarn Client 模式**

Client 模式将用于监控和调度的 Driver 模块在客户端执行，而不是在 Yarn 中，所以一
般用于测试。

- Driver在任务提交的本地机器上运行
- Driver启动后会和ResourceManager通讯申请启动ApplicationMaster
- ResourceManager分配container，在合适的NodeManager上启动ApplicationMaster，负责向 ResourceManager 申请 Executor 内存
- ResourceManager接到ApplicationMaster的资源申请后会分配container，然后ApplicationMaster 在资源分配指定的 NodeManager 上启动 Executor 进程
- Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行 main 函数

之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分stage，每个stage生 成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行。

2. **Yarn Cluster 模式**

Cluster 模式将用于监控和调度的 Driver 模块启动在 Yarn 集群资源中执行。一般应用于
实际生产环境。

- 在YARNCluster模式下，任务提交后会和ResourceManager通讯申请启动
  ApplicationMaster，
- 随后ResourceManager分配container，在合适的NodeManager上启动ApplicationMaster，
  此时的 ApplicationMaster 就是 Driver。
- Driver启动后向ResourceManager申请Executor内存，ResourceManager接到ApplicationMaster 的资源申请后会分配 container，然后在合适的 NodeManager 上启动Executor 进程
- Executor进程启动后会向Driver反向注册，Executor全部注册完成后Driver开始执行main 函数
- 之后执行到Action算子时，触发一个Job，并根据宽依赖开始划分stage，每个stage生
  成对应的 TaskSet，之后将 task 分发到各个 Executor 上执行。
