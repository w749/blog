---
title: 常见错误统计
author: 汪寻
date: 2020-11-07
tags:
 - Other
categories:
 - Language
---

常见错误统计，目前主要是Hive。

<!-- more -->

### Hive

#### Tez计算引擎导致的错误

```sh
Status: Failed
Invalid event on Vertex vertex_1622365407812_0028_1_00 [Map 1]
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.tez.TezTask. Invalid event on Vertex vertex_1622365407812_0028_1_00 [Map 1]
```

当在运行sql脚本或者sql语句的时候发生以上错误，而且yarn和nodemanageer以及任务日志基本看不出来哪里出了问题的时候，把Hive配置文件hive-site.xml中的计算引擎由tez改为mr（MapReduce）。

```xml
<property>
  <name>hive.execution.engine</name>
  <value>mr</value>
</property>
```

#### MySQL驱动版本不一致

`Loading class 'com.mysql.jdbc.Driver'. This is deprecated. The new driver class is 'com.mysql.cj.jdbc.Driver'. `

一般是链接MySQL的驱动版本太高而配置文件中的配置又太低的缘故，建议在Hive2.x中仍然使用`mysql-connect-5.1.x`版本的驱动，并且在配置文件中使用`com.mysql.jdbc.Driver`，Hive3.x版本使用`mysql-connect-8.0.x`版本的驱动，并且在配置文件中使用`com.mysql.cj.jdbc.Driver`，必须要对应，不然会报错

```xml
<!-- Hive2.x对应mysql-connect-5.1.x驱动 -->
<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.jdbc.Driver</value>
  <description>Driver class name for a JDBC metastore</description>
</property>

<!-- Hive3.x对应mysql-connect-8.0.x驱动 -->
<property>
  <name>javax.jdo.option.ConnectionDriverName</name>
  <value>com.mysql.cj.jdbc.Driver</value>
  <description>Driver class name for a JDBC metastore</description>
</property>
```

#### 连接DataGrip错误

新版DataGrip连接的时候不需要输入用户名和密码，网上搜出来的都是旧版的，可以参考所需的JAR包，一般出现错误都是JAR包不兼容，在Hive主机上使用beeline测试Hive可以连接后就不是Hive的问题了。不要使用DataGrip建议的包，自己下载需要的JAR包并导入，支持Hive1.2.1的包在服务器上。

#### 拼接两个无关的查询结果

```sql
-- 前提是两个查询结果的行数相同，而且必须group by，否则会报错
select
    t1.id, 
    t1.name,
    t2.id, 
    t2.name
from t1 
left join t2 
on 1=1
group by t2.id,t2.name;
```

#### When importing query results in parallel...

`When importing query results in parallel, you must specify --split-by.`

当`--num-mappers`设置的值大于1时，`--split-by`必须设置字段（需要是 int 类型的字段），如果不是 int类型的字段，则需要加上参数，例如`--split-by columnName`

#### Zero date value prohibited

这是因为数据库中的`0000-00-00 00:00:00`在Hive中是被禁止的，所以需要在JDBC中加入参数`zeroDateTimeBehavior=convertToNull`

```shell
# 报错
Caused by: com.mysql.cj.exceptions.DataReadException: Zero date value prohibited
# 修改参数
jdbc:mysql://192.168.1.111:3306/shanhy_demo?zeroDateTimeBehavior=convertToNull
```

#### java.sql.SQLException: HOUR_OF_DAY: 0 -> 1

查了半天也没查出来原因，应该是时区的原因，但是有解决办法

```shell
# 报错
Caused by: java.sql.SQLException: HOUR_OF_DAY: 0 -> 1
Caused by: com.mysql.cj.exceptions.WrongArgumentException: HOUR_OF_DAY: 0 -> 1
Caused by: java.lang.IllegalArgumentException: HOUR_OF_DAY: 0 -> 1
# 修改参数
jdbc:mysql://192.168.1.111:3306/shanhy_demo?zeroDateTimeBehavior=convertToNull&serverTimezone=GMT%2B8
```

#### ParseException line 1:40 cannot recognize input near...

```shell
# 报错
FAILED: ParseException line 1:40 cannot recognize input near 'with' 'tb' 'as' in statement
# SQL语法问题，with新建临时表之后，后面必须接select语句，而且with as语句后不能加分号，否则就会报错
with mm as (
  select 
    '$do_date' report_time, 
    source, 
    count(1) member_all, 
    sum(if(newoldmember='新会员',1,0)) new_member_all, 
    sum(if(newoldmember='老会员',1,0)) old_member_all
  from dws_Member
  group by source
)
select * from mm;
```

#### return code 1 from org.apache.hadoop.hive.ql.exec.tez.TezTask

这是使用tez引擎时出现的错误，产生这种问题的原因有很多

- tez-site.xml文件有放在hadoop配置文件目录下的，有让放在hive配置文件目录下的，最后还是两个文件夹都放了才解决这个问题。
- hive、tez和hadoop中的jar包彼此冲突，解决方法：将tez目录中lib目录下的两个hadoop相关的包改成和hadoop目录下一样的，然后再将tez目录下tez相关的包复制到hive安装目录下的lib目录下。
- hdfs根目录下的tmp文件夹最好给上所有权限

#### java.lang.NoSuchMethodError: com.google.common.base.Preconditions.checkArgument

错误原因：系统找不到这个类所在的jar包或者jar包的版本不一样系统不知道使用哪个。hive启动报错的原因是后者

解决办法：
1、com.google.common.base.Preconditions.checkArgument这个类所在的jar包为：guava.jar
2、hadoop-3.2.1（路径：hadoop\share\hadoop\common\lib）中该jar包为 guava-27.0-jre.jar；而hive-3.1.2(路径：hive/lib)中该jar包为guava-19.0.1.jar
3、将jar包变成一致的版本：删除hive中低版本jar包，将hadoop中高版本的复制到hive的lib中。

再次启动问题得到解决！

#### User: root is not allowed to impersonate root

beeline通过jdbc登录hive的时候需要输入服务器用户名和密码，报这个错需要在hadoop中core-site.xml配置文件中修改两个参数，而且记得要分发到所有集群并重启hadoop集群

```xml
<property>
      <name>hadoop.proxyuser.root.hosts</name>
      <value>*</value>
  </property>
  <property>
      <name>hadoop.proxyuser.root.groups</name>
      <value>*</value>
  </property>
```

#### 错误: 找不到或无法加载主类 org.apache.hadoop.mapreduce.v2.app.MRAppMaster

输入命令 hadoop classpath，将输出的内容直接复制到yarn-site.xml文件中：

```xml
<property>
    <name>yarn.application.classpath</name>
    <value>/usr/local/hadoop3.1.2/etc/hadoop:/usr/local/hadoop3.1.2/share/hadoop/common/lib/*:/usr/local/hadoop3.1.2/share/hadoop/common/*:/usr/local/hadoop3.1.2/share/hadoop/hdfs:.....</value>
</property>
```

#### Hive元数据中文乱码

因为初始化 MySQL 元数据库的时候默认的编码格式是 latin1，必须修改为 UTF-8 才可以正常显示，但是只需修改特定的字段即可。修改之前的数据还是乱码，不起作用。

```sql
#修改字段注释字符集
alter table COLUMNS_V2 modify column COMMENT varchar(256) character set utf8;
#修改表注释字符集
alter table TABLE_PARAMS modify column PARAM_VALUE varchar(20000) character set utf8;
#修改分区参数，支持分区建用中文表示
alter table PARTITION_PARAMS modify column PARAM_VALUE varchar(20000) character set utf8;
alter table PARTITION_KEYS modify column PKEY_COMMENT varchar(20000) character set utf8;
#修改索引名注释，支持中文表示
alter table INDEX_PARAMS modify column PARAM_VALUE varchar(4000) character set utf8;
#修改视图，支持视图中文
ALTER TABLE TBLS modify COLUMN VIEW_EXPANDED_TEXT mediumtext CHARACTER SET utf8;
ALTER TABLE TBLS modify COLUMN VIEW_ORIGINAL_TEXT mediumtext CHARACTER SET utf8;
```

#### FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask

执行 hive mapreduce 任务报这个错误，查看 hive 日志发现其实主要原因是`java.lang.OutOfMemoryError: Java heap space`这个错误，找了一圈发现设置`set io.sort.mb=10;`即可解决，默认给的是100，指定了排序使用的内存，大的内存可以加快 job 的处理速度。

#### Hive 建表后直接将数据导入 HDFS 查不到数据（DataX 导入到 HDFS）

如果有分区的话，将数据导入分区所在目录下，再使用`alter table test if not exists add partition(year='2021',month='10',day='12')`新增分区，其实就是同步到元数据库。如果表没有分区，那么就需要使用语句将导入进来的数据再插入到表中：`insert overwrite table test select * from test;`，如果不是全量数据，则需要加上查询语句：`insert into table test select * from test where ctime = '2021-10-12';`

### Linux

#### /bin/bash^M: 坏的解释器

一个linux的shell脚本在执行的时候出现错误：`/bin/bash^M: 坏的解释器: 没有那个文件或目录`

原因：这个文件在Windows 下编辑过，在Windows下每一行结尾是\n\r，而Linux下则是\n，所以才会有 多出来的\r。

解决：使用命令`sed -i 's/\r$//' 名称.sh`，上面的指令会把 `名称.sh` 中的\r 替换成空白！

### MySQL

#### datetime默认值设置0000-00-00失败

mysql5.7之后版本的sql_mode默认使用：
ONLY_FULL_GROUP_BY, STRICT_TRANS_TABLES, NO_ZERO_IN_DATE, NO_ZERO_DATE, ERROR_FOR_DIVISION_BY_ZERO, NO_AUTO_CREATE_USER, and NO_ENGINE_SUBSTITUTION

其中NO_ZERO_IN_DATE, NO_ZERO_DATE两个选项禁止了0000这样的日期和时间。因此在mysql的配置文件中，重新设置sql_mode，去掉这两项就可以了。

修改my.cnf文件，在[mysqld]中添加

```shell
sql-mode=ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION
```

### 其他

#### 本地编译出现异常：com.sun.tools.javac.code.TypeTags

找到问题是因为jdk版本是11，lombok版本太低导致的，将lombok升级即可

```xml
<dependency>
        <groupId>org.projectlombok</groupId>
        <artifactId>lombok</artifactId>
        <version>1.18.4</version>
</dependency>
```

#### IDEA 中 pom.xml 多个插件变红

检查 Maven 配置文件没问题之后，命令行输入`mvn -U idea:idea`
